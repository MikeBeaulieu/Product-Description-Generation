{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Product Description Generation\n",
    "In this project, you can generate descriptions of various apparel items like shirts, jeans, kurtas using RNNs. The descriptions of 10400 products for each apparel item has been used fro jabong.com to train the neural network. The Neural Network built will generate a product description for a product from the given sets of apparels i.e. shirts, jeans, shoes, watches and kurtas.\n",
    "## Get the Data\n",
    "The data consists of 10400 descriptions of different jeans obtained from jabong.com with distinguishing features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "import helper\n",
    "\n",
    "data_dir = './data/simpsons/t-shirts.txt'\n",
    "text = helper.load_data(data_dir)\n",
    "# Ignore notice, since we don't use it for analysing the data\n",
    "text = text[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Stats\n",
      "Roughly the number of unique words: 10308\n",
      "Number of scenes: 20484\n",
      "Average number of sentences in each scene: 0.9999511814098809\n",
      "Number of lines: 40967\n",
      "Average number of words in each line: 31.192374350086656\n",
      "\n",
      "The sentences 0 to 10:\n",
      "﻿HERE&NOW Charcoal Grey Regular Fit Faded Casual Shirt: Charcoal grey faded casual shirt, has a mandarin collar, button placket, long sleeves, curved hem\n",
      "\n",
      "\n",
      "Jack & Jones Rust Solid Slim Fit Casual Shirt: Breathe a new life into your casual wardrobe by adding this rust coloured casual shirt for men by Jack & Jones. This slim-fit shirt will ensure a comfortable fit, courtesy its cotton fabric. Furthermore, this shirt will go well with jeans and loafers.\n",
      "\n",
      "\n",
      "Wrangler Multicoloured Checked Regular Fit Casual Shirt: Multi in colour, this shirt from Wrangler is a basic essential in every urbane man’s casual collection. High on style, this lightweight cotton shirt will be an absolute delight to wear. This shirt can be teamed with a pair of black trousers and black casual shoes to complete your smart look.\n",
      "\n",
      "\n",
      "Nike Nike Black CF JSY CLUB KNIT Joggers: Nike Black CF JSY CLUB KNIT Joggers\n"
     ]
    }
   ],
   "source": [
    "view_sentence_range = (0, 10)\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "\n",
    "print('Dataset Stats')\n",
    "print('Roughly the number of unique words: {}'.format(len({word: None for word in text.split()})))\n",
    "scenes = text.split('\\n\\n')\n",
    "print('Number of scenes: {}'.format(len(scenes)))\n",
    "sentence_count_scene = [scene.count('\\n') for scene in scenes]\n",
    "print('Average number of sentences in each scene: {}'.format(np.average(sentence_count_scene)))\n",
    "\n",
    "sentences = [sentence for scene in scenes for sentence in scene.split('\\n')]\n",
    "print('Number of lines: {}'.format(len(sentences)))\n",
    "word_count_sentence = [len(sentence.split()) for sentence in sentences]\n",
    "print('Average number of words in each line: {}'.format(np.average(word_count_sentence)))\n",
    "\n",
    "print()\n",
    "print('The sentences {} to {}:'.format(*view_sentence_range))\n",
    "print('\\n'.join(text.split('\\n')[view_sentence_range[0]:view_sentence_range[1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement Preprocessing Functions\n",
    "The first thing to do to any dataset is preprocessing.  Implement the following preprocessing functions below:\n",
    "- Lookup Table\n",
    "- Tokenize Punctuation\n",
    "\n",
    "### Lookup Table\n",
    "To create a word embedding, you first need to transform the words to ids.  In this function, create two dictionaries:\n",
    "- Dictionary to go from the words to an id, we'll call `vocab_to_int`\n",
    "- Dictionary to go from the id to word, we'll call `int_to_vocab`\n",
    "\n",
    "Return these dictionaries in the following tuple `(vocab_to_int, int_to_vocab)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import problem_unittests as tests\n",
    "\n",
    "def create_lookup_tables(text):\n",
    "    \"\"\"\n",
    "    Create lookup tables for vocabulary\n",
    "    :param text: The text of tv scripts split into words\n",
    "    :return: A tuple of dicts (vocab_to_int, int_to_vocab)\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    text = list(set(text))   \n",
    "    index = range(len(text))    \n",
    "    int_to_vocab = dict(zip(index, text))   \n",
    "    vocab_to_int = dict(zip(text, index))       \n",
    "    return vocab_to_int, int_to_vocab\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_create_lookup_tables(create_lookup_tables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize Punctuation\n",
    "We'll be splitting the script into a word array using spaces as delimiters.  However, punctuations like periods and exclamation marks make it hard for the neural network to distinguish between the word \"bye\" and \"bye!\".\n",
    "\n",
    "Implement the function `token_lookup` to return a dict that will be used to tokenize symbols like \"!\" into \"||Exclamation_Mark||\".  Create a dictionary for the following symbols where the symbol is the key and value is the token:\n",
    "- Period ( . )\n",
    "- Comma ( , )\n",
    "- Quotation Mark ( \" )\n",
    "- Semicolon ( ; )\n",
    "- Exclamation mark ( ! )\n",
    "- Question mark ( ? )\n",
    "- Left Parentheses ( ( )\n",
    "- Right Parentheses ( ) )\n",
    "- Dash ( -- )\n",
    "- Return ( \\n )\n",
    "\n",
    "This dictionary will be used to token the symbols and add the delimiter (space) around it.  This separates the symbols as it's own word, making it easier for the neural network to predict on the next word. Make sure you don't use a token that could be confused as a word. Instead of using the token \"dash\", try using something like \"||dash||\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def token_lookup():\n",
    "    \"\"\"\n",
    "    Generate a dict to turn punctuation into a token.\n",
    "    :return: Tokenize dictionary where the key is the punctuation and the value is the token\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    token = {}\n",
    "    token['.']=\"||period||\"\n",
    "    token[',']=\"||comma||\"\n",
    "    token['\"']=\"||quotation_mark||\"\n",
    "    token[';']=\"||semicolon||\"\n",
    "    token['!']=\"||exclamation_mark||\"\n",
    "    token['?']=\"||question_mark||\"\n",
    "    token['(']=\"||left_parantheses||\"\n",
    "    token[')']=\"||right_parantheses||\"\n",
    "    token['--'] = \"||dash||\"\n",
    "    token['\\n'] = \"||return||\"\n",
    "    \n",
    "    return token\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_tokenize(token_lookup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess all the data and save it\n",
    "Running the code cell below will preprocess all the data and save it to file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "# Preprocess Training, Validation, and Testing Data\n",
    "helper.preprocess_and_save_data(data_dir, token_lookup, create_lookup_tables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Point\n",
    "This is your first checkpoint. If you ever decide to come back to this notebook or have to restart the notebook, you can start from here. The preprocessed data has been saved to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "import helper\n",
    "import numpy as np\n",
    "import problem_unittests as tests\n",
    "\n",
    "int_text, vocab_to_int, int_to_vocab, token_dict = helper.load_preprocess()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the Neural Network\n",
    "You'll build the components necessary to build a RNN by implementing the following functions below:\n",
    "- get_inputs\n",
    "- get_init_cell\n",
    "- get_embed\n",
    "- build_rnn\n",
    "- build_nn\n",
    "- get_batches\n",
    "\n",
    "### Check the Version of TensorFlow and Access to GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 1.1.0\n",
      "Default GPU Device: /gpu:0\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "from distutils.version import LooseVersion\n",
    "import warnings\n",
    "import tensorflow as tf\n",
    "\n",
    "# Check TensorFlow Version\n",
    "assert LooseVersion(tf.__version__) >= LooseVersion('1.0'), 'Please use TensorFlow version 1.0 or newer'\n",
    "print('TensorFlow Version: {}'.format(tf.__version__))\n",
    "\n",
    "# Check for a GPU\n",
    "if not tf.test.gpu_device_name():\n",
    "    warnings.warn('No GPU found. Please use a GPU to train your neural network.')\n",
    "else:\n",
    "    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input\n",
    "Implement the `get_inputs()` function to create TF Placeholders for the Neural Network.  It should create the following placeholders:\n",
    "- Input text placeholder named \"input\" using the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder) `name` parameter.\n",
    "- Targets placeholder\n",
    "- Learning Rate placeholder\n",
    "\n",
    "Return the placeholders in the following tuple `(Input, Targets, LearningRate)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def get_inputs():\n",
    "    \"\"\"\n",
    "    Create TF Placeholders for input, targets, and learning rate.\n",
    "    :return: Tuple (input, targets, learning rate)\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    INPUT = tf.placeholder(tf.int32, shape=(None,None), name=\"input\")\n",
    "    TARGETS = tf.placeholder(tf.int32, shape=(None,None), name='targets')\n",
    "    LEARNING_RATE = tf.placeholder(tf.float32, shape=None, name='learning_rate')\n",
    "    return INPUT, TARGETS, LEARNING_RATE\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_get_inputs(get_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build RNN Cell and Initialize\n",
    "Stack one or more [`BasicLSTMCells`](https://www.tensorflow.org/api_docs/python/tf/contrib/rnn/BasicLSTMCell) in a [`MultiRNNCell`](https://www.tensorflow.org/api_docs/python/tf/contrib/rnn/MultiRNNCell).\n",
    "- The Rnn size should be set using `rnn_size`\n",
    "- Initalize Cell State using the MultiRNNCell's [`zero_state()`](https://www.tensorflow.org/api_docs/python/tf/contrib/rnn/MultiRNNCell#zero_state) function\n",
    "    - Apply the name \"initial_state\" to the initial state using [`tf.identity()`](https://www.tensorflow.org/api_docs/python/tf/identity)\n",
    "\n",
    "Return the cell and initial state in the following tuple `(Cell, InitialState)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def get_init_cell(batch_size, rnn_size):\n",
    "    \"\"\"\n",
    "    Create an RNN Cell and initialize it.\n",
    "    :param batch_size: Size of batches\n",
    "    :param rnn_size: Size of RNNs\n",
    "    :return: Tuple (cell, initialize state)\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    lstm1 = tf.contrib.rnn.BasicLSTMCell(rnn_size)\n",
    "    lstm2 = tf.contrib.rnn.BasicLSTMCell(rnn_size)\n",
    "    lstm3 = tf.contrib.rnn.BasicLSTMCell(rnn_size)\n",
    "    lstm4 = tf.contrib.rnn.BasicLSTMCell(rnn_size)\n",
    "    cell = tf.contrib.rnn.MultiRNNCell([lstm1,lstm2,lstm3,lstm4])\n",
    "    initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "    initial_state = tf.identity(initial_state, name='initial_state')\n",
    "   \n",
    "     \n",
    "    return cell, initial_state\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_get_init_cell(get_init_cell)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Embedding\n",
    "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def get_embed(input_data, vocab_size, embed_dim):\n",
    "    \"\"\"\n",
    "    Create embedding for <input_data>.\n",
    "    :param input_data: TF placeholder for text input.\n",
    "    :param vocab_size: Number of words in vocabulary.\n",
    "    :param embed_dim: Number of embedding dimensionstea\n",
    "    :return: Embedded input.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    embedding = tf.Variable(tf.random_uniform((vocab_size, embed_dim), -1, 1))\n",
    "    embed = tf.nn.embedding_lookup(embedding, input_data)\n",
    "    return embed\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_get_embed(get_embed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build RNN\n",
    "You created a RNN Cell in the `get_init_cell()` function.  Time to use the cell to create a RNN.\n",
    "- Build the RNN using the [`tf.nn.dynamic_rnn()`](https://www.tensorflow.org/api_docs/python/tf/nn/dynamic_rnn)\n",
    " - Apply the name \"final_state\" to the final state using [`tf.identity()`](https://www.tensorflow.org/api_docs/python/tf/identity)\n",
    "\n",
    "Return the outputs and final_state state in the following tuple `(Outputs, FinalState)` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def build_rnn(cell, inputs):\n",
    "    \"\"\"\n",
    "    Create a RNN using a RNN Cell\n",
    "    :param cell: RNN Cell\n",
    "    :param inputs: Input text data\n",
    "    :return: Tuple (Outputs, Final State)\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    Outputs, States = tf.nn.dynamic_rnn(cell, inputs,dtype='float32')\n",
    "    FinalState = tf.identity(States,name=\"final_state\")\n",
    "    return Outputs, FinalState\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_build_rnn(build_rnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the Neural Network\n",
    "Apply the functions you implemented above to:\n",
    "- Apply embedding to `input_data` using your `get_embed(input_data, vocab_size, embed_dim)` function.\n",
    "- Build RNN using `cell` and your `build_rnn(cell, inputs)` function.\n",
    "- Apply a fully connected layer with a linear activation and `vocab_size` as the number of outputs.\n",
    "\n",
    "Return the logits and final state in the following tuple (Logits, FinalState) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def build_nn(cell, rnn_size, input_data, vocab_size, embed_dim):\n",
    "    \"\"\"\n",
    "    Build part of the neural network\n",
    "    :param cell: RNN cell\n",
    "    :param rnn_size: Size of rnns\n",
    "    :param input_data: Input data\n",
    "    :param vocab_size: Vocabulary size\n",
    "    :param embed_dim: Number of embedding dimensions\n",
    "    :return: Tuple (Logits, FinalState)\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    embed = get_embed(input_data, vocab_size, embed_dim)\n",
    "    outputs, final_state = build_rnn(cell, embed)\n",
    "    logits = tf.contrib.layers.fully_connected(outputs, vocab_size, activation_fn=None,\n",
    "                                               weights_initializer=tf.truncated_normal_initializer(stddev=0.1),\n",
    "                                               biases_initializer=tf.zeros_initializer())\n",
    "    return logits, final_state\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_build_nn(build_nn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batches\n",
    "Implement `get_batches` to create batches of input and targets using `int_text`.  The batches should be a Numpy array with the shape `(number of batches, 2, batch size, sequence length)`. Each batch contains two elements:\n",
    "- The first element is a single batch of **input** with the shape `[batch size, sequence length]`\n",
    "- The second element is a single batch of **targets** with the shape `[batch size, sequence length]`\n",
    "\n",
    "If you can't fill the last batch with enough data, drop the last batch.\n",
    "\n",
    "For example, `get_batches([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20], 3, 2)` would return a Numpy array of the following:\n",
    "```\n",
    "[\n",
    "  # First Batch\n",
    "  [\n",
    "    # Batch of Input\n",
    "    [[ 1  2], [ 7  8], [13 14]]\n",
    "    # Batch of targets\n",
    "    [[ 2  3], [ 8  9], [14 15]]\n",
    "  ]\n",
    "\n",
    "  # Second Batch\n",
    "  [\n",
    "    # Batch of Input\n",
    "    [[ 3  4], [ 9 10], [15 16]]\n",
    "    # Batch of targets\n",
    "    [[ 4  5], [10 11], [16 17]]\n",
    "  ]\n",
    "\n",
    "  # Third Batch\n",
    "  [\n",
    "    # Batch of Input\n",
    "    [[ 5  6], [11 12], [17 18]]\n",
    "    # Batch of targets\n",
    "    [[ 6  7], [12 13], [18  1]]\n",
    "  ]\n",
    "]\n",
    "```\n",
    "\n",
    "Notice that the last target value in the last batch is the first input value of the first batch. In this case, `1`. This is a common technique used when creating sequence batches, although it is rather unintuitive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def get_batches(int_text, batch_size, seq_length):\n",
    "    \"\"\"\n",
    "    Return batches of input and target\n",
    "    :param int_text: Text with the words replaced by their ids\n",
    "    :param batch_size: The size of batch\n",
    "    :param seq_length: The length of sequence\n",
    "    :return: Batches as a Numpy array\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    num_batches = int(len(int_text) / (batch_size * seq_length))\n",
    "    in_data = np.array(int_text[: num_batches * batch_size * seq_length])\n",
    "    out_data = np.array(int_text[1: num_batches * batch_size * seq_length + 1])\n",
    "    out_data[-1] = in_data[0]\n",
    "\n",
    "    in_batches = np.split(in_data.reshape(batch_size, -1), num_batches, 1)\n",
    "    out_batches = np.split(out_data.reshape(batch_size, -1), num_batches, 1)\n",
    "    return np.array(list(zip(in_batches, out_batches)))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_get_batches(get_batches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Training\n",
    "### Hyperparameters\n",
    "Tune the following parameters:\n",
    "\n",
    "- Set `num_epochs` to the number of epochs.\n",
    "- Set `batch_size` to the batch size.\n",
    "- Set `rnn_size` to the size of the RNNs.\n",
    "- Set `embed_dim` to the size of the embedding.\n",
    "- Set `seq_length` to the length of sequence.\n",
    "- Set `learning_rate` to the learning rate.\n",
    "- Set `show_every_n_batches` to the number of batches the neural network should print progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of Epochs\n",
    "num_epochs = 3000\n",
    "# Batch Size\n",
    "batch_size = 512\n",
    "# RNN Size\n",
    "rnn_size = 256\n",
    "# Embedding Dimension Size\n",
    "embed_dim = 256\n",
    "# Sequence Length\n",
    "seq_length = 16\n",
    "# Learning Rate\n",
    "learning_rate = 0.01\n",
    "# Show stats for every n number of batches\n",
    "show_every_n_batches = 32\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "save_dir = './savetshirts'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the Graph\n",
    "Build the graph using the neural network you implemented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "from tensorflow.contrib import seq2seq\n",
    "\n",
    "train_graph = tf.Graph()\n",
    "with train_graph.as_default():\n",
    "    vocab_size = len(int_to_vocab)\n",
    "    input_text, targets, lr = get_inputs()\n",
    "    input_data_shape = tf.shape(input_text)\n",
    "    cell, initial_state = get_init_cell(input_data_shape[0], rnn_size)\n",
    "    logits, final_state = build_nn(cell, rnn_size, input_text, vocab_size, embed_dim)\n",
    "\n",
    "    # Probabilities for generating words\n",
    "    probs = tf.nn.softmax(logits, name='probs')\n",
    "\n",
    "    # Loss function\n",
    "    cost = seq2seq.sequence_loss(\n",
    "        logits,\n",
    "        targets,\n",
    "        tf.ones([input_data_shape[0], input_data_shape[1]]))\n",
    "\n",
    "    # Optimizer\n",
    "    optimizer = tf.train.AdamOptimizer(lr)\n",
    "\n",
    "    # Gradient Clipping\n",
    "    gradients = optimizer.compute_gradients(cost)\n",
    "    capped_gradients = [(tf.clip_by_value(grad, -1., 1.), var) for grad, var in gradients if grad is not None]\n",
    "    train_op = optimizer.apply_gradients(capped_gradients)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train\n",
    "Train the neural network on the preprocessed data.  If you have a hard time getting a good loss, check the [forums](https://discussions.udacity.com/) to see if anyone is having the same problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   0 Batch    0/175   train_loss = 8.793\n",
      "Epoch   0 Batch   32/175   train_loss = 5.839\n",
      "Epoch   0 Batch   64/175   train_loss = 5.828\n",
      "Epoch   0 Batch   96/175   train_loss = 5.847\n",
      "Epoch   0 Batch  128/175   train_loss = 5.805\n",
      "Epoch   0 Batch  160/175   train_loss = 5.795\n",
      "Epoch   1 Batch   17/175   train_loss = 5.751\n",
      "Epoch   1 Batch   49/175   train_loss = 5.732\n",
      "Epoch   1 Batch   81/175   train_loss = 5.336\n",
      "Epoch   1 Batch  113/175   train_loss = 4.812\n",
      "Epoch   1 Batch  145/175   train_loss = 4.154\n",
      "Epoch   2 Batch    2/175   train_loss = 3.746\n",
      "Epoch   2 Batch   34/175   train_loss = 3.471\n",
      "Epoch   2 Batch   66/175   train_loss = 3.118\n",
      "Epoch   2 Batch   98/175   train_loss = 2.825\n",
      "Epoch   2 Batch  130/175   train_loss = 2.644\n",
      "Epoch   2 Batch  162/175   train_loss = 2.407\n",
      "Epoch   3 Batch   19/175   train_loss = 2.308\n",
      "Epoch   3 Batch   51/175   train_loss = 2.125\n",
      "Epoch   3 Batch   83/175   train_loss = 2.160\n",
      "Epoch   3 Batch  115/175   train_loss = 2.226\n",
      "Epoch   3 Batch  147/175   train_loss = 1.934\n",
      "Epoch   4 Batch    4/175   train_loss = 1.956\n",
      "Epoch   4 Batch   36/175   train_loss = 1.867\n",
      "Epoch   4 Batch   68/175   train_loss = 1.878\n",
      "Epoch   4 Batch  100/175   train_loss = 1.839\n",
      "Epoch   4 Batch  132/175   train_loss = 1.744\n",
      "Epoch   4 Batch  164/175   train_loss = 1.705\n",
      "Epoch   5 Batch   21/175   train_loss = 1.645\n",
      "Epoch   5 Batch   53/175   train_loss = 1.681\n",
      "Epoch   5 Batch   85/175   train_loss = 1.724\n",
      "Epoch   5 Batch  117/175   train_loss = 1.709\n",
      "Epoch   5 Batch  149/175   train_loss = 1.660\n",
      "Epoch   6 Batch    6/175   train_loss = 1.644\n",
      "Epoch   6 Batch   38/175   train_loss = 1.544\n",
      "Epoch   6 Batch   70/175   train_loss = 1.625\n",
      "Epoch   6 Batch  102/175   train_loss = 1.594\n",
      "Epoch   6 Batch  134/175   train_loss = 1.543\n",
      "Epoch   6 Batch  166/175   train_loss = 1.564\n",
      "Epoch   7 Batch   23/175   train_loss = 1.501\n",
      "Epoch   7 Batch   55/175   train_loss = 1.618\n",
      "Epoch   7 Batch   87/175   train_loss = 1.615\n",
      "Epoch   7 Batch  119/175   train_loss = 1.517\n",
      "Epoch   7 Batch  151/175   train_loss = 1.535\n",
      "Epoch   8 Batch    8/175   train_loss = 1.523\n",
      "Epoch   8 Batch   40/175   train_loss = 1.465\n",
      "Epoch   8 Batch   72/175   train_loss = 1.539\n",
      "Epoch   8 Batch  104/175   train_loss = 1.502\n",
      "Epoch   8 Batch  136/175   train_loss = 1.477\n",
      "Epoch   8 Batch  168/175   train_loss = 1.489\n",
      "Epoch   9 Batch   25/175   train_loss = 1.466\n",
      "Epoch   9 Batch   57/175   train_loss = 1.547\n",
      "Epoch   9 Batch   89/175   train_loss = 1.472\n",
      "Epoch   9 Batch  121/175   train_loss = 1.407\n",
      "Epoch   9 Batch  153/175   train_loss = 1.435\n",
      "Epoch  10 Batch   10/175   train_loss = 1.363\n",
      "Epoch  10 Batch   42/175   train_loss = 1.492\n",
      "Epoch  10 Batch   74/175   train_loss = 1.488\n",
      "Epoch  10 Batch  106/175   train_loss = 1.447\n",
      "Epoch  10 Batch  138/175   train_loss = 1.401\n",
      "Epoch  10 Batch  170/175   train_loss = 1.456\n",
      "Epoch  11 Batch   27/175   train_loss = 1.387\n",
      "Epoch  11 Batch   59/175   train_loss = 1.375\n",
      "Epoch  11 Batch   91/175   train_loss = 1.376\n",
      "Epoch  11 Batch  123/175   train_loss = 1.363\n",
      "Epoch  11 Batch  155/175   train_loss = 1.344\n",
      "Epoch  12 Batch   12/175   train_loss = 1.359\n",
      "Epoch  12 Batch   44/175   train_loss = 1.353\n",
      "Epoch  12 Batch   76/175   train_loss = 1.364\n",
      "Epoch  12 Batch  108/175   train_loss = 1.380\n",
      "Epoch  12 Batch  140/175   train_loss = 1.334\n",
      "Epoch  12 Batch  172/175   train_loss = 1.385\n",
      "Epoch  13 Batch   29/175   train_loss = 1.365\n",
      "Epoch  13 Batch   61/175   train_loss = 1.381\n",
      "Epoch  13 Batch   93/175   train_loss = 1.364\n",
      "Epoch  13 Batch  125/175   train_loss = 1.351\n",
      "Epoch  13 Batch  157/175   train_loss = 1.359\n",
      "Epoch  14 Batch   14/175   train_loss = 1.355\n",
      "Epoch  14 Batch   46/175   train_loss = 1.349\n",
      "Epoch  14 Batch   78/175   train_loss = 1.304\n",
      "Epoch  14 Batch  110/175   train_loss = 1.436\n",
      "Epoch  14 Batch  142/175   train_loss = 1.371\n",
      "Epoch  14 Batch  174/175   train_loss = 1.325\n",
      "Epoch  15 Batch   31/175   train_loss = 1.351\n",
      "Epoch  15 Batch   63/175   train_loss = 1.358\n",
      "Epoch  15 Batch   95/175   train_loss = 1.306\n",
      "Epoch  15 Batch  127/175   train_loss = 1.285\n",
      "Epoch  15 Batch  159/175   train_loss = 1.287\n",
      "Epoch  16 Batch   16/175   train_loss = 1.325\n",
      "Epoch  16 Batch   48/175   train_loss = 1.355\n",
      "Epoch  16 Batch   80/175   train_loss = 1.320\n",
      "Epoch  16 Batch  112/175   train_loss = 1.302\n",
      "Epoch  16 Batch  144/175   train_loss = 1.248\n",
      "Epoch  17 Batch    1/175   train_loss = 1.364\n",
      "Epoch  17 Batch   33/175   train_loss = 1.338\n",
      "Epoch  17 Batch   65/175   train_loss = 1.321\n",
      "Epoch  17 Batch   97/175   train_loss = 1.299\n",
      "Epoch  17 Batch  129/175   train_loss = 1.295\n",
      "Epoch  17 Batch  161/175   train_loss = 1.274\n",
      "Epoch  18 Batch   18/175   train_loss = 1.264\n",
      "Epoch  18 Batch   50/175   train_loss = 1.285\n",
      "Epoch  18 Batch   82/175   train_loss = 1.341\n",
      "Epoch  18 Batch  114/175   train_loss = 1.338\n",
      "Epoch  18 Batch  146/175   train_loss = 1.288\n",
      "Epoch  19 Batch    3/175   train_loss = 1.323\n",
      "Epoch  19 Batch   35/175   train_loss = 1.277\n",
      "Epoch  19 Batch   67/175   train_loss = 1.268\n",
      "Epoch  19 Batch   99/175   train_loss = 1.360\n",
      "Epoch  19 Batch  131/175   train_loss = 1.292\n",
      "Epoch  19 Batch  163/175   train_loss = 1.284\n",
      "Epoch  20 Batch   20/175   train_loss = 1.244\n",
      "Epoch  20 Batch   52/175   train_loss = 1.215\n",
      "Epoch  20 Batch   84/175   train_loss = 1.277\n",
      "Epoch  20 Batch  116/175   train_loss = 1.343\n",
      "Epoch  20 Batch  148/175   train_loss = 1.284\n",
      "Epoch  21 Batch    5/175   train_loss = 1.286\n",
      "Epoch  21 Batch   37/175   train_loss = 1.246\n",
      "Epoch  21 Batch   69/175   train_loss = 1.283\n",
      "Epoch  21 Batch  101/175   train_loss = 1.325\n",
      "Epoch  21 Batch  133/175   train_loss = 1.188\n",
      "Epoch  21 Batch  165/175   train_loss = 1.279\n",
      "Epoch  22 Batch   22/175   train_loss = 1.189\n",
      "Epoch  22 Batch   54/175   train_loss = 1.310\n",
      "Epoch  22 Batch   86/175   train_loss = 1.339\n",
      "Epoch  22 Batch  118/175   train_loss = 1.324\n",
      "Epoch  22 Batch  150/175   train_loss = 1.281\n",
      "Epoch  23 Batch    7/175   train_loss = 1.288\n",
      "Epoch  23 Batch   39/175   train_loss = 1.211\n",
      "Epoch  23 Batch   71/175   train_loss = 1.307\n",
      "Epoch  23 Batch  103/175   train_loss = 1.260\n",
      "Epoch  23 Batch  135/175   train_loss = 1.234\n",
      "Epoch  23 Batch  167/175   train_loss = 1.295\n",
      "Epoch  24 Batch   24/175   train_loss = 1.189\n",
      "Epoch  24 Batch   56/175   train_loss = 1.291\n",
      "Epoch  24 Batch   88/175   train_loss = 1.296\n",
      "Epoch  24 Batch  120/175   train_loss = 1.239\n",
      "Epoch  24 Batch  152/175   train_loss = 1.233\n",
      "Epoch  25 Batch    9/175   train_loss = 1.277\n",
      "Epoch  25 Batch   41/175   train_loss = 1.270\n",
      "Epoch  25 Batch   73/175   train_loss = 1.269\n",
      "Epoch  25 Batch  105/175   train_loss = 1.280\n",
      "Epoch  25 Batch  137/175   train_loss = 1.212\n",
      "Epoch  25 Batch  169/175   train_loss = 1.266\n",
      "Epoch  26 Batch   26/175   train_loss = 1.251\n",
      "Epoch  26 Batch   58/175   train_loss = 1.257\n",
      "Epoch  26 Batch   90/175   train_loss = 1.270\n",
      "Epoch  26 Batch  122/175   train_loss = 1.210\n",
      "Epoch  26 Batch  154/175   train_loss = 1.223\n",
      "Epoch  27 Batch   11/175   train_loss = 1.218\n",
      "Epoch  27 Batch   43/175   train_loss = 1.217\n",
      "Epoch  27 Batch   75/175   train_loss = 1.206\n",
      "Epoch  27 Batch  107/175   train_loss = 1.260\n",
      "Epoch  27 Batch  139/175   train_loss = 1.198\n",
      "Epoch  27 Batch  171/175   train_loss = 1.288\n",
      "Epoch  28 Batch   28/175   train_loss = 1.206\n",
      "Epoch  28 Batch   60/175   train_loss = 1.226\n",
      "Epoch  28 Batch   92/175   train_loss = 1.224\n",
      "Epoch  28 Batch  124/175   train_loss = 1.234\n",
      "Epoch  28 Batch  156/175   train_loss = 1.277\n",
      "Epoch  29 Batch   13/175   train_loss = 1.243\n",
      "Epoch  29 Batch   45/175   train_loss = 1.224\n",
      "Epoch  29 Batch   77/175   train_loss = 1.197\n",
      "Epoch  29 Batch  109/175   train_loss = 1.260\n",
      "Epoch  29 Batch  141/175   train_loss = 1.197\n",
      "Epoch  29 Batch  173/175   train_loss = 1.217\n",
      "Epoch  30 Batch   30/175   train_loss = 1.278\n",
      "Epoch  30 Batch   62/175   train_loss = 1.241\n",
      "Epoch  30 Batch   94/175   train_loss = 1.214\n",
      "Epoch  30 Batch  126/175   train_loss = 1.236\n",
      "Epoch  30 Batch  158/175   train_loss = 1.224\n",
      "Epoch  31 Batch   15/175   train_loss = 1.282\n",
      "Epoch  31 Batch   47/175   train_loss = 1.227\n",
      "Epoch  31 Batch   79/175   train_loss = 1.263\n",
      "Epoch  31 Batch  111/175   train_loss = 1.273\n",
      "Epoch  31 Batch  143/175   train_loss = 1.211\n",
      "Epoch  32 Batch    0/175   train_loss = 1.231\n",
      "Epoch  32 Batch   32/175   train_loss = 1.235\n",
      "Epoch  32 Batch   64/175   train_loss = 1.242\n",
      "Epoch  32 Batch   96/175   train_loss = 1.245\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  32 Batch  128/175   train_loss = 1.192\n",
      "Epoch  32 Batch  160/175   train_loss = 1.219\n",
      "Epoch  33 Batch   17/175   train_loss = 1.197\n",
      "Epoch  33 Batch   49/175   train_loss = 1.222\n",
      "Epoch  33 Batch   81/175   train_loss = 1.190\n",
      "Epoch  33 Batch  113/175   train_loss = 1.222\n",
      "Epoch  33 Batch  145/175   train_loss = 1.198\n",
      "Epoch  34 Batch    2/175   train_loss = 1.218\n",
      "Epoch  34 Batch   34/175   train_loss = 1.214\n",
      "Epoch  34 Batch   66/175   train_loss = 1.238\n",
      "Epoch  34 Batch   98/175   train_loss = 1.234\n",
      "Epoch  34 Batch  130/175   train_loss = 1.227\n",
      "Epoch  34 Batch  162/175   train_loss = 1.209\n",
      "Epoch  35 Batch   19/175   train_loss = 1.225\n",
      "Epoch  35 Batch   51/175   train_loss = 1.165\n",
      "Epoch  35 Batch   83/175   train_loss = 1.266\n",
      "Epoch  35 Batch  115/175   train_loss = 1.308\n",
      "Epoch  35 Batch  147/175   train_loss = 1.197\n",
      "Epoch  36 Batch    4/175   train_loss = 1.224\n",
      "Epoch  36 Batch   36/175   train_loss = 1.171\n",
      "Epoch  36 Batch   68/175   train_loss = 1.182\n",
      "Epoch  36 Batch  100/175   train_loss = 1.195\n",
      "Epoch  36 Batch  132/175   train_loss = 1.186\n",
      "Epoch  36 Batch  164/175   train_loss = 1.176\n",
      "Epoch  37 Batch   21/175   train_loss = 1.183\n",
      "Epoch  37 Batch   53/175   train_loss = 1.173\n",
      "Epoch  37 Batch   85/175   train_loss = 1.225\n",
      "Epoch  37 Batch  117/175   train_loss = 1.207\n",
      "Epoch  37 Batch  149/175   train_loss = 1.232\n",
      "Epoch  38 Batch    6/175   train_loss = 1.222\n",
      "Epoch  38 Batch   38/175   train_loss = 1.169\n",
      "Epoch  38 Batch   70/175   train_loss = 1.194\n",
      "Epoch  38 Batch  102/175   train_loss = 1.225\n",
      "Epoch  38 Batch  134/175   train_loss = 1.181\n",
      "Epoch  38 Batch  166/175   train_loss = 1.208\n",
      "Epoch  39 Batch   23/175   train_loss = 1.166\n",
      "Epoch  39 Batch   55/175   train_loss = 1.239\n",
      "Epoch  39 Batch   87/175   train_loss = 1.285\n",
      "Epoch  39 Batch  119/175   train_loss = 1.186\n",
      "Epoch  39 Batch  151/175   train_loss = 1.229\n",
      "Epoch  40 Batch    8/175   train_loss = 1.222\n",
      "Epoch  40 Batch   40/175   train_loss = 1.190\n",
      "Epoch  40 Batch   72/175   train_loss = 1.219\n",
      "Epoch  40 Batch  104/175   train_loss = 1.210\n",
      "Epoch  40 Batch  136/175   train_loss = 1.202\n",
      "Epoch  40 Batch  168/175   train_loss = 1.210\n",
      "Epoch  41 Batch   25/175   train_loss = 1.210\n",
      "Epoch  41 Batch   57/175   train_loss = 1.249\n",
      "Epoch  41 Batch   89/175   train_loss = 1.205\n",
      "Epoch  41 Batch  121/175   train_loss = 1.159\n",
      "Epoch  41 Batch  153/175   train_loss = 1.208\n",
      "Epoch  42 Batch   10/175   train_loss = 1.139\n",
      "Epoch  42 Batch   42/175   train_loss = 1.247\n",
      "Epoch  42 Batch   74/175   train_loss = 1.241\n",
      "Epoch  42 Batch  106/175   train_loss = 1.207\n",
      "Epoch  42 Batch  138/175   train_loss = 1.194\n",
      "Epoch  42 Batch  170/175   train_loss = 1.251\n",
      "Epoch  43 Batch   27/175   train_loss = 1.187\n",
      "Epoch  43 Batch   59/175   train_loss = 1.181\n",
      "Epoch  43 Batch   91/175   train_loss = 1.189\n",
      "Epoch  43 Batch  123/175   train_loss = 1.181\n",
      "Epoch  43 Batch  155/175   train_loss = 1.154\n",
      "Epoch  44 Batch   12/175   train_loss = 1.190\n",
      "Epoch  44 Batch   44/175   train_loss = 1.178\n",
      "Epoch  44 Batch   76/175   train_loss = 1.162\n",
      "Epoch  44 Batch  108/175   train_loss = 1.187\n",
      "Epoch  44 Batch  140/175   train_loss = 1.179\n",
      "Epoch  44 Batch  172/175   train_loss = 1.193\n",
      "Epoch  45 Batch   29/175   train_loss = 1.195\n",
      "Epoch  45 Batch   61/175   train_loss = 1.204\n",
      "Epoch  45 Batch   93/175   train_loss = 1.182\n",
      "Epoch  45 Batch  125/175   train_loss = 1.188\n",
      "Epoch  45 Batch  157/175   train_loss = 1.180\n",
      "Epoch  46 Batch   14/175   train_loss = 1.200\n",
      "Epoch  46 Batch   46/175   train_loss = 1.201\n",
      "Epoch  46 Batch   78/175   train_loss = 1.178\n",
      "Epoch  46 Batch  110/175   train_loss = 1.267\n",
      "Epoch  46 Batch  142/175   train_loss = 1.222\n",
      "Epoch  46 Batch  174/175   train_loss = 1.168\n",
      "Epoch  47 Batch   31/175   train_loss = 1.192\n",
      "Epoch  47 Batch   63/175   train_loss = 1.216\n",
      "Epoch  47 Batch   95/175   train_loss = 1.152\n",
      "Epoch  47 Batch  127/175   train_loss = 1.143\n",
      "Epoch  47 Batch  159/175   train_loss = 1.173\n",
      "Epoch  48 Batch   16/175   train_loss = 1.184\n",
      "Epoch  48 Batch   48/175   train_loss = 1.212\n",
      "Epoch  48 Batch   80/175   train_loss = 1.192\n",
      "Epoch  48 Batch  112/175   train_loss = 1.184\n",
      "Epoch  48 Batch  144/175   train_loss = 1.096\n",
      "Epoch  49 Batch    1/175   train_loss = 1.224\n",
      "Epoch  49 Batch   33/175   train_loss = 1.222\n",
      "Epoch  49 Batch   65/175   train_loss = 1.164\n",
      "Epoch  49 Batch   97/175   train_loss = 1.159\n",
      "Epoch  49 Batch  129/175   train_loss = 1.156\n",
      "Epoch  49 Batch  161/175   train_loss = 1.168\n",
      "Epoch  50 Batch   18/175   train_loss = 1.149\n",
      "Epoch  50 Batch   50/175   train_loss = 1.171\n",
      "Epoch  50 Batch   82/175   train_loss = 1.216\n",
      "Epoch  50 Batch  114/175   train_loss = 1.214\n",
      "Epoch  50 Batch  146/175   train_loss = 1.169\n",
      "Epoch  51 Batch    3/175   train_loss = 1.198\n",
      "Epoch  51 Batch   35/175   train_loss = 1.167\n",
      "Epoch  51 Batch   67/175   train_loss = 1.141\n",
      "Epoch  51 Batch   99/175   train_loss = 1.223\n",
      "Epoch  51 Batch  131/175   train_loss = 1.180\n",
      "Epoch  51 Batch  163/175   train_loss = 1.175\n",
      "Epoch  52 Batch   20/175   train_loss = 1.159\n",
      "Epoch  52 Batch   52/175   train_loss = 1.123\n",
      "Epoch  52 Batch   84/175   train_loss = 1.174\n",
      "Epoch  52 Batch  116/175   train_loss = 1.214\n",
      "Epoch  52 Batch  148/175   train_loss = 1.177\n",
      "Epoch  53 Batch    5/175   train_loss = 1.181\n",
      "Epoch  53 Batch   37/175   train_loss = 1.163\n",
      "Epoch  53 Batch   69/175   train_loss = 1.181\n",
      "Epoch  53 Batch  101/175   train_loss = 1.216\n",
      "Epoch  53 Batch  133/175   train_loss = 1.098\n",
      "Epoch  53 Batch  165/175   train_loss = 1.194\n",
      "Epoch  54 Batch   22/175   train_loss = 1.112\n",
      "Epoch  54 Batch   54/175   train_loss = 1.203\n",
      "Epoch  54 Batch   86/175   train_loss = 1.233\n",
      "Epoch  54 Batch  118/175   train_loss = 1.216\n",
      "Epoch  54 Batch  150/175   train_loss = 1.209\n",
      "Epoch  55 Batch    7/175   train_loss = 1.201\n",
      "Epoch  55 Batch   39/175   train_loss = 1.132\n",
      "Epoch  55 Batch   71/175   train_loss = 1.214\n",
      "Epoch  55 Batch  103/175   train_loss = 1.174\n",
      "Epoch  55 Batch  135/175   train_loss = 1.146\n",
      "Epoch  55 Batch  167/175   train_loss = 1.225\n",
      "Epoch  56 Batch   24/175   train_loss = 1.107\n",
      "Epoch  56 Batch   56/175   train_loss = 1.196\n",
      "Epoch  56 Batch   88/175   train_loss = 1.209\n",
      "Epoch  56 Batch  120/175   train_loss = 1.149\n",
      "Epoch  56 Batch  152/175   train_loss = 1.147\n",
      "Epoch  57 Batch    9/175   train_loss = 1.213\n",
      "Epoch  57 Batch   41/175   train_loss = 1.218\n",
      "Epoch  57 Batch   73/175   train_loss = 1.181\n",
      "Epoch  57 Batch  105/175   train_loss = 1.216\n",
      "Epoch  57 Batch  137/175   train_loss = 1.148\n",
      "Epoch  57 Batch  169/175   train_loss = 1.190\n",
      "Epoch  58 Batch   26/175   train_loss = 1.184\n",
      "Epoch  58 Batch   58/175   train_loss = 1.206\n",
      "Epoch  58 Batch   90/175   train_loss = 1.198\n",
      "Epoch  58 Batch  122/175   train_loss = 1.142\n",
      "Epoch  58 Batch  154/175   train_loss = 1.163\n",
      "Epoch  59 Batch   11/175   train_loss = 1.151\n",
      "Epoch  59 Batch   43/175   train_loss = 1.158\n",
      "Epoch  59 Batch   75/175   train_loss = 1.156\n",
      "Epoch  59 Batch  107/175   train_loss = 1.202\n",
      "Epoch  59 Batch  139/175   train_loss = 1.145\n",
      "Epoch  59 Batch  171/175   train_loss = 1.223\n",
      "Epoch  60 Batch   28/175   train_loss = 1.160\n",
      "Epoch  60 Batch   60/175   train_loss = 1.163\n",
      "Epoch  60 Batch   92/175   train_loss = 1.171\n",
      "Epoch  60 Batch  124/175   train_loss = 1.172\n",
      "Epoch  60 Batch  156/175   train_loss = 1.208\n",
      "Epoch  61 Batch   13/175   train_loss = 1.190\n",
      "Epoch  61 Batch   45/175   train_loss = 1.161\n",
      "Epoch  61 Batch   77/175   train_loss = 1.161\n",
      "Epoch  61 Batch  109/175   train_loss = 1.202\n",
      "Epoch  61 Batch  141/175   train_loss = 1.134\n",
      "Epoch  61 Batch  173/175   train_loss = 1.148\n",
      "Epoch  62 Batch   30/175   train_loss = 1.224\n",
      "Epoch  62 Batch   62/175   train_loss = 1.203\n",
      "Epoch  62 Batch   94/175   train_loss = 1.164\n",
      "Epoch  62 Batch  126/175   train_loss = 1.176\n",
      "Epoch  62 Batch  158/175   train_loss = 1.178\n",
      "Epoch  63 Batch   15/175   train_loss = 1.242\n",
      "Epoch  63 Batch   47/175   train_loss = 1.199\n",
      "Epoch  63 Batch   79/175   train_loss = 1.201\n",
      "Epoch  63 Batch  111/175   train_loss = 1.237\n",
      "Epoch  63 Batch  143/175   train_loss = 1.144\n",
      "Epoch  64 Batch    0/175   train_loss = 1.200\n",
      "Epoch  64 Batch   32/175   train_loss = 1.209\n",
      "Epoch  64 Batch   64/175   train_loss = 1.208\n",
      "Epoch  64 Batch   96/175   train_loss = 1.191\n",
      "Epoch  64 Batch  128/175   train_loss = 1.151\n",
      "Epoch  64 Batch  160/175   train_loss = 1.184\n",
      "Epoch  65 Batch   17/175   train_loss = 1.165\n",
      "Epoch  65 Batch   49/175   train_loss = 1.205\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  65 Batch   81/175   train_loss = 1.152\n",
      "Epoch  65 Batch  113/175   train_loss = 1.190\n",
      "Epoch  65 Batch  145/175   train_loss = 1.148\n",
      "Epoch  66 Batch    2/175   train_loss = 1.173\n",
      "Epoch  66 Batch   34/175   train_loss = 1.199\n",
      "Epoch  66 Batch   66/175   train_loss = 1.195\n",
      "Epoch  66 Batch   98/175   train_loss = 1.211\n",
      "Epoch  66 Batch  130/175   train_loss = 1.212\n",
      "Epoch  66 Batch  162/175   train_loss = 1.189\n",
      "Epoch  67 Batch   19/175   train_loss = 1.171\n",
      "Epoch  67 Batch   51/175   train_loss = 1.132\n",
      "Epoch  67 Batch   83/175   train_loss = 1.230\n",
      "Epoch  67 Batch  115/175   train_loss = 1.291\n",
      "Epoch  67 Batch  147/175   train_loss = 1.155\n",
      "Epoch  68 Batch    4/175   train_loss = 1.191\n",
      "Epoch  68 Batch   36/175   train_loss = 1.141\n",
      "Epoch  68 Batch   68/175   train_loss = 1.161\n",
      "Epoch  68 Batch  100/175   train_loss = 1.172\n",
      "Epoch  68 Batch  132/175   train_loss = 1.122\n",
      "Epoch  68 Batch  164/175   train_loss = 1.149\n",
      "Epoch  69 Batch   21/175   train_loss = 1.141\n",
      "Epoch  69 Batch   53/175   train_loss = 1.164\n",
      "Epoch  69 Batch   85/175   train_loss = 1.185\n",
      "Epoch  69 Batch  117/175   train_loss = 1.171\n",
      "Epoch  69 Batch  149/175   train_loss = 1.190\n",
      "Epoch  70 Batch    6/175   train_loss = 1.181\n",
      "Epoch  70 Batch   38/175   train_loss = 1.130\n",
      "Epoch  70 Batch   70/175   train_loss = 1.168\n",
      "Epoch  70 Batch  102/175   train_loss = 1.187\n",
      "Epoch  70 Batch  134/175   train_loss = 1.135\n",
      "Epoch  70 Batch  166/175   train_loss = 1.174\n",
      "Epoch  71 Batch   23/175   train_loss = 1.139\n",
      "Epoch  71 Batch   55/175   train_loss = 1.220\n",
      "Epoch  71 Batch   87/175   train_loss = 1.242\n",
      "Epoch  71 Batch  119/175   train_loss = 1.169\n",
      "Epoch  71 Batch  151/175   train_loss = 1.194\n",
      "Epoch  72 Batch    8/175   train_loss = 1.183\n",
      "Epoch  72 Batch   40/175   train_loss = 1.169\n",
      "Epoch  72 Batch   72/175   train_loss = 1.190\n",
      "Epoch  72 Batch  104/175   train_loss = 1.185\n",
      "Epoch  72 Batch  136/175   train_loss = 1.169\n",
      "Epoch  72 Batch  168/175   train_loss = 1.206\n",
      "Epoch  73 Batch   25/175   train_loss = 1.170\n",
      "Epoch  73 Batch   57/175   train_loss = 1.218\n",
      "Epoch  73 Batch   89/175   train_loss = 1.169\n",
      "Epoch  73 Batch  121/175   train_loss = 1.133\n",
      "Epoch  73 Batch  153/175   train_loss = 1.186\n",
      "Epoch  74 Batch   10/175   train_loss = 1.128\n",
      "Epoch  74 Batch   42/175   train_loss = 1.215\n",
      "Epoch  74 Batch   74/175   train_loss = 1.221\n",
      "Epoch  74 Batch  106/175   train_loss = 1.198\n",
      "Epoch  74 Batch  138/175   train_loss = 1.167\n",
      "Epoch  74 Batch  170/175   train_loss = 1.219\n",
      "Epoch  75 Batch   27/175   train_loss = 1.146\n",
      "Epoch  75 Batch   59/175   train_loss = 1.157\n",
      "Epoch  75 Batch   91/175   train_loss = 1.156\n",
      "Epoch  75 Batch  123/175   train_loss = 1.145\n",
      "Epoch  75 Batch  155/175   train_loss = 1.134\n",
      "Epoch  76 Batch   12/175   train_loss = 1.164\n",
      "Epoch  76 Batch   44/175   train_loss = 1.154\n",
      "Epoch  76 Batch   76/175   train_loss = 1.150\n",
      "Epoch  76 Batch  108/175   train_loss = 1.177\n",
      "Epoch  76 Batch  140/175   train_loss = 1.153\n",
      "Epoch  76 Batch  172/175   train_loss = 1.194\n",
      "Epoch  77 Batch   29/175   train_loss = 1.157\n",
      "Epoch  77 Batch   61/175   train_loss = 1.181\n",
      "Epoch  77 Batch   93/175   train_loss = 1.180\n",
      "Epoch  77 Batch  125/175   train_loss = 1.178\n",
      "Epoch  77 Batch  157/175   train_loss = 1.149\n",
      "Epoch  78 Batch   14/175   train_loss = 1.185\n",
      "Epoch  78 Batch   46/175   train_loss = 1.172\n",
      "Epoch  78 Batch   78/175   train_loss = 1.149\n",
      "Epoch  78 Batch  110/175   train_loss = 1.230\n",
      "Epoch  78 Batch  142/175   train_loss = 1.193\n",
      "Epoch  78 Batch  174/175   train_loss = 1.166\n",
      "Epoch  79 Batch   31/175   train_loss = 1.190\n",
      "Epoch  79 Batch   63/175   train_loss = 1.189\n",
      "Epoch  79 Batch   95/175   train_loss = 1.145\n",
      "Epoch  79 Batch  127/175   train_loss = 1.131\n",
      "Epoch  79 Batch  159/175   train_loss = 1.147\n",
      "Epoch  80 Batch   16/175   train_loss = 1.162\n",
      "Epoch  80 Batch   48/175   train_loss = 1.166\n",
      "Epoch  80 Batch   80/175   train_loss = 1.176\n",
      "Epoch  80 Batch  112/175   train_loss = 1.170\n",
      "Epoch  80 Batch  144/175   train_loss = 1.099\n",
      "Epoch  81 Batch    1/175   train_loss = 1.185\n",
      "Epoch  81 Batch   33/175   train_loss = 1.193\n",
      "Epoch  81 Batch   65/175   train_loss = 1.160\n",
      "Epoch  81 Batch   97/175   train_loss = 1.156\n",
      "Epoch  81 Batch  129/175   train_loss = 1.152\n",
      "Epoch  81 Batch  161/175   train_loss = 1.144\n",
      "Epoch  82 Batch   18/175   train_loss = 1.157\n",
      "Epoch  82 Batch   50/175   train_loss = 1.142\n",
      "Epoch  82 Batch   82/175   train_loss = 1.190\n",
      "Epoch  82 Batch  114/175   train_loss = 1.199\n",
      "Epoch  82 Batch  146/175   train_loss = 1.161\n",
      "Epoch  83 Batch    3/175   train_loss = 1.198\n",
      "Epoch  83 Batch   35/175   train_loss = 1.155\n",
      "Epoch  83 Batch   67/175   train_loss = 1.130\n",
      "Epoch  83 Batch   99/175   train_loss = 1.221\n",
      "Epoch  83 Batch  131/175   train_loss = 1.161\n",
      "Epoch  83 Batch  163/175   train_loss = 1.151\n",
      "Epoch  84 Batch   20/175   train_loss = 1.129\n",
      "Epoch  84 Batch   52/175   train_loss = 1.105\n",
      "Epoch  84 Batch   84/175   train_loss = 1.141\n",
      "Epoch  84 Batch  116/175   train_loss = 1.212\n",
      "Epoch  84 Batch  148/175   train_loss = 1.166\n",
      "Epoch  85 Batch    5/175   train_loss = 1.166\n",
      "Epoch  85 Batch   37/175   train_loss = 1.147\n",
      "Epoch  85 Batch   69/175   train_loss = 1.178\n",
      "Epoch  85 Batch  101/175   train_loss = 1.199\n",
      "Epoch  85 Batch  133/175   train_loss = 1.073\n",
      "Epoch  85 Batch  165/175   train_loss = 1.162\n",
      "Epoch  86 Batch   22/175   train_loss = 1.106\n",
      "Epoch  86 Batch   54/175   train_loss = 1.206\n",
      "Epoch  86 Batch   86/175   train_loss = 1.218\n",
      "Epoch  86 Batch  118/175   train_loss = 1.208\n",
      "Epoch  86 Batch  150/175   train_loss = 1.178\n",
      "Epoch  87 Batch    7/175   train_loss = 1.188\n",
      "Epoch  87 Batch   39/175   train_loss = 1.116\n",
      "Epoch  87 Batch   71/175   train_loss = 1.192\n",
      "Epoch  87 Batch  103/175   train_loss = 1.157\n",
      "Epoch  87 Batch  135/175   train_loss = 1.138\n",
      "Epoch  87 Batch  167/175   train_loss = 1.216\n",
      "Epoch  88 Batch   24/175   train_loss = 1.108\n",
      "Epoch  88 Batch   56/175   train_loss = 1.191\n",
      "Epoch  88 Batch   88/175   train_loss = 1.196\n",
      "Epoch  88 Batch  120/175   train_loss = 1.146\n",
      "Epoch  88 Batch  152/175   train_loss = 1.142\n",
      "Epoch  89 Batch    9/175   train_loss = 1.187\n",
      "Epoch  89 Batch   41/175   train_loss = 1.207\n",
      "Epoch  89 Batch   73/175   train_loss = 1.177\n",
      "Epoch  89 Batch  105/175   train_loss = 1.208\n",
      "Epoch  89 Batch  137/175   train_loss = 1.143\n",
      "Epoch  89 Batch  169/175   train_loss = 1.183\n",
      "Epoch  90 Batch   26/175   train_loss = 1.175\n",
      "Epoch  90 Batch   58/175   train_loss = 1.194\n",
      "Epoch  90 Batch   90/175   train_loss = 1.171\n",
      "Epoch  90 Batch  122/175   train_loss = 1.140\n",
      "Epoch  90 Batch  154/175   train_loss = 1.133\n",
      "Epoch  91 Batch   11/175   train_loss = 1.150\n",
      "Epoch  91 Batch   43/175   train_loss = 1.151\n",
      "Epoch  91 Batch   75/175   train_loss = 1.126\n",
      "Epoch  91 Batch  107/175   train_loss = 1.186\n",
      "Epoch  91 Batch  139/175   train_loss = 1.134\n",
      "Epoch  91 Batch  171/175   train_loss = 1.183\n",
      "Epoch  92 Batch   28/175   train_loss = 1.139\n",
      "Epoch  92 Batch   60/175   train_loss = 1.170\n",
      "Epoch  92 Batch   92/175   train_loss = 1.144\n",
      "Epoch  92 Batch  124/175   train_loss = 1.161\n",
      "Epoch  92 Batch  156/175   train_loss = 1.191\n",
      "Epoch  93 Batch   13/175   train_loss = 1.166\n",
      "Epoch  93 Batch   45/175   train_loss = 1.155\n",
      "Epoch  93 Batch   77/175   train_loss = 1.149\n",
      "Epoch  93 Batch  109/175   train_loss = 1.199\n",
      "Epoch  93 Batch  141/175   train_loss = 1.117\n",
      "Epoch  93 Batch  173/175   train_loss = 1.155\n",
      "Epoch  94 Batch   30/175   train_loss = 1.212\n",
      "Epoch  94 Batch   62/175   train_loss = 1.183\n",
      "Epoch  94 Batch   94/175   train_loss = 1.142\n",
      "Epoch  94 Batch  126/175   train_loss = 1.166\n",
      "Epoch  94 Batch  158/175   train_loss = 1.168\n",
      "Epoch  95 Batch   15/175   train_loss = 1.206\n",
      "Epoch  95 Batch   47/175   train_loss = 1.180\n",
      "Epoch  95 Batch   79/175   train_loss = 1.185\n",
      "Epoch  95 Batch  111/175   train_loss = 1.200\n",
      "Epoch  95 Batch  143/175   train_loss = 1.130\n",
      "Epoch  96 Batch    0/175   train_loss = 1.161\n",
      "Epoch  96 Batch   32/175   train_loss = 1.186\n",
      "Epoch  96 Batch   64/175   train_loss = 1.178\n",
      "Epoch  96 Batch   96/175   train_loss = 1.177\n",
      "Epoch  96 Batch  128/175   train_loss = 1.118\n",
      "Epoch  96 Batch  160/175   train_loss = 1.175\n",
      "Epoch  97 Batch   17/175   train_loss = 1.135\n",
      "Epoch  97 Batch   49/175   train_loss = 1.189\n",
      "Epoch  97 Batch   81/175   train_loss = 1.143\n",
      "Epoch  97 Batch  113/175   train_loss = 1.153\n",
      "Epoch  97 Batch  145/175   train_loss = 1.141\n",
      "Epoch  98 Batch    2/175   train_loss = 1.164\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  98 Batch   34/175   train_loss = 1.175\n",
      "Epoch  98 Batch   66/175   train_loss = 1.193\n",
      "Epoch  98 Batch   98/175   train_loss = 1.182\n",
      "Epoch  98 Batch  130/175   train_loss = 1.177\n",
      "Epoch  98 Batch  162/175   train_loss = 1.166\n",
      "Epoch  99 Batch   19/175   train_loss = 1.165\n",
      "Epoch  99 Batch   51/175   train_loss = 1.120\n",
      "Epoch  99 Batch   83/175   train_loss = 1.211\n",
      "Epoch  99 Batch  115/175   train_loss = 1.259\n",
      "Epoch  99 Batch  147/175   train_loss = 1.144\n",
      "Epoch 100 Batch    4/175   train_loss = 1.186\n",
      "Epoch 100 Batch   36/175   train_loss = 1.133\n",
      "Epoch 100 Batch   68/175   train_loss = 1.164\n",
      "Epoch 100 Batch  100/175   train_loss = 1.156\n",
      "Epoch 100 Batch  132/175   train_loss = 1.113\n",
      "Epoch 100 Batch  164/175   train_loss = 1.148\n",
      "Epoch 101 Batch   21/175   train_loss = 1.126\n",
      "Epoch 101 Batch   53/175   train_loss = 1.144\n",
      "Epoch 101 Batch   85/175   train_loss = 1.175\n",
      "Epoch 101 Batch  117/175   train_loss = 1.188\n",
      "Epoch 101 Batch  149/175   train_loss = 1.174\n",
      "Epoch 102 Batch    6/175   train_loss = 1.182\n",
      "Epoch 102 Batch   38/175   train_loss = 1.112\n",
      "Epoch 102 Batch   70/175   train_loss = 1.161\n",
      "Epoch 102 Batch  102/175   train_loss = 1.175\n",
      "Epoch 102 Batch  134/175   train_loss = 1.124\n",
      "Epoch 102 Batch  166/175   train_loss = 1.172\n",
      "Epoch 103 Batch   23/175   train_loss = 1.122\n",
      "Epoch 103 Batch   55/175   train_loss = 1.222\n",
      "Epoch 103 Batch   87/175   train_loss = 1.232\n",
      "Epoch 103 Batch  119/175   train_loss = 1.145\n",
      "Epoch 103 Batch  151/175   train_loss = 1.180\n",
      "Epoch 104 Batch    8/175   train_loss = 1.174\n",
      "Epoch 104 Batch   40/175   train_loss = 1.162\n",
      "Epoch 104 Batch   72/175   train_loss = 1.191\n",
      "Epoch 104 Batch  104/175   train_loss = 1.184\n",
      "Epoch 104 Batch  136/175   train_loss = 1.162\n",
      "Epoch 104 Batch  168/175   train_loss = 1.183\n",
      "Epoch 105 Batch   25/175   train_loss = 1.170\n",
      "Epoch 105 Batch   57/175   train_loss = 1.193\n",
      "Epoch 105 Batch   89/175   train_loss = 1.155\n",
      "Epoch 105 Batch  121/175   train_loss = 1.131\n",
      "Epoch 105 Batch  153/175   train_loss = 1.160\n",
      "Epoch 106 Batch   10/175   train_loss = 1.119\n",
      "Epoch 106 Batch   42/175   train_loss = 1.200\n",
      "Epoch 106 Batch   74/175   train_loss = 1.205\n",
      "Epoch 106 Batch  106/175   train_loss = 1.191\n",
      "Epoch 106 Batch  138/175   train_loss = 1.159\n",
      "Epoch 106 Batch  170/175   train_loss = 1.200\n",
      "Epoch 107 Batch   27/175   train_loss = 1.143\n",
      "Epoch 107 Batch   59/175   train_loss = 1.154\n",
      "Epoch 107 Batch   91/175   train_loss = 1.136\n",
      "Epoch 107 Batch  123/175   train_loss = 1.149\n",
      "Epoch 107 Batch  155/175   train_loss = 1.135\n",
      "Epoch 108 Batch   12/175   train_loss = 1.164\n",
      "Epoch 108 Batch   44/175   train_loss = 1.131\n",
      "Epoch 108 Batch   76/175   train_loss = 1.138\n",
      "Epoch 108 Batch  108/175   train_loss = 1.173\n",
      "Epoch 108 Batch  140/175   train_loss = 1.159\n",
      "Epoch 108 Batch  172/175   train_loss = 1.190\n",
      "Epoch 109 Batch   29/175   train_loss = 1.152\n",
      "Epoch 109 Batch   61/175   train_loss = 1.170\n",
      "Epoch 109 Batch   93/175   train_loss = 1.167\n",
      "Epoch 109 Batch  125/175   train_loss = 1.171\n",
      "Epoch 109 Batch  157/175   train_loss = 1.145\n",
      "Epoch 110 Batch   14/175   train_loss = 1.178\n",
      "Epoch 110 Batch   46/175   train_loss = 1.159\n",
      "Epoch 110 Batch   78/175   train_loss = 1.155\n",
      "Epoch 110 Batch  110/175   train_loss = 1.242\n",
      "Epoch 110 Batch  142/175   train_loss = 1.194\n",
      "Epoch 110 Batch  174/175   train_loss = 1.159\n",
      "Epoch 111 Batch   31/175   train_loss = 1.169\n",
      "Epoch 111 Batch   63/175   train_loss = 1.185\n",
      "Epoch 111 Batch   95/175   train_loss = 1.150\n",
      "Epoch 111 Batch  127/175   train_loss = 1.115\n",
      "Epoch 111 Batch  159/175   train_loss = 1.152\n",
      "Epoch 112 Batch   16/175   train_loss = 1.175\n",
      "Epoch 112 Batch   48/175   train_loss = 1.171\n",
      "Epoch 112 Batch   80/175   train_loss = 1.161\n",
      "Epoch 112 Batch  112/175   train_loss = 1.152\n",
      "Epoch 112 Batch  144/175   train_loss = 1.087\n",
      "Epoch 113 Batch    1/175   train_loss = 1.188\n",
      "Epoch 113 Batch   33/175   train_loss = 1.197\n",
      "Epoch 113 Batch   65/175   train_loss = 1.165\n",
      "Epoch 113 Batch   97/175   train_loss = 1.144\n",
      "Epoch 113 Batch  129/175   train_loss = 1.144\n",
      "Epoch 113 Batch  161/175   train_loss = 1.136\n",
      "Epoch 114 Batch   18/175   train_loss = 1.134\n",
      "Epoch 114 Batch   50/175   train_loss = 1.152\n",
      "Epoch 114 Batch   82/175   train_loss = 1.192\n",
      "Epoch 114 Batch  114/175   train_loss = 1.194\n",
      "Epoch 114 Batch  146/175   train_loss = 1.163\n",
      "Epoch 115 Batch    3/175   train_loss = 1.186\n",
      "Epoch 115 Batch   35/175   train_loss = 1.145\n",
      "Epoch 115 Batch   67/175   train_loss = 1.120\n",
      "Epoch 115 Batch   99/175   train_loss = 1.224\n",
      "Epoch 115 Batch  131/175   train_loss = 1.159\n",
      "Epoch 115 Batch  163/175   train_loss = 1.151\n",
      "Epoch 116 Batch   20/175   train_loss = 1.131\n",
      "Epoch 116 Batch   52/175   train_loss = 1.101\n",
      "Epoch 116 Batch   84/175   train_loss = 1.164\n",
      "Epoch 116 Batch  116/175   train_loss = 1.212\n",
      "Epoch 116 Batch  148/175   train_loss = 1.168\n",
      "Epoch 117 Batch    5/175   train_loss = 1.179\n",
      "Epoch 117 Batch   37/175   train_loss = 1.147\n",
      "Epoch 117 Batch   69/175   train_loss = 1.193\n",
      "Epoch 117 Batch  101/175   train_loss = 1.194\n",
      "Epoch 117 Batch  133/175   train_loss = 1.084\n",
      "Epoch 117 Batch  165/175   train_loss = 1.153\n",
      "Epoch 118 Batch   22/175   train_loss = 1.114\n",
      "Epoch 118 Batch   54/175   train_loss = 1.190\n",
      "Epoch 118 Batch   86/175   train_loss = 1.210\n",
      "Epoch 118 Batch  118/175   train_loss = 1.198\n",
      "Epoch 118 Batch  150/175   train_loss = 1.179\n",
      "Epoch 119 Batch    7/175   train_loss = 1.201\n",
      "Epoch 119 Batch   39/175   train_loss = 1.114\n",
      "Epoch 119 Batch   71/175   train_loss = 1.195\n",
      "Epoch 119 Batch  103/175   train_loss = 1.156\n",
      "Epoch 119 Batch  135/175   train_loss = 1.138\n",
      "Epoch 119 Batch  167/175   train_loss = 1.203\n",
      "Epoch 120 Batch   24/175   train_loss = 1.114\n",
      "Epoch 120 Batch   56/175   train_loss = 1.189\n",
      "Epoch 120 Batch   88/175   train_loss = 1.213\n",
      "Epoch 120 Batch  120/175   train_loss = 1.155\n",
      "Epoch 120 Batch  152/175   train_loss = 1.158\n",
      "Epoch 121 Batch    9/175   train_loss = 1.205\n",
      "Epoch 121 Batch   41/175   train_loss = 1.185\n",
      "Epoch 121 Batch   73/175   train_loss = 1.179\n",
      "Epoch 121 Batch  105/175   train_loss = 1.212\n",
      "Epoch 121 Batch  137/175   train_loss = 1.143\n",
      "Epoch 121 Batch  169/175   train_loss = 1.172\n",
      "Epoch 122 Batch   26/175   train_loss = 1.182\n",
      "Epoch 122 Batch   58/175   train_loss = 1.189\n",
      "Epoch 122 Batch   90/175   train_loss = 1.182\n",
      "Epoch 122 Batch  122/175   train_loss = 1.136\n",
      "Epoch 122 Batch  154/175   train_loss = 1.109\n",
      "Epoch 123 Batch   11/175   train_loss = 1.149\n",
      "Epoch 123 Batch   43/175   train_loss = 1.147\n",
      "Epoch 123 Batch   75/175   train_loss = 1.146\n",
      "Epoch 123 Batch  107/175   train_loss = 1.197\n",
      "Epoch 123 Batch  139/175   train_loss = 1.136\n",
      "Epoch 123 Batch  171/175   train_loss = 1.215\n",
      "Epoch 124 Batch   28/175   train_loss = 1.151\n",
      "Epoch 124 Batch   60/175   train_loss = 1.162\n",
      "Epoch 124 Batch   92/175   train_loss = 1.158\n",
      "Epoch 124 Batch  124/175   train_loss = 1.168\n",
      "Epoch 124 Batch  156/175   train_loss = 1.196\n",
      "Epoch 125 Batch   13/175   train_loss = 1.180\n",
      "Epoch 125 Batch   45/175   train_loss = 1.164\n",
      "Epoch 125 Batch   77/175   train_loss = 1.140\n",
      "Epoch 125 Batch  109/175   train_loss = 1.196\n",
      "Epoch 125 Batch  141/175   train_loss = 1.136\n",
      "Epoch 125 Batch  173/175   train_loss = 1.148\n",
      "Epoch 126 Batch   30/175   train_loss = 1.210\n",
      "Epoch 126 Batch   62/175   train_loss = 1.194\n",
      "Epoch 126 Batch   94/175   train_loss = 1.147\n",
      "Epoch 126 Batch  126/175   train_loss = 1.178\n",
      "Epoch 126 Batch  158/175   train_loss = 1.167\n",
      "Epoch 127 Batch   15/175   train_loss = 1.193\n",
      "Epoch 127 Batch   47/175   train_loss = 1.169\n",
      "Epoch 127 Batch   79/175   train_loss = 1.194\n",
      "Epoch 127 Batch  111/175   train_loss = 1.215\n",
      "Epoch 127 Batch  143/175   train_loss = 1.153\n",
      "Epoch 128 Batch    0/175   train_loss = 1.172\n",
      "Epoch 128 Batch   32/175   train_loss = 1.171\n",
      "Epoch 128 Batch   64/175   train_loss = 1.186\n",
      "Epoch 128 Batch   96/175   train_loss = 1.174\n",
      "Epoch 128 Batch  128/175   train_loss = 1.132\n",
      "Epoch 128 Batch  160/175   train_loss = 1.173\n",
      "Epoch 129 Batch   17/175   train_loss = 1.135\n",
      "Epoch 129 Batch   49/175   train_loss = 1.185\n",
      "Epoch 129 Batch   81/175   train_loss = 1.144\n",
      "Epoch 129 Batch  113/175   train_loss = 1.173\n",
      "Epoch 129 Batch  145/175   train_loss = 1.125\n",
      "Epoch 130 Batch    2/175   train_loss = 1.185\n",
      "Epoch 130 Batch   34/175   train_loss = 1.168\n",
      "Epoch 130 Batch   66/175   train_loss = 1.197\n",
      "Epoch 130 Batch   98/175   train_loss = 1.207\n",
      "Epoch 130 Batch  130/175   train_loss = 1.188\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 130 Batch  162/175   train_loss = 1.171\n",
      "Epoch 131 Batch   19/175   train_loss = 1.165\n",
      "Epoch 131 Batch   51/175   train_loss = 1.103\n",
      "Epoch 131 Batch   83/175   train_loss = 1.221\n",
      "Epoch 131 Batch  115/175   train_loss = 1.259\n",
      "Epoch 131 Batch  147/175   train_loss = 1.152\n",
      "Epoch 132 Batch    4/175   train_loss = 1.160\n",
      "Epoch 132 Batch   36/175   train_loss = 1.129\n",
      "Epoch 132 Batch   68/175   train_loss = 1.154\n",
      "Epoch 132 Batch  100/175   train_loss = 1.159\n",
      "Epoch 132 Batch  132/175   train_loss = 1.120\n",
      "Epoch 132 Batch  164/175   train_loss = 1.124\n",
      "Epoch 133 Batch   21/175   train_loss = 1.126\n",
      "Epoch 133 Batch   53/175   train_loss = 1.142\n",
      "Epoch 133 Batch   85/175   train_loss = 1.172\n",
      "Epoch 133 Batch  117/175   train_loss = 1.163\n",
      "Epoch 133 Batch  149/175   train_loss = 1.187\n",
      "Epoch 134 Batch    6/175   train_loss = 1.182\n",
      "Epoch 134 Batch   38/175   train_loss = 1.118\n",
      "Epoch 134 Batch   70/175   train_loss = 1.161\n",
      "Epoch 134 Batch  102/175   train_loss = 1.172\n",
      "Epoch 134 Batch  134/175   train_loss = 1.135\n",
      "Epoch 134 Batch  166/175   train_loss = 1.170\n",
      "Epoch 135 Batch   23/175   train_loss = 1.118\n",
      "Epoch 135 Batch   55/175   train_loss = 1.209\n",
      "Epoch 135 Batch   87/175   train_loss = 1.233\n",
      "Epoch 135 Batch  119/175   train_loss = 1.134\n",
      "Epoch 135 Batch  151/175   train_loss = 1.181\n",
      "Epoch 136 Batch    8/175   train_loss = 1.170\n",
      "Epoch 136 Batch   40/175   train_loss = 1.156\n",
      "Epoch 136 Batch   72/175   train_loss = 1.155\n",
      "Epoch 136 Batch  104/175   train_loss = 1.177\n",
      "Epoch 136 Batch  136/175   train_loss = 1.151\n",
      "Epoch 136 Batch  168/175   train_loss = 1.182\n",
      "Epoch 137 Batch   25/175   train_loss = 1.175\n",
      "Epoch 137 Batch   57/175   train_loss = 1.203\n",
      "Epoch 137 Batch   89/175   train_loss = 1.156\n",
      "Epoch 137 Batch  121/175   train_loss = 1.122\n",
      "Epoch 137 Batch  153/175   train_loss = 1.172\n",
      "Epoch 138 Batch   10/175   train_loss = 1.110\n",
      "Epoch 138 Batch   42/175   train_loss = 1.203\n",
      "Epoch 138 Batch   74/175   train_loss = 1.184\n",
      "Epoch 138 Batch  106/175   train_loss = 1.181\n",
      "Epoch 138 Batch  138/175   train_loss = 1.146\n",
      "Epoch 138 Batch  170/175   train_loss = 1.192\n",
      "Epoch 139 Batch   27/175   train_loss = 1.132\n",
      "Epoch 139 Batch   59/175   train_loss = 1.137\n",
      "Epoch 139 Batch   91/175   train_loss = 1.156\n",
      "Epoch 139 Batch  123/175   train_loss = 1.136\n",
      "Epoch 139 Batch  155/175   train_loss = 1.136\n",
      "Epoch 140 Batch   12/175   train_loss = 1.154\n",
      "Epoch 140 Batch   44/175   train_loss = 1.155\n",
      "Epoch 140 Batch   76/175   train_loss = 1.137\n",
      "Epoch 140 Batch  108/175   train_loss = 1.150\n",
      "Epoch 140 Batch  140/175   train_loss = 1.133\n",
      "Epoch 140 Batch  172/175   train_loss = 1.187\n",
      "Epoch 141 Batch   29/175   train_loss = 1.144\n",
      "Epoch 141 Batch   61/175   train_loss = 1.151\n",
      "Epoch 141 Batch   93/175   train_loss = 1.161\n",
      "Epoch 141 Batch  125/175   train_loss = 1.174\n",
      "Epoch 141 Batch  157/175   train_loss = 1.142\n",
      "Epoch 142 Batch   14/175   train_loss = 1.169\n",
      "Epoch 142 Batch   46/175   train_loss = 1.150\n",
      "Epoch 142 Batch   78/175   train_loss = 1.130\n",
      "Epoch 142 Batch  110/175   train_loss = 1.236\n",
      "Epoch 142 Batch  142/175   train_loss = 1.183\n",
      "Epoch 142 Batch  174/175   train_loss = 1.155\n",
      "Epoch 143 Batch   31/175   train_loss = 1.159\n",
      "Epoch 143 Batch   63/175   train_loss = 1.170\n",
      "Epoch 143 Batch   95/175   train_loss = 1.125\n",
      "Epoch 143 Batch  127/175   train_loss = 1.112\n",
      "Epoch 143 Batch  159/175   train_loss = 1.147\n",
      "Epoch 144 Batch   16/175   train_loss = 1.154\n",
      "Epoch 144 Batch   48/175   train_loss = 1.164\n",
      "Epoch 144 Batch   80/175   train_loss = 1.165\n",
      "Epoch 144 Batch  112/175   train_loss = 1.157\n",
      "Epoch 144 Batch  144/175   train_loss = 1.083\n",
      "Epoch 145 Batch    1/175   train_loss = 1.184\n",
      "Epoch 145 Batch   33/175   train_loss = 1.195\n",
      "Epoch 145 Batch   65/175   train_loss = 1.142\n",
      "Epoch 145 Batch   97/175   train_loss = 1.148\n",
      "Epoch 145 Batch  129/175   train_loss = 1.138\n",
      "Epoch 145 Batch  161/175   train_loss = 1.142\n",
      "Epoch 146 Batch   18/175   train_loss = 1.137\n",
      "Epoch 146 Batch   50/175   train_loss = 1.136\n",
      "Epoch 146 Batch   82/175   train_loss = 1.197\n",
      "Epoch 146 Batch  114/175   train_loss = 1.197\n",
      "Epoch 146 Batch  146/175   train_loss = 1.144\n",
      "Epoch 147 Batch    3/175   train_loss = 1.179\n",
      "Epoch 147 Batch   35/175   train_loss = 1.144\n",
      "Epoch 147 Batch   67/175   train_loss = 1.110\n",
      "Epoch 147 Batch   99/175   train_loss = 1.212\n",
      "Epoch 147 Batch  131/175   train_loss = 1.141\n",
      "Epoch 147 Batch  163/175   train_loss = 1.162\n",
      "Epoch 148 Batch   20/175   train_loss = 1.135\n",
      "Epoch 148 Batch   52/175   train_loss = 1.087\n",
      "Epoch 148 Batch   84/175   train_loss = 1.155\n",
      "Epoch 148 Batch  116/175   train_loss = 1.196\n",
      "Epoch 148 Batch  148/175   train_loss = 1.168\n",
      "Epoch 149 Batch    5/175   train_loss = 1.153\n",
      "Epoch 149 Batch   37/175   train_loss = 1.138\n",
      "Epoch 149 Batch   69/175   train_loss = 1.171\n",
      "Epoch 149 Batch  101/175   train_loss = 1.183\n",
      "Epoch 149 Batch  133/175   train_loss = 1.078\n",
      "Epoch 149 Batch  165/175   train_loss = 1.168\n",
      "Epoch 150 Batch   22/175   train_loss = 1.085\n",
      "Epoch 150 Batch   54/175   train_loss = 1.191\n",
      "Epoch 150 Batch   86/175   train_loss = 1.204\n",
      "Epoch 150 Batch  118/175   train_loss = 1.182\n",
      "Epoch 150 Batch  150/175   train_loss = 1.167\n",
      "Epoch 151 Batch    7/175   train_loss = 1.193\n",
      "Epoch 151 Batch   39/175   train_loss = 1.125\n",
      "Epoch 151 Batch   71/175   train_loss = 1.167\n",
      "Epoch 151 Batch  103/175   train_loss = 1.149\n",
      "Epoch 151 Batch  135/175   train_loss = 1.118\n",
      "Epoch 151 Batch  167/175   train_loss = 1.199\n",
      "Epoch 152 Batch   24/175   train_loss = 1.091\n",
      "Epoch 152 Batch   56/175   train_loss = 1.165\n",
      "Epoch 152 Batch   88/175   train_loss = 1.183\n",
      "Epoch 152 Batch  120/175   train_loss = 1.134\n",
      "Epoch 152 Batch  152/175   train_loss = 1.142\n",
      "Epoch 153 Batch    9/175   train_loss = 1.193\n",
      "Epoch 153 Batch   41/175   train_loss = 1.174\n",
      "Epoch 153 Batch   73/175   train_loss = 1.169\n",
      "Epoch 153 Batch  105/175   train_loss = 1.204\n",
      "Epoch 153 Batch  137/175   train_loss = 1.127\n",
      "Epoch 153 Batch  169/175   train_loss = 1.160\n",
      "Epoch 154 Batch   26/175   train_loss = 1.158\n",
      "Epoch 154 Batch   58/175   train_loss = 1.185\n",
      "Epoch 154 Batch   90/175   train_loss = 1.166\n",
      "Epoch 154 Batch  122/175   train_loss = 1.132\n",
      "Epoch 154 Batch  154/175   train_loss = 1.142\n",
      "Epoch 155 Batch   11/175   train_loss = 1.141\n",
      "Epoch 155 Batch   43/175   train_loss = 1.138\n",
      "Epoch 155 Batch   75/175   train_loss = 1.122\n",
      "Epoch 155 Batch  107/175   train_loss = 1.197\n",
      "Epoch 155 Batch  139/175   train_loss = 1.115\n",
      "Epoch 155 Batch  171/175   train_loss = 1.197\n",
      "Epoch 156 Batch   28/175   train_loss = 1.124\n",
      "Epoch 156 Batch   60/175   train_loss = 1.155\n",
      "Epoch 156 Batch   92/175   train_loss = 1.132\n",
      "Epoch 156 Batch  124/175   train_loss = 1.159\n",
      "Epoch 156 Batch  156/175   train_loss = 1.184\n",
      "Epoch 157 Batch   13/175   train_loss = 1.146\n",
      "Epoch 157 Batch   45/175   train_loss = 1.149\n",
      "Epoch 157 Batch   77/175   train_loss = 1.131\n",
      "Epoch 157 Batch  109/175   train_loss = 1.187\n",
      "Epoch 157 Batch  141/175   train_loss = 1.120\n",
      "Epoch 157 Batch  173/175   train_loss = 1.145\n",
      "Epoch 158 Batch   30/175   train_loss = 1.201\n",
      "Epoch 158 Batch   62/175   train_loss = 1.177\n",
      "Epoch 158 Batch   94/175   train_loss = 1.131\n",
      "Epoch 158 Batch  126/175   train_loss = 1.160\n",
      "Epoch 158 Batch  158/175   train_loss = 1.168\n",
      "Epoch 159 Batch   15/175   train_loss = 1.193\n",
      "Epoch 159 Batch   47/175   train_loss = 1.158\n",
      "Epoch 159 Batch   79/175   train_loss = 1.189\n",
      "Epoch 159 Batch  111/175   train_loss = 1.210\n",
      "Epoch 159 Batch  143/175   train_loss = 1.133\n",
      "Epoch 160 Batch    0/175   train_loss = 1.163\n",
      "Epoch 160 Batch   32/175   train_loss = 1.175\n",
      "Epoch 160 Batch   64/175   train_loss = 1.165\n",
      "Epoch 160 Batch   96/175   train_loss = 1.162\n",
      "Epoch 160 Batch  128/175   train_loss = 1.108\n",
      "Epoch 160 Batch  160/175   train_loss = 1.158\n",
      "Epoch 161 Batch   17/175   train_loss = 1.118\n",
      "Epoch 161 Batch   49/175   train_loss = 1.162\n",
      "Epoch 161 Batch   81/175   train_loss = 1.132\n",
      "Epoch 161 Batch  113/175   train_loss = 1.155\n",
      "Epoch 161 Batch  145/175   train_loss = 1.114\n",
      "Epoch 162 Batch    2/175   train_loss = 1.160\n",
      "Epoch 162 Batch   34/175   train_loss = 1.159\n",
      "Epoch 162 Batch   66/175   train_loss = 1.184\n",
      "Epoch 162 Batch   98/175   train_loss = 1.190\n",
      "Epoch 162 Batch  130/175   train_loss = 1.174\n",
      "Epoch 162 Batch  162/175   train_loss = 1.159\n",
      "Epoch 163 Batch   19/175   train_loss = 1.138\n",
      "Epoch 163 Batch   51/175   train_loss = 1.097\n",
      "Epoch 163 Batch   83/175   train_loss = 1.216\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 163 Batch  115/175   train_loss = 1.239\n",
      "Epoch 163 Batch  147/175   train_loss = 1.120\n",
      "Epoch 164 Batch    4/175   train_loss = 1.165\n",
      "Epoch 164 Batch   36/175   train_loss = 1.116\n",
      "Epoch 164 Batch   68/175   train_loss = 1.160\n",
      "Epoch 164 Batch  100/175   train_loss = 1.160\n",
      "Epoch 164 Batch  132/175   train_loss = 1.127\n",
      "Epoch 164 Batch  164/175   train_loss = 1.123\n",
      "Epoch 165 Batch   21/175   train_loss = 1.110\n",
      "Epoch 165 Batch   53/175   train_loss = 1.116\n",
      "Epoch 165 Batch   85/175   train_loss = 1.161\n",
      "Epoch 165 Batch  117/175   train_loss = 1.173\n",
      "Epoch 165 Batch  149/175   train_loss = 1.165\n",
      "Epoch 166 Batch    6/175   train_loss = 1.173\n",
      "Epoch 166 Batch   38/175   train_loss = 1.111\n",
      "Epoch 166 Batch   70/175   train_loss = 1.155\n",
      "Epoch 166 Batch  102/175   train_loss = 1.156\n",
      "Epoch 166 Batch  134/175   train_loss = 1.124\n",
      "Epoch 166 Batch  166/175   train_loss = 1.163\n",
      "Epoch 167 Batch   23/175   train_loss = 1.114\n",
      "Epoch 167 Batch   55/175   train_loss = 1.207\n",
      "Epoch 167 Batch   87/175   train_loss = 1.217\n",
      "Epoch 167 Batch  119/175   train_loss = 1.137\n",
      "Epoch 167 Batch  151/175   train_loss = 1.159\n",
      "Epoch 168 Batch    8/175   train_loss = 1.154\n",
      "Epoch 168 Batch   40/175   train_loss = 1.158\n",
      "Epoch 168 Batch   72/175   train_loss = 1.169\n",
      "Epoch 168 Batch  104/175   train_loss = 1.167\n",
      "Epoch 168 Batch  136/175   train_loss = 1.130\n",
      "Epoch 168 Batch  168/175   train_loss = 1.183\n",
      "Epoch 169 Batch   25/175   train_loss = 1.165\n",
      "Epoch 169 Batch   57/175   train_loss = 1.197\n",
      "Epoch 169 Batch   89/175   train_loss = 1.146\n",
      "Epoch 169 Batch  121/175   train_loss = 1.125\n",
      "Epoch 169 Batch  153/175   train_loss = 1.161\n",
      "Epoch 170 Batch   10/175   train_loss = 1.096\n",
      "Epoch 170 Batch   42/175   train_loss = 1.212\n",
      "Epoch 170 Batch   74/175   train_loss = 1.197\n",
      "Epoch 170 Batch  106/175   train_loss = 1.182\n",
      "Epoch 170 Batch  138/175   train_loss = 1.146\n",
      "Epoch 170 Batch  170/175   train_loss = 1.189\n",
      "Epoch 171 Batch   27/175   train_loss = 1.134\n",
      "Epoch 171 Batch   59/175   train_loss = 1.128\n",
      "Epoch 171 Batch   91/175   train_loss = 1.141\n",
      "Epoch 171 Batch  123/175   train_loss = 1.155\n",
      "Epoch 171 Batch  155/175   train_loss = 1.119\n",
      "Epoch 172 Batch   12/175   train_loss = 1.150\n",
      "Epoch 172 Batch   44/175   train_loss = 1.137\n",
      "Epoch 172 Batch   76/175   train_loss = 1.138\n",
      "Epoch 172 Batch  108/175   train_loss = 1.162\n",
      "Epoch 172 Batch  140/175   train_loss = 1.143\n",
      "Epoch 172 Batch  172/175   train_loss = 1.188\n",
      "Epoch 173 Batch   29/175   train_loss = 1.146\n",
      "Epoch 173 Batch   61/175   train_loss = 1.147\n",
      "Epoch 173 Batch   93/175   train_loss = 1.142\n",
      "Epoch 173 Batch  125/175   train_loss = 1.171\n",
      "Epoch 173 Batch  157/175   train_loss = 1.132\n",
      "Epoch 174 Batch   14/175   train_loss = 1.167\n",
      "Epoch 174 Batch   46/175   train_loss = 1.156\n",
      "Epoch 174 Batch   78/175   train_loss = 1.119\n",
      "Epoch 174 Batch  110/175   train_loss = 1.226\n",
      "Epoch 174 Batch  142/175   train_loss = 1.180\n",
      "Epoch 174 Batch  174/175   train_loss = 1.151\n",
      "Epoch 175 Batch   31/175   train_loss = 1.159\n",
      "Epoch 175 Batch   63/175   train_loss = 1.155\n",
      "Epoch 175 Batch   95/175   train_loss = 1.120\n",
      "Epoch 175 Batch  127/175   train_loss = 1.119\n",
      "Epoch 175 Batch  159/175   train_loss = 1.135\n",
      "Epoch 176 Batch   16/175   train_loss = 1.163\n",
      "Epoch 176 Batch   48/175   train_loss = 1.160\n",
      "Epoch 176 Batch   80/175   train_loss = 1.160\n",
      "Epoch 176 Batch  112/175   train_loss = 1.142\n",
      "Epoch 176 Batch  144/175   train_loss = 1.099\n",
      "Epoch 177 Batch    1/175   train_loss = 1.193\n",
      "Epoch 177 Batch   33/175   train_loss = 1.189\n",
      "Epoch 177 Batch   65/175   train_loss = 1.135\n",
      "Epoch 177 Batch   97/175   train_loss = 1.139\n",
      "Epoch 177 Batch  129/175   train_loss = 1.130\n",
      "Epoch 177 Batch  161/175   train_loss = 1.126\n",
      "Epoch 178 Batch   18/175   train_loss = 1.128\n",
      "Epoch 178 Batch   50/175   train_loss = 1.139\n",
      "Epoch 178 Batch   82/175   train_loss = 1.201\n",
      "Epoch 178 Batch  114/175   train_loss = 1.190\n",
      "Epoch 178 Batch  146/175   train_loss = 1.141\n",
      "Epoch 179 Batch    3/175   train_loss = 1.182\n",
      "Epoch 179 Batch   35/175   train_loss = 1.135\n",
      "Epoch 179 Batch   67/175   train_loss = 1.125\n",
      "Epoch 179 Batch   99/175   train_loss = 1.194\n",
      "Epoch 179 Batch  131/175   train_loss = 1.150\n",
      "Epoch 179 Batch  163/175   train_loss = 1.155\n",
      "Epoch 180 Batch   20/175   train_loss = 1.161\n",
      "Epoch 180 Batch   52/175   train_loss = 1.105\n",
      "Epoch 180 Batch   84/175   train_loss = 1.152\n",
      "Epoch 180 Batch  116/175   train_loss = 1.196\n",
      "Epoch 180 Batch  148/175   train_loss = 1.154\n",
      "Epoch 181 Batch    5/175   train_loss = 1.162\n",
      "Epoch 181 Batch   37/175   train_loss = 1.131\n",
      "Epoch 181 Batch   69/175   train_loss = 1.170\n",
      "Epoch 181 Batch  101/175   train_loss = 1.188\n",
      "Epoch 181 Batch  133/175   train_loss = 1.070\n",
      "Epoch 181 Batch  165/175   train_loss = 1.179\n",
      "Epoch 182 Batch   22/175   train_loss = 1.102\n",
      "Epoch 182 Batch   54/175   train_loss = 1.190\n",
      "Epoch 182 Batch   86/175   train_loss = 1.207\n",
      "Epoch 182 Batch  118/175   train_loss = 1.195\n",
      "Epoch 182 Batch  150/175   train_loss = 1.160\n",
      "Epoch 183 Batch    7/175   train_loss = 1.191\n",
      "Epoch 183 Batch   39/175   train_loss = 1.108\n",
      "Epoch 183 Batch   71/175   train_loss = 1.174\n",
      "Epoch 183 Batch  103/175   train_loss = 1.153\n",
      "Epoch 183 Batch  135/175   train_loss = 1.108\n",
      "Epoch 183 Batch  167/175   train_loss = 1.202\n",
      "Epoch 184 Batch   24/175   train_loss = 1.101\n",
      "Epoch 184 Batch   56/175   train_loss = 1.183\n",
      "Epoch 184 Batch   88/175   train_loss = 1.183\n",
      "Epoch 184 Batch  120/175   train_loss = 1.161\n",
      "Epoch 184 Batch  152/175   train_loss = 1.125\n",
      "Epoch 185 Batch    9/175   train_loss = 1.181\n",
      "Epoch 185 Batch   41/175   train_loss = 1.184\n",
      "Epoch 185 Batch   73/175   train_loss = 1.158\n",
      "Epoch 185 Batch  105/175   train_loss = 1.207\n",
      "Epoch 185 Batch  137/175   train_loss = 1.133\n",
      "Epoch 185 Batch  169/175   train_loss = 1.169\n",
      "Epoch 186 Batch   26/175   train_loss = 1.150\n",
      "Epoch 186 Batch   58/175   train_loss = 1.165\n",
      "Epoch 186 Batch   90/175   train_loss = 1.169\n",
      "Epoch 186 Batch  122/175   train_loss = 1.125\n",
      "Epoch 186 Batch  154/175   train_loss = 1.124\n",
      "Epoch 187 Batch   11/175   train_loss = 1.144\n",
      "Epoch 187 Batch   43/175   train_loss = 1.126\n",
      "Epoch 187 Batch   75/175   train_loss = 1.119\n",
      "Epoch 187 Batch  107/175   train_loss = 1.189\n",
      "Epoch 187 Batch  139/175   train_loss = 1.125\n",
      "Epoch 187 Batch  171/175   train_loss = 1.193\n",
      "Epoch 188 Batch   28/175   train_loss = 1.132\n",
      "Epoch 188 Batch   60/175   train_loss = 1.157\n",
      "Epoch 188 Batch   92/175   train_loss = 1.126\n",
      "Epoch 188 Batch  124/175   train_loss = 1.154\n",
      "Epoch 188 Batch  156/175   train_loss = 1.179\n",
      "Epoch 189 Batch   13/175   train_loss = 1.149\n",
      "Epoch 189 Batch   45/175   train_loss = 1.144\n",
      "Epoch 189 Batch   77/175   train_loss = 1.127\n",
      "Epoch 189 Batch  109/175   train_loss = 1.194\n",
      "Epoch 189 Batch  141/175   train_loss = 1.120\n",
      "Epoch 189 Batch  173/175   train_loss = 1.125\n",
      "Epoch 190 Batch   30/175   train_loss = 1.189\n",
      "Epoch 190 Batch   62/175   train_loss = 1.184\n",
      "Epoch 190 Batch   94/175   train_loss = 1.131\n",
      "Epoch 190 Batch  126/175   train_loss = 1.178\n",
      "Epoch 190 Batch  158/175   train_loss = 1.160\n",
      "Epoch 191 Batch   15/175   train_loss = 1.188\n",
      "Epoch 191 Batch   47/175   train_loss = 1.169\n",
      "Epoch 191 Batch   79/175   train_loss = 1.190\n",
      "Epoch 191 Batch  111/175   train_loss = 1.214\n",
      "Epoch 191 Batch  143/175   train_loss = 1.112\n",
      "Epoch 192 Batch    0/175   train_loss = 1.160\n",
      "Epoch 192 Batch   32/175   train_loss = 1.171\n",
      "Epoch 192 Batch   64/175   train_loss = 1.173\n",
      "Epoch 192 Batch   96/175   train_loss = 1.169\n",
      "Epoch 192 Batch  128/175   train_loss = 1.115\n",
      "Epoch 192 Batch  160/175   train_loss = 1.148\n",
      "Epoch 193 Batch   17/175   train_loss = 1.129\n",
      "Epoch 193 Batch   49/175   train_loss = 1.171\n",
      "Epoch 193 Batch   81/175   train_loss = 1.138\n",
      "Epoch 193 Batch  113/175   train_loss = 1.173\n",
      "Epoch 193 Batch  145/175   train_loss = 1.129\n",
      "Epoch 194 Batch    2/175   train_loss = 1.161\n",
      "Epoch 194 Batch   34/175   train_loss = 1.168\n",
      "Epoch 194 Batch   66/175   train_loss = 1.179\n",
      "Epoch 194 Batch   98/175   train_loss = 1.167\n",
      "Epoch 194 Batch  130/175   train_loss = 1.174\n",
      "Epoch 194 Batch  162/175   train_loss = 1.157\n",
      "Epoch 195 Batch   19/175   train_loss = 1.144\n",
      "Epoch 195 Batch   51/175   train_loss = 1.085\n",
      "Epoch 195 Batch   83/175   train_loss = 1.221\n",
      "Epoch 195 Batch  115/175   train_loss = 1.254\n",
      "Epoch 195 Batch  147/175   train_loss = 1.119\n",
      "Epoch 196 Batch    4/175   train_loss = 1.156\n",
      "Epoch 196 Batch   36/175   train_loss = 1.112\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 196 Batch   68/175   train_loss = 1.154\n",
      "Epoch 196 Batch  100/175   train_loss = 1.150\n",
      "Epoch 196 Batch  132/175   train_loss = 1.114\n",
      "Epoch 196 Batch  164/175   train_loss = 1.126\n",
      "Epoch 197 Batch   21/175   train_loss = 1.110\n",
      "Epoch 197 Batch   53/175   train_loss = 1.124\n",
      "Epoch 197 Batch   85/175   train_loss = 1.174\n",
      "Epoch 197 Batch  117/175   train_loss = 1.158\n",
      "Epoch 197 Batch  149/175   train_loss = 1.168\n",
      "Epoch 198 Batch    6/175   train_loss = 1.164\n",
      "Epoch 198 Batch   38/175   train_loss = 1.103\n",
      "Epoch 198 Batch   70/175   train_loss = 1.139\n",
      "Epoch 198 Batch  102/175   train_loss = 1.160\n",
      "Epoch 198 Batch  134/175   train_loss = 1.122\n",
      "Epoch 198 Batch  166/175   train_loss = 1.165\n",
      "Epoch 199 Batch   23/175   train_loss = 1.118\n",
      "Epoch 199 Batch   55/175   train_loss = 1.193\n",
      "Epoch 199 Batch   87/175   train_loss = 1.227\n",
      "Epoch 199 Batch  119/175   train_loss = 1.131\n",
      "Epoch 199 Batch  151/175   train_loss = 1.164\n",
      "Epoch 200 Batch    8/175   train_loss = 1.158\n",
      "Epoch 200 Batch   40/175   train_loss = 1.137\n",
      "Epoch 200 Batch   72/175   train_loss = 1.160\n",
      "Epoch 200 Batch  104/175   train_loss = 1.168\n",
      "Epoch 200 Batch  136/175   train_loss = 1.143\n",
      "Epoch 200 Batch  168/175   train_loss = 1.189\n",
      "Epoch 201 Batch   25/175   train_loss = 1.155\n",
      "Epoch 201 Batch   57/175   train_loss = 1.195\n",
      "Epoch 201 Batch   89/175   train_loss = 1.159\n",
      "Epoch 201 Batch  121/175   train_loss = 1.118\n",
      "Epoch 201 Batch  153/175   train_loss = 1.161\n",
      "Epoch 202 Batch   10/175   train_loss = 1.109\n",
      "Epoch 202 Batch   42/175   train_loss = 1.204\n",
      "Epoch 202 Batch   74/175   train_loss = 1.194\n",
      "Epoch 202 Batch  106/175   train_loss = 1.177\n",
      "Epoch 202 Batch  138/175   train_loss = 1.139\n",
      "Epoch 202 Batch  170/175   train_loss = 1.198\n",
      "Epoch 203 Batch   27/175   train_loss = 1.148\n",
      "Epoch 203 Batch   59/175   train_loss = 1.126\n",
      "Epoch 203 Batch   91/175   train_loss = 1.135\n",
      "Epoch 203 Batch  123/175   train_loss = 1.132\n",
      "Epoch 203 Batch  155/175   train_loss = 1.114\n",
      "Epoch 204 Batch   12/175   train_loss = 1.137\n",
      "Epoch 204 Batch   44/175   train_loss = 1.138\n",
      "Epoch 204 Batch   76/175   train_loss = 1.127\n",
      "Epoch 204 Batch  108/175   train_loss = 1.138\n",
      "Epoch 204 Batch  140/175   train_loss = 1.139\n",
      "Epoch 204 Batch  172/175   train_loss = 1.183\n",
      "Epoch 205 Batch   29/175   train_loss = 1.150\n",
      "Epoch 205 Batch   61/175   train_loss = 1.156\n",
      "Epoch 205 Batch   93/175   train_loss = 1.152\n",
      "Epoch 205 Batch  125/175   train_loss = 1.166\n",
      "Epoch 205 Batch  157/175   train_loss = 1.132\n",
      "Epoch 206 Batch   14/175   train_loss = 1.176\n",
      "Epoch 206 Batch   46/175   train_loss = 1.162\n",
      "Epoch 206 Batch   78/175   train_loss = 1.137\n",
      "Epoch 206 Batch  110/175   train_loss = 1.219\n",
      "Epoch 206 Batch  142/175   train_loss = 1.189\n",
      "Epoch 206 Batch  174/175   train_loss = 1.147\n",
      "Epoch 207 Batch   31/175   train_loss = 1.169\n",
      "Epoch 207 Batch   63/175   train_loss = 1.164\n",
      "Epoch 207 Batch   95/175   train_loss = 1.122\n",
      "Epoch 207 Batch  127/175   train_loss = 1.110\n",
      "Epoch 207 Batch  159/175   train_loss = 1.129\n",
      "Epoch 208 Batch   16/175   train_loss = 1.144\n",
      "Epoch 208 Batch   48/175   train_loss = 1.144\n",
      "Epoch 208 Batch   80/175   train_loss = 1.161\n",
      "Epoch 208 Batch  112/175   train_loss = 1.160\n",
      "Epoch 208 Batch  144/175   train_loss = 1.079\n",
      "Epoch 209 Batch    1/175   train_loss = 1.180\n",
      "Epoch 209 Batch   33/175   train_loss = 1.187\n",
      "Epoch 209 Batch   65/175   train_loss = 1.147\n",
      "Epoch 209 Batch   97/175   train_loss = 1.141\n",
      "Epoch 209 Batch  129/175   train_loss = 1.118\n",
      "Epoch 209 Batch  161/175   train_loss = 1.138\n",
      "Epoch 210 Batch   18/175   train_loss = 1.130\n",
      "Epoch 210 Batch   50/175   train_loss = 1.122\n",
      "Epoch 210 Batch   82/175   train_loss = 1.197\n",
      "Epoch 210 Batch  114/175   train_loss = 1.201\n",
      "Epoch 210 Batch  146/175   train_loss = 1.132\n",
      "Epoch 211 Batch    3/175   train_loss = 1.181\n",
      "Epoch 211 Batch   35/175   train_loss = 1.136\n",
      "Epoch 211 Batch   67/175   train_loss = 1.103\n",
      "Epoch 211 Batch   99/175   train_loss = 1.193\n",
      "Epoch 211 Batch  131/175   train_loss = 1.156\n",
      "Epoch 211 Batch  163/175   train_loss = 1.176\n",
      "Epoch 212 Batch   20/175   train_loss = 1.136\n",
      "Epoch 212 Batch   52/175   train_loss = 1.092\n",
      "Epoch 212 Batch   84/175   train_loss = 1.159\n",
      "Epoch 212 Batch  116/175   train_loss = 1.209\n",
      "Epoch 212 Batch  148/175   train_loss = 1.152\n",
      "Epoch 213 Batch    5/175   train_loss = 1.155\n",
      "Epoch 213 Batch   37/175   train_loss = 1.143\n",
      "Epoch 213 Batch   69/175   train_loss = 1.180\n",
      "Epoch 213 Batch  101/175   train_loss = 1.185\n",
      "Epoch 213 Batch  133/175   train_loss = 1.066\n",
      "Epoch 213 Batch  165/175   train_loss = 1.152\n",
      "Epoch 214 Batch   22/175   train_loss = 1.104\n",
      "Epoch 214 Batch   54/175   train_loss = 1.193\n",
      "Epoch 214 Batch   86/175   train_loss = 1.196\n",
      "Epoch 214 Batch  118/175   train_loss = 1.194\n",
      "Epoch 214 Batch  150/175   train_loss = 1.163\n",
      "Epoch 215 Batch    7/175   train_loss = 1.180\n",
      "Epoch 215 Batch   39/175   train_loss = 1.115\n",
      "Epoch 215 Batch   71/175   train_loss = 1.187\n",
      "Epoch 215 Batch  103/175   train_loss = 1.146\n",
      "Epoch 215 Batch  135/175   train_loss = 1.120\n",
      "Epoch 215 Batch  167/175   train_loss = 1.198\n",
      "Epoch 216 Batch   24/175   train_loss = 1.093\n",
      "Epoch 216 Batch   56/175   train_loss = 1.176\n",
      "Epoch 216 Batch   88/175   train_loss = 1.175\n",
      "Epoch 216 Batch  120/175   train_loss = 1.132\n",
      "Epoch 216 Batch  152/175   train_loss = 1.135\n",
      "Epoch 217 Batch    9/175   train_loss = 1.183\n",
      "Epoch 217 Batch   41/175   train_loss = 1.177\n",
      "Epoch 217 Batch   73/175   train_loss = 1.164\n",
      "Epoch 217 Batch  105/175   train_loss = 1.192\n",
      "Epoch 217 Batch  137/175   train_loss = 1.125\n",
      "Epoch 217 Batch  169/175   train_loss = 1.159\n",
      "Epoch 218 Batch   26/175   train_loss = 1.148\n",
      "Epoch 218 Batch   58/175   train_loss = 1.177\n",
      "Epoch 218 Batch   90/175   train_loss = 1.174\n",
      "Epoch 218 Batch  122/175   train_loss = 1.125\n",
      "Epoch 218 Batch  154/175   train_loss = 1.125\n",
      "Epoch 219 Batch   11/175   train_loss = 1.122\n",
      "Epoch 219 Batch   43/175   train_loss = 1.135\n",
      "Epoch 219 Batch   75/175   train_loss = 1.132\n",
      "Epoch 219 Batch  107/175   train_loss = 1.174\n",
      "Epoch 219 Batch  139/175   train_loss = 1.104\n",
      "Epoch 219 Batch  171/175   train_loss = 1.203\n",
      "Epoch 220 Batch   28/175   train_loss = 1.126\n",
      "Epoch 220 Batch   60/175   train_loss = 1.148\n",
      "Epoch 220 Batch   92/175   train_loss = 1.132\n",
      "Epoch 220 Batch  124/175   train_loss = 1.155\n",
      "Epoch 220 Batch  156/175   train_loss = 1.164\n",
      "Epoch 221 Batch   13/175   train_loss = 1.157\n",
      "Epoch 221 Batch   45/175   train_loss = 1.132\n",
      "Epoch 221 Batch   77/175   train_loss = 1.145\n",
      "Epoch 221 Batch  109/175   train_loss = 1.177\n",
      "Epoch 221 Batch  141/175   train_loss = 1.123\n",
      "Epoch 221 Batch  173/175   train_loss = 1.145\n",
      "Epoch 222 Batch   30/175   train_loss = 1.197\n",
      "Epoch 222 Batch   62/175   train_loss = 1.185\n",
      "Epoch 222 Batch   94/175   train_loss = 1.140\n",
      "Epoch 222 Batch  126/175   train_loss = 1.160\n",
      "Epoch 222 Batch  158/175   train_loss = 1.149\n",
      "Epoch 223 Batch   15/175   train_loss = 1.193\n",
      "Epoch 223 Batch   47/175   train_loss = 1.172\n",
      "Epoch 223 Batch   79/175   train_loss = 1.185\n",
      "Epoch 223 Batch  111/175   train_loss = 1.199\n",
      "Epoch 223 Batch  143/175   train_loss = 1.133\n",
      "Epoch 224 Batch    0/175   train_loss = 1.163\n",
      "Epoch 224 Batch   32/175   train_loss = 1.161\n",
      "Epoch 224 Batch   64/175   train_loss = 1.161\n",
      "Epoch 224 Batch   96/175   train_loss = 1.175\n",
      "Epoch 224 Batch  128/175   train_loss = 1.111\n",
      "Epoch 224 Batch  160/175   train_loss = 1.145\n",
      "Epoch 225 Batch   17/175   train_loss = 1.124\n",
      "Epoch 225 Batch   49/175   train_loss = 1.157\n",
      "Epoch 225 Batch   81/175   train_loss = 1.124\n",
      "Epoch 225 Batch  113/175   train_loss = 1.157\n",
      "Epoch 225 Batch  145/175   train_loss = 1.121\n",
      "Epoch 226 Batch    2/175   train_loss = 1.150\n",
      "Epoch 226 Batch   34/175   train_loss = 1.174\n",
      "Epoch 226 Batch   66/175   train_loss = 1.188\n",
      "Epoch 226 Batch   98/175   train_loss = 1.181\n",
      "Epoch 226 Batch  130/175   train_loss = 1.179\n",
      "Epoch 226 Batch  162/175   train_loss = 1.148\n",
      "Epoch 227 Batch   19/175   train_loss = 1.147\n",
      "Epoch 227 Batch   51/175   train_loss = 1.098\n",
      "Epoch 227 Batch   83/175   train_loss = 1.199\n",
      "Epoch 227 Batch  115/175   train_loss = 1.242\n",
      "Epoch 227 Batch  147/175   train_loss = 1.128\n",
      "Epoch 228 Batch    4/175   train_loss = 1.157\n",
      "Epoch 228 Batch   36/175   train_loss = 1.117\n",
      "Epoch 228 Batch   68/175   train_loss = 1.137\n",
      "Epoch 228 Batch  100/175   train_loss = 1.150\n",
      "Epoch 228 Batch  132/175   train_loss = 1.118\n",
      "Epoch 228 Batch  164/175   train_loss = 1.128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 229 Batch   21/175   train_loss = 1.106\n",
      "Epoch 229 Batch   53/175   train_loss = 1.116\n",
      "Epoch 229 Batch   85/175   train_loss = 1.163\n",
      "Epoch 229 Batch  117/175   train_loss = 1.151\n",
      "Epoch 229 Batch  149/175   train_loss = 1.150\n",
      "Epoch 230 Batch    6/175   train_loss = 1.159\n",
      "Epoch 230 Batch   38/175   train_loss = 1.108\n",
      "Epoch 230 Batch   70/175   train_loss = 1.142\n",
      "Epoch 230 Batch  102/175   train_loss = 1.156\n",
      "Epoch 230 Batch  134/175   train_loss = 1.119\n",
      "Epoch 230 Batch  166/175   train_loss = 1.163\n",
      "Epoch 231 Batch   23/175   train_loss = 1.093\n",
      "Epoch 231 Batch   55/175   train_loss = 1.197\n",
      "Epoch 231 Batch   87/175   train_loss = 1.214\n",
      "Epoch 231 Batch  119/175   train_loss = 1.128\n",
      "Epoch 231 Batch  151/175   train_loss = 1.155\n",
      "Epoch 232 Batch    8/175   train_loss = 1.159\n",
      "Epoch 232 Batch   40/175   train_loss = 1.140\n",
      "Epoch 232 Batch   72/175   train_loss = 1.167\n",
      "Epoch 232 Batch  104/175   train_loss = 1.155\n",
      "Epoch 232 Batch  136/175   train_loss = 1.124\n",
      "Epoch 232 Batch  168/175   train_loss = 1.193\n",
      "Epoch 233 Batch   25/175   train_loss = 1.147\n",
      "Epoch 233 Batch   57/175   train_loss = 1.179\n",
      "Epoch 233 Batch   89/175   train_loss = 1.147\n",
      "Epoch 233 Batch  121/175   train_loss = 1.128\n",
      "Epoch 233 Batch  153/175   train_loss = 1.148\n",
      "Epoch 234 Batch   10/175   train_loss = 1.104\n",
      "Epoch 234 Batch   42/175   train_loss = 1.201\n",
      "Epoch 234 Batch   74/175   train_loss = 1.190\n",
      "Epoch 234 Batch  106/175   train_loss = 1.175\n",
      "Epoch 234 Batch  138/175   train_loss = 1.154\n",
      "Epoch 234 Batch  170/175   train_loss = 1.187\n",
      "Epoch 235 Batch   27/175   train_loss = 1.133\n",
      "Epoch 235 Batch   59/175   train_loss = 1.128\n",
      "Epoch 235 Batch   91/175   train_loss = 1.134\n",
      "Epoch 235 Batch  123/175   train_loss = 1.142\n",
      "Epoch 235 Batch  155/175   train_loss = 1.105\n",
      "Epoch 236 Batch   12/175   train_loss = 1.145\n",
      "Epoch 236 Batch   44/175   train_loss = 1.133\n",
      "Epoch 236 Batch   76/175   train_loss = 1.132\n",
      "Epoch 236 Batch  108/175   train_loss = 1.152\n",
      "Epoch 236 Batch  140/175   train_loss = 1.146\n",
      "Epoch 236 Batch  172/175   train_loss = 1.190\n",
      "Epoch 237 Batch   29/175   train_loss = 1.142\n",
      "Epoch 237 Batch   61/175   train_loss = 1.142\n",
      "Epoch 237 Batch   93/175   train_loss = 1.152\n",
      "Epoch 237 Batch  125/175   train_loss = 1.156\n",
      "Epoch 237 Batch  157/175   train_loss = 1.137\n",
      "Epoch 238 Batch   14/175   train_loss = 1.165\n",
      "Epoch 238 Batch   46/175   train_loss = 1.151\n",
      "Epoch 238 Batch   78/175   train_loss = 1.126\n",
      "Epoch 238 Batch  110/175   train_loss = 1.217\n",
      "Epoch 238 Batch  142/175   train_loss = 1.167\n",
      "Epoch 238 Batch  174/175   train_loss = 1.139\n",
      "Epoch 239 Batch   31/175   train_loss = 1.157\n",
      "Epoch 239 Batch   63/175   train_loss = 1.164\n",
      "Epoch 239 Batch   95/175   train_loss = 1.123\n",
      "Epoch 239 Batch  127/175   train_loss = 1.097\n",
      "Epoch 239 Batch  159/175   train_loss = 1.112\n",
      "Epoch 240 Batch   16/175   train_loss = 1.150\n",
      "Epoch 240 Batch   48/175   train_loss = 1.141\n",
      "Epoch 240 Batch   80/175   train_loss = 1.154\n",
      "Epoch 240 Batch  112/175   train_loss = 1.157\n",
      "Epoch 240 Batch  144/175   train_loss = 1.086\n",
      "Epoch 241 Batch    1/175   train_loss = 1.204\n",
      "Epoch 241 Batch   33/175   train_loss = 1.193\n",
      "Epoch 241 Batch   65/175   train_loss = 1.152\n",
      "Epoch 241 Batch   97/175   train_loss = 1.160\n",
      "Epoch 241 Batch  129/175   train_loss = 1.140\n",
      "Epoch 241 Batch  161/175   train_loss = 1.141\n",
      "Epoch 242 Batch   18/175   train_loss = 1.127\n",
      "Epoch 242 Batch   50/175   train_loss = 1.142\n",
      "Epoch 242 Batch   82/175   train_loss = 1.189\n",
      "Epoch 242 Batch  114/175   train_loss = 1.182\n",
      "Epoch 242 Batch  146/175   train_loss = 1.136\n",
      "Epoch 243 Batch    3/175   train_loss = 1.192\n",
      "Epoch 243 Batch   35/175   train_loss = 1.145\n",
      "Epoch 243 Batch   67/175   train_loss = 1.129\n",
      "Epoch 243 Batch   99/175   train_loss = 1.224\n",
      "Epoch 243 Batch  131/175   train_loss = 1.148\n",
      "Epoch 243 Batch  163/175   train_loss = 1.162\n",
      "Epoch 244 Batch   20/175   train_loss = 1.141\n",
      "Epoch 244 Batch   52/175   train_loss = 1.095\n",
      "Epoch 244 Batch   84/175   train_loss = 1.146\n",
      "Epoch 244 Batch  116/175   train_loss = 1.203\n",
      "Epoch 244 Batch  148/175   train_loss = 1.152\n",
      "Epoch 245 Batch    5/175   train_loss = 1.163\n",
      "Epoch 245 Batch   37/175   train_loss = 1.150\n",
      "Epoch 245 Batch   69/175   train_loss = 1.170\n",
      "Epoch 245 Batch  101/175   train_loss = 1.196\n",
      "Epoch 245 Batch  133/175   train_loss = 1.076\n",
      "Epoch 245 Batch  165/175   train_loss = 1.174\n",
      "Epoch 246 Batch   22/175   train_loss = 1.110\n",
      "Epoch 246 Batch   54/175   train_loss = 1.189\n",
      "Epoch 246 Batch   86/175   train_loss = 1.193\n",
      "Epoch 246 Batch  118/175   train_loss = 1.198\n",
      "Epoch 246 Batch  150/175   train_loss = 1.177\n",
      "Epoch 247 Batch    7/175   train_loss = 1.184\n",
      "Epoch 247 Batch   39/175   train_loss = 1.117\n",
      "Epoch 247 Batch   71/175   train_loss = 1.181\n",
      "Epoch 247 Batch  103/175   train_loss = 1.155\n",
      "Epoch 247 Batch  135/175   train_loss = 1.110\n",
      "Epoch 247 Batch  167/175   train_loss = 1.211\n",
      "Epoch 248 Batch   24/175   train_loss = 1.110\n",
      "Epoch 248 Batch   56/175   train_loss = 1.175\n",
      "Epoch 248 Batch   88/175   train_loss = 1.180\n",
      "Epoch 248 Batch  120/175   train_loss = 1.142\n",
      "Epoch 248 Batch  152/175   train_loss = 1.145\n",
      "Epoch 249 Batch    9/175   train_loss = 1.188\n",
      "Epoch 249 Batch   41/175   train_loss = 1.186\n",
      "Epoch 249 Batch   73/175   train_loss = 1.172\n",
      "Epoch 249 Batch  105/175   train_loss = 1.211\n",
      "Epoch 249 Batch  137/175   train_loss = 1.128\n",
      "Epoch 249 Batch  169/175   train_loss = 1.177\n",
      "Epoch 250 Batch   26/175   train_loss = 1.160\n",
      "Epoch 250 Batch   58/175   train_loss = 1.183\n",
      "Epoch 250 Batch   90/175   train_loss = 1.162\n",
      "Epoch 250 Batch  122/175   train_loss = 1.132\n",
      "Epoch 250 Batch  154/175   train_loss = 1.156\n",
      "Epoch 251 Batch   11/175   train_loss = 1.138\n",
      "Epoch 251 Batch   43/175   train_loss = 1.142\n",
      "Epoch 251 Batch   75/175   train_loss = 1.126\n",
      "Epoch 251 Batch  107/175   train_loss = 1.186\n",
      "Epoch 251 Batch  139/175   train_loss = 1.129\n",
      "Epoch 251 Batch  171/175   train_loss = 1.201\n",
      "Epoch 252 Batch   28/175   train_loss = 1.141\n",
      "Epoch 252 Batch   60/175   train_loss = 1.154\n",
      "Epoch 252 Batch   92/175   train_loss = 1.142\n",
      "Epoch 252 Batch  124/175   train_loss = 1.153\n",
      "Epoch 252 Batch  156/175   train_loss = 1.178\n",
      "Epoch 253 Batch   13/175   train_loss = 1.154\n",
      "Epoch 253 Batch   45/175   train_loss = 1.141\n",
      "Epoch 253 Batch   77/175   train_loss = 1.136\n",
      "Epoch 253 Batch  109/175   train_loss = 1.189\n",
      "Epoch 253 Batch  141/175   train_loss = 1.114\n",
      "Epoch 253 Batch  173/175   train_loss = 1.137\n",
      "Epoch 254 Batch   30/175   train_loss = 1.209\n",
      "Epoch 254 Batch   62/175   train_loss = 1.180\n",
      "Epoch 254 Batch   94/175   train_loss = 1.129\n",
      "Epoch 254 Batch  126/175   train_loss = 1.170\n",
      "Epoch 254 Batch  158/175   train_loss = 1.159\n",
      "Epoch 255 Batch   15/175   train_loss = 1.180\n",
      "Epoch 255 Batch   47/175   train_loss = 1.173\n",
      "Epoch 255 Batch   79/175   train_loss = 1.204\n",
      "Epoch 255 Batch  111/175   train_loss = 1.205\n",
      "Epoch 255 Batch  143/175   train_loss = 1.127\n",
      "Epoch 256 Batch    0/175   train_loss = 1.180\n",
      "Epoch 256 Batch   32/175   train_loss = 1.177\n",
      "Epoch 256 Batch   64/175   train_loss = 1.182\n",
      "Epoch 256 Batch   96/175   train_loss = 1.175\n",
      "Epoch 256 Batch  128/175   train_loss = 1.102\n",
      "Epoch 256 Batch  160/175   train_loss = 1.157\n",
      "Epoch 257 Batch   17/175   train_loss = 1.139\n",
      "Epoch 257 Batch   49/175   train_loss = 1.152\n",
      "Epoch 257 Batch   81/175   train_loss = 1.130\n",
      "Epoch 257 Batch  113/175   train_loss = 1.169\n",
      "Epoch 257 Batch  145/175   train_loss = 1.127\n",
      "Epoch 258 Batch    2/175   train_loss = 1.152\n",
      "Epoch 258 Batch   34/175   train_loss = 1.179\n",
      "Epoch 258 Batch   66/175   train_loss = 1.183\n",
      "Epoch 258 Batch   98/175   train_loss = 1.194\n",
      "Epoch 258 Batch  130/175   train_loss = 1.181\n",
      "Epoch 258 Batch  162/175   train_loss = 1.160\n",
      "Epoch 259 Batch   19/175   train_loss = 1.155\n",
      "Epoch 259 Batch   51/175   train_loss = 1.107\n",
      "Epoch 259 Batch   83/175   train_loss = 1.205\n",
      "Epoch 259 Batch  115/175   train_loss = 1.240\n",
      "Epoch 259 Batch  147/175   train_loss = 1.125\n",
      "Epoch 260 Batch    4/175   train_loss = 1.154\n",
      "Epoch 260 Batch   36/175   train_loss = 1.115\n",
      "Epoch 260 Batch   68/175   train_loss = 1.153\n",
      "Epoch 260 Batch  100/175   train_loss = 1.144\n",
      "Epoch 260 Batch  132/175   train_loss = 1.127\n",
      "Epoch 260 Batch  164/175   train_loss = 1.129\n",
      "Epoch 261 Batch   21/175   train_loss = 1.115\n",
      "Epoch 261 Batch   53/175   train_loss = 1.130\n",
      "Epoch 261 Batch   85/175   train_loss = 1.158\n",
      "Epoch 261 Batch  117/175   train_loss = 1.159\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 261 Batch  149/175   train_loss = 1.166\n",
      "Epoch 262 Batch    6/175   train_loss = 1.169\n",
      "Epoch 262 Batch   38/175   train_loss = 1.107\n",
      "Epoch 262 Batch   70/175   train_loss = 1.153\n",
      "Epoch 262 Batch  102/175   train_loss = 1.174\n",
      "Epoch 262 Batch  134/175   train_loss = 1.115\n",
      "Epoch 262 Batch  166/175   train_loss = 1.173\n",
      "Epoch 263 Batch   23/175   train_loss = 1.113\n",
      "Epoch 263 Batch   55/175   train_loss = 1.202\n",
      "Epoch 263 Batch   87/175   train_loss = 1.220\n",
      "Epoch 263 Batch  119/175   train_loss = 1.136\n",
      "Epoch 263 Batch  151/175   train_loss = 1.156\n",
      "Epoch 264 Batch    8/175   train_loss = 1.163\n",
      "Epoch 264 Batch   40/175   train_loss = 1.138\n",
      "Epoch 264 Batch   72/175   train_loss = 1.155\n",
      "Epoch 264 Batch  104/175   train_loss = 1.176\n",
      "Epoch 264 Batch  136/175   train_loss = 1.130\n",
      "Epoch 264 Batch  168/175   train_loss = 1.184\n",
      "Epoch 265 Batch   25/175   train_loss = 1.162\n",
      "Epoch 265 Batch   57/175   train_loss = 1.185\n",
      "Epoch 265 Batch   89/175   train_loss = 1.154\n",
      "Epoch 265 Batch  121/175   train_loss = 1.122\n",
      "Epoch 265 Batch  153/175   train_loss = 1.173\n",
      "Epoch 266 Batch   10/175   train_loss = 1.092\n",
      "Epoch 266 Batch   42/175   train_loss = 1.186\n",
      "Epoch 266 Batch   74/175   train_loss = 1.197\n",
      "Epoch 266 Batch  106/175   train_loss = 1.178\n",
      "Epoch 266 Batch  138/175   train_loss = 1.141\n",
      "Epoch 266 Batch  170/175   train_loss = 1.171\n",
      "Epoch 267 Batch   27/175   train_loss = 1.140\n",
      "Epoch 267 Batch   59/175   train_loss = 1.131\n",
      "Epoch 267 Batch   91/175   train_loss = 1.130\n",
      "Epoch 267 Batch  123/175   train_loss = 1.147\n",
      "Epoch 267 Batch  155/175   train_loss = 1.113\n",
      "Epoch 268 Batch   12/175   train_loss = 1.150\n",
      "Epoch 268 Batch   44/175   train_loss = 1.125\n",
      "Epoch 268 Batch   76/175   train_loss = 1.141\n",
      "Epoch 268 Batch  108/175   train_loss = 1.157\n",
      "Epoch 268 Batch  140/175   train_loss = 1.129\n",
      "Epoch 268 Batch  172/175   train_loss = 1.182\n",
      "Epoch 269 Batch   29/175   train_loss = 1.145\n",
      "Epoch 269 Batch   61/175   train_loss = 1.165\n",
      "Epoch 269 Batch   93/175   train_loss = 1.146\n",
      "Epoch 269 Batch  125/175   train_loss = 1.171\n",
      "Epoch 269 Batch  157/175   train_loss = 1.143\n",
      "Epoch 270 Batch   14/175   train_loss = 1.175\n",
      "Epoch 270 Batch   46/175   train_loss = 1.152\n",
      "Epoch 270 Batch   78/175   train_loss = 1.140\n",
      "Epoch 270 Batch  110/175   train_loss = 1.217\n",
      "Epoch 270 Batch  142/175   train_loss = 1.175\n",
      "Epoch 270 Batch  174/175   train_loss = 1.141\n",
      "Epoch 271 Batch   31/175   train_loss = 1.155\n",
      "Epoch 271 Batch   63/175   train_loss = 1.159\n",
      "Epoch 271 Batch   95/175   train_loss = 1.129\n",
      "Epoch 271 Batch  127/175   train_loss = 1.112\n",
      "Epoch 271 Batch  159/175   train_loss = 1.128\n",
      "Epoch 272 Batch   16/175   train_loss = 1.157\n",
      "Epoch 272 Batch   48/175   train_loss = 1.156\n",
      "Epoch 272 Batch   80/175   train_loss = 1.158\n",
      "Epoch 272 Batch  112/175   train_loss = 1.152\n",
      "Epoch 272 Batch  144/175   train_loss = 1.098\n",
      "Epoch 273 Batch    1/175   train_loss = 1.199\n",
      "Epoch 273 Batch   33/175   train_loss = 1.195\n",
      "Epoch 273 Batch   65/175   train_loss = 1.139\n",
      "Epoch 273 Batch   97/175   train_loss = 1.138\n",
      "Epoch 273 Batch  129/175   train_loss = 1.148\n",
      "Epoch 273 Batch  161/175   train_loss = 1.130\n",
      "Epoch 274 Batch   18/175   train_loss = 1.115\n",
      "Epoch 274 Batch   50/175   train_loss = 1.138\n",
      "Epoch 274 Batch   82/175   train_loss = 1.184\n",
      "Epoch 274 Batch  114/175   train_loss = 1.198\n",
      "Epoch 274 Batch  146/175   train_loss = 1.156\n",
      "Epoch 275 Batch    3/175   train_loss = 1.192\n",
      "Epoch 275 Batch   35/175   train_loss = 1.140\n",
      "Epoch 275 Batch   67/175   train_loss = 1.109\n",
      "Epoch 275 Batch   99/175   train_loss = 1.213\n",
      "Epoch 275 Batch  131/175   train_loss = 1.145\n",
      "Epoch 275 Batch  163/175   train_loss = 1.161\n",
      "Epoch 276 Batch   20/175   train_loss = 1.136\n",
      "Epoch 276 Batch   52/175   train_loss = 1.088\n",
      "Epoch 276 Batch   84/175   train_loss = 1.145\n",
      "Epoch 276 Batch  116/175   train_loss = 1.195\n",
      "Epoch 276 Batch  148/175   train_loss = 1.164\n",
      "Epoch 277 Batch    5/175   train_loss = 1.161\n",
      "Epoch 277 Batch   37/175   train_loss = 1.130\n",
      "Epoch 277 Batch   69/175   train_loss = 1.157\n",
      "Epoch 277 Batch  101/175   train_loss = 1.187\n",
      "Epoch 277 Batch  133/175   train_loss = 1.073\n",
      "Epoch 277 Batch  165/175   train_loss = 1.154\n",
      "Epoch 278 Batch   22/175   train_loss = 1.088\n",
      "Epoch 278 Batch   54/175   train_loss = 1.181\n",
      "Epoch 278 Batch   86/175   train_loss = 1.202\n",
      "Epoch 278 Batch  118/175   train_loss = 1.202\n",
      "Epoch 278 Batch  150/175   train_loss = 1.160\n",
      "Epoch 279 Batch    7/175   train_loss = 1.179\n",
      "Epoch 279 Batch   39/175   train_loss = 1.115\n",
      "Epoch 279 Batch   71/175   train_loss = 1.177\n",
      "Epoch 279 Batch  103/175   train_loss = 1.155\n",
      "Epoch 279 Batch  135/175   train_loss = 1.127\n",
      "Epoch 279 Batch  167/175   train_loss = 1.203\n",
      "Epoch 280 Batch   24/175   train_loss = 1.093\n",
      "Epoch 280 Batch   56/175   train_loss = 1.167\n",
      "Epoch 280 Batch   88/175   train_loss = 1.186\n",
      "Epoch 280 Batch  120/175   train_loss = 1.140\n",
      "Epoch 280 Batch  152/175   train_loss = 1.126\n",
      "Epoch 281 Batch    9/175   train_loss = 1.178\n",
      "Epoch 281 Batch   41/175   train_loss = 1.174\n",
      "Epoch 281 Batch   73/175   train_loss = 1.158\n",
      "Epoch 281 Batch  105/175   train_loss = 1.205\n",
      "Epoch 281 Batch  137/175   train_loss = 1.125\n",
      "Epoch 281 Batch  169/175   train_loss = 1.161\n",
      "Epoch 282 Batch   26/175   train_loss = 1.164\n",
      "Epoch 282 Batch   58/175   train_loss = 1.189\n",
      "Epoch 282 Batch   90/175   train_loss = 1.191\n",
      "Epoch 282 Batch  122/175   train_loss = 1.142\n",
      "Epoch 282 Batch  154/175   train_loss = 1.148\n",
      "Epoch 283 Batch   11/175   train_loss = 1.128\n",
      "Epoch 283 Batch   43/175   train_loss = 1.133\n",
      "Epoch 283 Batch   75/175   train_loss = 1.114\n",
      "Epoch 283 Batch  107/175   train_loss = 1.192\n",
      "Epoch 283 Batch  139/175   train_loss = 1.107\n",
      "Epoch 283 Batch  171/175   train_loss = 1.185\n",
      "Epoch 284 Batch   28/175   train_loss = 1.138\n",
      "Epoch 284 Batch   60/175   train_loss = 1.140\n",
      "Epoch 284 Batch   92/175   train_loss = 1.139\n",
      "Epoch 284 Batch  124/175   train_loss = 1.182\n",
      "Epoch 284 Batch  156/175   train_loss = 1.164\n",
      "Epoch 285 Batch   13/175   train_loss = 1.183\n",
      "Epoch 285 Batch   45/175   train_loss = 1.156\n",
      "Epoch 285 Batch   77/175   train_loss = 1.123\n",
      "Epoch 285 Batch  109/175   train_loss = 1.202\n",
      "Epoch 285 Batch  141/175   train_loss = 1.122\n",
      "Epoch 285 Batch  173/175   train_loss = 1.137\n",
      "Epoch 286 Batch   30/175   train_loss = 1.196\n",
      "Epoch 286 Batch   62/175   train_loss = 1.169\n",
      "Epoch 286 Batch   94/175   train_loss = 1.130\n",
      "Epoch 286 Batch  126/175   train_loss = 1.167\n",
      "Epoch 286 Batch  158/175   train_loss = 1.154\n",
      "Epoch 287 Batch   15/175   train_loss = 1.188\n",
      "Epoch 287 Batch   47/175   train_loss = 1.167\n",
      "Epoch 287 Batch   79/175   train_loss = 1.198\n",
      "Epoch 287 Batch  111/175   train_loss = 1.208\n",
      "Epoch 287 Batch  143/175   train_loss = 1.128\n",
      "Epoch 288 Batch    0/175   train_loss = 1.165\n",
      "Epoch 288 Batch   32/175   train_loss = 1.167\n",
      "Epoch 288 Batch   64/175   train_loss = 1.169\n",
      "Epoch 288 Batch   96/175   train_loss = 1.180\n",
      "Epoch 288 Batch  128/175   train_loss = 1.117\n",
      "Epoch 288 Batch  160/175   train_loss = 1.153\n",
      "Epoch 289 Batch   17/175   train_loss = 1.124\n",
      "Epoch 289 Batch   49/175   train_loss = 1.165\n",
      "Epoch 289 Batch   81/175   train_loss = 1.130\n",
      "Epoch 289 Batch  113/175   train_loss = 1.168\n",
      "Epoch 289 Batch  145/175   train_loss = 1.123\n",
      "Epoch 290 Batch    2/175   train_loss = 1.152\n",
      "Epoch 290 Batch   34/175   train_loss = 1.168\n",
      "Epoch 290 Batch   66/175   train_loss = 1.184\n",
      "Epoch 290 Batch   98/175   train_loss = 1.173\n",
      "Epoch 290 Batch  130/175   train_loss = 1.164\n",
      "Epoch 290 Batch  162/175   train_loss = 1.144\n",
      "Epoch 291 Batch   19/175   train_loss = 1.140\n",
      "Epoch 291 Batch   51/175   train_loss = 1.102\n",
      "Epoch 291 Batch   83/175   train_loss = 1.175\n",
      "Epoch 291 Batch  115/175   train_loss = 1.249\n",
      "Epoch 291 Batch  147/175   train_loss = 1.120\n",
      "Epoch 292 Batch    4/175   train_loss = 1.144\n",
      "Epoch 292 Batch   36/175   train_loss = 1.102\n",
      "Epoch 292 Batch   68/175   train_loss = 1.141\n",
      "Epoch 292 Batch  100/175   train_loss = 1.143\n",
      "Epoch 292 Batch  132/175   train_loss = 1.134\n",
      "Epoch 292 Batch  164/175   train_loss = 1.135\n",
      "Epoch 293 Batch   21/175   train_loss = 1.123\n",
      "Epoch 293 Batch   53/175   train_loss = 1.118\n",
      "Epoch 293 Batch   85/175   train_loss = 1.167\n",
      "Epoch 293 Batch  117/175   train_loss = 1.134\n",
      "Epoch 293 Batch  149/175   train_loss = 1.169\n",
      "Epoch 294 Batch    6/175   train_loss = 1.157\n",
      "Epoch 294 Batch   38/175   train_loss = 1.091\n",
      "Epoch 294 Batch   70/175   train_loss = 1.135\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 294 Batch  102/175   train_loss = 1.152\n",
      "Epoch 294 Batch  134/175   train_loss = 1.112\n",
      "Epoch 294 Batch  166/175   train_loss = 1.163\n",
      "Epoch 295 Batch   23/175   train_loss = 1.105\n",
      "Epoch 295 Batch   55/175   train_loss = 1.182\n",
      "Epoch 295 Batch   87/175   train_loss = 1.216\n",
      "Epoch 295 Batch  119/175   train_loss = 1.136\n",
      "Epoch 295 Batch  151/175   train_loss = 1.140\n",
      "Epoch 296 Batch    8/175   train_loss = 1.143\n",
      "Epoch 296 Batch   40/175   train_loss = 1.130\n",
      "Epoch 296 Batch   72/175   train_loss = 1.149\n",
      "Epoch 296 Batch  104/175   train_loss = 1.155\n",
      "Epoch 296 Batch  136/175   train_loss = 1.127\n",
      "Epoch 296 Batch  168/175   train_loss = 1.184\n",
      "Epoch 297 Batch   25/175   train_loss = 1.158\n",
      "Epoch 297 Batch   57/175   train_loss = 1.175\n",
      "Epoch 297 Batch   89/175   train_loss = 1.155\n",
      "Epoch 297 Batch  121/175   train_loss = 1.121\n",
      "Epoch 297 Batch  153/175   train_loss = 1.168\n",
      "Epoch 298 Batch   10/175   train_loss = 1.100\n",
      "Epoch 298 Batch   42/175   train_loss = 1.176\n",
      "Epoch 298 Batch   74/175   train_loss = 1.188\n",
      "Epoch 298 Batch  106/175   train_loss = 1.174\n",
      "Epoch 298 Batch  138/175   train_loss = 1.155\n",
      "Epoch 298 Batch  170/175   train_loss = 1.177\n",
      "Epoch 299 Batch   27/175   train_loss = 1.130\n",
      "Epoch 299 Batch   59/175   train_loss = 1.127\n",
      "Epoch 299 Batch   91/175   train_loss = 1.132\n",
      "Epoch 299 Batch  123/175   train_loss = 1.144\n",
      "Epoch 299 Batch  155/175   train_loss = 1.128\n",
      "Epoch 300 Batch   12/175   train_loss = 1.147\n",
      "Epoch 300 Batch   44/175   train_loss = 1.133\n",
      "Epoch 300 Batch   76/175   train_loss = 1.124\n",
      "Epoch 300 Batch  108/175   train_loss = 1.148\n",
      "Epoch 300 Batch  140/175   train_loss = 1.142\n",
      "Epoch 300 Batch  172/175   train_loss = 1.178\n",
      "Epoch 301 Batch   29/175   train_loss = 1.145\n",
      "Epoch 301 Batch   61/175   train_loss = 1.157\n",
      "Epoch 301 Batch   93/175   train_loss = 1.143\n",
      "Epoch 301 Batch  125/175   train_loss = 1.173\n",
      "Epoch 301 Batch  157/175   train_loss = 1.126\n",
      "Epoch 302 Batch   14/175   train_loss = 1.175\n",
      "Epoch 302 Batch   46/175   train_loss = 1.151\n",
      "Epoch 302 Batch   78/175   train_loss = 1.129\n",
      "Epoch 302 Batch  110/175   train_loss = 1.205\n",
      "Epoch 302 Batch  142/175   train_loss = 1.174\n",
      "Epoch 302 Batch  174/175   train_loss = 1.161\n",
      "Epoch 303 Batch   31/175   train_loss = 1.161\n",
      "Epoch 303 Batch   63/175   train_loss = 1.160\n",
      "Epoch 303 Batch   95/175   train_loss = 1.115\n",
      "Epoch 303 Batch  127/175   train_loss = 1.117\n",
      "Epoch 303 Batch  159/175   train_loss = 1.125\n",
      "Epoch 304 Batch   16/175   train_loss = 1.154\n",
      "Epoch 304 Batch   48/175   train_loss = 1.152\n",
      "Epoch 304 Batch   80/175   train_loss = 1.159\n",
      "Epoch 304 Batch  112/175   train_loss = 1.141\n",
      "Epoch 304 Batch  144/175   train_loss = 1.085\n",
      "Epoch 305 Batch    1/175   train_loss = 1.196\n",
      "Epoch 305 Batch   33/175   train_loss = 1.189\n",
      "Epoch 305 Batch   65/175   train_loss = 1.130\n",
      "Epoch 305 Batch   97/175   train_loss = 1.148\n",
      "Epoch 305 Batch  129/175   train_loss = 1.140\n",
      "Epoch 305 Batch  161/175   train_loss = 1.149\n",
      "Epoch 306 Batch   18/175   train_loss = 1.131\n",
      "Epoch 306 Batch   50/175   train_loss = 1.129\n",
      "Epoch 306 Batch   82/175   train_loss = 1.181\n",
      "Epoch 306 Batch  114/175   train_loss = 1.190\n",
      "Epoch 306 Batch  146/175   train_loss = 1.147\n",
      "Epoch 307 Batch    3/175   train_loss = 1.188\n",
      "Epoch 307 Batch   35/175   train_loss = 1.134\n",
      "Epoch 307 Batch   67/175   train_loss = 1.106\n",
      "Epoch 307 Batch   99/175   train_loss = 1.200\n",
      "Epoch 307 Batch  131/175   train_loss = 1.152\n",
      "Epoch 307 Batch  163/175   train_loss = 1.151\n",
      "Epoch 308 Batch   20/175   train_loss = 1.143\n",
      "Epoch 308 Batch   52/175   train_loss = 1.095\n",
      "Epoch 308 Batch   84/175   train_loss = 1.146\n",
      "Epoch 308 Batch  116/175   train_loss = 1.194\n",
      "Epoch 308 Batch  148/175   train_loss = 1.149\n",
      "Epoch 309 Batch    5/175   train_loss = 1.149\n",
      "Epoch 309 Batch   37/175   train_loss = 1.117\n",
      "Epoch 309 Batch   69/175   train_loss = 1.165\n",
      "Epoch 309 Batch  101/175   train_loss = 1.190\n",
      "Epoch 309 Batch  133/175   train_loss = 1.076\n",
      "Epoch 309 Batch  165/175   train_loss = 1.162\n",
      "Epoch 310 Batch   22/175   train_loss = 1.083\n",
      "Epoch 310 Batch   54/175   train_loss = 1.181\n",
      "Epoch 310 Batch   86/175   train_loss = 1.210\n",
      "Epoch 310 Batch  118/175   train_loss = 1.188\n",
      "Epoch 310 Batch  150/175   train_loss = 1.152\n",
      "Epoch 311 Batch    7/175   train_loss = 1.183\n",
      "Epoch 311 Batch   39/175   train_loss = 1.102\n",
      "Epoch 311 Batch   71/175   train_loss = 1.173\n",
      "Epoch 311 Batch  103/175   train_loss = 1.149\n",
      "Epoch 311 Batch  135/175   train_loss = 1.125\n",
      "Epoch 311 Batch  167/175   train_loss = 1.209\n",
      "Epoch 312 Batch   24/175   train_loss = 1.097\n",
      "Epoch 312 Batch   56/175   train_loss = 1.170\n",
      "Epoch 312 Batch   88/175   train_loss = 1.177\n",
      "Epoch 312 Batch  120/175   train_loss = 1.132\n",
      "Epoch 312 Batch  152/175   train_loss = 1.129\n",
      "Epoch 313 Batch    9/175   train_loss = 1.183\n",
      "Epoch 313 Batch   41/175   train_loss = 1.175\n",
      "Epoch 313 Batch   73/175   train_loss = 1.149\n",
      "Epoch 313 Batch  105/175   train_loss = 1.200\n",
      "Epoch 313 Batch  137/175   train_loss = 1.128\n",
      "Epoch 313 Batch  169/175   train_loss = 1.174\n",
      "Epoch 314 Batch   26/175   train_loss = 1.151\n",
      "Epoch 314 Batch   58/175   train_loss = 1.178\n",
      "Epoch 314 Batch   90/175   train_loss = 1.156\n",
      "Epoch 314 Batch  122/175   train_loss = 1.126\n",
      "Epoch 314 Batch  154/175   train_loss = 1.140\n",
      "Epoch 315 Batch   11/175   train_loss = 1.145\n",
      "Epoch 315 Batch   43/175   train_loss = 1.134\n",
      "Epoch 315 Batch   75/175   train_loss = 1.114\n",
      "Epoch 315 Batch  107/175   train_loss = 1.172\n",
      "Epoch 315 Batch  139/175   train_loss = 1.113\n",
      "Epoch 315 Batch  171/175   train_loss = 1.206\n",
      "Epoch 316 Batch   28/175   train_loss = 1.158\n",
      "Epoch 316 Batch   60/175   train_loss = 1.150\n",
      "Epoch 316 Batch   92/175   train_loss = 1.125\n",
      "Epoch 316 Batch  124/175   train_loss = 1.163\n",
      "Epoch 316 Batch  156/175   train_loss = 1.164\n",
      "Epoch 317 Batch   13/175   train_loss = 1.145\n",
      "Epoch 317 Batch   45/175   train_loss = 1.161\n",
      "Epoch 317 Batch   77/175   train_loss = 1.139\n",
      "Epoch 317 Batch  109/175   train_loss = 1.178\n",
      "Epoch 317 Batch  141/175   train_loss = 1.110\n",
      "Epoch 317 Batch  173/175   train_loss = 1.122\n",
      "Epoch 318 Batch   30/175   train_loss = 1.206\n",
      "Epoch 318 Batch   62/175   train_loss = 1.191\n",
      "Epoch 318 Batch   94/175   train_loss = 1.128\n",
      "Epoch 318 Batch  126/175   train_loss = 1.147\n",
      "Epoch 318 Batch  158/175   train_loss = 1.147\n",
      "Epoch 319 Batch   15/175   train_loss = 1.179\n",
      "Epoch 319 Batch   47/175   train_loss = 1.150\n",
      "Epoch 319 Batch   79/175   train_loss = 1.178\n",
      "Epoch 319 Batch  111/175   train_loss = 1.184\n",
      "Epoch 319 Batch  143/175   train_loss = 1.129\n",
      "Epoch 320 Batch    0/175   train_loss = 1.151\n",
      "Epoch 320 Batch   32/175   train_loss = 1.158\n",
      "Epoch 320 Batch   64/175   train_loss = 1.158\n",
      "Epoch 320 Batch   96/175   train_loss = 1.152\n",
      "Epoch 320 Batch  128/175   train_loss = 1.104\n",
      "Epoch 320 Batch  160/175   train_loss = 1.159\n",
      "Epoch 321 Batch   17/175   train_loss = 1.118\n",
      "Epoch 321 Batch   49/175   train_loss = 1.162\n",
      "Epoch 321 Batch   81/175   train_loss = 1.130\n",
      "Epoch 321 Batch  113/175   train_loss = 1.137\n",
      "Epoch 321 Batch  145/175   train_loss = 1.121\n",
      "Epoch 322 Batch    2/175   train_loss = 1.159\n",
      "Epoch 322 Batch   34/175   train_loss = 1.171\n",
      "Epoch 322 Batch   66/175   train_loss = 1.171\n",
      "Epoch 322 Batch   98/175   train_loss = 1.182\n",
      "Epoch 322 Batch  130/175   train_loss = 1.183\n",
      "Epoch 322 Batch  162/175   train_loss = 1.156\n",
      "Epoch 323 Batch   19/175   train_loss = 1.133\n",
      "Epoch 323 Batch   51/175   train_loss = 1.097\n",
      "Epoch 323 Batch   83/175   train_loss = 1.201\n",
      "Epoch 323 Batch  115/175   train_loss = 1.245\n",
      "Epoch 323 Batch  147/175   train_loss = 1.114\n",
      "Epoch 324 Batch    4/175   train_loss = 1.151\n",
      "Epoch 324 Batch   36/175   train_loss = 1.096\n",
      "Epoch 324 Batch   68/175   train_loss = 1.126\n",
      "Epoch 324 Batch  100/175   train_loss = 1.170\n",
      "Epoch 324 Batch  132/175   train_loss = 1.122\n",
      "Epoch 324 Batch  164/175   train_loss = 1.114\n",
      "Epoch 325 Batch   21/175   train_loss = 1.108\n",
      "Epoch 325 Batch   53/175   train_loss = 1.127\n",
      "Epoch 325 Batch   85/175   train_loss = 1.156\n",
      "Epoch 325 Batch  117/175   train_loss = 1.159\n",
      "Epoch 325 Batch  149/175   train_loss = 1.156\n",
      "Epoch 326 Batch    6/175   train_loss = 1.172\n",
      "Epoch 326 Batch   38/175   train_loss = 1.113\n",
      "Epoch 326 Batch   70/175   train_loss = 1.142\n",
      "Epoch 326 Batch  102/175   train_loss = 1.152\n",
      "Epoch 326 Batch  134/175   train_loss = 1.118\n",
      "Epoch 326 Batch  166/175   train_loss = 1.154\n",
      "Epoch 327 Batch   23/175   train_loss = 1.105\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 327 Batch   55/175   train_loss = 1.187\n",
      "Epoch 327 Batch   87/175   train_loss = 1.204\n",
      "Epoch 327 Batch  119/175   train_loss = 1.137\n",
      "Epoch 327 Batch  151/175   train_loss = 1.144\n",
      "Epoch 328 Batch    8/175   train_loss = 1.158\n",
      "Epoch 328 Batch   40/175   train_loss = 1.145\n",
      "Epoch 328 Batch   72/175   train_loss = 1.146\n",
      "Epoch 328 Batch  104/175   train_loss = 1.152\n",
      "Epoch 328 Batch  136/175   train_loss = 1.129\n",
      "Epoch 328 Batch  168/175   train_loss = 1.168\n",
      "Epoch 329 Batch   25/175   train_loss = 1.157\n",
      "Epoch 329 Batch   57/175   train_loss = 1.203\n",
      "Epoch 329 Batch   89/175   train_loss = 1.135\n",
      "Epoch 329 Batch  121/175   train_loss = 1.131\n",
      "Epoch 329 Batch  153/175   train_loss = 1.173\n",
      "Epoch 330 Batch   10/175   train_loss = 1.092\n",
      "Epoch 330 Batch   42/175   train_loss = 1.187\n",
      "Epoch 330 Batch   74/175   train_loss = 1.194\n",
      "Epoch 330 Batch  106/175   train_loss = 1.178\n",
      "Epoch 330 Batch  138/175   train_loss = 1.146\n",
      "Epoch 330 Batch  170/175   train_loss = 1.174\n",
      "Epoch 331 Batch   27/175   train_loss = 1.135\n",
      "Epoch 331 Batch   59/175   train_loss = 1.114\n",
      "Epoch 331 Batch   91/175   train_loss = 1.123\n",
      "Epoch 331 Batch  123/175   train_loss = 1.146\n",
      "Epoch 331 Batch  155/175   train_loss = 1.096\n",
      "Epoch 332 Batch   12/175   train_loss = 1.149\n",
      "Epoch 332 Batch   44/175   train_loss = 1.112\n",
      "Epoch 332 Batch   76/175   train_loss = 1.118\n",
      "Epoch 332 Batch  108/175   train_loss = 1.149\n",
      "Epoch 332 Batch  140/175   train_loss = 1.139\n",
      "Epoch 332 Batch  172/175   train_loss = 1.162\n",
      "Epoch 333 Batch   29/175   train_loss = 1.152\n",
      "Epoch 333 Batch   61/175   train_loss = 1.132\n",
      "Epoch 333 Batch   93/175   train_loss = 1.135\n",
      "Epoch 333 Batch  125/175   train_loss = 1.160\n",
      "Epoch 333 Batch  157/175   train_loss = 1.128\n",
      "Epoch 334 Batch   14/175   train_loss = 1.171\n",
      "Epoch 334 Batch   46/175   train_loss = 1.148\n",
      "Epoch 334 Batch   78/175   train_loss = 1.122\n",
      "Epoch 334 Batch  110/175   train_loss = 1.216\n",
      "Epoch 334 Batch  142/175   train_loss = 1.171\n",
      "Epoch 334 Batch  174/175   train_loss = 1.131\n",
      "Epoch 335 Batch   31/175   train_loss = 1.154\n",
      "Epoch 335 Batch   63/175   train_loss = 1.173\n",
      "Epoch 335 Batch   95/175   train_loss = 1.122\n",
      "Epoch 335 Batch  127/175   train_loss = 1.113\n",
      "Epoch 335 Batch  159/175   train_loss = 1.111\n",
      "Epoch 336 Batch   16/175   train_loss = 1.152\n",
      "Epoch 336 Batch   48/175   train_loss = 1.146\n",
      "Epoch 336 Batch   80/175   train_loss = 1.167\n",
      "Epoch 336 Batch  112/175   train_loss = 1.138\n",
      "Epoch 336 Batch  144/175   train_loss = 1.097\n",
      "Epoch 337 Batch    1/175   train_loss = 1.170\n",
      "Epoch 337 Batch   33/175   train_loss = 1.191\n",
      "Epoch 337 Batch   65/175   train_loss = 1.137\n",
      "Epoch 337 Batch   97/175   train_loss = 1.136\n",
      "Epoch 337 Batch  129/175   train_loss = 1.115\n",
      "Epoch 337 Batch  161/175   train_loss = 1.128\n",
      "Epoch 338 Batch   18/175   train_loss = 1.132\n",
      "Epoch 338 Batch   50/175   train_loss = 1.129\n",
      "Epoch 338 Batch   82/175   train_loss = 1.165\n",
      "Epoch 338 Batch  114/175   train_loss = 1.180\n",
      "Epoch 338 Batch  146/175   train_loss = 1.124\n",
      "Epoch 339 Batch    3/175   train_loss = 1.190\n",
      "Epoch 339 Batch   35/175   train_loss = 1.145\n",
      "Epoch 339 Batch   67/175   train_loss = 1.091\n",
      "Epoch 339 Batch   99/175   train_loss = 1.185\n",
      "Epoch 339 Batch  131/175   train_loss = 1.148\n",
      "Epoch 339 Batch  163/175   train_loss = 1.162\n",
      "Epoch 340 Batch   20/175   train_loss = 1.131\n",
      "Epoch 340 Batch   52/175   train_loss = 1.080\n",
      "Epoch 340 Batch   84/175   train_loss = 1.139\n",
      "Epoch 340 Batch  116/175   train_loss = 1.191\n",
      "Epoch 340 Batch  148/175   train_loss = 1.142\n",
      "Epoch 341 Batch    5/175   train_loss = 1.152\n",
      "Epoch 341 Batch   37/175   train_loss = 1.123\n",
      "Epoch 341 Batch   69/175   train_loss = 1.163\n",
      "Epoch 341 Batch  101/175   train_loss = 1.179\n",
      "Epoch 341 Batch  133/175   train_loss = 1.059\n",
      "Epoch 341 Batch  165/175   train_loss = 1.139\n",
      "Epoch 342 Batch   22/175   train_loss = 1.085\n",
      "Epoch 342 Batch   54/175   train_loss = 1.166\n",
      "Epoch 342 Batch   86/175   train_loss = 1.190\n",
      "Epoch 342 Batch  118/175   train_loss = 1.176\n",
      "Epoch 342 Batch  150/175   train_loss = 1.155\n",
      "Epoch 343 Batch    7/175   train_loss = 1.168\n",
      "Epoch 343 Batch   39/175   train_loss = 1.086\n",
      "Epoch 343 Batch   71/175   train_loss = 1.161\n",
      "Epoch 343 Batch  103/175   train_loss = 1.151\n",
      "Epoch 343 Batch  135/175   train_loss = 1.115\n",
      "Epoch 343 Batch  167/175   train_loss = 1.182\n",
      "Epoch 344 Batch   24/175   train_loss = 1.091\n",
      "Epoch 344 Batch   56/175   train_loss = 1.157\n",
      "Epoch 344 Batch   88/175   train_loss = 1.169\n",
      "Epoch 344 Batch  120/175   train_loss = 1.119\n",
      "Epoch 344 Batch  152/175   train_loss = 1.131\n",
      "Epoch 345 Batch    9/175   train_loss = 1.171\n",
      "Epoch 345 Batch   41/175   train_loss = 1.169\n",
      "Epoch 345 Batch   73/175   train_loss = 1.154\n",
      "Epoch 345 Batch  105/175   train_loss = 1.198\n",
      "Epoch 345 Batch  137/175   train_loss = 1.129\n",
      "Epoch 345 Batch  169/175   train_loss = 1.173\n",
      "Epoch 346 Batch   26/175   train_loss = 1.151\n",
      "Epoch 346 Batch   58/175   train_loss = 1.156\n",
      "Epoch 346 Batch   90/175   train_loss = 1.167\n",
      "Epoch 346 Batch  122/175   train_loss = 1.132\n",
      "Epoch 346 Batch  154/175   train_loss = 1.118\n",
      "Epoch 347 Batch   11/175   train_loss = 1.119\n",
      "Epoch 347 Batch   43/175   train_loss = 1.122\n",
      "Epoch 347 Batch   75/175   train_loss = 1.116\n",
      "Epoch 347 Batch  107/175   train_loss = 1.168\n",
      "Epoch 347 Batch  139/175   train_loss = 1.115\n",
      "Epoch 347 Batch  171/175   train_loss = 1.172\n",
      "Epoch 348 Batch   28/175   train_loss = 1.135\n",
      "Epoch 348 Batch   60/175   train_loss = 1.155\n",
      "Epoch 348 Batch   92/175   train_loss = 1.129\n",
      "Epoch 348 Batch  124/175   train_loss = 1.142\n",
      "Epoch 348 Batch  156/175   train_loss = 1.164\n",
      "Epoch 349 Batch   13/175   train_loss = 1.130\n",
      "Epoch 349 Batch   45/175   train_loss = 1.146\n",
      "Epoch 349 Batch   77/175   train_loss = 1.129\n",
      "Epoch 349 Batch  109/175   train_loss = 1.172\n",
      "Epoch 349 Batch  141/175   train_loss = 1.092\n",
      "Epoch 349 Batch  173/175   train_loss = 1.117\n",
      "Epoch 350 Batch   30/175   train_loss = 1.190\n",
      "Epoch 350 Batch   62/175   train_loss = 1.178\n",
      "Epoch 350 Batch   94/175   train_loss = 1.131\n",
      "Epoch 350 Batch  126/175   train_loss = 1.140\n",
      "Epoch 350 Batch  158/175   train_loss = 1.140\n",
      "Epoch 351 Batch   15/175   train_loss = 1.179\n",
      "Epoch 351 Batch   47/175   train_loss = 1.148\n",
      "Epoch 351 Batch   79/175   train_loss = 1.189\n",
      "Epoch 351 Batch  111/175   train_loss = 1.184\n",
      "Epoch 351 Batch  143/175   train_loss = 1.119\n",
      "Epoch 352 Batch    0/175   train_loss = 1.140\n",
      "Epoch 352 Batch   32/175   train_loss = 1.156\n",
      "Epoch 352 Batch   64/175   train_loss = 1.158\n",
      "Epoch 352 Batch   96/175   train_loss = 1.151\n",
      "Epoch 352 Batch  128/175   train_loss = 1.106\n",
      "Epoch 352 Batch  160/175   train_loss = 1.143\n",
      "Epoch 353 Batch   17/175   train_loss = 1.123\n",
      "Epoch 353 Batch   49/175   train_loss = 1.163\n",
      "Epoch 353 Batch   81/175   train_loss = 1.118\n",
      "Epoch 353 Batch  113/175   train_loss = 1.144\n",
      "Epoch 353 Batch  145/175   train_loss = 1.118\n",
      "Epoch 354 Batch    2/175   train_loss = 1.151\n",
      "Epoch 354 Batch   34/175   train_loss = 1.152\n",
      "Epoch 354 Batch   66/175   train_loss = 1.164\n",
      "Epoch 354 Batch   98/175   train_loss = 1.186\n",
      "Epoch 354 Batch  130/175   train_loss = 1.171\n",
      "Epoch 354 Batch  162/175   train_loss = 1.136\n",
      "Epoch 355 Batch   19/175   train_loss = 1.129\n",
      "Epoch 355 Batch   51/175   train_loss = 1.091\n",
      "Epoch 355 Batch   83/175   train_loss = 1.184\n",
      "Epoch 355 Batch  115/175   train_loss = 1.236\n",
      "Epoch 355 Batch  147/175   train_loss = 1.114\n",
      "Epoch 356 Batch    4/175   train_loss = 1.148\n",
      "Epoch 356 Batch   36/175   train_loss = 1.107\n",
      "Epoch 356 Batch   68/175   train_loss = 1.146\n",
      "Epoch 356 Batch  100/175   train_loss = 1.149\n",
      "Epoch 356 Batch  132/175   train_loss = 1.116\n",
      "Epoch 356 Batch  164/175   train_loss = 1.134\n",
      "Epoch 357 Batch   21/175   train_loss = 1.120\n",
      "Epoch 357 Batch   53/175   train_loss = 1.121\n",
      "Epoch 357 Batch   85/175   train_loss = 1.170\n",
      "Epoch 357 Batch  117/175   train_loss = 1.160\n",
      "Epoch 357 Batch  149/175   train_loss = 1.155\n",
      "Epoch 358 Batch    6/175   train_loss = 1.175\n",
      "Epoch 358 Batch   38/175   train_loss = 1.104\n",
      "Epoch 358 Batch   70/175   train_loss = 1.143\n",
      "Epoch 358 Batch  102/175   train_loss = 1.162\n",
      "Epoch 358 Batch  134/175   train_loss = 1.128\n",
      "Epoch 358 Batch  166/175   train_loss = 1.153\n",
      "Epoch 359 Batch   23/175   train_loss = 1.094\n",
      "Epoch 359 Batch   55/175   train_loss = 1.194\n",
      "Epoch 359 Batch   87/175   train_loss = 1.219\n",
      "Epoch 359 Batch  119/175   train_loss = 1.125\n",
      "Epoch 359 Batch  151/175   train_loss = 1.148\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 360 Batch    8/175   train_loss = 1.160\n",
      "Epoch 360 Batch   40/175   train_loss = 1.128\n",
      "Epoch 360 Batch   72/175   train_loss = 1.160\n",
      "Epoch 360 Batch  104/175   train_loss = 1.170\n",
      "Epoch 360 Batch  136/175   train_loss = 1.145\n",
      "Epoch 360 Batch  168/175   train_loss = 1.177\n",
      "Epoch 361 Batch   25/175   train_loss = 1.161\n",
      "Epoch 361 Batch   57/175   train_loss = 1.197\n",
      "Epoch 361 Batch   89/175   train_loss = 1.143\n",
      "Epoch 361 Batch  121/175   train_loss = 1.130\n",
      "Epoch 361 Batch  153/175   train_loss = 1.169\n",
      "Epoch 362 Batch   10/175   train_loss = 1.106\n",
      "Epoch 362 Batch   42/175   train_loss = 1.183\n",
      "Epoch 362 Batch   74/175   train_loss = 1.184\n",
      "Epoch 362 Batch  106/175   train_loss = 1.172\n",
      "Epoch 362 Batch  138/175   train_loss = 1.138\n",
      "Epoch 362 Batch  170/175   train_loss = 1.182\n",
      "Epoch 363 Batch   27/175   train_loss = 1.140\n",
      "Epoch 363 Batch   59/175   train_loss = 1.126\n",
      "Epoch 363 Batch   91/175   train_loss = 1.126\n",
      "Epoch 363 Batch  123/175   train_loss = 1.133\n",
      "Epoch 363 Batch  155/175   train_loss = 1.123\n",
      "Epoch 364 Batch   12/175   train_loss = 1.128\n",
      "Epoch 364 Batch   44/175   train_loss = 1.129\n",
      "Epoch 364 Batch   76/175   train_loss = 1.131\n",
      "Epoch 364 Batch  108/175   train_loss = 1.137\n",
      "Epoch 364 Batch  140/175   train_loss = 1.125\n",
      "Epoch 364 Batch  172/175   train_loss = 1.156\n",
      "Epoch 365 Batch   29/175   train_loss = 1.136\n",
      "Epoch 365 Batch   61/175   train_loss = 1.153\n",
      "Epoch 365 Batch   93/175   train_loss = 1.141\n",
      "Epoch 365 Batch  125/175   train_loss = 1.144\n",
      "Epoch 365 Batch  157/175   train_loss = 1.111\n",
      "Epoch 366 Batch   14/175   train_loss = 1.163\n",
      "Epoch 366 Batch   46/175   train_loss = 1.144\n",
      "Epoch 366 Batch   78/175   train_loss = 1.124\n",
      "Epoch 366 Batch  110/175   train_loss = 1.216\n",
      "Epoch 366 Batch  142/175   train_loss = 1.168\n",
      "Epoch 366 Batch  174/175   train_loss = 1.127\n",
      "Epoch 367 Batch   31/175   train_loss = 1.157\n",
      "Epoch 367 Batch   63/175   train_loss = 1.161\n",
      "Epoch 367 Batch   95/175   train_loss = 1.115\n",
      "Epoch 367 Batch  127/175   train_loss = 1.090\n",
      "Epoch 367 Batch  159/175   train_loss = 1.125\n",
      "Epoch 368 Batch   16/175   train_loss = 1.158\n",
      "Epoch 368 Batch   48/175   train_loss = 1.149\n",
      "Epoch 368 Batch   80/175   train_loss = 1.167\n",
      "Epoch 368 Batch  112/175   train_loss = 1.137\n",
      "Epoch 368 Batch  144/175   train_loss = 1.075\n",
      "Epoch 369 Batch    1/175   train_loss = 1.177\n",
      "Epoch 369 Batch   33/175   train_loss = 1.176\n",
      "Epoch 369 Batch   65/175   train_loss = 1.139\n",
      "Epoch 369 Batch   97/175   train_loss = 1.137\n",
      "Epoch 369 Batch  129/175   train_loss = 1.117\n",
      "Epoch 369 Batch  161/175   train_loss = 1.125\n",
      "Epoch 370 Batch   18/175   train_loss = 1.118\n",
      "Epoch 370 Batch   50/175   train_loss = 1.135\n",
      "Epoch 370 Batch   82/175   train_loss = 1.175\n",
      "Epoch 370 Batch  114/175   train_loss = 1.170\n",
      "Epoch 370 Batch  146/175   train_loss = 1.137\n",
      "Epoch 371 Batch    3/175   train_loss = 1.163\n",
      "Epoch 371 Batch   35/175   train_loss = 1.137\n",
      "Epoch 371 Batch   67/175   train_loss = 1.102\n",
      "Epoch 371 Batch   99/175   train_loss = 1.188\n",
      "Epoch 371 Batch  131/175   train_loss = 1.150\n",
      "Epoch 371 Batch  163/175   train_loss = 1.145\n",
      "Epoch 372 Batch   20/175   train_loss = 1.130\n",
      "Epoch 372 Batch   52/175   train_loss = 1.090\n",
      "Epoch 372 Batch   84/175   train_loss = 1.135\n",
      "Epoch 372 Batch  116/175   train_loss = 1.191\n",
      "Epoch 372 Batch  148/175   train_loss = 1.139\n",
      "Epoch 373 Batch    5/175   train_loss = 1.132\n",
      "Epoch 373 Batch   37/175   train_loss = 1.112\n",
      "Epoch 373 Batch   69/175   train_loss = 1.158\n",
      "Epoch 373 Batch  101/175   train_loss = 1.193\n",
      "Epoch 373 Batch  133/175   train_loss = 1.081\n",
      "Epoch 373 Batch  165/175   train_loss = 1.147\n",
      "Epoch 374 Batch   22/175   train_loss = 1.067\n",
      "Epoch 374 Batch   54/175   train_loss = 1.178\n",
      "Epoch 374 Batch   86/175   train_loss = 1.176\n",
      "Epoch 374 Batch  118/175   train_loss = 1.183\n",
      "Epoch 374 Batch  150/175   train_loss = 1.152\n",
      "Epoch 375 Batch    7/175   train_loss = 1.165\n",
      "Epoch 375 Batch   39/175   train_loss = 1.101\n",
      "Epoch 375 Batch   71/175   train_loss = 1.146\n",
      "Epoch 375 Batch  103/175   train_loss = 1.135\n",
      "Epoch 375 Batch  135/175   train_loss = 1.115\n",
      "Epoch 375 Batch  167/175   train_loss = 1.189\n",
      "Epoch 376 Batch   24/175   train_loss = 1.087\n",
      "Epoch 376 Batch   56/175   train_loss = 1.159\n",
      "Epoch 376 Batch   88/175   train_loss = 1.168\n",
      "Epoch 376 Batch  120/175   train_loss = 1.130\n",
      "Epoch 376 Batch  152/175   train_loss = 1.120\n",
      "Epoch 377 Batch    9/175   train_loss = 1.166\n",
      "Epoch 377 Batch   41/175   train_loss = 1.188\n",
      "Epoch 377 Batch   73/175   train_loss = 1.158\n",
      "Epoch 377 Batch  105/175   train_loss = 1.193\n",
      "Epoch 377 Batch  137/175   train_loss = 1.135\n",
      "Epoch 377 Batch  169/175   train_loss = 1.165\n",
      "Epoch 378 Batch   26/175   train_loss = 1.153\n",
      "Epoch 378 Batch   58/175   train_loss = 1.160\n",
      "Epoch 378 Batch   90/175   train_loss = 1.157\n",
      "Epoch 378 Batch  122/175   train_loss = 1.125\n",
      "Epoch 378 Batch  154/175   train_loss = 1.121\n",
      "Epoch 379 Batch   11/175   train_loss = 1.132\n",
      "Epoch 379 Batch   43/175   train_loss = 1.132\n",
      "Epoch 379 Batch   75/175   train_loss = 1.118\n",
      "Epoch 379 Batch  107/175   train_loss = 1.167\n",
      "Epoch 379 Batch  139/175   train_loss = 1.109\n",
      "Epoch 379 Batch  171/175   train_loss = 1.186\n",
      "Epoch 380 Batch   28/175   train_loss = 1.125\n",
      "Epoch 380 Batch   60/175   train_loss = 1.165\n",
      "Epoch 380 Batch   92/175   train_loss = 1.140\n",
      "Epoch 380 Batch  124/175   train_loss = 1.133\n",
      "Epoch 380 Batch  156/175   train_loss = 1.170\n",
      "Epoch 381 Batch   13/175   train_loss = 1.141\n",
      "Epoch 381 Batch   45/175   train_loss = 1.137\n",
      "Epoch 381 Batch   77/175   train_loss = 1.111\n",
      "Epoch 381 Batch  109/175   train_loss = 1.196\n",
      "Epoch 381 Batch  141/175   train_loss = 1.116\n",
      "Epoch 381 Batch  173/175   train_loss = 1.116\n",
      "Epoch 382 Batch   30/175   train_loss = 1.180\n",
      "Epoch 382 Batch   62/175   train_loss = 1.155\n",
      "Epoch 382 Batch   94/175   train_loss = 1.128\n",
      "Epoch 382 Batch  126/175   train_loss = 1.153\n",
      "Epoch 382 Batch  158/175   train_loss = 1.130\n",
      "Epoch 383 Batch   15/175   train_loss = 1.169\n",
      "Epoch 383 Batch   47/175   train_loss = 1.149\n",
      "Epoch 383 Batch   79/175   train_loss = 1.190\n",
      "Epoch 383 Batch  111/175   train_loss = 1.205\n",
      "Epoch 383 Batch  143/175   train_loss = 1.119\n",
      "Epoch 384 Batch    0/175   train_loss = 1.139\n",
      "Epoch 384 Batch   32/175   train_loss = 1.156\n",
      "Epoch 384 Batch   64/175   train_loss = 1.170\n",
      "Epoch 384 Batch   96/175   train_loss = 1.155\n",
      "Epoch 384 Batch  128/175   train_loss = 1.109\n",
      "Epoch 384 Batch  160/175   train_loss = 1.123\n",
      "Epoch 385 Batch   17/175   train_loss = 1.128\n",
      "Epoch 385 Batch   49/175   train_loss = 1.163\n",
      "Epoch 385 Batch   81/175   train_loss = 1.128\n",
      "Epoch 385 Batch  113/175   train_loss = 1.139\n",
      "Epoch 385 Batch  145/175   train_loss = 1.126\n",
      "Epoch 386 Batch    2/175   train_loss = 1.144\n",
      "Epoch 386 Batch   34/175   train_loss = 1.155\n",
      "Epoch 386 Batch   66/175   train_loss = 1.172\n",
      "Epoch 386 Batch   98/175   train_loss = 1.162\n",
      "Epoch 386 Batch  130/175   train_loss = 1.168\n",
      "Epoch 386 Batch  162/175   train_loss = 1.143\n",
      "Epoch 387 Batch   19/175   train_loss = 1.122\n",
      "Epoch 387 Batch   51/175   train_loss = 1.102\n",
      "Epoch 387 Batch   83/175   train_loss = 1.183\n",
      "Epoch 387 Batch  115/175   train_loss = 1.246\n",
      "Epoch 387 Batch  147/175   train_loss = 1.120\n",
      "Epoch 388 Batch    4/175   train_loss = 1.162\n",
      "Epoch 388 Batch   36/175   train_loss = 1.105\n",
      "Epoch 388 Batch   68/175   train_loss = 1.151\n",
      "Epoch 388 Batch  100/175   train_loss = 1.142\n",
      "Epoch 388 Batch  132/175   train_loss = 1.110\n",
      "Epoch 388 Batch  164/175   train_loss = 1.124\n",
      "Epoch 389 Batch   21/175   train_loss = 1.105\n",
      "Epoch 389 Batch   53/175   train_loss = 1.127\n",
      "Epoch 389 Batch   85/175   train_loss = 1.162\n",
      "Epoch 389 Batch  117/175   train_loss = 1.164\n",
      "Epoch 389 Batch  149/175   train_loss = 1.160\n",
      "Epoch 390 Batch    6/175   train_loss = 1.163\n",
      "Epoch 390 Batch   38/175   train_loss = 1.103\n",
      "Epoch 390 Batch   70/175   train_loss = 1.149\n",
      "Epoch 390 Batch  102/175   train_loss = 1.154\n",
      "Epoch 390 Batch  134/175   train_loss = 1.115\n",
      "Epoch 390 Batch  166/175   train_loss = 1.153\n",
      "Epoch 391 Batch   23/175   train_loss = 1.091\n",
      "Epoch 391 Batch   55/175   train_loss = 1.183\n",
      "Epoch 391 Batch   87/175   train_loss = 1.198\n",
      "Epoch 391 Batch  119/175   train_loss = 1.115\n",
      "Epoch 391 Batch  151/175   train_loss = 1.157\n",
      "Epoch 392 Batch    8/175   train_loss = 1.144\n",
      "Epoch 392 Batch   40/175   train_loss = 1.139\n",
      "Epoch 392 Batch   72/175   train_loss = 1.161\n",
      "Epoch 392 Batch  104/175   train_loss = 1.167\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 392 Batch  136/175   train_loss = 1.132\n",
      "Epoch 392 Batch  168/175   train_loss = 1.183\n",
      "Epoch 393 Batch   25/175   train_loss = 1.174\n",
      "Epoch 393 Batch   57/175   train_loss = 1.200\n",
      "Epoch 393 Batch   89/175   train_loss = 1.147\n",
      "Epoch 393 Batch  121/175   train_loss = 1.125\n",
      "Epoch 393 Batch  153/175   train_loss = 1.155\n",
      "Epoch 394 Batch   10/175   train_loss = 1.094\n",
      "Epoch 394 Batch   42/175   train_loss = 1.186\n",
      "Epoch 394 Batch   74/175   train_loss = 1.176\n",
      "Epoch 394 Batch  106/175   train_loss = 1.181\n",
      "Epoch 394 Batch  138/175   train_loss = 1.120\n",
      "Epoch 394 Batch  170/175   train_loss = 1.161\n",
      "Epoch 395 Batch   27/175   train_loss = 1.129\n",
      "Epoch 395 Batch   59/175   train_loss = 1.116\n",
      "Epoch 395 Batch   91/175   train_loss = 1.125\n",
      "Epoch 395 Batch  123/175   train_loss = 1.140\n",
      "Epoch 395 Batch  155/175   train_loss = 1.111\n",
      "Epoch 396 Batch   12/175   train_loss = 1.123\n",
      "Epoch 396 Batch   44/175   train_loss = 1.127\n",
      "Epoch 396 Batch   76/175   train_loss = 1.115\n",
      "Epoch 396 Batch  108/175   train_loss = 1.145\n",
      "Epoch 396 Batch  140/175   train_loss = 1.127\n",
      "Epoch 396 Batch  172/175   train_loss = 1.170\n",
      "Epoch 397 Batch   29/175   train_loss = 1.133\n",
      "Epoch 397 Batch   61/175   train_loss = 1.154\n",
      "Epoch 397 Batch   93/175   train_loss = 1.121\n",
      "Epoch 397 Batch  125/175   train_loss = 1.145\n",
      "Epoch 397 Batch  157/175   train_loss = 1.124\n",
      "Epoch 398 Batch   14/175   train_loss = 1.158\n",
      "Epoch 398 Batch   46/175   train_loss = 1.130\n",
      "Epoch 398 Batch   78/175   train_loss = 1.118\n",
      "Epoch 398 Batch  110/175   train_loss = 1.210\n",
      "Epoch 398 Batch  142/175   train_loss = 1.152\n",
      "Epoch 398 Batch  174/175   train_loss = 1.123\n",
      "Epoch 399 Batch   31/175   train_loss = 1.148\n",
      "Epoch 399 Batch   63/175   train_loss = 1.150\n",
      "Epoch 399 Batch   95/175   train_loss = 1.111\n",
      "Epoch 399 Batch  127/175   train_loss = 1.095\n",
      "Epoch 399 Batch  159/175   train_loss = 1.105\n",
      "Epoch 400 Batch   16/175   train_loss = 1.139\n",
      "Epoch 400 Batch   48/175   train_loss = 1.147\n",
      "Epoch 400 Batch   80/175   train_loss = 1.155\n",
      "Epoch 400 Batch  112/175   train_loss = 1.142\n",
      "Epoch 400 Batch  144/175   train_loss = 1.085\n",
      "Epoch 401 Batch    1/175   train_loss = 1.172\n",
      "Epoch 401 Batch   33/175   train_loss = 1.182\n",
      "Epoch 401 Batch   65/175   train_loss = 1.143\n",
      "Epoch 401 Batch   97/175   train_loss = 1.144\n",
      "Epoch 401 Batch  129/175   train_loss = 1.132\n",
      "Epoch 401 Batch  161/175   train_loss = 1.124\n",
      "Epoch 402 Batch   18/175   train_loss = 1.123\n",
      "Epoch 402 Batch   50/175   train_loss = 1.122\n",
      "Epoch 402 Batch   82/175   train_loss = 1.167\n",
      "Epoch 402 Batch  114/175   train_loss = 1.170\n",
      "Epoch 402 Batch  146/175   train_loss = 1.136\n",
      "Epoch 403 Batch    3/175   train_loss = 1.191\n",
      "Epoch 403 Batch   35/175   train_loss = 1.138\n",
      "Epoch 403 Batch   67/175   train_loss = 1.107\n",
      "Epoch 403 Batch   99/175   train_loss = 1.196\n",
      "Epoch 403 Batch  131/175   train_loss = 1.130\n",
      "Epoch 403 Batch  163/175   train_loss = 1.144\n",
      "Epoch 404 Batch   20/175   train_loss = 1.129\n",
      "Epoch 404 Batch   52/175   train_loss = 1.091\n",
      "Epoch 404 Batch   84/175   train_loss = 1.151\n",
      "Epoch 404 Batch  116/175   train_loss = 1.189\n",
      "Epoch 404 Batch  148/175   train_loss = 1.145\n",
      "Epoch 405 Batch    5/175   train_loss = 1.146\n",
      "Epoch 405 Batch   37/175   train_loss = 1.113\n",
      "Epoch 405 Batch   69/175   train_loss = 1.168\n",
      "Epoch 405 Batch  101/175   train_loss = 1.192\n",
      "Epoch 405 Batch  133/175   train_loss = 1.049\n",
      "Epoch 405 Batch  165/175   train_loss = 1.153\n",
      "Epoch 406 Batch   22/175   train_loss = 1.080\n",
      "Epoch 406 Batch   54/175   train_loss = 1.186\n",
      "Epoch 406 Batch   86/175   train_loss = 1.182\n",
      "Epoch 406 Batch  118/175   train_loss = 1.175\n",
      "Epoch 406 Batch  150/175   train_loss = 1.143\n",
      "Epoch 407 Batch    7/175   train_loss = 1.170\n",
      "Epoch 407 Batch   39/175   train_loss = 1.102\n",
      "Epoch 407 Batch   71/175   train_loss = 1.160\n",
      "Epoch 407 Batch  103/175   train_loss = 1.131\n",
      "Epoch 407 Batch  135/175   train_loss = 1.112\n",
      "Epoch 407 Batch  167/175   train_loss = 1.190\n",
      "Epoch 408 Batch   24/175   train_loss = 1.093\n",
      "Epoch 408 Batch   56/175   train_loss = 1.156\n",
      "Epoch 408 Batch   88/175   train_loss = 1.183\n",
      "Epoch 408 Batch  120/175   train_loss = 1.123\n",
      "Epoch 408 Batch  152/175   train_loss = 1.120\n",
      "Epoch 409 Batch    9/175   train_loss = 1.181\n",
      "Epoch 409 Batch   41/175   train_loss = 1.186\n",
      "Epoch 409 Batch   73/175   train_loss = 1.141\n",
      "Epoch 409 Batch  105/175   train_loss = 1.194\n",
      "Epoch 409 Batch  137/175   train_loss = 1.127\n",
      "Epoch 409 Batch  169/175   train_loss = 1.164\n",
      "Epoch 410 Batch   26/175   train_loss = 1.154\n",
      "Epoch 410 Batch   58/175   train_loss = 1.181\n",
      "Epoch 410 Batch   90/175   train_loss = 1.157\n",
      "Epoch 410 Batch  122/175   train_loss = 1.130\n",
      "Epoch 410 Batch  154/175   train_loss = 1.131\n",
      "Epoch 411 Batch   11/175   train_loss = 1.136\n",
      "Epoch 411 Batch   43/175   train_loss = 1.121\n",
      "Epoch 411 Batch   75/175   train_loss = 1.121\n",
      "Epoch 411 Batch  107/175   train_loss = 1.180\n",
      "Epoch 411 Batch  139/175   train_loss = 1.105\n",
      "Epoch 411 Batch  171/175   train_loss = 1.180\n",
      "Epoch 412 Batch   28/175   train_loss = 1.127\n",
      "Epoch 412 Batch   60/175   train_loss = 1.147\n",
      "Epoch 412 Batch   92/175   train_loss = 1.129\n",
      "Epoch 412 Batch  124/175   train_loss = 1.154\n",
      "Epoch 412 Batch  156/175   train_loss = 1.171\n",
      "Epoch 413 Batch   13/175   train_loss = 1.138\n",
      "Epoch 413 Batch   45/175   train_loss = 1.135\n",
      "Epoch 413 Batch   77/175   train_loss = 1.123\n",
      "Epoch 413 Batch  109/175   train_loss = 1.177\n",
      "Epoch 413 Batch  141/175   train_loss = 1.107\n",
      "Epoch 413 Batch  173/175   train_loss = 1.117\n",
      "Epoch 414 Batch   30/175   train_loss = 1.172\n",
      "Epoch 414 Batch   62/175   train_loss = 1.161\n",
      "Epoch 414 Batch   94/175   train_loss = 1.129\n",
      "Epoch 414 Batch  126/175   train_loss = 1.152\n",
      "Epoch 414 Batch  158/175   train_loss = 1.136\n",
      "Epoch 415 Batch   15/175   train_loss = 1.161\n",
      "Epoch 415 Batch   47/175   train_loss = 1.152\n",
      "Epoch 415 Batch   79/175   train_loss = 1.206\n",
      "Epoch 415 Batch  111/175   train_loss = 1.202\n",
      "Epoch 415 Batch  143/175   train_loss = 1.136\n",
      "Epoch 416 Batch    0/175   train_loss = 1.129\n",
      "Epoch 416 Batch   32/175   train_loss = 1.156\n",
      "Epoch 416 Batch   64/175   train_loss = 1.161\n",
      "Epoch 416 Batch   96/175   train_loss = 1.155\n",
      "Epoch 416 Batch  128/175   train_loss = 1.108\n",
      "Epoch 416 Batch  160/175   train_loss = 1.153\n",
      "Epoch 417 Batch   17/175   train_loss = 1.122\n",
      "Epoch 417 Batch   49/175   train_loss = 1.167\n",
      "Epoch 417 Batch   81/175   train_loss = 1.130\n",
      "Epoch 417 Batch  113/175   train_loss = 1.152\n",
      "Epoch 417 Batch  145/175   train_loss = 1.134\n",
      "Epoch 418 Batch    2/175   train_loss = 1.142\n",
      "Epoch 418 Batch   34/175   train_loss = 1.165\n",
      "Epoch 418 Batch   66/175   train_loss = 1.185\n",
      "Epoch 418 Batch   98/175   train_loss = 1.169\n",
      "Epoch 418 Batch  130/175   train_loss = 1.174\n",
      "Epoch 418 Batch  162/175   train_loss = 1.146\n",
      "Epoch 419 Batch   19/175   train_loss = 1.135\n",
      "Epoch 419 Batch   51/175   train_loss = 1.097\n",
      "Epoch 419 Batch   83/175   train_loss = 1.180\n",
      "Epoch 419 Batch  115/175   train_loss = 1.239\n",
      "Epoch 419 Batch  147/175   train_loss = 1.120\n",
      "Epoch 420 Batch    4/175   train_loss = 1.148\n",
      "Epoch 420 Batch   36/175   train_loss = 1.102\n",
      "Epoch 420 Batch   68/175   train_loss = 1.131\n",
      "Epoch 420 Batch  100/175   train_loss = 1.134\n",
      "Epoch 420 Batch  132/175   train_loss = 1.103\n",
      "Epoch 420 Batch  164/175   train_loss = 1.128\n",
      "Epoch 421 Batch   21/175   train_loss = 1.114\n",
      "Epoch 421 Batch   53/175   train_loss = 1.127\n",
      "Epoch 421 Batch   85/175   train_loss = 1.158\n",
      "Epoch 421 Batch  117/175   train_loss = 1.140\n",
      "Epoch 421 Batch  149/175   train_loss = 1.162\n",
      "Epoch 422 Batch    6/175   train_loss = 1.154\n",
      "Epoch 422 Batch   38/175   train_loss = 1.095\n",
      "Epoch 422 Batch   70/175   train_loss = 1.142\n",
      "Epoch 422 Batch  102/175   train_loss = 1.159\n",
      "Epoch 422 Batch  134/175   train_loss = 1.119\n",
      "Epoch 422 Batch  166/175   train_loss = 1.151\n",
      "Epoch 423 Batch   23/175   train_loss = 1.092\n",
      "Epoch 423 Batch   55/175   train_loss = 1.201\n",
      "Epoch 423 Batch   87/175   train_loss = 1.215\n",
      "Epoch 423 Batch  119/175   train_loss = 1.128\n",
      "Epoch 423 Batch  151/175   train_loss = 1.169\n",
      "Epoch 424 Batch    8/175   train_loss = 1.143\n",
      "Epoch 424 Batch   40/175   train_loss = 1.125\n",
      "Epoch 424 Batch   72/175   train_loss = 1.150\n",
      "Epoch 424 Batch  104/175   train_loss = 1.158\n",
      "Epoch 424 Batch  136/175   train_loss = 1.140\n",
      "Epoch 424 Batch  168/175   train_loss = 1.166\n",
      "Epoch 425 Batch   25/175   train_loss = 1.145\n",
      "Epoch 425 Batch   57/175   train_loss = 1.190\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 425 Batch   89/175   train_loss = 1.138\n",
      "Epoch 425 Batch  121/175   train_loss = 1.119\n",
      "Epoch 425 Batch  153/175   train_loss = 1.170\n",
      "Epoch 426 Batch   10/175   train_loss = 1.104\n",
      "Epoch 426 Batch   42/175   train_loss = 1.174\n",
      "Epoch 426 Batch   74/175   train_loss = 1.189\n",
      "Epoch 426 Batch  106/175   train_loss = 1.184\n",
      "Epoch 426 Batch  138/175   train_loss = 1.145\n",
      "Epoch 426 Batch  170/175   train_loss = 1.172\n",
      "Epoch 427 Batch   27/175   train_loss = 1.132\n",
      "Epoch 427 Batch   59/175   train_loss = 1.100\n",
      "Epoch 427 Batch   91/175   train_loss = 1.131\n",
      "Epoch 427 Batch  123/175   train_loss = 1.143\n",
      "Epoch 427 Batch  155/175   train_loss = 1.100\n",
      "Epoch 428 Batch   12/175   train_loss = 1.135\n",
      "Epoch 428 Batch   44/175   train_loss = 1.131\n",
      "Epoch 428 Batch   76/175   train_loss = 1.126\n",
      "Epoch 428 Batch  108/175   train_loss = 1.162\n",
      "Epoch 428 Batch  140/175   train_loss = 1.137\n",
      "Epoch 428 Batch  172/175   train_loss = 1.172\n",
      "Epoch 429 Batch   29/175   train_loss = 1.125\n",
      "Epoch 429 Batch   61/175   train_loss = 1.150\n",
      "Epoch 429 Batch   93/175   train_loss = 1.143\n",
      "Epoch 429 Batch  125/175   train_loss = 1.168\n",
      "Epoch 429 Batch  157/175   train_loss = 1.122\n",
      "Epoch 430 Batch   14/175   train_loss = 1.177\n",
      "Epoch 430 Batch   46/175   train_loss = 1.151\n",
      "Epoch 430 Batch   78/175   train_loss = 1.123\n",
      "Epoch 430 Batch  110/175   train_loss = 1.205\n",
      "Epoch 430 Batch  142/175   train_loss = 1.166\n",
      "Epoch 430 Batch  174/175   train_loss = 1.127\n",
      "Epoch 431 Batch   31/175   train_loss = 1.157\n",
      "Epoch 431 Batch   63/175   train_loss = 1.169\n",
      "Epoch 431 Batch   95/175   train_loss = 1.122\n",
      "Epoch 431 Batch  127/175   train_loss = 1.114\n",
      "Epoch 431 Batch  159/175   train_loss = 1.118\n",
      "Epoch 432 Batch   16/175   train_loss = 1.148\n",
      "Epoch 432 Batch   48/175   train_loss = 1.150\n",
      "Epoch 432 Batch   80/175   train_loss = 1.148\n",
      "Epoch 432 Batch  112/175   train_loss = 1.149\n",
      "Epoch 432 Batch  144/175   train_loss = 1.102\n",
      "Epoch 433 Batch    1/175   train_loss = 1.160\n",
      "Epoch 433 Batch   33/175   train_loss = 1.178\n",
      "Epoch 433 Batch   65/175   train_loss = 1.144\n",
      "Epoch 433 Batch   97/175   train_loss = 1.135\n",
      "Epoch 433 Batch  129/175   train_loss = 1.125\n",
      "Epoch 433 Batch  161/175   train_loss = 1.136\n",
      "Epoch 434 Batch   18/175   train_loss = 1.138\n",
      "Epoch 434 Batch   50/175   train_loss = 1.121\n",
      "Epoch 434 Batch   82/175   train_loss = 1.184\n",
      "Epoch 434 Batch  114/175   train_loss = 1.171\n",
      "Epoch 434 Batch  146/175   train_loss = 1.150\n",
      "Epoch 435 Batch    3/175   train_loss = 1.180\n",
      "Epoch 435 Batch   35/175   train_loss = 1.146\n",
      "Epoch 435 Batch   67/175   train_loss = 1.110\n",
      "Epoch 435 Batch   99/175   train_loss = 1.193\n",
      "Epoch 435 Batch  131/175   train_loss = 1.159\n",
      "Epoch 435 Batch  163/175   train_loss = 1.138\n",
      "Epoch 436 Batch   20/175   train_loss = 1.122\n",
      "Epoch 436 Batch   52/175   train_loss = 1.072\n",
      "Epoch 436 Batch   84/175   train_loss = 1.135\n",
      "Epoch 436 Batch  116/175   train_loss = 1.187\n",
      "Epoch 436 Batch  148/175   train_loss = 1.142\n",
      "Epoch 437 Batch    5/175   train_loss = 1.134\n",
      "Epoch 437 Batch   37/175   train_loss = 1.110\n",
      "Epoch 437 Batch   69/175   train_loss = 1.157\n",
      "Epoch 437 Batch  101/175   train_loss = 1.185\n",
      "Epoch 437 Batch  133/175   train_loss = 1.060\n",
      "Epoch 437 Batch  165/175   train_loss = 1.159\n",
      "Epoch 438 Batch   22/175   train_loss = 1.089\n",
      "Epoch 438 Batch   54/175   train_loss = 1.192\n",
      "Epoch 438 Batch   86/175   train_loss = 1.198\n",
      "Epoch 438 Batch  118/175   train_loss = 1.177\n",
      "Epoch 438 Batch  150/175   train_loss = 1.143\n",
      "Epoch 439 Batch    7/175   train_loss = 1.177\n",
      "Epoch 439 Batch   39/175   train_loss = 1.091\n",
      "Epoch 439 Batch   71/175   train_loss = 1.177\n",
      "Epoch 439 Batch  103/175   train_loss = 1.151\n",
      "Epoch 439 Batch  135/175   train_loss = 1.121\n",
      "Epoch 439 Batch  167/175   train_loss = 1.190\n",
      "Epoch 440 Batch   24/175   train_loss = 1.097\n",
      "Epoch 440 Batch   56/175   train_loss = 1.177\n",
      "Epoch 440 Batch   88/175   train_loss = 1.177\n",
      "Epoch 440 Batch  120/175   train_loss = 1.124\n",
      "Epoch 440 Batch  152/175   train_loss = 1.129\n",
      "Epoch 441 Batch    9/175   train_loss = 1.173\n",
      "Epoch 441 Batch   41/175   train_loss = 1.181\n",
      "Epoch 441 Batch   73/175   train_loss = 1.158\n",
      "Epoch 441 Batch  105/175   train_loss = 1.192\n",
      "Epoch 441 Batch  137/175   train_loss = 1.133\n",
      "Epoch 441 Batch  169/175   train_loss = 1.181\n",
      "Epoch 442 Batch   26/175   train_loss = 1.156\n",
      "Epoch 442 Batch   58/175   train_loss = 1.173\n",
      "Epoch 442 Batch   90/175   train_loss = 1.174\n",
      "Epoch 442 Batch  122/175   train_loss = 1.125\n",
      "Epoch 442 Batch  154/175   train_loss = 1.131\n",
      "Epoch 443 Batch   11/175   train_loss = 1.137\n",
      "Epoch 443 Batch   43/175   train_loss = 1.133\n",
      "Epoch 443 Batch   75/175   train_loss = 1.120\n",
      "Epoch 443 Batch  107/175   train_loss = 1.183\n",
      "Epoch 443 Batch  139/175   train_loss = 1.127\n",
      "Epoch 443 Batch  171/175   train_loss = 1.184\n",
      "Epoch 444 Batch   28/175   train_loss = 1.145\n",
      "Epoch 444 Batch   60/175   train_loss = 1.144\n",
      "Epoch 444 Batch   92/175   train_loss = 1.127\n",
      "Epoch 444 Batch  124/175   train_loss = 1.159\n",
      "Epoch 444 Batch  156/175   train_loss = 1.174\n",
      "Epoch 445 Batch   13/175   train_loss = 1.138\n",
      "Epoch 445 Batch   45/175   train_loss = 1.139\n",
      "Epoch 445 Batch   77/175   train_loss = 1.123\n",
      "Epoch 445 Batch  109/175   train_loss = 1.185\n",
      "Epoch 445 Batch  141/175   train_loss = 1.123\n",
      "Epoch 445 Batch  173/175   train_loss = 1.124\n",
      "Epoch 446 Batch   30/175   train_loss = 1.189\n",
      "Epoch 446 Batch   62/175   train_loss = 1.175\n",
      "Epoch 446 Batch   94/175   train_loss = 1.141\n",
      "Epoch 446 Batch  126/175   train_loss = 1.141\n",
      "Epoch 446 Batch  158/175   train_loss = 1.141\n",
      "Epoch 447 Batch   15/175   train_loss = 1.163\n",
      "Epoch 447 Batch   47/175   train_loss = 1.155\n",
      "Epoch 447 Batch   79/175   train_loss = 1.175\n",
      "Epoch 447 Batch  111/175   train_loss = 1.192\n",
      "Epoch 447 Batch  143/175   train_loss = 1.128\n",
      "Epoch 448 Batch    0/175   train_loss = 1.153\n",
      "Epoch 448 Batch   32/175   train_loss = 1.164\n",
      "Epoch 448 Batch   64/175   train_loss = 1.164\n",
      "Epoch 448 Batch   96/175   train_loss = 1.152\n",
      "Epoch 448 Batch  128/175   train_loss = 1.112\n",
      "Epoch 448 Batch  160/175   train_loss = 1.151\n",
      "Epoch 449 Batch   17/175   train_loss = 1.116\n",
      "Epoch 449 Batch   49/175   train_loss = 1.162\n",
      "Epoch 449 Batch   81/175   train_loss = 1.108\n",
      "Epoch 449 Batch  113/175   train_loss = 1.149\n",
      "Epoch 449 Batch  145/175   train_loss = 1.132\n",
      "Epoch 450 Batch    2/175   train_loss = 1.136\n",
      "Epoch 450 Batch   34/175   train_loss = 1.161\n",
      "Epoch 450 Batch   66/175   train_loss = 1.165\n",
      "Epoch 450 Batch   98/175   train_loss = 1.169\n",
      "Epoch 450 Batch  130/175   train_loss = 1.164\n",
      "Epoch 450 Batch  162/175   train_loss = 1.142\n",
      "Epoch 451 Batch   19/175   train_loss = 1.122\n",
      "Epoch 451 Batch   51/175   train_loss = 1.099\n",
      "Epoch 451 Batch   83/175   train_loss = 1.188\n",
      "Epoch 451 Batch  115/175   train_loss = 1.245\n",
      "Epoch 451 Batch  147/175   train_loss = 1.128\n",
      "Epoch 452 Batch    4/175   train_loss = 1.144\n",
      "Epoch 452 Batch   36/175   train_loss = 1.123\n",
      "Epoch 452 Batch   68/175   train_loss = 1.133\n",
      "Epoch 452 Batch  100/175   train_loss = 1.142\n",
      "Epoch 452 Batch  132/175   train_loss = 1.112\n",
      "Epoch 452 Batch  164/175   train_loss = 1.115\n",
      "Epoch 453 Batch   21/175   train_loss = 1.105\n",
      "Epoch 453 Batch   53/175   train_loss = 1.116\n",
      "Epoch 453 Batch   85/175   train_loss = 1.148\n",
      "Epoch 453 Batch  117/175   train_loss = 1.150\n",
      "Epoch 453 Batch  149/175   train_loss = 1.145\n",
      "Epoch 454 Batch    6/175   train_loss = 1.149\n",
      "Epoch 454 Batch   38/175   train_loss = 1.105\n",
      "Epoch 454 Batch   70/175   train_loss = 1.146\n",
      "Epoch 454 Batch  102/175   train_loss = 1.140\n",
      "Epoch 454 Batch  134/175   train_loss = 1.112\n",
      "Epoch 454 Batch  166/175   train_loss = 1.152\n",
      "Epoch 455 Batch   23/175   train_loss = 1.092\n",
      "Epoch 455 Batch   55/175   train_loss = 1.175\n",
      "Epoch 455 Batch   87/175   train_loss = 1.203\n",
      "Epoch 455 Batch  119/175   train_loss = 1.111\n",
      "Epoch 455 Batch  151/175   train_loss = 1.155\n",
      "Epoch 456 Batch    8/175   train_loss = 1.147\n",
      "Epoch 456 Batch   40/175   train_loss = 1.143\n",
      "Epoch 456 Batch   72/175   train_loss = 1.157\n",
      "Epoch 456 Batch  104/175   train_loss = 1.169\n",
      "Epoch 456 Batch  136/175   train_loss = 1.121\n",
      "Epoch 456 Batch  168/175   train_loss = 1.161\n",
      "Epoch 457 Batch   25/175   train_loss = 1.136\n",
      "Epoch 457 Batch   57/175   train_loss = 1.182\n",
      "Epoch 457 Batch   89/175   train_loss = 1.140\n",
      "Epoch 457 Batch  121/175   train_loss = 1.111\n",
      "Epoch 457 Batch  153/175   train_loss = 1.164\n",
      "Epoch 458 Batch   10/175   train_loss = 1.099\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 458 Batch   42/175   train_loss = 1.175\n",
      "Epoch 458 Batch   74/175   train_loss = 1.188\n",
      "Epoch 458 Batch  106/175   train_loss = 1.175\n",
      "Epoch 458 Batch  138/175   train_loss = 1.142\n",
      "Epoch 458 Batch  170/175   train_loss = 1.180\n",
      "Epoch 459 Batch   27/175   train_loss = 1.118\n",
      "Epoch 459 Batch   59/175   train_loss = 1.115\n",
      "Epoch 459 Batch   91/175   train_loss = 1.131\n",
      "Epoch 459 Batch  123/175   train_loss = 1.147\n",
      "Epoch 459 Batch  155/175   train_loss = 1.112\n",
      "Epoch 460 Batch   12/175   train_loss = 1.140\n",
      "Epoch 460 Batch   44/175   train_loss = 1.135\n",
      "Epoch 460 Batch   76/175   train_loss = 1.117\n",
      "Epoch 460 Batch  108/175   train_loss = 1.143\n",
      "Epoch 460 Batch  140/175   train_loss = 1.134\n",
      "Epoch 460 Batch  172/175   train_loss = 1.174\n",
      "Epoch 461 Batch   29/175   train_loss = 1.131\n",
      "Epoch 461 Batch   61/175   train_loss = 1.147\n",
      "Epoch 461 Batch   93/175   train_loss = 1.149\n",
      "Epoch 461 Batch  125/175   train_loss = 1.146\n",
      "Epoch 461 Batch  157/175   train_loss = 1.125\n",
      "Epoch 462 Batch   14/175   train_loss = 1.161\n",
      "Epoch 462 Batch   46/175   train_loss = 1.149\n",
      "Epoch 462 Batch   78/175   train_loss = 1.124\n",
      "Epoch 462 Batch  110/175   train_loss = 1.215\n",
      "Epoch 462 Batch  142/175   train_loss = 1.170\n",
      "Epoch 462 Batch  174/175   train_loss = 1.130\n",
      "Epoch 463 Batch   31/175   train_loss = 1.159\n",
      "Epoch 463 Batch   63/175   train_loss = 1.166\n",
      "Epoch 463 Batch   95/175   train_loss = 1.115\n",
      "Epoch 463 Batch  127/175   train_loss = 1.100\n",
      "Epoch 463 Batch  159/175   train_loss = 1.096\n",
      "Epoch 464 Batch   16/175   train_loss = 1.157\n",
      "Epoch 464 Batch   48/175   train_loss = 1.144\n",
      "Epoch 464 Batch   80/175   train_loss = 1.164\n",
      "Epoch 464 Batch  112/175   train_loss = 1.123\n",
      "Epoch 464 Batch  144/175   train_loss = 1.080\n",
      "Epoch 465 Batch    1/175   train_loss = 1.161\n",
      "Epoch 465 Batch   33/175   train_loss = 1.180\n",
      "Epoch 465 Batch   65/175   train_loss = 1.140\n",
      "Epoch 465 Batch   97/175   train_loss = 1.159\n",
      "Epoch 465 Batch  129/175   train_loss = 1.130\n",
      "Epoch 465 Batch  161/175   train_loss = 1.119\n",
      "Epoch 466 Batch   18/175   train_loss = 1.127\n",
      "Epoch 466 Batch   50/175   train_loss = 1.129\n",
      "Epoch 466 Batch   82/175   train_loss = 1.173\n",
      "Epoch 466 Batch  114/175   train_loss = 1.172\n",
      "Epoch 466 Batch  146/175   train_loss = 1.134\n",
      "Epoch 467 Batch    3/175   train_loss = 1.184\n",
      "Epoch 467 Batch   35/175   train_loss = 1.135\n",
      "Epoch 467 Batch   67/175   train_loss = 1.104\n",
      "Epoch 467 Batch   99/175   train_loss = 1.183\n",
      "Epoch 467 Batch  131/175   train_loss = 1.146\n",
      "Epoch 467 Batch  163/175   train_loss = 1.140\n",
      "Epoch 468 Batch   20/175   train_loss = 1.117\n",
      "Epoch 468 Batch   52/175   train_loss = 1.094\n",
      "Epoch 468 Batch   84/175   train_loss = 1.148\n",
      "Epoch 468 Batch  116/175   train_loss = 1.190\n",
      "Epoch 468 Batch  148/175   train_loss = 1.143\n",
      "Epoch 469 Batch    5/175   train_loss = 1.134\n",
      "Epoch 469 Batch   37/175   train_loss = 1.126\n",
      "Epoch 469 Batch   69/175   train_loss = 1.176\n",
      "Epoch 469 Batch  101/175   train_loss = 1.189\n",
      "Epoch 469 Batch  133/175   train_loss = 1.073\n",
      "Epoch 469 Batch  165/175   train_loss = 1.151\n",
      "Epoch 470 Batch   22/175   train_loss = 1.091\n",
      "Epoch 470 Batch   54/175   train_loss = 1.178\n",
      "Epoch 470 Batch   86/175   train_loss = 1.184\n",
      "Epoch 470 Batch  118/175   train_loss = 1.193\n",
      "Epoch 470 Batch  150/175   train_loss = 1.150\n",
      "Epoch 471 Batch    7/175   train_loss = 1.168\n",
      "Epoch 471 Batch   39/175   train_loss = 1.087\n",
      "Epoch 471 Batch   71/175   train_loss = 1.152\n",
      "Epoch 471 Batch  103/175   train_loss = 1.130\n",
      "Epoch 471 Batch  135/175   train_loss = 1.110\n",
      "Epoch 471 Batch  167/175   train_loss = 1.179\n",
      "Epoch 472 Batch   24/175   train_loss = 1.101\n",
      "Epoch 472 Batch   56/175   train_loss = 1.173\n",
      "Epoch 472 Batch   88/175   train_loss = 1.171\n",
      "Epoch 472 Batch  120/175   train_loss = 1.119\n",
      "Epoch 472 Batch  152/175   train_loss = 1.129\n",
      "Epoch 473 Batch    9/175   train_loss = 1.164\n",
      "Epoch 473 Batch   41/175   train_loss = 1.165\n",
      "Epoch 473 Batch   73/175   train_loss = 1.144\n",
      "Epoch 473 Batch  105/175   train_loss = 1.186\n",
      "Epoch 473 Batch  137/175   train_loss = 1.127\n",
      "Epoch 473 Batch  169/175   train_loss = 1.150\n",
      "Epoch 474 Batch   26/175   train_loss = 1.151\n",
      "Epoch 474 Batch   58/175   train_loss = 1.173\n",
      "Epoch 474 Batch   90/175   train_loss = 1.170\n",
      "Epoch 474 Batch  122/175   train_loss = 1.125\n",
      "Epoch 474 Batch  154/175   train_loss = 1.139\n",
      "Epoch 475 Batch   11/175   train_loss = 1.142\n",
      "Epoch 475 Batch   43/175   train_loss = 1.129\n",
      "Epoch 475 Batch   75/175   train_loss = 1.120\n",
      "Epoch 475 Batch  107/175   train_loss = 1.179\n",
      "Epoch 475 Batch  139/175   train_loss = 1.126\n",
      "Epoch 475 Batch  171/175   train_loss = 1.178\n",
      "Epoch 476 Batch   28/175   train_loss = 1.138\n",
      "Epoch 476 Batch   60/175   train_loss = 1.153\n",
      "Epoch 476 Batch   92/175   train_loss = 1.127\n",
      "Epoch 476 Batch  124/175   train_loss = 1.153\n",
      "Epoch 476 Batch  156/175   train_loss = 1.186\n",
      "Epoch 477 Batch   13/175   train_loss = 1.145\n",
      "Epoch 477 Batch   45/175   train_loss = 1.136\n",
      "Epoch 477 Batch   77/175   train_loss = 1.116\n",
      "Epoch 477 Batch  109/175   train_loss = 1.192\n",
      "Epoch 477 Batch  141/175   train_loss = 1.089\n",
      "Epoch 477 Batch  173/175   train_loss = 1.108\n",
      "Epoch 478 Batch   30/175   train_loss = 1.197\n",
      "Epoch 478 Batch   62/175   train_loss = 1.160\n",
      "Epoch 478 Batch   94/175   train_loss = 1.134\n",
      "Epoch 478 Batch  126/175   train_loss = 1.136\n",
      "Epoch 478 Batch  158/175   train_loss = 1.147\n",
      "Epoch 479 Batch   15/175   train_loss = 1.178\n",
      "Epoch 479 Batch   47/175   train_loss = 1.147\n",
      "Epoch 479 Batch   79/175   train_loss = 1.174\n",
      "Epoch 479 Batch  111/175   train_loss = 1.211\n",
      "Epoch 479 Batch  143/175   train_loss = 1.143\n",
      "Epoch 480 Batch    0/175   train_loss = 1.144\n",
      "Epoch 480 Batch   32/175   train_loss = 1.159\n",
      "Epoch 480 Batch   64/175   train_loss = 1.173\n",
      "Epoch 480 Batch   96/175   train_loss = 1.151\n",
      "Epoch 480 Batch  128/175   train_loss = 1.107\n",
      "Epoch 480 Batch  160/175   train_loss = 1.141\n",
      "Epoch 481 Batch   17/175   train_loss = 1.118\n",
      "Epoch 481 Batch   49/175   train_loss = 1.167\n",
      "Epoch 481 Batch   81/175   train_loss = 1.118\n",
      "Epoch 481 Batch  113/175   train_loss = 1.154\n",
      "Epoch 481 Batch  145/175   train_loss = 1.139\n",
      "Epoch 482 Batch    2/175   train_loss = 1.144\n",
      "Epoch 482 Batch   34/175   train_loss = 1.158\n",
      "Epoch 482 Batch   66/175   train_loss = 1.181\n",
      "Epoch 482 Batch   98/175   train_loss = 1.179\n",
      "Epoch 482 Batch  130/175   train_loss = 1.161\n",
      "Epoch 482 Batch  162/175   train_loss = 1.155\n",
      "Epoch 483 Batch   19/175   train_loss = 1.121\n",
      "Epoch 483 Batch   51/175   train_loss = 1.101\n",
      "Epoch 483 Batch   83/175   train_loss = 1.182\n",
      "Epoch 483 Batch  115/175   train_loss = 1.245\n",
      "Epoch 483 Batch  147/175   train_loss = 1.127\n",
      "Epoch 484 Batch    4/175   train_loss = 1.151\n",
      "Epoch 484 Batch   36/175   train_loss = 1.118\n",
      "Epoch 484 Batch   68/175   train_loss = 1.147\n",
      "Epoch 484 Batch  100/175   train_loss = 1.142\n",
      "Epoch 484 Batch  132/175   train_loss = 1.118\n",
      "Epoch 484 Batch  164/175   train_loss = 1.114\n",
      "Epoch 485 Batch   21/175   train_loss = 1.128\n",
      "Epoch 485 Batch   53/175   train_loss = 1.119\n",
      "Epoch 485 Batch   85/175   train_loss = 1.163\n",
      "Epoch 485 Batch  117/175   train_loss = 1.173\n",
      "Epoch 485 Batch  149/175   train_loss = 1.159\n",
      "Epoch 486 Batch    6/175   train_loss = 1.161\n",
      "Epoch 486 Batch   38/175   train_loss = 1.116\n",
      "Epoch 486 Batch   70/175   train_loss = 1.144\n",
      "Epoch 486 Batch  102/175   train_loss = 1.155\n",
      "Epoch 486 Batch  134/175   train_loss = 1.131\n",
      "Epoch 486 Batch  166/175   train_loss = 1.152\n",
      "Epoch 487 Batch   23/175   train_loss = 1.105\n",
      "Epoch 487 Batch   55/175   train_loss = 1.206\n",
      "Epoch 487 Batch   87/175   train_loss = 1.194\n",
      "Epoch 487 Batch  119/175   train_loss = 1.125\n",
      "Epoch 487 Batch  151/175   train_loss = 1.168\n",
      "Epoch 488 Batch    8/175   train_loss = 1.158\n",
      "Epoch 488 Batch   40/175   train_loss = 1.141\n",
      "Epoch 488 Batch   72/175   train_loss = 1.165\n",
      "Epoch 488 Batch  104/175   train_loss = 1.161\n",
      "Epoch 488 Batch  136/175   train_loss = 1.139\n",
      "Epoch 488 Batch  168/175   train_loss = 1.181\n",
      "Epoch 489 Batch   25/175   train_loss = 1.160\n",
      "Epoch 489 Batch   57/175   train_loss = 1.192\n",
      "Epoch 489 Batch   89/175   train_loss = 1.139\n",
      "Epoch 489 Batch  121/175   train_loss = 1.134\n",
      "Epoch 489 Batch  153/175   train_loss = 1.158\n",
      "Epoch 490 Batch   10/175   train_loss = 1.107\n",
      "Epoch 490 Batch   42/175   train_loss = 1.200\n",
      "Epoch 490 Batch   74/175   train_loss = 1.186\n",
      "Epoch 490 Batch  106/175   train_loss = 1.175\n",
      "Epoch 490 Batch  138/175   train_loss = 1.143\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 490 Batch  170/175   train_loss = 1.189\n",
      "Epoch 491 Batch   27/175   train_loss = 1.137\n",
      "Epoch 491 Batch   59/175   train_loss = 1.116\n",
      "Epoch 491 Batch   91/175   train_loss = 1.133\n",
      "Epoch 491 Batch  123/175   train_loss = 1.143\n",
      "Epoch 491 Batch  155/175   train_loss = 1.111\n",
      "Epoch 492 Batch   12/175   train_loss = 1.127\n",
      "Epoch 492 Batch   44/175   train_loss = 1.146\n",
      "Epoch 492 Batch   76/175   train_loss = 1.135\n",
      "Epoch 492 Batch  108/175   train_loss = 1.171\n",
      "Epoch 492 Batch  140/175   train_loss = 1.144\n",
      "Epoch 492 Batch  172/175   train_loss = 1.172\n",
      "Epoch 493 Batch   29/175   train_loss = 1.144\n",
      "Epoch 493 Batch   61/175   train_loss = 1.159\n",
      "Epoch 493 Batch   93/175   train_loss = 1.166\n",
      "Epoch 493 Batch  125/175   train_loss = 1.161\n",
      "Epoch 493 Batch  157/175   train_loss = 1.120\n",
      "Epoch 494 Batch   14/175   train_loss = 1.163\n",
      "Epoch 494 Batch   46/175   train_loss = 1.147\n",
      "Epoch 494 Batch   78/175   train_loss = 1.127\n",
      "Epoch 494 Batch  110/175   train_loss = 1.204\n",
      "Epoch 494 Batch  142/175   train_loss = 1.183\n",
      "Epoch 494 Batch  174/175   train_loss = 1.121\n",
      "Epoch 495 Batch   31/175   train_loss = 1.153\n",
      "Epoch 495 Batch   63/175   train_loss = 1.145\n",
      "Epoch 495 Batch   95/175   train_loss = 1.120\n",
      "Epoch 495 Batch  127/175   train_loss = 1.101\n",
      "Epoch 495 Batch  159/175   train_loss = 1.127\n",
      "Epoch 496 Batch   16/175   train_loss = 1.146\n",
      "Epoch 496 Batch   48/175   train_loss = 1.150\n",
      "Epoch 496 Batch   80/175   train_loss = 1.164\n",
      "Epoch 496 Batch  112/175   train_loss = 1.149\n",
      "Epoch 496 Batch  144/175   train_loss = 1.099\n",
      "Epoch 497 Batch    1/175   train_loss = 1.187\n",
      "Epoch 497 Batch   33/175   train_loss = 1.194\n",
      "Epoch 497 Batch   65/175   train_loss = 1.146\n",
      "Epoch 497 Batch   97/175   train_loss = 1.162\n",
      "Epoch 497 Batch  129/175   train_loss = 1.147\n",
      "Epoch 497 Batch  161/175   train_loss = 1.144\n",
      "Epoch 498 Batch   18/175   train_loss = 1.145\n",
      "Epoch 498 Batch   50/175   train_loss = 1.136\n",
      "Epoch 498 Batch   82/175   train_loss = 1.177\n",
      "Epoch 498 Batch  114/175   train_loss = 1.202\n",
      "Epoch 498 Batch  146/175   train_loss = 1.155\n",
      "Epoch 499 Batch    3/175   train_loss = 1.180\n",
      "Epoch 499 Batch   35/175   train_loss = 1.146\n",
      "Epoch 499 Batch   67/175   train_loss = 1.112\n",
      "Epoch 499 Batch   99/175   train_loss = 1.193\n",
      "Epoch 499 Batch  131/175   train_loss = 1.159\n",
      "Epoch 499 Batch  163/175   train_loss = 1.148\n",
      "Epoch 500 Batch   20/175   train_loss = 1.132\n",
      "Epoch 500 Batch   52/175   train_loss = 1.102\n",
      "Epoch 500 Batch   84/175   train_loss = 1.139\n",
      "Epoch 500 Batch  116/175   train_loss = 1.199\n",
      "Epoch 500 Batch  148/175   train_loss = 1.155\n",
      "Epoch 501 Batch    5/175   train_loss = 1.150\n",
      "Epoch 501 Batch   37/175   train_loss = 1.133\n",
      "Epoch 501 Batch   69/175   train_loss = 1.160\n",
      "Epoch 501 Batch  101/175   train_loss = 1.194\n",
      "Epoch 501 Batch  133/175   train_loss = 1.062\n",
      "Epoch 501 Batch  165/175   train_loss = 1.163\n",
      "Epoch 502 Batch   22/175   train_loss = 1.091\n",
      "Epoch 502 Batch   54/175   train_loss = 1.171\n",
      "Epoch 502 Batch   86/175   train_loss = 1.205\n",
      "Epoch 502 Batch  118/175   train_loss = 1.199\n",
      "Epoch 502 Batch  150/175   train_loss = 1.157\n",
      "Epoch 503 Batch    7/175   train_loss = 1.166\n",
      "Epoch 503 Batch   39/175   train_loss = 1.104\n",
      "Epoch 503 Batch   71/175   train_loss = 1.175\n",
      "Epoch 503 Batch  103/175   train_loss = 1.147\n",
      "Epoch 503 Batch  135/175   train_loss = 1.127\n",
      "Epoch 503 Batch  167/175   train_loss = 1.192\n",
      "Epoch 504 Batch   24/175   train_loss = 1.085\n",
      "Epoch 504 Batch   56/175   train_loss = 1.172\n",
      "Epoch 504 Batch   88/175   train_loss = 1.174\n",
      "Epoch 504 Batch  120/175   train_loss = 1.132\n",
      "Epoch 504 Batch  152/175   train_loss = 1.118\n",
      "Epoch 505 Batch    9/175   train_loss = 1.165\n",
      "Epoch 505 Batch   41/175   train_loss = 1.185\n",
      "Epoch 505 Batch   73/175   train_loss = 1.154\n",
      "Epoch 505 Batch  105/175   train_loss = 1.187\n",
      "Epoch 505 Batch  137/175   train_loss = 1.113\n",
      "Epoch 505 Batch  169/175   train_loss = 1.161\n",
      "Epoch 506 Batch   26/175   train_loss = 1.152\n",
      "Epoch 506 Batch   58/175   train_loss = 1.194\n",
      "Epoch 506 Batch   90/175   train_loss = 1.157\n",
      "Epoch 506 Batch  122/175   train_loss = 1.132\n",
      "Epoch 506 Batch  154/175   train_loss = 1.121\n",
      "Epoch 507 Batch   11/175   train_loss = 1.137\n",
      "Epoch 507 Batch   43/175   train_loss = 1.127\n",
      "Epoch 507 Batch   75/175   train_loss = 1.136\n",
      "Epoch 507 Batch  107/175   train_loss = 1.183\n",
      "Epoch 507 Batch  139/175   train_loss = 1.116\n",
      "Epoch 507 Batch  171/175   train_loss = 1.179\n",
      "Epoch 508 Batch   28/175   train_loss = 1.152\n",
      "Epoch 508 Batch   60/175   train_loss = 1.154\n",
      "Epoch 508 Batch   92/175   train_loss = 1.134\n",
      "Epoch 508 Batch  124/175   train_loss = 1.159\n",
      "Epoch 508 Batch  156/175   train_loss = 1.184\n",
      "Epoch 509 Batch   13/175   train_loss = 1.132\n",
      "Epoch 509 Batch   45/175   train_loss = 1.134\n",
      "Epoch 509 Batch   77/175   train_loss = 1.111\n",
      "Epoch 509 Batch  109/175   train_loss = 1.176\n",
      "Epoch 509 Batch  141/175   train_loss = 1.104\n",
      "Epoch 509 Batch  173/175   train_loss = 1.122\n",
      "Epoch 510 Batch   30/175   train_loss = 1.192\n",
      "Epoch 510 Batch   62/175   train_loss = 1.172\n",
      "Epoch 510 Batch   94/175   train_loss = 1.143\n",
      "Epoch 510 Batch  126/175   train_loss = 1.140\n",
      "Epoch 510 Batch  158/175   train_loss = 1.151\n",
      "Epoch 511 Batch   15/175   train_loss = 1.179\n",
      "Epoch 511 Batch   47/175   train_loss = 1.162\n",
      "Epoch 511 Batch   79/175   train_loss = 1.188\n",
      "Epoch 511 Batch  111/175   train_loss = 1.195\n",
      "Epoch 511 Batch  143/175   train_loss = 1.146\n",
      "Epoch 512 Batch    0/175   train_loss = 1.144\n",
      "Epoch 512 Batch   32/175   train_loss = 1.171\n",
      "Epoch 512 Batch   64/175   train_loss = 1.176\n",
      "Epoch 512 Batch   96/175   train_loss = 1.158\n",
      "Epoch 512 Batch  128/175   train_loss = 1.106\n",
      "Epoch 512 Batch  160/175   train_loss = 1.158\n",
      "Epoch 513 Batch   17/175   train_loss = 1.122\n",
      "Epoch 513 Batch   49/175   train_loss = 1.167\n",
      "Epoch 513 Batch   81/175   train_loss = 1.121\n",
      "Epoch 513 Batch  113/175   train_loss = 1.157\n",
      "Epoch 513 Batch  145/175   train_loss = 1.116\n",
      "Epoch 514 Batch    2/175   train_loss = 1.161\n",
      "Epoch 514 Batch   34/175   train_loss = 1.153\n",
      "Epoch 514 Batch   66/175   train_loss = 1.172\n",
      "Epoch 514 Batch   98/175   train_loss = 1.175\n",
      "Epoch 514 Batch  130/175   train_loss = 1.176\n",
      "Epoch 514 Batch  162/175   train_loss = 1.159\n",
      "Epoch 515 Batch   19/175   train_loss = 1.132\n",
      "Epoch 515 Batch   51/175   train_loss = 1.089\n",
      "Epoch 515 Batch   83/175   train_loss = 1.188\n",
      "Epoch 515 Batch  115/175   train_loss = 1.238\n",
      "Epoch 515 Batch  147/175   train_loss = 1.116\n",
      "Epoch 516 Batch    4/175   train_loss = 1.143\n",
      "Epoch 516 Batch   36/175   train_loss = 1.113\n",
      "Epoch 516 Batch   68/175   train_loss = 1.130\n",
      "Epoch 516 Batch  100/175   train_loss = 1.156\n",
      "Epoch 516 Batch  132/175   train_loss = 1.120\n",
      "Epoch 516 Batch  164/175   train_loss = 1.129\n",
      "Epoch 517 Batch   21/175   train_loss = 1.112\n",
      "Epoch 517 Batch   53/175   train_loss = 1.132\n",
      "Epoch 517 Batch   85/175   train_loss = 1.157\n",
      "Epoch 517 Batch  117/175   train_loss = 1.161\n",
      "Epoch 517 Batch  149/175   train_loss = 1.161\n",
      "Epoch 518 Batch    6/175   train_loss = 1.167\n",
      "Epoch 518 Batch   38/175   train_loss = 1.108\n",
      "Epoch 518 Batch   70/175   train_loss = 1.144\n",
      "Epoch 518 Batch  102/175   train_loss = 1.159\n",
      "Epoch 518 Batch  134/175   train_loss = 1.112\n",
      "Epoch 518 Batch  166/175   train_loss = 1.155\n",
      "Epoch 519 Batch   23/175   train_loss = 1.100\n",
      "Epoch 519 Batch   55/175   train_loss = 1.203\n",
      "Epoch 519 Batch   87/175   train_loss = 1.219\n",
      "Epoch 519 Batch  119/175   train_loss = 1.113\n",
      "Epoch 519 Batch  151/175   train_loss = 1.163\n",
      "Epoch 520 Batch    8/175   train_loss = 1.153\n",
      "Epoch 520 Batch   40/175   train_loss = 1.157\n",
      "Epoch 520 Batch   72/175   train_loss = 1.161\n",
      "Epoch 520 Batch  104/175   train_loss = 1.158\n",
      "Epoch 520 Batch  136/175   train_loss = 1.135\n",
      "Epoch 520 Batch  168/175   train_loss = 1.166\n",
      "Epoch 521 Batch   25/175   train_loss = 1.164\n",
      "Epoch 521 Batch   57/175   train_loss = 1.186\n",
      "Epoch 521 Batch   89/175   train_loss = 1.137\n",
      "Epoch 521 Batch  121/175   train_loss = 1.115\n",
      "Epoch 521 Batch  153/175   train_loss = 1.165\n",
      "Epoch 522 Batch   10/175   train_loss = 1.102\n",
      "Epoch 522 Batch   42/175   train_loss = 1.202\n",
      "Epoch 522 Batch   74/175   train_loss = 1.177\n",
      "Epoch 522 Batch  106/175   train_loss = 1.187\n",
      "Epoch 522 Batch  138/175   train_loss = 1.136\n",
      "Epoch 522 Batch  170/175   train_loss = 1.204\n",
      "Epoch 523 Batch   27/175   train_loss = 1.138\n",
      "Epoch 523 Batch   59/175   train_loss = 1.127\n",
      "Epoch 523 Batch   91/175   train_loss = 1.127\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 523 Batch  123/175   train_loss = 1.144\n",
      "Epoch 523 Batch  155/175   train_loss = 1.116\n",
      "Epoch 524 Batch   12/175   train_loss = 1.137\n",
      "Epoch 524 Batch   44/175   train_loss = 1.132\n",
      "Epoch 524 Batch   76/175   train_loss = 1.120\n",
      "Epoch 524 Batch  108/175   train_loss = 1.144\n",
      "Epoch 524 Batch  140/175   train_loss = 1.123\n",
      "Epoch 524 Batch  172/175   train_loss = 1.180\n",
      "Epoch 525 Batch   29/175   train_loss = 1.153\n",
      "Epoch 525 Batch   61/175   train_loss = 1.163\n",
      "Epoch 525 Batch   93/175   train_loss = 1.145\n",
      "Epoch 525 Batch  125/175   train_loss = 1.161\n",
      "Epoch 525 Batch  157/175   train_loss = 1.125\n",
      "Epoch 526 Batch   14/175   train_loss = 1.160\n",
      "Epoch 526 Batch   46/175   train_loss = 1.153\n",
      "Epoch 526 Batch   78/175   train_loss = 1.111\n",
      "Epoch 526 Batch  110/175   train_loss = 1.212\n",
      "Epoch 526 Batch  142/175   train_loss = 1.166\n",
      "Epoch 526 Batch  174/175   train_loss = 1.128\n",
      "Epoch 527 Batch   31/175   train_loss = 1.140\n",
      "Epoch 527 Batch   63/175   train_loss = 1.151\n",
      "Epoch 527 Batch   95/175   train_loss = 1.123\n",
      "Epoch 527 Batch  127/175   train_loss = 1.113\n",
      "Epoch 527 Batch  159/175   train_loss = 1.105\n",
      "Epoch 528 Batch   16/175   train_loss = 1.159\n",
      "Epoch 528 Batch   48/175   train_loss = 1.160\n",
      "Epoch 528 Batch   80/175   train_loss = 1.165\n",
      "Epoch 528 Batch  112/175   train_loss = 1.140\n",
      "Epoch 528 Batch  144/175   train_loss = 1.079\n",
      "Epoch 529 Batch    1/175   train_loss = 1.188\n",
      "Epoch 529 Batch   33/175   train_loss = 1.197\n",
      "Epoch 529 Batch   65/175   train_loss = 1.153\n",
      "Epoch 529 Batch   97/175   train_loss = 1.136\n",
      "Epoch 529 Batch  129/175   train_loss = 1.126\n",
      "Epoch 529 Batch  161/175   train_loss = 1.129\n",
      "Epoch 530 Batch   18/175   train_loss = 1.113\n",
      "Epoch 530 Batch   50/175   train_loss = 1.108\n",
      "Epoch 530 Batch   82/175   train_loss = 1.167\n",
      "Epoch 530 Batch  114/175   train_loss = 1.173\n",
      "Epoch 530 Batch  146/175   train_loss = 1.150\n",
      "Epoch 531 Batch    3/175   train_loss = 1.171\n",
      "Epoch 531 Batch   35/175   train_loss = 1.135\n",
      "Epoch 531 Batch   67/175   train_loss = 1.119\n",
      "Epoch 531 Batch   99/175   train_loss = 1.190\n",
      "Epoch 531 Batch  131/175   train_loss = 1.136\n",
      "Epoch 531 Batch  163/175   train_loss = 1.145\n",
      "Epoch 532 Batch   20/175   train_loss = 1.126\n",
      "Epoch 532 Batch   52/175   train_loss = 1.078\n",
      "Epoch 532 Batch   84/175   train_loss = 1.146\n",
      "Epoch 532 Batch  116/175   train_loss = 1.174\n",
      "Epoch 532 Batch  148/175   train_loss = 1.146\n",
      "Epoch 533 Batch    5/175   train_loss = 1.135\n",
      "Epoch 533 Batch   37/175   train_loss = 1.123\n",
      "Epoch 533 Batch   69/175   train_loss = 1.162\n",
      "Epoch 533 Batch  101/175   train_loss = 1.188\n",
      "Epoch 533 Batch  133/175   train_loss = 1.051\n",
      "Epoch 533 Batch  165/175   train_loss = 1.144\n",
      "Epoch 534 Batch   22/175   train_loss = 1.096\n",
      "Epoch 534 Batch   54/175   train_loss = 1.161\n",
      "Epoch 534 Batch   86/175   train_loss = 1.201\n",
      "Epoch 534 Batch  118/175   train_loss = 1.180\n",
      "Epoch 534 Batch  150/175   train_loss = 1.152\n",
      "Epoch 535 Batch    7/175   train_loss = 1.152\n",
      "Epoch 535 Batch   39/175   train_loss = 1.095\n",
      "Epoch 535 Batch   71/175   train_loss = 1.169\n",
      "Epoch 535 Batch  103/175   train_loss = 1.145\n",
      "Epoch 535 Batch  135/175   train_loss = 1.117\n",
      "Epoch 535 Batch  167/175   train_loss = 1.186\n",
      "Epoch 536 Batch   24/175   train_loss = 1.089\n",
      "Epoch 536 Batch   56/175   train_loss = 1.170\n",
      "Epoch 536 Batch   88/175   train_loss = 1.175\n",
      "Epoch 536 Batch  120/175   train_loss = 1.133\n",
      "Epoch 536 Batch  152/175   train_loss = 1.125\n",
      "Epoch 537 Batch    9/175   train_loss = 1.179\n",
      "Epoch 537 Batch   41/175   train_loss = 1.180\n",
      "Epoch 537 Batch   73/175   train_loss = 1.166\n",
      "Epoch 537 Batch  105/175   train_loss = 1.188\n",
      "Epoch 537 Batch  137/175   train_loss = 1.119\n",
      "Epoch 537 Batch  169/175   train_loss = 1.158\n",
      "Epoch 538 Batch   26/175   train_loss = 1.131\n",
      "Epoch 538 Batch   58/175   train_loss = 1.184\n",
      "Epoch 538 Batch   90/175   train_loss = 1.170\n",
      "Epoch 538 Batch  122/175   train_loss = 1.114\n",
      "Epoch 538 Batch  154/175   train_loss = 1.126\n",
      "Epoch 539 Batch   11/175   train_loss = 1.141\n",
      "Epoch 539 Batch   43/175   train_loss = 1.142\n",
      "Epoch 539 Batch   75/175   train_loss = 1.129\n",
      "Epoch 539 Batch  107/175   train_loss = 1.188\n",
      "Epoch 539 Batch  139/175   train_loss = 1.103\n",
      "Epoch 539 Batch  171/175   train_loss = 1.181\n",
      "Epoch 540 Batch   28/175   train_loss = 1.137\n",
      "Epoch 540 Batch   60/175   train_loss = 1.150\n",
      "Epoch 540 Batch   92/175   train_loss = 1.135\n",
      "Epoch 540 Batch  124/175   train_loss = 1.150\n",
      "Epoch 540 Batch  156/175   train_loss = 1.171\n",
      "Epoch 541 Batch   13/175   train_loss = 1.145\n",
      "Epoch 541 Batch   45/175   train_loss = 1.127\n",
      "Epoch 541 Batch   77/175   train_loss = 1.133\n",
      "Epoch 541 Batch  109/175   train_loss = 1.164\n",
      "Epoch 541 Batch  141/175   train_loss = 1.099\n",
      "Epoch 541 Batch  173/175   train_loss = 1.115\n",
      "Epoch 542 Batch   30/175   train_loss = 1.191\n",
      "Epoch 542 Batch   62/175   train_loss = 1.173\n",
      "Epoch 542 Batch   94/175   train_loss = 1.139\n",
      "Epoch 542 Batch  126/175   train_loss = 1.151\n",
      "Epoch 542 Batch  158/175   train_loss = 1.153\n",
      "Epoch 543 Batch   15/175   train_loss = 1.184\n",
      "Epoch 543 Batch   47/175   train_loss = 1.157\n",
      "Epoch 543 Batch   79/175   train_loss = 1.184\n",
      "Epoch 543 Batch  111/175   train_loss = 1.206\n",
      "Epoch 543 Batch  143/175   train_loss = 1.148\n",
      "Epoch 544 Batch    0/175   train_loss = 1.138\n",
      "Epoch 544 Batch   32/175   train_loss = 1.158\n",
      "Epoch 544 Batch   64/175   train_loss = 1.165\n",
      "Epoch 544 Batch   96/175   train_loss = 1.154\n",
      "Epoch 544 Batch  128/175   train_loss = 1.102\n",
      "Epoch 544 Batch  160/175   train_loss = 1.125\n",
      "Epoch 545 Batch   17/175   train_loss = 1.117\n",
      "Epoch 545 Batch   49/175   train_loss = 1.156\n",
      "Epoch 545 Batch   81/175   train_loss = 1.113\n",
      "Epoch 545 Batch  113/175   train_loss = 1.146\n",
      "Epoch 545 Batch  145/175   train_loss = 1.097\n",
      "Epoch 546 Batch    2/175   train_loss = 1.155\n",
      "Epoch 546 Batch   34/175   train_loss = 1.160\n",
      "Epoch 546 Batch   66/175   train_loss = 1.190\n",
      "Epoch 546 Batch   98/175   train_loss = 1.170\n",
      "Epoch 546 Batch  130/175   train_loss = 1.164\n",
      "Epoch 546 Batch  162/175   train_loss = 1.149\n",
      "Epoch 547 Batch   19/175   train_loss = 1.123\n",
      "Epoch 547 Batch   51/175   train_loss = 1.088\n",
      "Epoch 547 Batch   83/175   train_loss = 1.202\n",
      "Epoch 547 Batch  115/175   train_loss = 1.242\n",
      "Epoch 547 Batch  147/175   train_loss = 1.115\n",
      "Epoch 548 Batch    4/175   train_loss = 1.148\n",
      "Epoch 548 Batch   36/175   train_loss = 1.099\n",
      "Epoch 548 Batch   68/175   train_loss = 1.142\n",
      "Epoch 548 Batch  100/175   train_loss = 1.142\n",
      "Epoch 548 Batch  132/175   train_loss = 1.124\n",
      "Epoch 548 Batch  164/175   train_loss = 1.104\n",
      "Epoch 549 Batch   21/175   train_loss = 1.112\n",
      "Epoch 549 Batch   53/175   train_loss = 1.120\n",
      "Epoch 549 Batch   85/175   train_loss = 1.140\n",
      "Epoch 549 Batch  117/175   train_loss = 1.156\n",
      "Epoch 549 Batch  149/175   train_loss = 1.172\n",
      "Epoch 550 Batch    6/175   train_loss = 1.157\n",
      "Epoch 550 Batch   38/175   train_loss = 1.114\n",
      "Epoch 550 Batch   70/175   train_loss = 1.150\n",
      "Epoch 550 Batch  102/175   train_loss = 1.156\n",
      "Epoch 550 Batch  134/175   train_loss = 1.121\n",
      "Epoch 550 Batch  166/175   train_loss = 1.152\n",
      "Epoch 551 Batch   23/175   train_loss = 1.088\n",
      "Epoch 551 Batch   55/175   train_loss = 1.198\n",
      "Epoch 551 Batch   87/175   train_loss = 1.221\n",
      "Epoch 551 Batch  119/175   train_loss = 1.129\n",
      "Epoch 551 Batch  151/175   train_loss = 1.165\n",
      "Epoch 552 Batch    8/175   train_loss = 1.151\n",
      "Epoch 552 Batch   40/175   train_loss = 1.127\n",
      "Epoch 552 Batch   72/175   train_loss = 1.155\n",
      "Epoch 552 Batch  104/175   train_loss = 1.145\n",
      "Epoch 552 Batch  136/175   train_loss = 1.151\n",
      "Epoch 552 Batch  168/175   train_loss = 1.160\n",
      "Epoch 553 Batch   25/175   train_loss = 1.139\n",
      "Epoch 553 Batch   57/175   train_loss = 1.192\n",
      "Epoch 553 Batch   89/175   train_loss = 1.142\n",
      "Epoch 553 Batch  121/175   train_loss = 1.126\n",
      "Epoch 553 Batch  153/175   train_loss = 1.154\n",
      "Epoch 554 Batch   10/175   train_loss = 1.105\n",
      "Epoch 554 Batch   42/175   train_loss = 1.200\n",
      "Epoch 554 Batch   74/175   train_loss = 1.196\n",
      "Epoch 554 Batch  106/175   train_loss = 1.190\n",
      "Epoch 554 Batch  138/175   train_loss = 1.130\n",
      "Epoch 554 Batch  170/175   train_loss = 1.178\n",
      "Epoch 555 Batch   27/175   train_loss = 1.130\n",
      "Epoch 555 Batch   59/175   train_loss = 1.137\n",
      "Epoch 555 Batch   91/175   train_loss = 1.125\n",
      "Epoch 555 Batch  123/175   train_loss = 1.138\n",
      "Epoch 555 Batch  155/175   train_loss = 1.116\n",
      "Epoch 556 Batch   12/175   train_loss = 1.132\n",
      "Epoch 556 Batch   44/175   train_loss = 1.125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 556 Batch   76/175   train_loss = 1.118\n",
      "Epoch 556 Batch  108/175   train_loss = 1.148\n",
      "Epoch 556 Batch  140/175   train_loss = 1.140\n",
      "Epoch 556 Batch  172/175   train_loss = 1.157\n",
      "Epoch 557 Batch   29/175   train_loss = 1.136\n",
      "Epoch 557 Batch   61/175   train_loss = 1.162\n",
      "Epoch 557 Batch   93/175   train_loss = 1.158\n",
      "Epoch 557 Batch  125/175   train_loss = 1.165\n",
      "Epoch 557 Batch  157/175   train_loss = 1.128\n",
      "Epoch 558 Batch   14/175   train_loss = 1.152\n",
      "Epoch 558 Batch   46/175   train_loss = 1.162\n",
      "Epoch 558 Batch   78/175   train_loss = 1.116\n",
      "Epoch 558 Batch  110/175   train_loss = 1.212\n",
      "Epoch 558 Batch  142/175   train_loss = 1.172\n",
      "Epoch 558 Batch  174/175   train_loss = 1.111\n",
      "Epoch 559 Batch   31/175   train_loss = 1.139\n",
      "Epoch 559 Batch   63/175   train_loss = 1.143\n",
      "Epoch 559 Batch   95/175   train_loss = 1.126\n",
      "Epoch 559 Batch  127/175   train_loss = 1.108\n",
      "Epoch 559 Batch  159/175   train_loss = 1.126\n",
      "Epoch 560 Batch   16/175   train_loss = 1.155\n",
      "Epoch 560 Batch   48/175   train_loss = 1.136\n",
      "Epoch 560 Batch   80/175   train_loss = 1.161\n",
      "Epoch 560 Batch  112/175   train_loss = 1.154\n",
      "Epoch 560 Batch  144/175   train_loss = 1.087\n",
      "Epoch 561 Batch    1/175   train_loss = 1.187\n",
      "Epoch 561 Batch   33/175   train_loss = 1.186\n",
      "Epoch 561 Batch   65/175   train_loss = 1.140\n",
      "Epoch 561 Batch   97/175   train_loss = 1.158\n",
      "Epoch 561 Batch  129/175   train_loss = 1.126\n",
      "Epoch 561 Batch  161/175   train_loss = 1.124\n",
      "Epoch 562 Batch   18/175   train_loss = 1.110\n",
      "Epoch 562 Batch   50/175   train_loss = 1.124\n",
      "Epoch 562 Batch   82/175   train_loss = 1.168\n",
      "Epoch 562 Batch  114/175   train_loss = 1.177\n",
      "Epoch 562 Batch  146/175   train_loss = 1.135\n",
      "Epoch 563 Batch    3/175   train_loss = 1.175\n",
      "Epoch 563 Batch   35/175   train_loss = 1.145\n",
      "Epoch 563 Batch   67/175   train_loss = 1.105\n",
      "Epoch 563 Batch   99/175   train_loss = 1.190\n",
      "Epoch 563 Batch  131/175   train_loss = 1.140\n",
      "Epoch 563 Batch  163/175   train_loss = 1.134\n",
      "Epoch 564 Batch   20/175   train_loss = 1.119\n",
      "Epoch 564 Batch   52/175   train_loss = 1.078\n",
      "Epoch 564 Batch   84/175   train_loss = 1.147\n",
      "Epoch 564 Batch  116/175   train_loss = 1.167\n",
      "Epoch 564 Batch  148/175   train_loss = 1.140\n",
      "Epoch 565 Batch    5/175   train_loss = 1.135\n",
      "Epoch 565 Batch   37/175   train_loss = 1.130\n",
      "Epoch 565 Batch   69/175   train_loss = 1.157\n",
      "Epoch 565 Batch  101/175   train_loss = 1.181\n",
      "Epoch 565 Batch  133/175   train_loss = 1.069\n",
      "Epoch 565 Batch  165/175   train_loss = 1.142\n",
      "Epoch 566 Batch   22/175   train_loss = 1.088\n",
      "Epoch 566 Batch   54/175   train_loss = 1.183\n",
      "Epoch 566 Batch   86/175   train_loss = 1.217\n",
      "Epoch 566 Batch  118/175   train_loss = 1.187\n",
      "Epoch 566 Batch  150/175   train_loss = 1.139\n",
      "Epoch 567 Batch    7/175   train_loss = 1.170\n",
      "Epoch 567 Batch   39/175   train_loss = 1.083\n",
      "Epoch 567 Batch   71/175   train_loss = 1.174\n",
      "Epoch 567 Batch  103/175   train_loss = 1.139\n",
      "Epoch 567 Batch  135/175   train_loss = 1.124\n",
      "Epoch 567 Batch  167/175   train_loss = 1.187\n",
      "Epoch 568 Batch   24/175   train_loss = 1.072\n",
      "Epoch 568 Batch   56/175   train_loss = 1.182\n",
      "Epoch 568 Batch   88/175   train_loss = 1.187\n",
      "Epoch 568 Batch  120/175   train_loss = 1.121\n",
      "Epoch 568 Batch  152/175   train_loss = 1.119\n",
      "Epoch 569 Batch    9/175   train_loss = 1.184\n",
      "Epoch 569 Batch   41/175   train_loss = 1.173\n",
      "Epoch 569 Batch   73/175   train_loss = 1.164\n",
      "Epoch 569 Batch  105/175   train_loss = 1.193\n",
      "Epoch 569 Batch  137/175   train_loss = 1.115\n",
      "Epoch 569 Batch  169/175   train_loss = 1.176\n",
      "Epoch 570 Batch   26/175   train_loss = 1.136\n",
      "Epoch 570 Batch   58/175   train_loss = 1.190\n",
      "Epoch 570 Batch   90/175   train_loss = 1.174\n",
      "Epoch 570 Batch  122/175   train_loss = 1.121\n",
      "Epoch 570 Batch  154/175   train_loss = 1.126\n",
      "Epoch 571 Batch   11/175   train_loss = 1.120\n",
      "Epoch 571 Batch   43/175   train_loss = 1.118\n",
      "Epoch 571 Batch   75/175   train_loss = 1.135\n",
      "Epoch 571 Batch  107/175   train_loss = 1.194\n",
      "Epoch 571 Batch  139/175   train_loss = 1.094\n",
      "Epoch 571 Batch  171/175   train_loss = 1.197\n",
      "Epoch 572 Batch   28/175   train_loss = 1.134\n",
      "Epoch 572 Batch   60/175   train_loss = 1.154\n",
      "Epoch 572 Batch   92/175   train_loss = 1.114\n",
      "Epoch 572 Batch  124/175   train_loss = 1.150\n",
      "Epoch 572 Batch  156/175   train_loss = 1.174\n",
      "Epoch 573 Batch   13/175   train_loss = 1.141\n",
      "Epoch 573 Batch   45/175   train_loss = 1.133\n",
      "Epoch 573 Batch   77/175   train_loss = 1.129\n",
      "Epoch 573 Batch  109/175   train_loss = 1.179\n",
      "Epoch 573 Batch  141/175   train_loss = 1.101\n",
      "Epoch 573 Batch  173/175   train_loss = 1.121\n",
      "Epoch 574 Batch   30/175   train_loss = 1.181\n",
      "Epoch 574 Batch   62/175   train_loss = 1.172\n",
      "Epoch 574 Batch   94/175   train_loss = 1.145\n",
      "Epoch 574 Batch  126/175   train_loss = 1.143\n",
      "Epoch 574 Batch  158/175   train_loss = 1.146\n",
      "Epoch 575 Batch   15/175   train_loss = 1.187\n",
      "Epoch 575 Batch   47/175   train_loss = 1.155\n",
      "Epoch 575 Batch   79/175   train_loss = 1.174\n",
      "Epoch 575 Batch  111/175   train_loss = 1.194\n",
      "Epoch 575 Batch  143/175   train_loss = 1.127\n",
      "Epoch 576 Batch    0/175   train_loss = 1.145\n",
      "Epoch 576 Batch   32/175   train_loss = 1.181\n",
      "Epoch 576 Batch   64/175   train_loss = 1.188\n",
      "Epoch 576 Batch   96/175   train_loss = 1.187\n",
      "Epoch 576 Batch  128/175   train_loss = 1.112\n",
      "Epoch 576 Batch  160/175   train_loss = 1.137\n",
      "Epoch 577 Batch   17/175   train_loss = 1.125\n",
      "Epoch 577 Batch   49/175   train_loss = 1.174\n",
      "Epoch 577 Batch   81/175   train_loss = 1.106\n",
      "Epoch 577 Batch  113/175   train_loss = 1.152\n",
      "Epoch 577 Batch  145/175   train_loss = 1.110\n",
      "Epoch 578 Batch    2/175   train_loss = 1.155\n",
      "Epoch 578 Batch   34/175   train_loss = 1.164\n",
      "Epoch 578 Batch   66/175   train_loss = 1.183\n",
      "Epoch 578 Batch   98/175   train_loss = 1.168\n",
      "Epoch 578 Batch  130/175   train_loss = 1.189\n",
      "Epoch 578 Batch  162/175   train_loss = 1.153\n",
      "Epoch 579 Batch   19/175   train_loss = 1.135\n",
      "Epoch 579 Batch   51/175   train_loss = 1.084\n",
      "Epoch 579 Batch   83/175   train_loss = 1.196\n",
      "Epoch 579 Batch  115/175   train_loss = 1.246\n",
      "Epoch 579 Batch  147/175   train_loss = 1.130\n",
      "Epoch 580 Batch    4/175   train_loss = 1.149\n",
      "Epoch 580 Batch   36/175   train_loss = 1.117\n",
      "Epoch 580 Batch   68/175   train_loss = 1.145\n",
      "Epoch 580 Batch  100/175   train_loss = 1.163\n",
      "Epoch 580 Batch  132/175   train_loss = 1.120\n",
      "Epoch 580 Batch  164/175   train_loss = 1.124\n",
      "Epoch 581 Batch   21/175   train_loss = 1.122\n",
      "Epoch 581 Batch   53/175   train_loss = 1.120\n",
      "Epoch 581 Batch   85/175   train_loss = 1.156\n",
      "Epoch 581 Batch  117/175   train_loss = 1.172\n",
      "Epoch 581 Batch  149/175   train_loss = 1.160\n",
      "Epoch 582 Batch    6/175   train_loss = 1.158\n",
      "Epoch 582 Batch   38/175   train_loss = 1.110\n",
      "Epoch 582 Batch   70/175   train_loss = 1.138\n",
      "Epoch 582 Batch  102/175   train_loss = 1.160\n",
      "Epoch 582 Batch  134/175   train_loss = 1.117\n",
      "Epoch 582 Batch  166/175   train_loss = 1.158\n",
      "Epoch 583 Batch   23/175   train_loss = 1.104\n",
      "Epoch 583 Batch   55/175   train_loss = 1.190\n",
      "Epoch 583 Batch   87/175   train_loss = 1.217\n",
      "Epoch 583 Batch  119/175   train_loss = 1.142\n",
      "Epoch 583 Batch  151/175   train_loss = 1.167\n",
      "Epoch 584 Batch    8/175   train_loss = 1.139\n",
      "Epoch 584 Batch   40/175   train_loss = 1.137\n",
      "Epoch 584 Batch   72/175   train_loss = 1.170\n",
      "Epoch 584 Batch  104/175   train_loss = 1.169\n",
      "Epoch 584 Batch  136/175   train_loss = 1.140\n",
      "Epoch 584 Batch  168/175   train_loss = 1.174\n",
      "Epoch 585 Batch   25/175   train_loss = 1.149\n",
      "Epoch 585 Batch   57/175   train_loss = 1.204\n",
      "Epoch 585 Batch   89/175   train_loss = 1.144\n",
      "Epoch 585 Batch  121/175   train_loss = 1.140\n",
      "Epoch 585 Batch  153/175   train_loss = 1.157\n",
      "Epoch 586 Batch   10/175   train_loss = 1.120\n",
      "Epoch 586 Batch   42/175   train_loss = 1.192\n",
      "Epoch 586 Batch   74/175   train_loss = 1.181\n",
      "Epoch 586 Batch  106/175   train_loss = 1.187\n",
      "Epoch 586 Batch  138/175   train_loss = 1.129\n",
      "Epoch 586 Batch  170/175   train_loss = 1.188\n",
      "Epoch 587 Batch   27/175   train_loss = 1.141\n",
      "Epoch 587 Batch   59/175   train_loss = 1.153\n",
      "Epoch 587 Batch   91/175   train_loss = 1.120\n",
      "Epoch 587 Batch  123/175   train_loss = 1.147\n",
      "Epoch 587 Batch  155/175   train_loss = 1.114\n",
      "Epoch 588 Batch   12/175   train_loss = 1.131\n",
      "Epoch 588 Batch   44/175   train_loss = 1.120\n",
      "Epoch 588 Batch   76/175   train_loss = 1.140\n",
      "Epoch 588 Batch  108/175   train_loss = 1.155\n",
      "Epoch 588 Batch  140/175   train_loss = 1.141\n",
      "Epoch 588 Batch  172/175   train_loss = 1.162\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 589 Batch   29/175   train_loss = 1.137\n",
      "Epoch 589 Batch   61/175   train_loss = 1.178\n",
      "Epoch 589 Batch   93/175   train_loss = 1.155\n",
      "Epoch 589 Batch  125/175   train_loss = 1.170\n",
      "Epoch 589 Batch  157/175   train_loss = 1.131\n",
      "Epoch 590 Batch   14/175   train_loss = 1.152\n",
      "Epoch 590 Batch   46/175   train_loss = 1.146\n",
      "Epoch 590 Batch   78/175   train_loss = 1.114\n",
      "Epoch 590 Batch  110/175   train_loss = 1.220\n",
      "Epoch 590 Batch  142/175   train_loss = 1.163\n",
      "Epoch 590 Batch  174/175   train_loss = 1.122\n",
      "Epoch 591 Batch   31/175   train_loss = 1.147\n",
      "Epoch 591 Batch   63/175   train_loss = 1.140\n",
      "Epoch 591 Batch   95/175   train_loss = 1.128\n",
      "Epoch 591 Batch  127/175   train_loss = 1.105\n",
      "Epoch 591 Batch  159/175   train_loss = 1.110\n",
      "Epoch 592 Batch   16/175   train_loss = 1.141\n",
      "Epoch 592 Batch   48/175   train_loss = 1.140\n",
      "Epoch 592 Batch   80/175   train_loss = 1.156\n",
      "Epoch 592 Batch  112/175   train_loss = 1.138\n",
      "Epoch 592 Batch  144/175   train_loss = 1.086\n",
      "Epoch 593 Batch    1/175   train_loss = 1.181\n",
      "Epoch 593 Batch   33/175   train_loss = 1.193\n",
      "Epoch 593 Batch   65/175   train_loss = 1.143\n",
      "Epoch 593 Batch   97/175   train_loss = 1.147\n",
      "Epoch 593 Batch  129/175   train_loss = 1.157\n",
      "Epoch 593 Batch  161/175   train_loss = 1.134\n",
      "Epoch 594 Batch   18/175   train_loss = 1.116\n",
      "Epoch 594 Batch   50/175   train_loss = 1.125\n",
      "Epoch 594 Batch   82/175   train_loss = 1.183\n",
      "Epoch 594 Batch  114/175   train_loss = 1.174\n",
      "Epoch 594 Batch  146/175   train_loss = 1.125\n",
      "Epoch 595 Batch    3/175   train_loss = 1.191\n",
      "Epoch 595 Batch   35/175   train_loss = 1.134\n",
      "Epoch 595 Batch   67/175   train_loss = 1.111\n",
      "Epoch 595 Batch   99/175   train_loss = 1.192\n",
      "Epoch 595 Batch  131/175   train_loss = 1.137\n",
      "Epoch 595 Batch  163/175   train_loss = 1.146\n",
      "Epoch 596 Batch   20/175   train_loss = 1.140\n",
      "Epoch 596 Batch   52/175   train_loss = 1.095\n",
      "Epoch 596 Batch   84/175   train_loss = 1.134\n",
      "Epoch 596 Batch  116/175   train_loss = 1.186\n",
      "Epoch 596 Batch  148/175   train_loss = 1.149\n",
      "Epoch 597 Batch    5/175   train_loss = 1.144\n",
      "Epoch 597 Batch   37/175   train_loss = 1.119\n",
      "Epoch 597 Batch   69/175   train_loss = 1.153\n",
      "Epoch 597 Batch  101/175   train_loss = 1.174\n",
      "Epoch 597 Batch  133/175   train_loss = 1.069\n",
      "Epoch 597 Batch  165/175   train_loss = 1.154\n",
      "Epoch 598 Batch   22/175   train_loss = 1.101\n",
      "Epoch 598 Batch   54/175   train_loss = 1.180\n",
      "Epoch 598 Batch   86/175   train_loss = 1.198\n",
      "Epoch 598 Batch  118/175   train_loss = 1.199\n",
      "Epoch 598 Batch  150/175   train_loss = 1.166\n",
      "Epoch 599 Batch    7/175   train_loss = 1.181\n",
      "Epoch 599 Batch   39/175   train_loss = 1.076\n",
      "Epoch 599 Batch   71/175   train_loss = 1.179\n",
      "Epoch 599 Batch  103/175   train_loss = 1.153\n",
      "Epoch 599 Batch  135/175   train_loss = 1.127\n",
      "Epoch 599 Batch  167/175   train_loss = 1.203\n",
      "Epoch 600 Batch   24/175   train_loss = 1.091\n",
      "Epoch 600 Batch   56/175   train_loss = 1.169\n",
      "Epoch 600 Batch   88/175   train_loss = 1.194\n",
      "Epoch 600 Batch  120/175   train_loss = 1.145\n",
      "Epoch 600 Batch  152/175   train_loss = 1.132\n",
      "Epoch 601 Batch    9/175   train_loss = 1.166\n",
      "Epoch 601 Batch   41/175   train_loss = 1.183\n",
      "Epoch 601 Batch   73/175   train_loss = 1.148\n",
      "Epoch 601 Batch  105/175   train_loss = 1.192\n",
      "Epoch 601 Batch  137/175   train_loss = 1.110\n",
      "Epoch 601 Batch  169/175   train_loss = 1.164\n",
      "Epoch 602 Batch   26/175   train_loss = 1.157\n",
      "Epoch 602 Batch   58/175   train_loss = 1.189\n",
      "Epoch 602 Batch   90/175   train_loss = 1.174\n",
      "Epoch 602 Batch  122/175   train_loss = 1.133\n",
      "Epoch 602 Batch  154/175   train_loss = 1.120\n",
      "Epoch 603 Batch   11/175   train_loss = 1.149\n",
      "Epoch 603 Batch   43/175   train_loss = 1.123\n",
      "Epoch 603 Batch   75/175   train_loss = 1.127\n",
      "Epoch 603 Batch  107/175   train_loss = 1.196\n",
      "Epoch 603 Batch  139/175   train_loss = 1.131\n",
      "Epoch 603 Batch  171/175   train_loss = 1.175\n",
      "Epoch 604 Batch   28/175   train_loss = 1.144\n",
      "Epoch 604 Batch   60/175   train_loss = 1.164\n",
      "Epoch 604 Batch   92/175   train_loss = 1.113\n",
      "Epoch 604 Batch  124/175   train_loss = 1.161\n",
      "Epoch 604 Batch  156/175   train_loss = 1.167\n",
      "Epoch 605 Batch   13/175   train_loss = 1.140\n",
      "Epoch 605 Batch   45/175   train_loss = 1.144\n",
      "Epoch 605 Batch   77/175   train_loss = 1.137\n",
      "Epoch 605 Batch  109/175   train_loss = 1.179\n",
      "Epoch 605 Batch  141/175   train_loss = 1.123\n",
      "Epoch 605 Batch  173/175   train_loss = 1.131\n",
      "Epoch 606 Batch   30/175   train_loss = 1.196\n",
      "Epoch 606 Batch   62/175   train_loss = 1.183\n",
      "Epoch 606 Batch   94/175   train_loss = 1.144\n",
      "Epoch 606 Batch  126/175   train_loss = 1.150\n",
      "Epoch 606 Batch  158/175   train_loss = 1.149\n",
      "Epoch 607 Batch   15/175   train_loss = 1.183\n",
      "Epoch 607 Batch   47/175   train_loss = 1.159\n",
      "Epoch 607 Batch   79/175   train_loss = 1.183\n",
      "Epoch 607 Batch  111/175   train_loss = 1.189\n",
      "Epoch 607 Batch  143/175   train_loss = 1.123\n",
      "Epoch 608 Batch    0/175   train_loss = 1.167\n",
      "Epoch 608 Batch   32/175   train_loss = 1.171\n",
      "Epoch 608 Batch   64/175   train_loss = 1.186\n",
      "Epoch 608 Batch   96/175   train_loss = 1.169\n",
      "Epoch 608 Batch  128/175   train_loss = 1.114\n",
      "Epoch 608 Batch  160/175   train_loss = 1.146\n",
      "Epoch 609 Batch   17/175   train_loss = 1.114\n",
      "Epoch 609 Batch   49/175   train_loss = 1.156\n",
      "Epoch 609 Batch   81/175   train_loss = 1.116\n",
      "Epoch 609 Batch  113/175   train_loss = 1.155\n",
      "Epoch 609 Batch  145/175   train_loss = 1.113\n",
      "Epoch 610 Batch    2/175   train_loss = 1.160\n",
      "Epoch 610 Batch   34/175   train_loss = 1.165\n",
      "Epoch 610 Batch   66/175   train_loss = 1.190\n",
      "Epoch 610 Batch   98/175   train_loss = 1.178\n",
      "Epoch 610 Batch  130/175   train_loss = 1.164\n",
      "Epoch 610 Batch  162/175   train_loss = 1.152\n",
      "Epoch 611 Batch   19/175   train_loss = 1.128\n",
      "Epoch 611 Batch   51/175   train_loss = 1.080\n",
      "Epoch 611 Batch   83/175   train_loss = 1.184\n",
      "Epoch 611 Batch  115/175   train_loss = 1.239\n",
      "Epoch 611 Batch  147/175   train_loss = 1.120\n",
      "Epoch 612 Batch    4/175   train_loss = 1.156\n",
      "Epoch 612 Batch   36/175   train_loss = 1.112\n",
      "Epoch 612 Batch   68/175   train_loss = 1.136\n",
      "Epoch 612 Batch  100/175   train_loss = 1.145\n",
      "Epoch 612 Batch  132/175   train_loss = 1.119\n",
      "Epoch 612 Batch  164/175   train_loss = 1.119\n",
      "Epoch 613 Batch   21/175   train_loss = 1.102\n",
      "Epoch 613 Batch   53/175   train_loss = 1.129\n",
      "Epoch 613 Batch   85/175   train_loss = 1.160\n",
      "Epoch 613 Batch  117/175   train_loss = 1.154\n",
      "Epoch 613 Batch  149/175   train_loss = 1.162\n",
      "Epoch 614 Batch    6/175   train_loss = 1.161\n",
      "Epoch 614 Batch   38/175   train_loss = 1.119\n",
      "Epoch 614 Batch   70/175   train_loss = 1.148\n",
      "Epoch 614 Batch  102/175   train_loss = 1.156\n",
      "Epoch 614 Batch  134/175   train_loss = 1.102\n",
      "Epoch 614 Batch  166/175   train_loss = 1.153\n",
      "Epoch 615 Batch   23/175   train_loss = 1.124\n",
      "Epoch 615 Batch   55/175   train_loss = 1.189\n",
      "Epoch 615 Batch   87/175   train_loss = 1.217\n",
      "Epoch 615 Batch  119/175   train_loss = 1.130\n",
      "Epoch 615 Batch  151/175   train_loss = 1.169\n",
      "Epoch 616 Batch    8/175   train_loss = 1.152\n",
      "Epoch 616 Batch   40/175   train_loss = 1.149\n",
      "Epoch 616 Batch   72/175   train_loss = 1.182\n",
      "Epoch 616 Batch  104/175   train_loss = 1.160\n",
      "Epoch 616 Batch  136/175   train_loss = 1.125\n",
      "Epoch 616 Batch  168/175   train_loss = 1.174\n",
      "Epoch 617 Batch   25/175   train_loss = 1.164\n",
      "Epoch 617 Batch   57/175   train_loss = 1.190\n",
      "Epoch 617 Batch   89/175   train_loss = 1.140\n",
      "Epoch 617 Batch  121/175   train_loss = 1.130\n",
      "Epoch 617 Batch  153/175   train_loss = 1.153\n",
      "Epoch 618 Batch   10/175   train_loss = 1.107\n",
      "Epoch 618 Batch   42/175   train_loss = 1.190\n",
      "Epoch 618 Batch   74/175   train_loss = 1.189\n",
      "Epoch 618 Batch  106/175   train_loss = 1.187\n",
      "Epoch 618 Batch  138/175   train_loss = 1.132\n",
      "Epoch 618 Batch  170/175   train_loss = 1.192\n",
      "Epoch 619 Batch   27/175   train_loss = 1.134\n",
      "Epoch 619 Batch   59/175   train_loss = 1.138\n",
      "Epoch 619 Batch   91/175   train_loss = 1.145\n",
      "Epoch 619 Batch  123/175   train_loss = 1.134\n",
      "Epoch 619 Batch  155/175   train_loss = 1.099\n",
      "Epoch 620 Batch   12/175   train_loss = 1.133\n",
      "Epoch 620 Batch   44/175   train_loss = 1.130\n",
      "Epoch 620 Batch   76/175   train_loss = 1.123\n",
      "Epoch 620 Batch  108/175   train_loss = 1.166\n",
      "Epoch 620 Batch  140/175   train_loss = 1.132\n",
      "Epoch 620 Batch  172/175   train_loss = 1.161\n",
      "Epoch 621 Batch   29/175   train_loss = 1.159\n",
      "Epoch 621 Batch   61/175   train_loss = 1.171\n",
      "Epoch 621 Batch   93/175   train_loss = 1.152\n",
      "Epoch 621 Batch  125/175   train_loss = 1.156\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 621 Batch  157/175   train_loss = 1.128\n",
      "Epoch 622 Batch   14/175   train_loss = 1.161\n",
      "Epoch 622 Batch   46/175   train_loss = 1.157\n",
      "Epoch 622 Batch   78/175   train_loss = 1.118\n",
      "Epoch 622 Batch  110/175   train_loss = 1.216\n",
      "Epoch 622 Batch  142/175   train_loss = 1.161\n",
      "Epoch 622 Batch  174/175   train_loss = 1.130\n",
      "Epoch 623 Batch   31/175   train_loss = 1.148\n",
      "Epoch 623 Batch   63/175   train_loss = 1.154\n",
      "Epoch 623 Batch   95/175   train_loss = 1.118\n",
      "Epoch 623 Batch  127/175   train_loss = 1.106\n",
      "Epoch 623 Batch  159/175   train_loss = 1.127\n",
      "Epoch 624 Batch   16/175   train_loss = 1.155\n",
      "Epoch 624 Batch   48/175   train_loss = 1.132\n",
      "Epoch 624 Batch   80/175   train_loss = 1.171\n",
      "Epoch 624 Batch  112/175   train_loss = 1.136\n",
      "Epoch 624 Batch  144/175   train_loss = 1.087\n",
      "Epoch 625 Batch    1/175   train_loss = 1.170\n",
      "Epoch 625 Batch   33/175   train_loss = 1.186\n",
      "Epoch 625 Batch   65/175   train_loss = 1.148\n",
      "Epoch 625 Batch   97/175   train_loss = 1.153\n",
      "Epoch 625 Batch  129/175   train_loss = 1.124\n",
      "Epoch 625 Batch  161/175   train_loss = 1.137\n",
      "Epoch 626 Batch   18/175   train_loss = 1.131\n",
      "Epoch 626 Batch   50/175   train_loss = 1.115\n",
      "Epoch 626 Batch   82/175   train_loss = 1.187\n",
      "Epoch 626 Batch  114/175   train_loss = 1.184\n",
      "Epoch 626 Batch  146/175   train_loss = 1.133\n",
      "Epoch 627 Batch    3/175   train_loss = 1.179\n",
      "Epoch 627 Batch   35/175   train_loss = 1.160\n",
      "Epoch 627 Batch   67/175   train_loss = 1.125\n",
      "Epoch 627 Batch   99/175   train_loss = 1.214\n",
      "Epoch 627 Batch  131/175   train_loss = 1.161\n",
      "Epoch 627 Batch  163/175   train_loss = 1.142\n",
      "Epoch 628 Batch   20/175   train_loss = 1.142\n",
      "Epoch 628 Batch   52/175   train_loss = 1.097\n",
      "Epoch 628 Batch   84/175   train_loss = 1.149\n",
      "Epoch 628 Batch  116/175   train_loss = 1.209\n",
      "Epoch 628 Batch  148/175   train_loss = 1.149\n",
      "Epoch 629 Batch    5/175   train_loss = 1.150\n",
      "Epoch 629 Batch   37/175   train_loss = 1.133\n",
      "Epoch 629 Batch   69/175   train_loss = 1.156\n",
      "Epoch 629 Batch  101/175   train_loss = 1.190\n",
      "Epoch 629 Batch  133/175   train_loss = 1.064\n",
      "Epoch 629 Batch  165/175   train_loss = 1.145\n",
      "Epoch 630 Batch   22/175   train_loss = 1.085\n",
      "Epoch 630 Batch   54/175   train_loss = 1.186\n",
      "Epoch 630 Batch   86/175   train_loss = 1.216\n",
      "Epoch 630 Batch  118/175   train_loss = 1.182\n",
      "Epoch 630 Batch  150/175   train_loss = 1.161\n",
      "Epoch 631 Batch    7/175   train_loss = 1.168\n",
      "Epoch 631 Batch   39/175   train_loss = 1.091\n",
      "Epoch 631 Batch   71/175   train_loss = 1.173\n",
      "Epoch 631 Batch  103/175   train_loss = 1.156\n",
      "Epoch 631 Batch  135/175   train_loss = 1.122\n",
      "Epoch 631 Batch  167/175   train_loss = 1.204\n",
      "Epoch 632 Batch   24/175   train_loss = 1.087\n",
      "Epoch 632 Batch   56/175   train_loss = 1.164\n",
      "Epoch 632 Batch   88/175   train_loss = 1.197\n",
      "Epoch 632 Batch  120/175   train_loss = 1.137\n",
      "Epoch 632 Batch  152/175   train_loss = 1.120\n",
      "Epoch 633 Batch    9/175   train_loss = 1.177\n",
      "Epoch 633 Batch   41/175   train_loss = 1.179\n",
      "Epoch 633 Batch   73/175   train_loss = 1.159\n",
      "Epoch 633 Batch  105/175   train_loss = 1.218\n",
      "Epoch 633 Batch  137/175   train_loss = 1.129\n",
      "Epoch 633 Batch  169/175   train_loss = 1.181\n",
      "Epoch 634 Batch   26/175   train_loss = 1.153\n",
      "Epoch 634 Batch   58/175   train_loss = 1.183\n",
      "Epoch 634 Batch   90/175   train_loss = 1.174\n",
      "Epoch 634 Batch  122/175   train_loss = 1.119\n",
      "Epoch 634 Batch  154/175   train_loss = 1.123\n",
      "Epoch 635 Batch   11/175   train_loss = 1.151\n",
      "Epoch 635 Batch   43/175   train_loss = 1.114\n",
      "Epoch 635 Batch   75/175   train_loss = 1.133\n",
      "Epoch 635 Batch  107/175   train_loss = 1.200\n",
      "Epoch 635 Batch  139/175   train_loss = 1.102\n",
      "Epoch 635 Batch  171/175   train_loss = 1.182\n",
      "Epoch 636 Batch   28/175   train_loss = 1.162\n",
      "Epoch 636 Batch   60/175   train_loss = 1.153\n",
      "Epoch 636 Batch   92/175   train_loss = 1.131\n",
      "Epoch 636 Batch  124/175   train_loss = 1.163\n",
      "Epoch 636 Batch  156/175   train_loss = 1.184\n",
      "Epoch 637 Batch   13/175   train_loss = 1.140\n",
      "Epoch 637 Batch   45/175   train_loss = 1.152\n",
      "Epoch 637 Batch   77/175   train_loss = 1.134\n",
      "Epoch 637 Batch  109/175   train_loss = 1.183\n",
      "Epoch 637 Batch  141/175   train_loss = 1.110\n",
      "Epoch 637 Batch  173/175   train_loss = 1.130\n",
      "Epoch 638 Batch   30/175   train_loss = 1.196\n",
      "Epoch 638 Batch   62/175   train_loss = 1.175\n",
      "Epoch 638 Batch   94/175   train_loss = 1.137\n",
      "Epoch 638 Batch  126/175   train_loss = 1.150\n",
      "Epoch 638 Batch  158/175   train_loss = 1.134\n",
      "Epoch 639 Batch   15/175   train_loss = 1.188\n",
      "Epoch 639 Batch   47/175   train_loss = 1.162\n",
      "Epoch 639 Batch   79/175   train_loss = 1.183\n",
      "Epoch 639 Batch  111/175   train_loss = 1.206\n",
      "Epoch 639 Batch  143/175   train_loss = 1.129\n",
      "Epoch 640 Batch    0/175   train_loss = 1.154\n",
      "Epoch 640 Batch   32/175   train_loss = 1.178\n",
      "Epoch 640 Batch   64/175   train_loss = 1.179\n",
      "Epoch 640 Batch   96/175   train_loss = 1.178\n",
      "Epoch 640 Batch  128/175   train_loss = 1.110\n",
      "Epoch 640 Batch  160/175   train_loss = 1.153\n",
      "Epoch 641 Batch   17/175   train_loss = 1.120\n",
      "Epoch 641 Batch   49/175   train_loss = 1.158\n",
      "Epoch 641 Batch   81/175   train_loss = 1.144\n",
      "Epoch 641 Batch  113/175   train_loss = 1.141\n",
      "Epoch 641 Batch  145/175   train_loss = 1.123\n",
      "Epoch 642 Batch    2/175   train_loss = 1.170\n",
      "Epoch 642 Batch   34/175   train_loss = 1.172\n",
      "Epoch 642 Batch   66/175   train_loss = 1.176\n",
      "Epoch 642 Batch   98/175   train_loss = 1.185\n",
      "Epoch 642 Batch  130/175   train_loss = 1.182\n",
      "Epoch 642 Batch  162/175   train_loss = 1.160\n",
      "Epoch 643 Batch   19/175   train_loss = 1.147\n",
      "Epoch 643 Batch   51/175   train_loss = 1.105\n",
      "Epoch 643 Batch   83/175   train_loss = 1.200\n",
      "Epoch 643 Batch  115/175   train_loss = 1.237\n",
      "Epoch 643 Batch  147/175   train_loss = 1.123\n",
      "Epoch 644 Batch    4/175   train_loss = 1.163\n",
      "Epoch 644 Batch   36/175   train_loss = 1.113\n",
      "Epoch 644 Batch   68/175   train_loss = 1.136\n",
      "Epoch 644 Batch  100/175   train_loss = 1.169\n",
      "Epoch 644 Batch  132/175   train_loss = 1.134\n",
      "Epoch 644 Batch  164/175   train_loss = 1.124\n",
      "Epoch 645 Batch   21/175   train_loss = 1.120\n",
      "Epoch 645 Batch   53/175   train_loss = 1.139\n",
      "Epoch 645 Batch   85/175   train_loss = 1.160\n",
      "Epoch 645 Batch  117/175   train_loss = 1.170\n",
      "Epoch 645 Batch  149/175   train_loss = 1.160\n",
      "Epoch 646 Batch    6/175   train_loss = 1.180\n",
      "Epoch 646 Batch   38/175   train_loss = 1.119\n",
      "Epoch 646 Batch   70/175   train_loss = 1.152\n",
      "Epoch 646 Batch  102/175   train_loss = 1.163\n",
      "Epoch 646 Batch  134/175   train_loss = 1.126\n",
      "Epoch 646 Batch  166/175   train_loss = 1.166\n",
      "Epoch 647 Batch   23/175   train_loss = 1.114\n",
      "Epoch 647 Batch   55/175   train_loss = 1.223\n",
      "Epoch 647 Batch   87/175   train_loss = 1.237\n",
      "Epoch 647 Batch  119/175   train_loss = 1.139\n",
      "Epoch 647 Batch  151/175   train_loss = 1.182\n",
      "Epoch 648 Batch    8/175   train_loss = 1.167\n",
      "Epoch 648 Batch   40/175   train_loss = 1.143\n",
      "Epoch 648 Batch   72/175   train_loss = 1.180\n",
      "Epoch 648 Batch  104/175   train_loss = 1.170\n",
      "Epoch 648 Batch  136/175   train_loss = 1.141\n",
      "Epoch 648 Batch  168/175   train_loss = 1.167\n",
      "Epoch 649 Batch   25/175   train_loss = 1.173\n",
      "Epoch 649 Batch   57/175   train_loss = 1.203\n",
      "Epoch 649 Batch   89/175   train_loss = 1.165\n",
      "Epoch 649 Batch  121/175   train_loss = 1.142\n",
      "Epoch 649 Batch  153/175   train_loss = 1.155\n",
      "Epoch 650 Batch   10/175   train_loss = 1.127\n",
      "Epoch 650 Batch   42/175   train_loss = 1.205\n",
      "Epoch 650 Batch   74/175   train_loss = 1.191\n",
      "Epoch 650 Batch  106/175   train_loss = 1.194\n",
      "Epoch 650 Batch  138/175   train_loss = 1.143\n",
      "Epoch 650 Batch  170/175   train_loss = 1.186\n",
      "Epoch 651 Batch   27/175   train_loss = 1.154\n",
      "Epoch 651 Batch   59/175   train_loss = 1.131\n",
      "Epoch 651 Batch   91/175   train_loss = 1.142\n",
      "Epoch 651 Batch  123/175   train_loss = 1.155\n",
      "Epoch 651 Batch  155/175   train_loss = 1.118\n",
      "Epoch 652 Batch   12/175   train_loss = 1.155\n",
      "Epoch 652 Batch   44/175   train_loss = 1.142\n",
      "Epoch 652 Batch   76/175   train_loss = 1.119\n",
      "Epoch 652 Batch  108/175   train_loss = 1.166\n",
      "Epoch 652 Batch  140/175   train_loss = 1.138\n",
      "Epoch 652 Batch  172/175   train_loss = 1.182\n",
      "Epoch 653 Batch   29/175   train_loss = 1.149\n",
      "Epoch 653 Batch   61/175   train_loss = 1.166\n",
      "Epoch 653 Batch   93/175   train_loss = 1.162\n",
      "Epoch 653 Batch  125/175   train_loss = 1.169\n",
      "Epoch 653 Batch  157/175   train_loss = 1.144\n",
      "Epoch 654 Batch   14/175   train_loss = 1.165\n",
      "Epoch 654 Batch   46/175   train_loss = 1.172\n",
      "Epoch 654 Batch   78/175   train_loss = 1.141\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 654 Batch  110/175   train_loss = 1.223\n",
      "Epoch 654 Batch  142/175   train_loss = 1.158\n",
      "Epoch 654 Batch  174/175   train_loss = 1.128\n",
      "Epoch 655 Batch   31/175   train_loss = 1.159\n",
      "Epoch 655 Batch   63/175   train_loss = 1.167\n",
      "Epoch 655 Batch   95/175   train_loss = 1.146\n",
      "Epoch 655 Batch  127/175   train_loss = 1.118\n",
      "Epoch 655 Batch  159/175   train_loss = 1.120\n",
      "Epoch 656 Batch   16/175   train_loss = 1.144\n",
      "Epoch 656 Batch   48/175   train_loss = 1.148\n",
      "Epoch 656 Batch   80/175   train_loss = 1.181\n",
      "Epoch 656 Batch  112/175   train_loss = 1.137\n",
      "Epoch 656 Batch  144/175   train_loss = 1.085\n",
      "Epoch 657 Batch    1/175   train_loss = 1.186\n",
      "Epoch 657 Batch   33/175   train_loss = 1.187\n",
      "Epoch 657 Batch   65/175   train_loss = 1.146\n",
      "Epoch 657 Batch   97/175   train_loss = 1.149\n",
      "Epoch 657 Batch  129/175   train_loss = 1.119\n",
      "Epoch 657 Batch  161/175   train_loss = 1.132\n",
      "Epoch 658 Batch   18/175   train_loss = 1.121\n",
      "Epoch 658 Batch   50/175   train_loss = 1.129\n",
      "Epoch 658 Batch   82/175   train_loss = 1.191\n",
      "Epoch 658 Batch  114/175   train_loss = 1.180\n",
      "Epoch 658 Batch  146/175   train_loss = 1.147\n",
      "Epoch 659 Batch    3/175   train_loss = 1.187\n",
      "Epoch 659 Batch   35/175   train_loss = 1.154\n",
      "Epoch 659 Batch   67/175   train_loss = 1.144\n",
      "Epoch 659 Batch   99/175   train_loss = 1.204\n",
      "Epoch 659 Batch  131/175   train_loss = 1.154\n",
      "Epoch 659 Batch  163/175   train_loss = 1.160\n",
      "Epoch 660 Batch   20/175   train_loss = 1.149\n",
      "Epoch 660 Batch   52/175   train_loss = 1.095\n",
      "Epoch 660 Batch   84/175   train_loss = 1.154\n",
      "Epoch 660 Batch  116/175   train_loss = 1.207\n",
      "Epoch 660 Batch  148/175   train_loss = 1.162\n",
      "Epoch 661 Batch    5/175   train_loss = 1.164\n",
      "Epoch 661 Batch   37/175   train_loss = 1.138\n",
      "Epoch 661 Batch   69/175   train_loss = 1.175\n",
      "Epoch 661 Batch  101/175   train_loss = 1.215\n",
      "Epoch 661 Batch  133/175   train_loss = 1.073\n",
      "Epoch 661 Batch  165/175   train_loss = 1.166\n",
      "Epoch 662 Batch   22/175   train_loss = 1.110\n",
      "Epoch 662 Batch   54/175   train_loss = 1.196\n",
      "Epoch 662 Batch   86/175   train_loss = 1.230\n",
      "Epoch 662 Batch  118/175   train_loss = 1.189\n",
      "Epoch 662 Batch  150/175   train_loss = 1.154\n",
      "Epoch 663 Batch    7/175   train_loss = 1.184\n",
      "Epoch 663 Batch   39/175   train_loss = 1.108\n",
      "Epoch 663 Batch   71/175   train_loss = 1.189\n",
      "Epoch 663 Batch  103/175   train_loss = 1.157\n",
      "Epoch 663 Batch  135/175   train_loss = 1.136\n",
      "Epoch 663 Batch  167/175   train_loss = 1.182\n",
      "Epoch 664 Batch   24/175   train_loss = 1.105\n",
      "Epoch 664 Batch   56/175   train_loss = 1.175\n",
      "Epoch 664 Batch   88/175   train_loss = 1.197\n",
      "Epoch 664 Batch  120/175   train_loss = 1.135\n",
      "Epoch 664 Batch  152/175   train_loss = 1.130\n",
      "Epoch 665 Batch    9/175   train_loss = 1.176\n",
      "Epoch 665 Batch   41/175   train_loss = 1.172\n",
      "Epoch 665 Batch   73/175   train_loss = 1.167\n",
      "Epoch 665 Batch  105/175   train_loss = 1.198\n",
      "Epoch 665 Batch  137/175   train_loss = 1.132\n",
      "Epoch 665 Batch  169/175   train_loss = 1.195\n",
      "Epoch 666 Batch   26/175   train_loss = 1.179\n",
      "Epoch 666 Batch   58/175   train_loss = 1.189\n",
      "Epoch 666 Batch   90/175   train_loss = 1.170\n",
      "Epoch 666 Batch  122/175   train_loss = 1.120\n",
      "Epoch 666 Batch  154/175   train_loss = 1.147\n",
      "Epoch 667 Batch   11/175   train_loss = 1.137\n",
      "Epoch 667 Batch   43/175   train_loss = 1.144\n",
      "Epoch 667 Batch   75/175   train_loss = 1.143\n",
      "Epoch 667 Batch  107/175   train_loss = 1.201\n",
      "Epoch 667 Batch  139/175   train_loss = 1.120\n",
      "Epoch 667 Batch  171/175   train_loss = 1.189\n",
      "Epoch 668 Batch   28/175   train_loss = 1.153\n",
      "Epoch 668 Batch   60/175   train_loss = 1.165\n",
      "Epoch 668 Batch   92/175   train_loss = 1.128\n",
      "Epoch 668 Batch  124/175   train_loss = 1.170\n",
      "Epoch 668 Batch  156/175   train_loss = 1.206\n",
      "Epoch 669 Batch   13/175   train_loss = 1.157\n",
      "Epoch 669 Batch   45/175   train_loss = 1.144\n",
      "Epoch 669 Batch   77/175   train_loss = 1.138\n",
      "Epoch 669 Batch  109/175   train_loss = 1.193\n",
      "Epoch 669 Batch  141/175   train_loss = 1.120\n",
      "Epoch 669 Batch  173/175   train_loss = 1.135\n",
      "Epoch 670 Batch   30/175   train_loss = 1.190\n",
      "Epoch 670 Batch   62/175   train_loss = 1.189\n",
      "Epoch 670 Batch   94/175   train_loss = 1.140\n",
      "Epoch 670 Batch  126/175   train_loss = 1.167\n",
      "Epoch 670 Batch  158/175   train_loss = 1.150\n",
      "Epoch 671 Batch   15/175   train_loss = 1.182\n",
      "Epoch 671 Batch   47/175   train_loss = 1.165\n",
      "Epoch 671 Batch   79/175   train_loss = 1.202\n",
      "Epoch 671 Batch  111/175   train_loss = 1.219\n",
      "Epoch 671 Batch  143/175   train_loss = 1.137\n",
      "Epoch 672 Batch    0/175   train_loss = 1.164\n",
      "Epoch 672 Batch   32/175   train_loss = 1.173\n",
      "Epoch 672 Batch   64/175   train_loss = 1.182\n",
      "Epoch 672 Batch   96/175   train_loss = 1.176\n",
      "Epoch 672 Batch  128/175   train_loss = 1.126\n",
      "Epoch 672 Batch  160/175   train_loss = 1.151\n",
      "Epoch 673 Batch   17/175   train_loss = 1.134\n",
      "Epoch 673 Batch   49/175   train_loss = 1.164\n",
      "Epoch 673 Batch   81/175   train_loss = 1.130\n",
      "Epoch 673 Batch  113/175   train_loss = 1.166\n",
      "Epoch 673 Batch  145/175   train_loss = 1.113\n",
      "Epoch 674 Batch    2/175   train_loss = 1.169\n",
      "Epoch 674 Batch   34/175   train_loss = 1.157\n",
      "Epoch 674 Batch   66/175   train_loss = 1.178\n",
      "Epoch 674 Batch   98/175   train_loss = 1.169\n",
      "Epoch 674 Batch  130/175   train_loss = 1.180\n",
      "Epoch 674 Batch  162/175   train_loss = 1.161\n",
      "Epoch 675 Batch   19/175   train_loss = 1.132\n",
      "Epoch 675 Batch   51/175   train_loss = 1.109\n",
      "Epoch 675 Batch   83/175   train_loss = 1.200\n",
      "Epoch 675 Batch  115/175   train_loss = 1.260\n",
      "Epoch 675 Batch  147/175   train_loss = 1.129\n",
      "Epoch 676 Batch    4/175   train_loss = 1.158\n",
      "Epoch 676 Batch   36/175   train_loss = 1.121\n",
      "Epoch 676 Batch   68/175   train_loss = 1.145\n",
      "Epoch 676 Batch  100/175   train_loss = 1.161\n",
      "Epoch 676 Batch  132/175   train_loss = 1.133\n",
      "Epoch 676 Batch  164/175   train_loss = 1.134\n",
      "Epoch 677 Batch   21/175   train_loss = 1.124\n",
      "Epoch 677 Batch   53/175   train_loss = 1.138\n",
      "Epoch 677 Batch   85/175   train_loss = 1.163\n",
      "Epoch 677 Batch  117/175   train_loss = 1.175\n",
      "Epoch 677 Batch  149/175   train_loss = 1.162\n",
      "Epoch 678 Batch    6/175   train_loss = 1.186\n",
      "Epoch 678 Batch   38/175   train_loss = 1.122\n",
      "Epoch 678 Batch   70/175   train_loss = 1.147\n",
      "Epoch 678 Batch  102/175   train_loss = 1.170\n",
      "Epoch 678 Batch  134/175   train_loss = 1.123\n",
      "Epoch 678 Batch  166/175   train_loss = 1.167\n",
      "Epoch 679 Batch   23/175   train_loss = 1.094\n",
      "Epoch 679 Batch   55/175   train_loss = 1.197\n",
      "Epoch 679 Batch   87/175   train_loss = 1.214\n",
      "Epoch 679 Batch  119/175   train_loss = 1.145\n",
      "Epoch 679 Batch  151/175   train_loss = 1.166\n",
      "Epoch 680 Batch    8/175   train_loss = 1.157\n",
      "Epoch 680 Batch   40/175   train_loss = 1.138\n",
      "Epoch 680 Batch   72/175   train_loss = 1.166\n",
      "Epoch 680 Batch  104/175   train_loss = 1.136\n",
      "Epoch 680 Batch  136/175   train_loss = 1.137\n",
      "Epoch 680 Batch  168/175   train_loss = 1.172\n",
      "Epoch 681 Batch   25/175   train_loss = 1.152\n",
      "Epoch 681 Batch   57/175   train_loss = 1.192\n",
      "Epoch 681 Batch   89/175   train_loss = 1.152\n",
      "Epoch 681 Batch  121/175   train_loss = 1.143\n",
      "Epoch 681 Batch  153/175   train_loss = 1.150\n",
      "Epoch 682 Batch   10/175   train_loss = 1.112\n",
      "Epoch 682 Batch   42/175   train_loss = 1.194\n",
      "Epoch 682 Batch   74/175   train_loss = 1.188\n",
      "Epoch 682 Batch  106/175   train_loss = 1.186\n",
      "Epoch 682 Batch  138/175   train_loss = 1.136\n",
      "Epoch 682 Batch  170/175   train_loss = 1.194\n",
      "Epoch 683 Batch   27/175   train_loss = 1.151\n",
      "Epoch 683 Batch   59/175   train_loss = 1.144\n",
      "Epoch 683 Batch   91/175   train_loss = 1.139\n",
      "Epoch 683 Batch  123/175   train_loss = 1.143\n",
      "Epoch 683 Batch  155/175   train_loss = 1.109\n",
      "Epoch 684 Batch   12/175   train_loss = 1.160\n",
      "Epoch 684 Batch   44/175   train_loss = 1.150\n",
      "Epoch 684 Batch   76/175   train_loss = 1.122\n",
      "Epoch 684 Batch  108/175   train_loss = 1.174\n",
      "Epoch 684 Batch  140/175   train_loss = 1.144\n",
      "Epoch 684 Batch  172/175   train_loss = 1.196\n",
      "Epoch 685 Batch   29/175   train_loss = 1.146\n",
      "Epoch 685 Batch   61/175   train_loss = 1.165\n",
      "Epoch 685 Batch   93/175   train_loss = 1.158\n",
      "Epoch 685 Batch  125/175   train_loss = 1.180\n",
      "Epoch 685 Batch  157/175   train_loss = 1.160\n",
      "Epoch 686 Batch   14/175   train_loss = 1.164\n",
      "Epoch 686 Batch   46/175   train_loss = 1.169\n",
      "Epoch 686 Batch   78/175   train_loss = 1.139\n",
      "Epoch 686 Batch  110/175   train_loss = 1.232\n",
      "Epoch 686 Batch  142/175   train_loss = 1.174\n",
      "Epoch 686 Batch  174/175   train_loss = 1.129\n",
      "Epoch 687 Batch   31/175   train_loss = 1.174\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 687 Batch   63/175   train_loss = 1.191\n",
      "Epoch 687 Batch   95/175   train_loss = 1.149\n",
      "Epoch 687 Batch  127/175   train_loss = 1.112\n",
      "Epoch 687 Batch  159/175   train_loss = 1.135\n",
      "Epoch 688 Batch   16/175   train_loss = 1.167\n",
      "Epoch 688 Batch   48/175   train_loss = 1.150\n",
      "Epoch 688 Batch   80/175   train_loss = 1.181\n",
      "Epoch 688 Batch  112/175   train_loss = 1.157\n",
      "Epoch 688 Batch  144/175   train_loss = 1.090\n",
      "Epoch 689 Batch    1/175   train_loss = 1.184\n",
      "Epoch 689 Batch   33/175   train_loss = 1.199\n",
      "Epoch 689 Batch   65/175   train_loss = 1.155\n",
      "Epoch 689 Batch   97/175   train_loss = 1.155\n",
      "Epoch 689 Batch  129/175   train_loss = 1.129\n",
      "Epoch 689 Batch  161/175   train_loss = 1.131\n",
      "Epoch 690 Batch   18/175   train_loss = 1.134\n",
      "Epoch 690 Batch   50/175   train_loss = 1.131\n",
      "Epoch 690 Batch   82/175   train_loss = 1.198\n",
      "Epoch 690 Batch  114/175   train_loss = 1.199\n",
      "Epoch 690 Batch  146/175   train_loss = 1.154\n",
      "Epoch 691 Batch    3/175   train_loss = 1.185\n",
      "Epoch 691 Batch   35/175   train_loss = 1.141\n",
      "Epoch 691 Batch   67/175   train_loss = 1.137\n",
      "Epoch 691 Batch   99/175   train_loss = 1.203\n",
      "Epoch 691 Batch  131/175   train_loss = 1.166\n",
      "Epoch 691 Batch  163/175   train_loss = 1.145\n",
      "Epoch 692 Batch   20/175   train_loss = 1.158\n",
      "Epoch 692 Batch   52/175   train_loss = 1.090\n",
      "Epoch 692 Batch   84/175   train_loss = 1.159\n",
      "Epoch 692 Batch  116/175   train_loss = 1.186\n",
      "Epoch 692 Batch  148/175   train_loss = 1.154\n",
      "Epoch 693 Batch    5/175   train_loss = 1.146\n",
      "Epoch 693 Batch   37/175   train_loss = 1.165\n",
      "Epoch 693 Batch   69/175   train_loss = 1.150\n",
      "Epoch 693 Batch  101/175   train_loss = 1.200\n",
      "Epoch 693 Batch  133/175   train_loss = 1.082\n",
      "Epoch 693 Batch  165/175   train_loss = 1.171\n",
      "Epoch 694 Batch   22/175   train_loss = 1.101\n",
      "Epoch 694 Batch   54/175   train_loss = 1.180\n",
      "Epoch 694 Batch   86/175   train_loss = 1.217\n",
      "Epoch 694 Batch  118/175   train_loss = 1.200\n",
      "Epoch 694 Batch  150/175   train_loss = 1.168\n",
      "Epoch 695 Batch    7/175   train_loss = 1.175\n",
      "Epoch 695 Batch   39/175   train_loss = 1.090\n",
      "Epoch 695 Batch   71/175   train_loss = 1.193\n",
      "Epoch 695 Batch  103/175   train_loss = 1.157\n",
      "Epoch 695 Batch  135/175   train_loss = 1.135\n",
      "Epoch 695 Batch  167/175   train_loss = 1.195\n",
      "Epoch 696 Batch   24/175   train_loss = 1.125\n",
      "Epoch 696 Batch   56/175   train_loss = 1.181\n",
      "Epoch 696 Batch   88/175   train_loss = 1.200\n",
      "Epoch 696 Batch  120/175   train_loss = 1.151\n",
      "Epoch 696 Batch  152/175   train_loss = 1.140\n",
      "Epoch 697 Batch    9/175   train_loss = 1.208\n",
      "Epoch 697 Batch   41/175   train_loss = 1.185\n",
      "Epoch 697 Batch   73/175   train_loss = 1.174\n",
      "Epoch 697 Batch  105/175   train_loss = 1.218\n",
      "Epoch 697 Batch  137/175   train_loss = 1.129\n",
      "Epoch 697 Batch  169/175   train_loss = 1.175\n",
      "Epoch 698 Batch   26/175   train_loss = 1.183\n",
      "Epoch 698 Batch   58/175   train_loss = 1.167\n",
      "Epoch 698 Batch   90/175   train_loss = 1.189\n",
      "Epoch 698 Batch  122/175   train_loss = 1.133\n",
      "Epoch 698 Batch  154/175   train_loss = 1.146\n",
      "Epoch 699 Batch   11/175   train_loss = 1.140\n",
      "Epoch 699 Batch   43/175   train_loss = 1.143\n",
      "Epoch 699 Batch   75/175   train_loss = 1.139\n",
      "Epoch 699 Batch  107/175   train_loss = 1.203\n",
      "Epoch 699 Batch  139/175   train_loss = 1.117\n",
      "Epoch 699 Batch  171/175   train_loss = 1.203\n",
      "Epoch 700 Batch   28/175   train_loss = 1.169\n",
      "Epoch 700 Batch   60/175   train_loss = 1.177\n",
      "Epoch 700 Batch   92/175   train_loss = 1.151\n",
      "Epoch 700 Batch  124/175   train_loss = 1.163\n",
      "Epoch 700 Batch  156/175   train_loss = 1.207\n",
      "Epoch 701 Batch   13/175   train_loss = 1.167\n",
      "Epoch 701 Batch   45/175   train_loss = 1.155\n",
      "Epoch 701 Batch   77/175   train_loss = 1.130\n",
      "Epoch 701 Batch  109/175   train_loss = 1.193\n",
      "Epoch 701 Batch  141/175   train_loss = 1.139\n",
      "Epoch 701 Batch  173/175   train_loss = 1.134\n",
      "Epoch 702 Batch   30/175   train_loss = 1.195\n",
      "Epoch 702 Batch   62/175   train_loss = 1.180\n",
      "Epoch 702 Batch   94/175   train_loss = 1.159\n",
      "Epoch 702 Batch  126/175   train_loss = 1.166\n",
      "Epoch 702 Batch  158/175   train_loss = 1.160\n",
      "Epoch 703 Batch   15/175   train_loss = 1.198\n",
      "Epoch 703 Batch   47/175   train_loss = 1.175\n",
      "Epoch 703 Batch   79/175   train_loss = 1.203\n",
      "Epoch 703 Batch  111/175   train_loss = 1.223\n",
      "Epoch 703 Batch  143/175   train_loss = 1.150\n",
      "Epoch 704 Batch    0/175   train_loss = 1.185\n",
      "Epoch 704 Batch   32/175   train_loss = 1.197\n",
      "Epoch 704 Batch   64/175   train_loss = 1.196\n",
      "Epoch 704 Batch   96/175   train_loss = 1.179\n",
      "Epoch 704 Batch  128/175   train_loss = 1.118\n",
      "Epoch 704 Batch  160/175   train_loss = 1.143\n",
      "Epoch 705 Batch   17/175   train_loss = 1.126\n",
      "Epoch 705 Batch   49/175   train_loss = 1.171\n",
      "Epoch 705 Batch   81/175   train_loss = 1.139\n",
      "Epoch 705 Batch  113/175   train_loss = 1.163\n",
      "Epoch 705 Batch  145/175   train_loss = 1.116\n",
      "Epoch 706 Batch    2/175   train_loss = 1.177\n",
      "Epoch 706 Batch   34/175   train_loss = 1.182\n",
      "Epoch 706 Batch   66/175   train_loss = 1.201\n",
      "Epoch 706 Batch   98/175   train_loss = 1.195\n",
      "Epoch 706 Batch  130/175   train_loss = 1.202\n",
      "Epoch 706 Batch  162/175   train_loss = 1.148\n",
      "Epoch 707 Batch   19/175   train_loss = 1.150\n",
      "Epoch 707 Batch   51/175   train_loss = 1.108\n",
      "Epoch 707 Batch   83/175   train_loss = 1.199\n",
      "Epoch 707 Batch  115/175   train_loss = 1.262\n",
      "Epoch 707 Batch  147/175   train_loss = 1.145\n",
      "Epoch 708 Batch    4/175   train_loss = 1.174\n",
      "Epoch 708 Batch   36/175   train_loss = 1.128\n",
      "Epoch 708 Batch   68/175   train_loss = 1.160\n",
      "Epoch 708 Batch  100/175   train_loss = 1.173\n",
      "Epoch 708 Batch  132/175   train_loss = 1.136\n",
      "Epoch 708 Batch  164/175   train_loss = 1.133\n",
      "Epoch 709 Batch   21/175   train_loss = 1.119\n",
      "Epoch 709 Batch   53/175   train_loss = 1.140\n",
      "Epoch 709 Batch   85/175   train_loss = 1.170\n",
      "Epoch 709 Batch  117/175   train_loss = 1.170\n",
      "Epoch 709 Batch  149/175   train_loss = 1.168\n",
      "Epoch 710 Batch    6/175   train_loss = 1.176\n",
      "Epoch 710 Batch   38/175   train_loss = 1.124\n",
      "Epoch 710 Batch   70/175   train_loss = 1.146\n",
      "Epoch 710 Batch  102/175   train_loss = 1.174\n",
      "Epoch 710 Batch  134/175   train_loss = 1.126\n",
      "Epoch 710 Batch  166/175   train_loss = 1.174\n",
      "Epoch 711 Batch   23/175   train_loss = 1.115\n",
      "Epoch 711 Batch   55/175   train_loss = 1.203\n",
      "Epoch 711 Batch   87/175   train_loss = 1.226\n",
      "Epoch 711 Batch  119/175   train_loss = 1.153\n",
      "Epoch 711 Batch  151/175   train_loss = 1.172\n",
      "Epoch 712 Batch    8/175   train_loss = 1.163\n",
      "Epoch 712 Batch   40/175   train_loss = 1.131\n",
      "Epoch 712 Batch   72/175   train_loss = 1.187\n",
      "Epoch 712 Batch  104/175   train_loss = 1.165\n",
      "Epoch 712 Batch  136/175   train_loss = 1.133\n",
      "Epoch 712 Batch  168/175   train_loss = 1.176\n",
      "Epoch 713 Batch   25/175   train_loss = 1.156\n",
      "Epoch 713 Batch   57/175   train_loss = 1.194\n",
      "Epoch 713 Batch   89/175   train_loss = 1.167\n",
      "Epoch 713 Batch  121/175   train_loss = 1.141\n",
      "Epoch 713 Batch  153/175   train_loss = 1.163\n",
      "Epoch 714 Batch   10/175   train_loss = 1.114\n",
      "Epoch 714 Batch   42/175   train_loss = 1.200\n",
      "Epoch 714 Batch   74/175   train_loss = 1.182\n",
      "Epoch 714 Batch  106/175   train_loss = 1.207\n",
      "Epoch 714 Batch  138/175   train_loss = 1.166\n",
      "Epoch 714 Batch  170/175   train_loss = 1.183\n",
      "Epoch 715 Batch   27/175   train_loss = 1.138\n",
      "Epoch 715 Batch   59/175   train_loss = 1.136\n",
      "Epoch 715 Batch   91/175   train_loss = 1.141\n",
      "Epoch 715 Batch  123/175   train_loss = 1.154\n",
      "Epoch 715 Batch  155/175   train_loss = 1.118\n",
      "Epoch 716 Batch   12/175   train_loss = 1.144\n",
      "Epoch 716 Batch   44/175   train_loss = 1.127\n",
      "Epoch 716 Batch   76/175   train_loss = 1.130\n",
      "Epoch 716 Batch  108/175   train_loss = 1.166\n",
      "Epoch 716 Batch  140/175   train_loss = 1.149\n",
      "Epoch 716 Batch  172/175   train_loss = 1.180\n",
      "Epoch 717 Batch   29/175   train_loss = 1.142\n",
      "Epoch 717 Batch   61/175   train_loss = 1.172\n",
      "Epoch 717 Batch   93/175   train_loss = 1.161\n",
      "Epoch 717 Batch  125/175   train_loss = 1.173\n",
      "Epoch 717 Batch  157/175   train_loss = 1.145\n",
      "Epoch 718 Batch   14/175   train_loss = 1.179\n",
      "Epoch 718 Batch   46/175   train_loss = 1.152\n",
      "Epoch 718 Batch   78/175   train_loss = 1.126\n",
      "Epoch 718 Batch  110/175   train_loss = 1.228\n",
      "Epoch 718 Batch  142/175   train_loss = 1.170\n",
      "Epoch 718 Batch  174/175   train_loss = 1.137\n",
      "Epoch 719 Batch   31/175   train_loss = 1.164\n",
      "Epoch 719 Batch   63/175   train_loss = 1.169\n",
      "Epoch 719 Batch   95/175   train_loss = 1.144\n",
      "Epoch 719 Batch  127/175   train_loss = 1.119\n",
      "Epoch 719 Batch  159/175   train_loss = 1.126\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 720 Batch   16/175   train_loss = 1.157\n",
      "Epoch 720 Batch   48/175   train_loss = 1.156\n",
      "Epoch 720 Batch   80/175   train_loss = 1.170\n",
      "Epoch 720 Batch  112/175   train_loss = 1.142\n",
      "Epoch 720 Batch  144/175   train_loss = 1.110\n",
      "Epoch 721 Batch    1/175   train_loss = 1.181\n",
      "Epoch 721 Batch   33/175   train_loss = 1.190\n",
      "Epoch 721 Batch   65/175   train_loss = 1.168\n",
      "Epoch 721 Batch   97/175   train_loss = 1.164\n",
      "Epoch 721 Batch  129/175   train_loss = 1.141\n",
      "Epoch 721 Batch  161/175   train_loss = 1.148\n",
      "Epoch 722 Batch   18/175   train_loss = 1.129\n",
      "Epoch 722 Batch   50/175   train_loss = 1.143\n",
      "Epoch 722 Batch   82/175   train_loss = 1.165\n",
      "Epoch 722 Batch  114/175   train_loss = 1.200\n",
      "Epoch 722 Batch  146/175   train_loss = 1.138\n",
      "Epoch 723 Batch    3/175   train_loss = 1.187\n",
      "Epoch 723 Batch   35/175   train_loss = 1.151\n",
      "Epoch 723 Batch   67/175   train_loss = 1.122\n",
      "Epoch 723 Batch   99/175   train_loss = 1.203\n",
      "Epoch 723 Batch  131/175   train_loss = 1.155\n",
      "Epoch 723 Batch  163/175   train_loss = 1.156\n",
      "Epoch 724 Batch   20/175   train_loss = 1.170\n",
      "Epoch 724 Batch   52/175   train_loss = 1.097\n",
      "Epoch 724 Batch   84/175   train_loss = 1.166\n",
      "Epoch 724 Batch  116/175   train_loss = 1.202\n",
      "Epoch 724 Batch  148/175   train_loss = 1.153\n",
      "Epoch 725 Batch    5/175   train_loss = 1.147\n",
      "Epoch 725 Batch   37/175   train_loss = 1.134\n",
      "Epoch 725 Batch   69/175   train_loss = 1.172\n",
      "Epoch 725 Batch  101/175   train_loss = 1.206\n",
      "Epoch 725 Batch  133/175   train_loss = 1.073\n",
      "Epoch 725 Batch  165/175   train_loss = 1.175\n",
      "Epoch 726 Batch   22/175   train_loss = 1.107\n",
      "Epoch 726 Batch   54/175   train_loss = 1.198\n",
      "Epoch 726 Batch   86/175   train_loss = 1.227\n",
      "Epoch 726 Batch  118/175   train_loss = 1.183\n",
      "Epoch 726 Batch  150/175   train_loss = 1.161\n",
      "Epoch 727 Batch    7/175   train_loss = 1.183\n",
      "Epoch 727 Batch   39/175   train_loss = 1.124\n",
      "Epoch 727 Batch   71/175   train_loss = 1.203\n",
      "Epoch 727 Batch  103/175   train_loss = 1.146\n",
      "Epoch 727 Batch  135/175   train_loss = 1.131\n",
      "Epoch 727 Batch  167/175   train_loss = 1.192\n",
      "Epoch 728 Batch   24/175   train_loss = 1.103\n",
      "Epoch 728 Batch   56/175   train_loss = 1.180\n",
      "Epoch 728 Batch   88/175   train_loss = 1.200\n",
      "Epoch 728 Batch  120/175   train_loss = 1.160\n",
      "Epoch 728 Batch  152/175   train_loss = 1.139\n",
      "Epoch 729 Batch    9/175   train_loss = 1.196\n",
      "Epoch 729 Batch   41/175   train_loss = 1.181\n",
      "Epoch 729 Batch   73/175   train_loss = 1.169\n",
      "Epoch 729 Batch  105/175   train_loss = 1.199\n",
      "Epoch 729 Batch  137/175   train_loss = 1.142\n",
      "Epoch 729 Batch  169/175   train_loss = 1.189\n",
      "Epoch 730 Batch   26/175   train_loss = 1.179\n",
      "Epoch 730 Batch   58/175   train_loss = 1.183\n",
      "Epoch 730 Batch   90/175   train_loss = 1.177\n",
      "Epoch 730 Batch  122/175   train_loss = 1.150\n",
      "Epoch 730 Batch  154/175   train_loss = 1.129\n",
      "Epoch 731 Batch   11/175   train_loss = 1.154\n",
      "Epoch 731 Batch   43/175   train_loss = 1.154\n",
      "Epoch 731 Batch   75/175   train_loss = 1.139\n",
      "Epoch 731 Batch  107/175   train_loss = 1.205\n",
      "Epoch 731 Batch  139/175   train_loss = 1.136\n",
      "Epoch 731 Batch  171/175   train_loss = 1.193\n",
      "Epoch 732 Batch   28/175   train_loss = 1.137\n",
      "Epoch 732 Batch   60/175   train_loss = 1.173\n",
      "Epoch 732 Batch   92/175   train_loss = 1.148\n",
      "Epoch 732 Batch  124/175   train_loss = 1.173\n",
      "Epoch 732 Batch  156/175   train_loss = 1.199\n",
      "Epoch 733 Batch   13/175   train_loss = 1.161\n",
      "Epoch 733 Batch   45/175   train_loss = 1.158\n",
      "Epoch 733 Batch   77/175   train_loss = 1.142\n",
      "Epoch 733 Batch  109/175   train_loss = 1.185\n",
      "Epoch 733 Batch  141/175   train_loss = 1.115\n",
      "Epoch 733 Batch  173/175   train_loss = 1.121\n",
      "Epoch 734 Batch   30/175   train_loss = 1.207\n",
      "Epoch 734 Batch   62/175   train_loss = 1.189\n",
      "Epoch 734 Batch   94/175   train_loss = 1.157\n",
      "Epoch 734 Batch  126/175   train_loss = 1.161\n",
      "Epoch 734 Batch  158/175   train_loss = 1.162\n",
      "Epoch 735 Batch   15/175   train_loss = 1.201\n",
      "Epoch 735 Batch   47/175   train_loss = 1.179\n",
      "Epoch 735 Batch   79/175   train_loss = 1.192\n",
      "Epoch 735 Batch  111/175   train_loss = 1.214\n",
      "Epoch 735 Batch  143/175   train_loss = 1.150\n",
      "Epoch 736 Batch    0/175   train_loss = 1.177\n",
      "Epoch 736 Batch   32/175   train_loss = 1.194\n",
      "Epoch 736 Batch   64/175   train_loss = 1.204\n",
      "Epoch 736 Batch   96/175   train_loss = 1.173\n",
      "Epoch 736 Batch  128/175   train_loss = 1.117\n",
      "Epoch 736 Batch  160/175   train_loss = 1.151\n",
      "Epoch 737 Batch   17/175   train_loss = 1.128\n",
      "Epoch 737 Batch   49/175   train_loss = 1.165\n",
      "Epoch 737 Batch   81/175   train_loss = 1.126\n",
      "Epoch 737 Batch  113/175   train_loss = 1.175\n",
      "Epoch 737 Batch  145/175   train_loss = 1.135\n",
      "Epoch 738 Batch    2/175   train_loss = 1.163\n",
      "Epoch 738 Batch   34/175   train_loss = 1.184\n",
      "Epoch 738 Batch   66/175   train_loss = 1.188\n",
      "Epoch 738 Batch   98/175   train_loss = 1.190\n",
      "Epoch 738 Batch  130/175   train_loss = 1.189\n",
      "Epoch 738 Batch  162/175   train_loss = 1.155\n",
      "Epoch 739 Batch   19/175   train_loss = 1.163\n",
      "Epoch 739 Batch   51/175   train_loss = 1.111\n",
      "Epoch 739 Batch   83/175   train_loss = 1.197\n",
      "Epoch 739 Batch  115/175   train_loss = 1.262\n",
      "Epoch 739 Batch  147/175   train_loss = 1.140\n",
      "Epoch 740 Batch    4/175   train_loss = 1.176\n",
      "Epoch 740 Batch   36/175   train_loss = 1.133\n",
      "Epoch 740 Batch   68/175   train_loss = 1.151\n",
      "Epoch 740 Batch  100/175   train_loss = 1.167\n",
      "Epoch 740 Batch  132/175   train_loss = 1.133\n",
      "Epoch 740 Batch  164/175   train_loss = 1.127\n",
      "Epoch 741 Batch   21/175   train_loss = 1.121\n",
      "Epoch 741 Batch   53/175   train_loss = 1.136\n",
      "Epoch 741 Batch   85/175   train_loss = 1.163\n",
      "Epoch 741 Batch  117/175   train_loss = 1.175\n",
      "Epoch 741 Batch  149/175   train_loss = 1.185\n",
      "Epoch 742 Batch    6/175   train_loss = 1.208\n",
      "Epoch 742 Batch   38/175   train_loss = 1.135\n",
      "Epoch 742 Batch   70/175   train_loss = 1.157\n",
      "Epoch 742 Batch  102/175   train_loss = 1.185\n",
      "Epoch 742 Batch  134/175   train_loss = 1.131\n",
      "Epoch 742 Batch  166/175   train_loss = 1.172\n",
      "Epoch 743 Batch   23/175   train_loss = 1.112\n",
      "Epoch 743 Batch   55/175   train_loss = 1.198\n",
      "Epoch 743 Batch   87/175   train_loss = 1.249\n",
      "Epoch 743 Batch  119/175   train_loss = 1.168\n",
      "Epoch 743 Batch  151/175   train_loss = 1.182\n",
      "Epoch 744 Batch    8/175   train_loss = 1.175\n",
      "Epoch 744 Batch   40/175   train_loss = 1.150\n",
      "Epoch 744 Batch   72/175   train_loss = 1.176\n",
      "Epoch 744 Batch  104/175   train_loss = 1.177\n",
      "Epoch 744 Batch  136/175   train_loss = 1.129\n",
      "Epoch 744 Batch  168/175   train_loss = 1.176\n",
      "Epoch 745 Batch   25/175   train_loss = 1.171\n",
      "Epoch 745 Batch   57/175   train_loss = 1.181\n",
      "Epoch 745 Batch   89/175   train_loss = 1.156\n",
      "Epoch 745 Batch  121/175   train_loss = 1.150\n",
      "Epoch 745 Batch  153/175   train_loss = 1.160\n",
      "Epoch 746 Batch   10/175   train_loss = 1.105\n",
      "Epoch 746 Batch   42/175   train_loss = 1.193\n",
      "Epoch 746 Batch   74/175   train_loss = 1.199\n",
      "Epoch 746 Batch  106/175   train_loss = 1.204\n",
      "Epoch 746 Batch  138/175   train_loss = 1.146\n",
      "Epoch 746 Batch  170/175   train_loss = 1.196\n",
      "Epoch 747 Batch   27/175   train_loss = 1.171\n",
      "Epoch 747 Batch   59/175   train_loss = 1.140\n",
      "Epoch 747 Batch   91/175   train_loss = 1.139\n",
      "Epoch 747 Batch  123/175   train_loss = 1.160\n",
      "Epoch 747 Batch  155/175   train_loss = 1.117\n",
      "Epoch 748 Batch   12/175   train_loss = 1.142\n",
      "Epoch 748 Batch   44/175   train_loss = 1.139\n",
      "Epoch 748 Batch   76/175   train_loss = 1.154\n",
      "Epoch 748 Batch  108/175   train_loss = 1.180\n",
      "Epoch 748 Batch  140/175   train_loss = 1.153\n",
      "Epoch 748 Batch  172/175   train_loss = 1.184\n",
      "Epoch 749 Batch   29/175   train_loss = 1.154\n",
      "Epoch 749 Batch   61/175   train_loss = 1.166\n",
      "Epoch 749 Batch   93/175   train_loss = 1.165\n",
      "Epoch 749 Batch  125/175   train_loss = 1.181\n",
      "Epoch 749 Batch  157/175   train_loss = 1.148\n",
      "Epoch 750 Batch   14/175   train_loss = 1.190\n",
      "Epoch 750 Batch   46/175   train_loss = 1.170\n",
      "Epoch 750 Batch   78/175   train_loss = 1.138\n",
      "Epoch 750 Batch  110/175   train_loss = 1.231\n",
      "Epoch 750 Batch  142/175   train_loss = 1.167\n",
      "Epoch 750 Batch  174/175   train_loss = 1.129\n",
      "Epoch 751 Batch   31/175   train_loss = 1.163\n",
      "Epoch 751 Batch   63/175   train_loss = 1.188\n",
      "Epoch 751 Batch   95/175   train_loss = 1.140\n",
      "Epoch 751 Batch  127/175   train_loss = 1.110\n",
      "Epoch 751 Batch  159/175   train_loss = 1.119\n",
      "Epoch 752 Batch   16/175   train_loss = 1.156\n",
      "Epoch 752 Batch   48/175   train_loss = 1.169\n",
      "Epoch 752 Batch   80/175   train_loss = 1.183\n",
      "Epoch 752 Batch  112/175   train_loss = 1.157\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 752 Batch  144/175   train_loss = 1.096\n",
      "Epoch 753 Batch    1/175   train_loss = 1.179\n",
      "Epoch 753 Batch   33/175   train_loss = 1.194\n",
      "Epoch 753 Batch   65/175   train_loss = 1.160\n",
      "Epoch 753 Batch   97/175   train_loss = 1.140\n",
      "Epoch 753 Batch  129/175   train_loss = 1.144\n",
      "Epoch 753 Batch  161/175   train_loss = 1.141\n",
      "Epoch 754 Batch   18/175   train_loss = 1.136\n",
      "Epoch 754 Batch   50/175   train_loss = 1.154\n",
      "Epoch 754 Batch   82/175   train_loss = 1.186\n",
      "Epoch 754 Batch  114/175   train_loss = 1.196\n",
      "Epoch 754 Batch  146/175   train_loss = 1.155\n",
      "Epoch 755 Batch    3/175   train_loss = 1.187\n",
      "Epoch 755 Batch   35/175   train_loss = 1.150\n",
      "Epoch 755 Batch   67/175   train_loss = 1.129\n",
      "Epoch 755 Batch   99/175   train_loss = 1.210\n",
      "Epoch 755 Batch  131/175   train_loss = 1.145\n",
      "Epoch 755 Batch  163/175   train_loss = 1.157\n",
      "Epoch 756 Batch   20/175   train_loss = 1.151\n",
      "Epoch 756 Batch   52/175   train_loss = 1.111\n",
      "Epoch 756 Batch   84/175   train_loss = 1.156\n",
      "Epoch 756 Batch  116/175   train_loss = 1.194\n",
      "Epoch 756 Batch  148/175   train_loss = 1.153\n",
      "Epoch 757 Batch    5/175   train_loss = 1.154\n",
      "Epoch 757 Batch   37/175   train_loss = 1.130\n",
      "Epoch 757 Batch   69/175   train_loss = 1.168\n",
      "Epoch 757 Batch  101/175   train_loss = 1.208\n",
      "Epoch 757 Batch  133/175   train_loss = 1.076\n",
      "Epoch 757 Batch  165/175   train_loss = 1.178\n",
      "Epoch 758 Batch   22/175   train_loss = 1.115\n",
      "Epoch 758 Batch   54/175   train_loss = 1.180\n",
      "Epoch 758 Batch   86/175   train_loss = 1.232\n",
      "Epoch 758 Batch  118/175   train_loss = 1.211\n",
      "Epoch 758 Batch  150/175   train_loss = 1.172\n",
      "Epoch 759 Batch    7/175   train_loss = 1.177\n",
      "Epoch 759 Batch   39/175   train_loss = 1.124\n",
      "Epoch 759 Batch   71/175   train_loss = 1.200\n",
      "Epoch 759 Batch  103/175   train_loss = 1.165\n",
      "Epoch 759 Batch  135/175   train_loss = 1.141\n",
      "Epoch 759 Batch  167/175   train_loss = 1.200\n",
      "Epoch 760 Batch   24/175   train_loss = 1.124\n",
      "Epoch 760 Batch   56/175   train_loss = 1.201\n",
      "Epoch 760 Batch   88/175   train_loss = 1.204\n",
      "Epoch 760 Batch  120/175   train_loss = 1.138\n",
      "Epoch 760 Batch  152/175   train_loss = 1.155\n",
      "Epoch 761 Batch    9/175   train_loss = 1.193\n",
      "Epoch 761 Batch   41/175   train_loss = 1.186\n",
      "Epoch 761 Batch   73/175   train_loss = 1.163\n",
      "Epoch 761 Batch  105/175   train_loss = 1.211\n",
      "Epoch 761 Batch  137/175   train_loss = 1.132\n",
      "Epoch 761 Batch  169/175   train_loss = 1.179\n",
      "Epoch 762 Batch   26/175   train_loss = 1.194\n",
      "Epoch 762 Batch   58/175   train_loss = 1.199\n",
      "Epoch 762 Batch   90/175   train_loss = 1.181\n",
      "Epoch 762 Batch  122/175   train_loss = 1.128\n",
      "Epoch 762 Batch  154/175   train_loss = 1.157\n",
      "Epoch 763 Batch   11/175   train_loss = 1.162\n",
      "Epoch 763 Batch   43/175   train_loss = 1.149\n",
      "Epoch 763 Batch   75/175   train_loss = 1.139\n",
      "Epoch 763 Batch  107/175   train_loss = 1.214\n",
      "Epoch 763 Batch  139/175   train_loss = 1.131\n",
      "Epoch 763 Batch  171/175   train_loss = 1.194\n",
      "Epoch 764 Batch   28/175   train_loss = 1.140\n",
      "Epoch 764 Batch   60/175   train_loss = 1.169\n",
      "Epoch 764 Batch   92/175   train_loss = 1.130\n",
      "Epoch 764 Batch  124/175   train_loss = 1.159\n",
      "Epoch 764 Batch  156/175   train_loss = 1.192\n",
      "Epoch 765 Batch   13/175   train_loss = 1.158\n",
      "Epoch 765 Batch   45/175   train_loss = 1.148\n",
      "Epoch 765 Batch   77/175   train_loss = 1.151\n",
      "Epoch 765 Batch  109/175   train_loss = 1.187\n",
      "Epoch 765 Batch  141/175   train_loss = 1.123\n",
      "Epoch 765 Batch  173/175   train_loss = 1.133\n",
      "Epoch 766 Batch   30/175   train_loss = 1.198\n",
      "Epoch 766 Batch   62/175   train_loss = 1.170\n",
      "Epoch 766 Batch   94/175   train_loss = 1.141\n",
      "Epoch 766 Batch  126/175   train_loss = 1.153\n",
      "Epoch 766 Batch  158/175   train_loss = 1.154\n",
      "Epoch 767 Batch   15/175   train_loss = 1.192\n",
      "Epoch 767 Batch   47/175   train_loss = 1.166\n",
      "Epoch 767 Batch   79/175   train_loss = 1.194\n",
      "Epoch 767 Batch  111/175   train_loss = 1.198\n",
      "Epoch 767 Batch  143/175   train_loss = 1.130\n",
      "Epoch 768 Batch    0/175   train_loss = 1.188\n",
      "Epoch 768 Batch   32/175   train_loss = 1.167\n",
      "Epoch 768 Batch   64/175   train_loss = 1.184\n",
      "Epoch 768 Batch   96/175   train_loss = 1.181\n",
      "Epoch 768 Batch  128/175   train_loss = 1.123\n",
      "Epoch 768 Batch  160/175   train_loss = 1.160\n",
      "Epoch 769 Batch   17/175   train_loss = 1.120\n",
      "Epoch 769 Batch   49/175   train_loss = 1.171\n",
      "Epoch 769 Batch   81/175   train_loss = 1.146\n",
      "Epoch 769 Batch  113/175   train_loss = 1.182\n",
      "Epoch 769 Batch  145/175   train_loss = 1.118\n",
      "Epoch 770 Batch    2/175   train_loss = 1.170\n",
      "Epoch 770 Batch   34/175   train_loss = 1.182\n",
      "Epoch 770 Batch   66/175   train_loss = 1.202\n",
      "Epoch 770 Batch   98/175   train_loss = 1.191\n",
      "Epoch 770 Batch  130/175   train_loss = 1.189\n",
      "Epoch 770 Batch  162/175   train_loss = 1.166\n",
      "Epoch 771 Batch   19/175   train_loss = 1.143\n",
      "Epoch 771 Batch   51/175   train_loss = 1.121\n",
      "Epoch 771 Batch   83/175   train_loss = 1.190\n",
      "Epoch 771 Batch  115/175   train_loss = 1.262\n",
      "Epoch 771 Batch  147/175   train_loss = 1.146\n",
      "Epoch 772 Batch    4/175   train_loss = 1.159\n",
      "Epoch 772 Batch   36/175   train_loss = 1.127\n",
      "Epoch 772 Batch   68/175   train_loss = 1.165\n",
      "Epoch 772 Batch  100/175   train_loss = 1.186\n",
      "Epoch 772 Batch  132/175   train_loss = 1.124\n",
      "Epoch 772 Batch  164/175   train_loss = 1.140\n",
      "Epoch 773 Batch   21/175   train_loss = 1.120\n",
      "Epoch 773 Batch   53/175   train_loss = 1.113\n",
      "Epoch 773 Batch   85/175   train_loss = 1.174\n",
      "Epoch 773 Batch  117/175   train_loss = 1.168\n",
      "Epoch 773 Batch  149/175   train_loss = 1.192\n",
      "Epoch 774 Batch    6/175   train_loss = 1.174\n",
      "Epoch 774 Batch   38/175   train_loss = 1.130\n",
      "Epoch 774 Batch   70/175   train_loss = 1.169\n",
      "Epoch 774 Batch  102/175   train_loss = 1.173\n",
      "Epoch 774 Batch  134/175   train_loss = 1.135\n",
      "Epoch 774 Batch  166/175   train_loss = 1.169\n",
      "Epoch 775 Batch   23/175   train_loss = 1.119\n",
      "Epoch 775 Batch   55/175   train_loss = 1.185\n",
      "Epoch 775 Batch   87/175   train_loss = 1.229\n",
      "Epoch 775 Batch  119/175   train_loss = 1.152\n",
      "Epoch 775 Batch  151/175   train_loss = 1.169\n",
      "Epoch 776 Batch    8/175   train_loss = 1.171\n",
      "Epoch 776 Batch   40/175   train_loss = 1.155\n",
      "Epoch 776 Batch   72/175   train_loss = 1.172\n",
      "Epoch 776 Batch  104/175   train_loss = 1.177\n",
      "Epoch 776 Batch  136/175   train_loss = 1.131\n",
      "Epoch 776 Batch  168/175   train_loss = 1.188\n",
      "Epoch 777 Batch   25/175   train_loss = 1.187\n",
      "Epoch 777 Batch   57/175   train_loss = 1.198\n",
      "Epoch 777 Batch   89/175   train_loss = 1.160\n",
      "Epoch 777 Batch  121/175   train_loss = 1.126\n",
      "Epoch 777 Batch  153/175   train_loss = 1.165\n",
      "Epoch 778 Batch   10/175   train_loss = 1.122\n",
      "Epoch 778 Batch   42/175   train_loss = 1.209\n",
      "Epoch 778 Batch   74/175   train_loss = 1.206\n",
      "Epoch 778 Batch  106/175   train_loss = 1.190\n",
      "Epoch 778 Batch  138/175   train_loss = 1.166\n",
      "Epoch 778 Batch  170/175   train_loss = 1.200\n",
      "Epoch 779 Batch   27/175   train_loss = 1.168\n",
      "Epoch 779 Batch   59/175   train_loss = 1.133\n",
      "Epoch 779 Batch   91/175   train_loss = 1.149\n",
      "Epoch 779 Batch  123/175   train_loss = 1.149\n",
      "Epoch 779 Batch  155/175   train_loss = 1.123\n",
      "Epoch 780 Batch   12/175   train_loss = 1.160\n",
      "Epoch 780 Batch   44/175   train_loss = 1.143\n",
      "Epoch 780 Batch   76/175   train_loss = 1.134\n",
      "Epoch 780 Batch  108/175   train_loss = 1.160\n",
      "Epoch 780 Batch  140/175   train_loss = 1.149\n",
      "Epoch 780 Batch  172/175   train_loss = 1.186\n",
      "Epoch 781 Batch   29/175   train_loss = 1.152\n",
      "Epoch 781 Batch   61/175   train_loss = 1.180\n",
      "Epoch 781 Batch   93/175   train_loss = 1.165\n",
      "Epoch 781 Batch  125/175   train_loss = 1.199\n",
      "Epoch 781 Batch  157/175   train_loss = 1.139\n",
      "Epoch 782 Batch   14/175   train_loss = 1.181\n",
      "Epoch 782 Batch   46/175   train_loss = 1.175\n",
      "Epoch 782 Batch   78/175   train_loss = 1.130\n",
      "Epoch 782 Batch  110/175   train_loss = 1.245\n",
      "Epoch 782 Batch  142/175   train_loss = 1.180\n",
      "Epoch 782 Batch  174/175   train_loss = 1.141\n",
      "Epoch 783 Batch   31/175   train_loss = 1.158\n",
      "Epoch 783 Batch   63/175   train_loss = 1.188\n",
      "Epoch 783 Batch   95/175   train_loss = 1.138\n",
      "Epoch 783 Batch  127/175   train_loss = 1.110\n",
      "Epoch 783 Batch  159/175   train_loss = 1.129\n",
      "Epoch 784 Batch   16/175   train_loss = 1.156\n",
      "Epoch 784 Batch   48/175   train_loss = 1.158\n",
      "Epoch 784 Batch   80/175   train_loss = 1.197\n",
      "Epoch 784 Batch  112/175   train_loss = 1.147\n",
      "Epoch 784 Batch  144/175   train_loss = 1.111\n",
      "Epoch 785 Batch    1/175   train_loss = 1.201\n",
      "Epoch 785 Batch   33/175   train_loss = 1.221\n",
      "Epoch 785 Batch   65/175   train_loss = 1.164\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 785 Batch   97/175   train_loss = 1.152\n",
      "Epoch 785 Batch  129/175   train_loss = 1.153\n",
      "Epoch 785 Batch  161/175   train_loss = 1.154\n",
      "Epoch 786 Batch   18/175   train_loss = 1.134\n",
      "Epoch 786 Batch   50/175   train_loss = 1.148\n",
      "Epoch 786 Batch   82/175   train_loss = 1.201\n",
      "Epoch 786 Batch  114/175   train_loss = 1.199\n",
      "Epoch 786 Batch  146/175   train_loss = 1.146\n",
      "Epoch 787 Batch    3/175   train_loss = 1.199\n",
      "Epoch 787 Batch   35/175   train_loss = 1.159\n",
      "Epoch 787 Batch   67/175   train_loss = 1.141\n",
      "Epoch 787 Batch   99/175   train_loss = 1.227\n",
      "Epoch 787 Batch  131/175   train_loss = 1.150\n",
      "Epoch 787 Batch  163/175   train_loss = 1.162\n",
      "Epoch 788 Batch   20/175   train_loss = 1.151\n",
      "Epoch 788 Batch   52/175   train_loss = 1.106\n",
      "Epoch 788 Batch   84/175   train_loss = 1.151\n",
      "Epoch 788 Batch  116/175   train_loss = 1.195\n",
      "Epoch 788 Batch  148/175   train_loss = 1.173\n",
      "Epoch 789 Batch    5/175   train_loss = 1.154\n",
      "Epoch 789 Batch   37/175   train_loss = 1.158\n",
      "Epoch 789 Batch   69/175   train_loss = 1.164\n",
      "Epoch 789 Batch  101/175   train_loss = 1.209\n",
      "Epoch 789 Batch  133/175   train_loss = 1.089\n",
      "Epoch 789 Batch  165/175   train_loss = 1.180\n",
      "Epoch 790 Batch   22/175   train_loss = 1.106\n",
      "Epoch 790 Batch   54/175   train_loss = 1.194\n",
      "Epoch 790 Batch   86/175   train_loss = 1.221\n",
      "Epoch 790 Batch  118/175   train_loss = 1.197\n",
      "Epoch 790 Batch  150/175   train_loss = 1.159\n",
      "Epoch 791 Batch    7/175   train_loss = 1.170\n",
      "Epoch 791 Batch   39/175   train_loss = 1.106\n",
      "Epoch 791 Batch   71/175   train_loss = 1.197\n",
      "Epoch 791 Batch  103/175   train_loss = 1.161\n",
      "Epoch 791 Batch  135/175   train_loss = 1.134\n",
      "Epoch 791 Batch  167/175   train_loss = 1.195\n",
      "Epoch 792 Batch   24/175   train_loss = 1.122\n",
      "Epoch 792 Batch   56/175   train_loss = 1.175\n",
      "Epoch 792 Batch   88/175   train_loss = 1.180\n",
      "Epoch 792 Batch  120/175   train_loss = 1.127\n",
      "Epoch 792 Batch  152/175   train_loss = 1.143\n",
      "Epoch 793 Batch    9/175   train_loss = 1.197\n",
      "Epoch 793 Batch   41/175   train_loss = 1.191\n",
      "Epoch 793 Batch   73/175   train_loss = 1.181\n",
      "Epoch 793 Batch  105/175   train_loss = 1.206\n",
      "Epoch 793 Batch  137/175   train_loss = 1.123\n",
      "Epoch 793 Batch  169/175   train_loss = 1.186\n",
      "Epoch 794 Batch   26/175   train_loss = 1.158\n",
      "Epoch 794 Batch   58/175   train_loss = 1.192\n",
      "Epoch 794 Batch   90/175   train_loss = 1.170\n",
      "Epoch 794 Batch  122/175   train_loss = 1.143\n",
      "Epoch 794 Batch  154/175   train_loss = 1.133\n",
      "Epoch 795 Batch   11/175   train_loss = 1.138\n",
      "Epoch 795 Batch   43/175   train_loss = 1.155\n",
      "Epoch 795 Batch   75/175   train_loss = 1.158\n",
      "Epoch 795 Batch  107/175   train_loss = 1.213\n",
      "Epoch 795 Batch  139/175   train_loss = 1.130\n",
      "Epoch 795 Batch  171/175   train_loss = 1.193\n",
      "Epoch 796 Batch   28/175   train_loss = 1.144\n",
      "Epoch 796 Batch   60/175   train_loss = 1.172\n",
      "Epoch 796 Batch   92/175   train_loss = 1.151\n",
      "Epoch 796 Batch  124/175   train_loss = 1.169\n",
      "Epoch 796 Batch  156/175   train_loss = 1.205\n",
      "Epoch 797 Batch   13/175   train_loss = 1.160\n",
      "Epoch 797 Batch   45/175   train_loss = 1.164\n",
      "Epoch 797 Batch   77/175   train_loss = 1.152\n",
      "Epoch 797 Batch  109/175   train_loss = 1.209\n",
      "Epoch 797 Batch  141/175   train_loss = 1.139\n",
      "Epoch 797 Batch  173/175   train_loss = 1.147\n",
      "Epoch 798 Batch   30/175   train_loss = 1.220\n",
      "Epoch 798 Batch   62/175   train_loss = 1.189\n",
      "Epoch 798 Batch   94/175   train_loss = 1.152\n",
      "Epoch 798 Batch  126/175   train_loss = 1.158\n",
      "Epoch 798 Batch  158/175   train_loss = 1.145\n",
      "Epoch 799 Batch   15/175   train_loss = 1.207\n",
      "Epoch 799 Batch   47/175   train_loss = 1.181\n",
      "Epoch 799 Batch   79/175   train_loss = 1.205\n",
      "Epoch 799 Batch  111/175   train_loss = 1.218\n",
      "Epoch 799 Batch  143/175   train_loss = 1.138\n",
      "Epoch 800 Batch    0/175   train_loss = 1.192\n",
      "Epoch 800 Batch   32/175   train_loss = 1.182\n",
      "Epoch 800 Batch   64/175   train_loss = 1.185\n",
      "Epoch 800 Batch   96/175   train_loss = 1.174\n",
      "Epoch 800 Batch  128/175   train_loss = 1.118\n",
      "Epoch 800 Batch  160/175   train_loss = 1.150\n",
      "Epoch 801 Batch   17/175   train_loss = 1.138\n",
      "Epoch 801 Batch   49/175   train_loss = 1.174\n",
      "Epoch 801 Batch   81/175   train_loss = 1.137\n",
      "Epoch 801 Batch  113/175   train_loss = 1.169\n",
      "Epoch 801 Batch  145/175   train_loss = 1.119\n",
      "Epoch 802 Batch    2/175   train_loss = 1.158\n",
      "Epoch 802 Batch   34/175   train_loss = 1.200\n",
      "Epoch 802 Batch   66/175   train_loss = 1.194\n",
      "Epoch 802 Batch   98/175   train_loss = 1.200\n",
      "Epoch 802 Batch  130/175   train_loss = 1.181\n",
      "Epoch 802 Batch  162/175   train_loss = 1.168\n",
      "Epoch 803 Batch   19/175   train_loss = 1.142\n",
      "Epoch 803 Batch   51/175   train_loss = 1.111\n",
      "Epoch 803 Batch   83/175   train_loss = 1.211\n",
      "Epoch 803 Batch  115/175   train_loss = 1.270\n",
      "Epoch 803 Batch  147/175   train_loss = 1.130\n",
      "Epoch 804 Batch    4/175   train_loss = 1.164\n",
      "Epoch 804 Batch   36/175   train_loss = 1.119\n",
      "Epoch 804 Batch   68/175   train_loss = 1.156\n",
      "Epoch 804 Batch  100/175   train_loss = 1.164\n",
      "Epoch 804 Batch  132/175   train_loss = 1.130\n",
      "Epoch 804 Batch  164/175   train_loss = 1.134\n",
      "Epoch 805 Batch   21/175   train_loss = 1.106\n",
      "Epoch 805 Batch   53/175   train_loss = 1.124\n",
      "Epoch 805 Batch   85/175   train_loss = 1.175\n",
      "Epoch 805 Batch  117/175   train_loss = 1.183\n",
      "Epoch 805 Batch  149/175   train_loss = 1.177\n",
      "Epoch 806 Batch    6/175   train_loss = 1.187\n",
      "Epoch 806 Batch   38/175   train_loss = 1.117\n",
      "Epoch 806 Batch   70/175   train_loss = 1.161\n",
      "Epoch 806 Batch  102/175   train_loss = 1.168\n",
      "Epoch 806 Batch  134/175   train_loss = 1.137\n",
      "Epoch 806 Batch  166/175   train_loss = 1.170\n",
      "Epoch 807 Batch   23/175   train_loss = 1.100\n",
      "Epoch 807 Batch   55/175   train_loss = 1.210\n",
      "Epoch 807 Batch   87/175   train_loss = 1.228\n",
      "Epoch 807 Batch  119/175   train_loss = 1.145\n",
      "Epoch 807 Batch  151/175   train_loss = 1.171\n",
      "Epoch 808 Batch    8/175   train_loss = 1.164\n",
      "Epoch 808 Batch   40/175   train_loss = 1.149\n",
      "Epoch 808 Batch   72/175   train_loss = 1.188\n",
      "Epoch 808 Batch  104/175   train_loss = 1.181\n",
      "Epoch 808 Batch  136/175   train_loss = 1.150\n",
      "Epoch 808 Batch  168/175   train_loss = 1.192\n",
      "Epoch 809 Batch   25/175   train_loss = 1.166\n",
      "Epoch 809 Batch   57/175   train_loss = 1.204\n",
      "Epoch 809 Batch   89/175   train_loss = 1.159\n",
      "Epoch 809 Batch  121/175   train_loss = 1.152\n",
      "Epoch 809 Batch  153/175   train_loss = 1.173\n",
      "Epoch 810 Batch   10/175   train_loss = 1.113\n",
      "Epoch 810 Batch   42/175   train_loss = 1.231\n",
      "Epoch 810 Batch   74/175   train_loss = 1.217\n",
      "Epoch 810 Batch  106/175   train_loss = 1.194\n",
      "Epoch 810 Batch  138/175   train_loss = 1.140\n",
      "Epoch 810 Batch  170/175   train_loss = 1.201\n",
      "Epoch 811 Batch   27/175   train_loss = 1.151\n",
      "Epoch 811 Batch   59/175   train_loss = 1.151\n",
      "Epoch 811 Batch   91/175   train_loss = 1.147\n",
      "Epoch 811 Batch  123/175   train_loss = 1.170\n",
      "Epoch 811 Batch  155/175   train_loss = 1.128\n",
      "Epoch 812 Batch   12/175   train_loss = 1.158\n",
      "Epoch 812 Batch   44/175   train_loss = 1.141\n",
      "Epoch 812 Batch   76/175   train_loss = 1.143\n",
      "Epoch 812 Batch  108/175   train_loss = 1.174\n",
      "Epoch 812 Batch  140/175   train_loss = 1.152\n",
      "Epoch 812 Batch  172/175   train_loss = 1.175\n",
      "Epoch 813 Batch   29/175   train_loss = 1.176\n",
      "Epoch 813 Batch   61/175   train_loss = 1.172\n",
      "Epoch 813 Batch   93/175   train_loss = 1.171\n",
      "Epoch 813 Batch  125/175   train_loss = 1.184\n",
      "Epoch 813 Batch  157/175   train_loss = 1.158\n",
      "Epoch 814 Batch   14/175   train_loss = 1.181\n",
      "Epoch 814 Batch   46/175   train_loss = 1.175\n",
      "Epoch 814 Batch   78/175   train_loss = 1.134\n",
      "Epoch 814 Batch  110/175   train_loss = 1.216\n",
      "Epoch 814 Batch  142/175   train_loss = 1.186\n",
      "Epoch 814 Batch  174/175   train_loss = 1.162\n",
      "Epoch 815 Batch   31/175   train_loss = 1.162\n",
      "Epoch 815 Batch   63/175   train_loss = 1.189\n",
      "Epoch 815 Batch   95/175   train_loss = 1.134\n",
      "Epoch 815 Batch  127/175   train_loss = 1.118\n",
      "Epoch 815 Batch  159/175   train_loss = 1.134\n",
      "Epoch 816 Batch   16/175   train_loss = 1.145\n",
      "Epoch 816 Batch   48/175   train_loss = 1.161\n",
      "Epoch 816 Batch   80/175   train_loss = 1.197\n",
      "Epoch 816 Batch  112/175   train_loss = 1.174\n",
      "Epoch 816 Batch  144/175   train_loss = 1.127\n",
      "Epoch 817 Batch    1/175   train_loss = 1.212\n",
      "Epoch 817 Batch   33/175   train_loss = 1.218\n",
      "Epoch 817 Batch   65/175   train_loss = 1.168\n",
      "Epoch 817 Batch   97/175   train_loss = 1.161\n",
      "Epoch 817 Batch  129/175   train_loss = 1.147\n",
      "Epoch 817 Batch  161/175   train_loss = 1.161\n",
      "Epoch 818 Batch   18/175   train_loss = 1.137\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 818 Batch   50/175   train_loss = 1.141\n",
      "Epoch 818 Batch   82/175   train_loss = 1.198\n",
      "Epoch 818 Batch  114/175   train_loss = 1.197\n",
      "Epoch 818 Batch  146/175   train_loss = 1.148\n",
      "Epoch 819 Batch    3/175   train_loss = 1.193\n",
      "Epoch 819 Batch   35/175   train_loss = 1.168\n",
      "Epoch 819 Batch   67/175   train_loss = 1.124\n",
      "Epoch 819 Batch   99/175   train_loss = 1.215\n",
      "Epoch 819 Batch  131/175   train_loss = 1.145\n",
      "Epoch 819 Batch  163/175   train_loss = 1.166\n",
      "Epoch 820 Batch   20/175   train_loss = 1.157\n",
      "Epoch 820 Batch   52/175   train_loss = 1.101\n",
      "Epoch 820 Batch   84/175   train_loss = 1.162\n",
      "Epoch 820 Batch  116/175   train_loss = 1.199\n",
      "Epoch 820 Batch  148/175   train_loss = 1.152\n",
      "Epoch 821 Batch    5/175   train_loss = 1.159\n",
      "Epoch 821 Batch   37/175   train_loss = 1.152\n",
      "Epoch 821 Batch   69/175   train_loss = 1.160\n",
      "Epoch 821 Batch  101/175   train_loss = 1.216\n",
      "Epoch 821 Batch  133/175   train_loss = 1.103\n",
      "Epoch 821 Batch  165/175   train_loss = 1.175\n",
      "Epoch 822 Batch   22/175   train_loss = 1.110\n",
      "Epoch 822 Batch   54/175   train_loss = 1.211\n",
      "Epoch 822 Batch   86/175   train_loss = 1.238\n",
      "Epoch 822 Batch  118/175   train_loss = 1.210\n",
      "Epoch 822 Batch  150/175   train_loss = 1.167\n",
      "Epoch 823 Batch    7/175   train_loss = 1.187\n",
      "Epoch 823 Batch   39/175   train_loss = 1.108\n",
      "Epoch 823 Batch   71/175   train_loss = 1.201\n",
      "Epoch 823 Batch  103/175   train_loss = 1.172\n",
      "Epoch 823 Batch  135/175   train_loss = 1.131\n",
      "Epoch 823 Batch  167/175   train_loss = 1.206\n",
      "Epoch 824 Batch   24/175   train_loss = 1.141\n",
      "Epoch 824 Batch   56/175   train_loss = 1.193\n",
      "Epoch 824 Batch   88/175   train_loss = 1.188\n",
      "Epoch 824 Batch  120/175   train_loss = 1.153\n",
      "Epoch 824 Batch  152/175   train_loss = 1.139\n",
      "Epoch 825 Batch    9/175   train_loss = 1.175\n",
      "Epoch 825 Batch   41/175   train_loss = 1.192\n",
      "Epoch 825 Batch   73/175   train_loss = 1.192\n",
      "Epoch 825 Batch  105/175   train_loss = 1.210\n",
      "Epoch 825 Batch  137/175   train_loss = 1.132\n",
      "Epoch 825 Batch  169/175   train_loss = 1.185\n",
      "Epoch 826 Batch   26/175   train_loss = 1.152\n",
      "Epoch 826 Batch   58/175   train_loss = 1.198\n",
      "Epoch 826 Batch   90/175   train_loss = 1.168\n",
      "Epoch 826 Batch  122/175   train_loss = 1.135\n",
      "Epoch 826 Batch  154/175   train_loss = 1.139\n",
      "Epoch 827 Batch   11/175   train_loss = 1.135\n",
      "Epoch 827 Batch   43/175   train_loss = 1.152\n",
      "Epoch 827 Batch   75/175   train_loss = 1.140\n",
      "Epoch 827 Batch  107/175   train_loss = 1.213\n",
      "Epoch 827 Batch  139/175   train_loss = 1.122\n",
      "Epoch 827 Batch  171/175   train_loss = 1.206\n",
      "Epoch 828 Batch   28/175   train_loss = 1.150\n",
      "Epoch 828 Batch   60/175   train_loss = 1.190\n",
      "Epoch 828 Batch   92/175   train_loss = 1.149\n",
      "Epoch 828 Batch  124/175   train_loss = 1.183\n",
      "Epoch 828 Batch  156/175   train_loss = 1.211\n",
      "Epoch 829 Batch   13/175   train_loss = 1.162\n",
      "Epoch 829 Batch   45/175   train_loss = 1.149\n",
      "Epoch 829 Batch   77/175   train_loss = 1.140\n",
      "Epoch 829 Batch  109/175   train_loss = 1.208\n",
      "Epoch 829 Batch  141/175   train_loss = 1.124\n",
      "Epoch 829 Batch  173/175   train_loss = 1.140\n",
      "Epoch 830 Batch   30/175   train_loss = 1.198\n",
      "Epoch 830 Batch   62/175   train_loss = 1.197\n",
      "Epoch 830 Batch   94/175   train_loss = 1.155\n",
      "Epoch 830 Batch  126/175   train_loss = 1.179\n",
      "Epoch 830 Batch  158/175   train_loss = 1.155\n",
      "Epoch 831 Batch   15/175   train_loss = 1.192\n",
      "Epoch 831 Batch   47/175   train_loss = 1.189\n",
      "Epoch 831 Batch   79/175   train_loss = 1.197\n",
      "Epoch 831 Batch  111/175   train_loss = 1.212\n",
      "Epoch 831 Batch  143/175   train_loss = 1.148\n",
      "Epoch 832 Batch    0/175   train_loss = 1.167\n",
      "Epoch 832 Batch   32/175   train_loss = 1.172\n",
      "Epoch 832 Batch   64/175   train_loss = 1.186\n",
      "Epoch 832 Batch   96/175   train_loss = 1.181\n",
      "Epoch 832 Batch  128/175   train_loss = 1.125\n",
      "Epoch 832 Batch  160/175   train_loss = 1.146\n",
      "Epoch 833 Batch   17/175   train_loss = 1.141\n",
      "Epoch 833 Batch   49/175   train_loss = 1.176\n",
      "Epoch 833 Batch   81/175   train_loss = 1.148\n",
      "Epoch 833 Batch  113/175   train_loss = 1.179\n",
      "Epoch 833 Batch  145/175   train_loss = 1.133\n",
      "Epoch 834 Batch    2/175   train_loss = 1.179\n",
      "Epoch 834 Batch   34/175   train_loss = 1.215\n",
      "Epoch 834 Batch   66/175   train_loss = 1.203\n",
      "Epoch 834 Batch   98/175   train_loss = 1.198\n",
      "Epoch 834 Batch  130/175   train_loss = 1.195\n",
      "Epoch 834 Batch  162/175   train_loss = 1.175\n",
      "Epoch 835 Batch   19/175   train_loss = 1.147\n",
      "Epoch 835 Batch   51/175   train_loss = 1.115\n",
      "Epoch 835 Batch   83/175   train_loss = 1.214\n",
      "Epoch 835 Batch  115/175   train_loss = 1.277\n",
      "Epoch 835 Batch  147/175   train_loss = 1.173\n",
      "Epoch 836 Batch    4/175   train_loss = 1.183\n",
      "Epoch 836 Batch   36/175   train_loss = 1.145\n",
      "Epoch 836 Batch   68/175   train_loss = 1.172\n",
      "Epoch 836 Batch  100/175   train_loss = 1.173\n",
      "Epoch 836 Batch  132/175   train_loss = 1.139\n",
      "Epoch 836 Batch  164/175   train_loss = 1.148\n",
      "Epoch 837 Batch   21/175   train_loss = 1.115\n",
      "Epoch 837 Batch   53/175   train_loss = 1.130\n",
      "Epoch 837 Batch   85/175   train_loss = 1.193\n",
      "Epoch 837 Batch  117/175   train_loss = 1.170\n",
      "Epoch 837 Batch  149/175   train_loss = 1.174\n",
      "Epoch 838 Batch    6/175   train_loss = 1.191\n",
      "Epoch 838 Batch   38/175   train_loss = 1.137\n",
      "Epoch 838 Batch   70/175   train_loss = 1.163\n",
      "Epoch 838 Batch  102/175   train_loss = 1.191\n",
      "Epoch 838 Batch  134/175   train_loss = 1.135\n",
      "Epoch 838 Batch  166/175   train_loss = 1.196\n",
      "Epoch 839 Batch   23/175   train_loss = 1.108\n",
      "Epoch 839 Batch   55/175   train_loss = 1.221\n",
      "Epoch 839 Batch   87/175   train_loss = 1.246\n",
      "Epoch 839 Batch  119/175   train_loss = 1.172\n",
      "Epoch 839 Batch  151/175   train_loss = 1.181\n",
      "Epoch 840 Batch    8/175   train_loss = 1.180\n",
      "Epoch 840 Batch   40/175   train_loss = 1.145\n",
      "Epoch 840 Batch   72/175   train_loss = 1.190\n",
      "Epoch 840 Batch  104/175   train_loss = 1.189\n",
      "Epoch 840 Batch  136/175   train_loss = 1.158\n",
      "Epoch 840 Batch  168/175   train_loss = 1.213\n",
      "Epoch 841 Batch   25/175   train_loss = 1.186\n",
      "Epoch 841 Batch   57/175   train_loss = 1.222\n",
      "Epoch 841 Batch   89/175   train_loss = 1.192\n",
      "Epoch 841 Batch  121/175   train_loss = 1.146\n",
      "Epoch 841 Batch  153/175   train_loss = 1.164\n",
      "Epoch 842 Batch   10/175   train_loss = 1.130\n",
      "Epoch 842 Batch   42/175   train_loss = 1.206\n",
      "Epoch 842 Batch   74/175   train_loss = 1.222\n",
      "Epoch 842 Batch  106/175   train_loss = 1.224\n",
      "Epoch 842 Batch  138/175   train_loss = 1.165\n",
      "Epoch 842 Batch  170/175   train_loss = 1.214\n",
      "Epoch 843 Batch   27/175   train_loss = 1.169\n",
      "Epoch 843 Batch   59/175   train_loss = 1.144\n",
      "Epoch 843 Batch   91/175   train_loss = 1.156\n",
      "Epoch 843 Batch  123/175   train_loss = 1.159\n",
      "Epoch 843 Batch  155/175   train_loss = 1.140\n",
      "Epoch 844 Batch   12/175   train_loss = 1.170\n",
      "Epoch 844 Batch   44/175   train_loss = 1.142\n",
      "Epoch 844 Batch   76/175   train_loss = 1.144\n",
      "Epoch 844 Batch  108/175   train_loss = 1.188\n",
      "Epoch 844 Batch  140/175   train_loss = 1.155\n",
      "Epoch 844 Batch  172/175   train_loss = 1.185\n",
      "Epoch 845 Batch   29/175   train_loss = 1.162\n",
      "Epoch 845 Batch   61/175   train_loss = 1.189\n",
      "Epoch 845 Batch   93/175   train_loss = 1.195\n",
      "Epoch 845 Batch  125/175   train_loss = 1.200\n",
      "Epoch 845 Batch  157/175   train_loss = 1.161\n",
      "Epoch 846 Batch   14/175   train_loss = 1.183\n",
      "Epoch 846 Batch   46/175   train_loss = 1.172\n",
      "Epoch 846 Batch   78/175   train_loss = 1.143\n",
      "Epoch 846 Batch  110/175   train_loss = 1.211\n",
      "Epoch 846 Batch  142/175   train_loss = 1.158\n",
      "Epoch 846 Batch  174/175   train_loss = 1.140\n",
      "Epoch 847 Batch   31/175   train_loss = 1.166\n",
      "Epoch 847 Batch   63/175   train_loss = 1.172\n",
      "Epoch 847 Batch   95/175   train_loss = 1.134\n",
      "Epoch 847 Batch  127/175   train_loss = 1.114\n",
      "Epoch 847 Batch  159/175   train_loss = 1.120\n",
      "Epoch 848 Batch   16/175   train_loss = 1.155\n",
      "Epoch 848 Batch   48/175   train_loss = 1.157\n",
      "Epoch 848 Batch   80/175   train_loss = 1.186\n",
      "Epoch 848 Batch  112/175   train_loss = 1.149\n",
      "Epoch 848 Batch  144/175   train_loss = 1.111\n",
      "Epoch 849 Batch    1/175   train_loss = 1.200\n",
      "Epoch 849 Batch   33/175   train_loss = 1.206\n",
      "Epoch 849 Batch   65/175   train_loss = 1.176\n",
      "Epoch 849 Batch   97/175   train_loss = 1.173\n",
      "Epoch 849 Batch  129/175   train_loss = 1.154\n",
      "Epoch 849 Batch  161/175   train_loss = 1.164\n",
      "Epoch 850 Batch   18/175   train_loss = 1.136\n",
      "Epoch 850 Batch   50/175   train_loss = 1.161\n",
      "Epoch 850 Batch   82/175   train_loss = 1.217\n",
      "Epoch 850 Batch  114/175   train_loss = 1.213\n",
      "Epoch 850 Batch  146/175   train_loss = 1.158\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 851 Batch    3/175   train_loss = 1.205\n",
      "Epoch 851 Batch   35/175   train_loss = 1.175\n",
      "Epoch 851 Batch   67/175   train_loss = 1.143\n",
      "Epoch 851 Batch   99/175   train_loss = 1.223\n",
      "Epoch 851 Batch  131/175   train_loss = 1.155\n",
      "Epoch 851 Batch  163/175   train_loss = 1.168\n",
      "Epoch 852 Batch   20/175   train_loss = 1.146\n",
      "Epoch 852 Batch   52/175   train_loss = 1.122\n",
      "Epoch 852 Batch   84/175   train_loss = 1.174\n",
      "Epoch 852 Batch  116/175   train_loss = 1.214\n",
      "Epoch 852 Batch  148/175   train_loss = 1.171\n",
      "Epoch 853 Batch    5/175   train_loss = 1.171\n",
      "Epoch 853 Batch   37/175   train_loss = 1.151\n",
      "Epoch 853 Batch   69/175   train_loss = 1.172\n",
      "Epoch 853 Batch  101/175   train_loss = 1.223\n",
      "Epoch 853 Batch  133/175   train_loss = 1.100\n",
      "Epoch 853 Batch  165/175   train_loss = 1.180\n",
      "Epoch 854 Batch   22/175   train_loss = 1.131\n",
      "Epoch 854 Batch   54/175   train_loss = 1.208\n",
      "Epoch 854 Batch   86/175   train_loss = 1.233\n",
      "Epoch 854 Batch  118/175   train_loss = 1.222\n",
      "Epoch 854 Batch  150/175   train_loss = 1.186\n",
      "Epoch 855 Batch    7/175   train_loss = 1.211\n",
      "Epoch 855 Batch   39/175   train_loss = 1.133\n",
      "Epoch 855 Batch   71/175   train_loss = 1.214\n",
      "Epoch 855 Batch  103/175   train_loss = 1.173\n",
      "Epoch 855 Batch  135/175   train_loss = 1.134\n",
      "Epoch 855 Batch  167/175   train_loss = 1.212\n",
      "Epoch 856 Batch   24/175   train_loss = 1.160\n",
      "Epoch 856 Batch   56/175   train_loss = 1.219\n",
      "Epoch 856 Batch   88/175   train_loss = 1.218\n",
      "Epoch 856 Batch  120/175   train_loss = 1.154\n",
      "Epoch 856 Batch  152/175   train_loss = 1.148\n",
      "Epoch 857 Batch    9/175   train_loss = 1.213\n",
      "Epoch 857 Batch   41/175   train_loss = 1.186\n",
      "Epoch 857 Batch   73/175   train_loss = 1.187\n",
      "Epoch 857 Batch  105/175   train_loss = 1.208\n",
      "Epoch 857 Batch  137/175   train_loss = 1.123\n",
      "Epoch 857 Batch  169/175   train_loss = 1.178\n",
      "Epoch 858 Batch   26/175   train_loss = 1.173\n",
      "Epoch 858 Batch   58/175   train_loss = 1.175\n",
      "Epoch 858 Batch   90/175   train_loss = 1.174\n",
      "Epoch 858 Batch  122/175   train_loss = 1.141\n",
      "Epoch 858 Batch  154/175   train_loss = 1.154\n",
      "Epoch 859 Batch   11/175   train_loss = 1.151\n",
      "Epoch 859 Batch   43/175   train_loss = 1.158\n",
      "Epoch 859 Batch   75/175   train_loss = 1.150\n",
      "Epoch 859 Batch  107/175   train_loss = 1.210\n",
      "Epoch 859 Batch  139/175   train_loss = 1.144\n",
      "Epoch 859 Batch  171/175   train_loss = 1.225\n",
      "Epoch 860 Batch   28/175   train_loss = 1.150\n",
      "Epoch 860 Batch   60/175   train_loss = 1.169\n",
      "Epoch 860 Batch   92/175   train_loss = 1.144\n",
      "Epoch 860 Batch  124/175   train_loss = 1.160\n",
      "Epoch 860 Batch  156/175   train_loss = 1.225\n",
      "Epoch 861 Batch   13/175   train_loss = 1.177\n",
      "Epoch 861 Batch   45/175   train_loss = 1.173\n",
      "Epoch 861 Batch   77/175   train_loss = 1.170\n",
      "Epoch 861 Batch  109/175   train_loss = 1.246\n",
      "Epoch 861 Batch  141/175   train_loss = 1.158\n",
      "Epoch 861 Batch  173/175   train_loss = 1.157\n",
      "Epoch 862 Batch   30/175   train_loss = 1.213\n",
      "Epoch 862 Batch   62/175   train_loss = 1.193\n",
      "Epoch 862 Batch   94/175   train_loss = 1.177\n",
      "Epoch 862 Batch  126/175   train_loss = 1.180\n",
      "Epoch 862 Batch  158/175   train_loss = 1.180\n",
      "Epoch 863 Batch   15/175   train_loss = 1.217\n",
      "Epoch 863 Batch   47/175   train_loss = 1.186\n",
      "Epoch 863 Batch   79/175   train_loss = 1.213\n",
      "Epoch 863 Batch  111/175   train_loss = 1.229\n",
      "Epoch 863 Batch  143/175   train_loss = 1.157\n",
      "Epoch 864 Batch    0/175   train_loss = 1.189\n",
      "Epoch 864 Batch   32/175   train_loss = 1.204\n",
      "Epoch 864 Batch   64/175   train_loss = 1.210\n",
      "Epoch 864 Batch   96/175   train_loss = 1.179\n",
      "Epoch 864 Batch  128/175   train_loss = 1.135\n",
      "Epoch 864 Batch  160/175   train_loss = 1.172\n",
      "Epoch 865 Batch   17/175   train_loss = 1.153\n",
      "Epoch 865 Batch   49/175   train_loss = 1.186\n",
      "Epoch 865 Batch   81/175   train_loss = 1.152\n",
      "Epoch 865 Batch  113/175   train_loss = 1.189\n",
      "Epoch 865 Batch  145/175   train_loss = 1.141\n",
      "Epoch 866 Batch    2/175   train_loss = 1.189\n",
      "Epoch 866 Batch   34/175   train_loss = 1.207\n",
      "Epoch 866 Batch   66/175   train_loss = 1.207\n",
      "Epoch 866 Batch   98/175   train_loss = 1.194\n",
      "Epoch 866 Batch  130/175   train_loss = 1.184\n",
      "Epoch 866 Batch  162/175   train_loss = 1.167\n",
      "Epoch 867 Batch   19/175   train_loss = 1.168\n",
      "Epoch 867 Batch   51/175   train_loss = 1.117\n",
      "Epoch 867 Batch   83/175   train_loss = 1.224\n",
      "Epoch 867 Batch  115/175   train_loss = 1.303\n",
      "Epoch 867 Batch  147/175   train_loss = 1.143\n",
      "Epoch 868 Batch    4/175   train_loss = 1.184\n",
      "Epoch 868 Batch   36/175   train_loss = 1.142\n",
      "Epoch 868 Batch   68/175   train_loss = 1.168\n",
      "Epoch 868 Batch  100/175   train_loss = 1.161\n",
      "Epoch 868 Batch  132/175   train_loss = 1.142\n",
      "Epoch 868 Batch  164/175   train_loss = 1.132\n",
      "Epoch 869 Batch   21/175   train_loss = 1.133\n",
      "Epoch 869 Batch   53/175   train_loss = 1.150\n",
      "Epoch 869 Batch   85/175   train_loss = 1.186\n",
      "Epoch 869 Batch  117/175   train_loss = 1.190\n",
      "Epoch 869 Batch  149/175   train_loss = 1.184\n",
      "Epoch 870 Batch    6/175   train_loss = 1.197\n",
      "Epoch 870 Batch   38/175   train_loss = 1.137\n",
      "Epoch 870 Batch   70/175   train_loss = 1.164\n",
      "Epoch 870 Batch  102/175   train_loss = 1.169\n",
      "Epoch 870 Batch  134/175   train_loss = 1.138\n",
      "Epoch 870 Batch  166/175   train_loss = 1.198\n",
      "Epoch 871 Batch   23/175   train_loss = 1.116\n",
      "Epoch 871 Batch   55/175   train_loss = 1.218\n",
      "Epoch 871 Batch   87/175   train_loss = 1.241\n",
      "Epoch 871 Batch  119/175   train_loss = 1.166\n",
      "Epoch 871 Batch  151/175   train_loss = 1.178\n",
      "Epoch 872 Batch    8/175   train_loss = 1.194\n",
      "Epoch 872 Batch   40/175   train_loss = 1.156\n",
      "Epoch 872 Batch   72/175   train_loss = 1.207\n",
      "Epoch 872 Batch  104/175   train_loss = 1.198\n",
      "Epoch 872 Batch  136/175   train_loss = 1.164\n",
      "Epoch 872 Batch  168/175   train_loss = 1.203\n",
      "Epoch 873 Batch   25/175   train_loss = 1.173\n",
      "Epoch 873 Batch   57/175   train_loss = 1.198\n",
      "Epoch 873 Batch   89/175   train_loss = 1.171\n",
      "Epoch 873 Batch  121/175   train_loss = 1.153\n",
      "Epoch 873 Batch  153/175   train_loss = 1.168\n",
      "Epoch 874 Batch   10/175   train_loss = 1.114\n",
      "Epoch 874 Batch   42/175   train_loss = 1.203\n",
      "Epoch 874 Batch   74/175   train_loss = 1.209\n",
      "Epoch 874 Batch  106/175   train_loss = 1.231\n",
      "Epoch 874 Batch  138/175   train_loss = 1.173\n",
      "Epoch 874 Batch  170/175   train_loss = 1.215\n",
      "Epoch 875 Batch   27/175   train_loss = 1.164\n",
      "Epoch 875 Batch   59/175   train_loss = 1.149\n",
      "Epoch 875 Batch   91/175   train_loss = 1.151\n",
      "Epoch 875 Batch  123/175   train_loss = 1.147\n",
      "Epoch 875 Batch  155/175   train_loss = 1.131\n",
      "Epoch 876 Batch   12/175   train_loss = 1.160\n",
      "Epoch 876 Batch   44/175   train_loss = 1.138\n",
      "Epoch 876 Batch   76/175   train_loss = 1.154\n",
      "Epoch 876 Batch  108/175   train_loss = 1.179\n",
      "Epoch 876 Batch  140/175   train_loss = 1.143\n",
      "Epoch 876 Batch  172/175   train_loss = 1.175\n",
      "Epoch 877 Batch   29/175   train_loss = 1.169\n",
      "Epoch 877 Batch   61/175   train_loss = 1.184\n",
      "Epoch 877 Batch   93/175   train_loss = 1.165\n",
      "Epoch 877 Batch  125/175   train_loss = 1.191\n",
      "Epoch 877 Batch  157/175   train_loss = 1.151\n",
      "Epoch 878 Batch   14/175   train_loss = 1.183\n",
      "Epoch 878 Batch   46/175   train_loss = 1.176\n",
      "Epoch 878 Batch   78/175   train_loss = 1.143\n",
      "Epoch 878 Batch  110/175   train_loss = 1.241\n",
      "Epoch 878 Batch  142/175   train_loss = 1.174\n",
      "Epoch 878 Batch  174/175   train_loss = 1.152\n",
      "Epoch 879 Batch   31/175   train_loss = 1.193\n",
      "Epoch 879 Batch   63/175   train_loss = 1.168\n",
      "Epoch 879 Batch   95/175   train_loss = 1.145\n",
      "Epoch 879 Batch  127/175   train_loss = 1.127\n",
      "Epoch 879 Batch  159/175   train_loss = 1.129\n",
      "Epoch 880 Batch   16/175   train_loss = 1.174\n",
      "Epoch 880 Batch   48/175   train_loss = 1.160\n",
      "Epoch 880 Batch   80/175   train_loss = 1.190\n",
      "Epoch 880 Batch  112/175   train_loss = 1.162\n",
      "Epoch 880 Batch  144/175   train_loss = 1.096\n",
      "Epoch 881 Batch    1/175   train_loss = 1.196\n",
      "Epoch 881 Batch   33/175   train_loss = 1.207\n",
      "Epoch 881 Batch   65/175   train_loss = 1.160\n",
      "Epoch 881 Batch   97/175   train_loss = 1.177\n",
      "Epoch 881 Batch  129/175   train_loss = 1.146\n",
      "Epoch 881 Batch  161/175   train_loss = 1.141\n",
      "Epoch 882 Batch   18/175   train_loss = 1.127\n",
      "Epoch 882 Batch   50/175   train_loss = 1.160\n",
      "Epoch 882 Batch   82/175   train_loss = 1.196\n",
      "Epoch 882 Batch  114/175   train_loss = 1.243\n",
      "Epoch 882 Batch  146/175   train_loss = 1.165\n",
      "Epoch 883 Batch    3/175   train_loss = 1.215\n",
      "Epoch 883 Batch   35/175   train_loss = 1.170\n",
      "Epoch 883 Batch   67/175   train_loss = 1.127\n",
      "Epoch 883 Batch   99/175   train_loss = 1.231\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 883 Batch  131/175   train_loss = 1.143\n",
      "Epoch 883 Batch  163/175   train_loss = 1.176\n",
      "Epoch 884 Batch   20/175   train_loss = 1.154\n",
      "Epoch 884 Batch   52/175   train_loss = 1.116\n",
      "Epoch 884 Batch   84/175   train_loss = 1.175\n",
      "Epoch 884 Batch  116/175   train_loss = 1.212\n",
      "Epoch 884 Batch  148/175   train_loss = 1.169\n",
      "Epoch 885 Batch    5/175   train_loss = 1.168\n",
      "Epoch 885 Batch   37/175   train_loss = 1.144\n",
      "Epoch 885 Batch   69/175   train_loss = 1.174\n",
      "Epoch 885 Batch  101/175   train_loss = 1.194\n",
      "Epoch 885 Batch  133/175   train_loss = 1.095\n",
      "Epoch 885 Batch  165/175   train_loss = 1.185\n",
      "Epoch 886 Batch   22/175   train_loss = 1.117\n",
      "Epoch 886 Batch   54/175   train_loss = 1.195\n",
      "Epoch 886 Batch   86/175   train_loss = 1.235\n",
      "Epoch 886 Batch  118/175   train_loss = 1.215\n",
      "Epoch 886 Batch  150/175   train_loss = 1.172\n",
      "Epoch 887 Batch    7/175   train_loss = 1.192\n",
      "Epoch 887 Batch   39/175   train_loss = 1.117\n",
      "Epoch 887 Batch   71/175   train_loss = 1.209\n",
      "Epoch 887 Batch  103/175   train_loss = 1.190\n",
      "Epoch 887 Batch  135/175   train_loss = 1.157\n",
      "Epoch 887 Batch  167/175   train_loss = 1.219\n",
      "Epoch 888 Batch   24/175   train_loss = 1.138\n",
      "Epoch 888 Batch   56/175   train_loss = 1.202\n",
      "Epoch 888 Batch   88/175   train_loss = 1.193\n",
      "Epoch 888 Batch  120/175   train_loss = 1.157\n",
      "Epoch 888 Batch  152/175   train_loss = 1.145\n",
      "Epoch 889 Batch    9/175   train_loss = 1.191\n",
      "Epoch 889 Batch   41/175   train_loss = 1.204\n",
      "Epoch 889 Batch   73/175   train_loss = 1.165\n",
      "Epoch 889 Batch  105/175   train_loss = 1.215\n",
      "Epoch 889 Batch  137/175   train_loss = 1.149\n",
      "Epoch 889 Batch  169/175   train_loss = 1.188\n",
      "Epoch 890 Batch   26/175   train_loss = 1.181\n",
      "Epoch 890 Batch   58/175   train_loss = 1.197\n",
      "Epoch 890 Batch   90/175   train_loss = 1.160\n",
      "Epoch 890 Batch  122/175   train_loss = 1.146\n",
      "Epoch 890 Batch  154/175   train_loss = 1.148\n",
      "Epoch 891 Batch   11/175   train_loss = 1.151\n",
      "Epoch 891 Batch   43/175   train_loss = 1.144\n",
      "Epoch 891 Batch   75/175   train_loss = 1.151\n",
      "Epoch 891 Batch  107/175   train_loss = 1.212\n",
      "Epoch 891 Batch  139/175   train_loss = 1.126\n",
      "Epoch 891 Batch  171/175   train_loss = 1.207\n",
      "Epoch 892 Batch   28/175   train_loss = 1.144\n",
      "Epoch 892 Batch   60/175   train_loss = 1.172\n",
      "Epoch 892 Batch   92/175   train_loss = 1.139\n",
      "Epoch 892 Batch  124/175   train_loss = 1.173\n",
      "Epoch 892 Batch  156/175   train_loss = 1.221\n",
      "Epoch 893 Batch   13/175   train_loss = 1.176\n",
      "Epoch 893 Batch   45/175   train_loss = 1.165\n",
      "Epoch 893 Batch   77/175   train_loss = 1.146\n",
      "Epoch 893 Batch  109/175   train_loss = 1.187\n",
      "Epoch 893 Batch  141/175   train_loss = 1.129\n",
      "Epoch 893 Batch  173/175   train_loss = 1.126\n",
      "Epoch 894 Batch   30/175   train_loss = 1.224\n",
      "Epoch 894 Batch   62/175   train_loss = 1.192\n",
      "Epoch 894 Batch   94/175   train_loss = 1.147\n",
      "Epoch 894 Batch  126/175   train_loss = 1.185\n",
      "Epoch 894 Batch  158/175   train_loss = 1.152\n",
      "Epoch 895 Batch   15/175   train_loss = 1.208\n",
      "Epoch 895 Batch   47/175   train_loss = 1.177\n",
      "Epoch 895 Batch   79/175   train_loss = 1.200\n",
      "Epoch 895 Batch  111/175   train_loss = 1.210\n",
      "Epoch 895 Batch  143/175   train_loss = 1.152\n",
      "Epoch 896 Batch    0/175   train_loss = 1.173\n",
      "Epoch 896 Batch   32/175   train_loss = 1.171\n",
      "Epoch 896 Batch   64/175   train_loss = 1.181\n",
      "Epoch 896 Batch   96/175   train_loss = 1.192\n",
      "Epoch 896 Batch  128/175   train_loss = 1.125\n",
      "Epoch 896 Batch  160/175   train_loss = 1.166\n",
      "Epoch 897 Batch   17/175   train_loss = 1.138\n",
      "Epoch 897 Batch   49/175   train_loss = 1.182\n",
      "Epoch 897 Batch   81/175   train_loss = 1.140\n",
      "Epoch 897 Batch  113/175   train_loss = 1.170\n",
      "Epoch 897 Batch  145/175   train_loss = 1.124\n",
      "Epoch 898 Batch    2/175   train_loss = 1.163\n",
      "Epoch 898 Batch   34/175   train_loss = 1.201\n",
      "Epoch 898 Batch   66/175   train_loss = 1.180\n",
      "Epoch 898 Batch   98/175   train_loss = 1.171\n",
      "Epoch 898 Batch  130/175   train_loss = 1.204\n",
      "Epoch 898 Batch  162/175   train_loss = 1.142\n",
      "Epoch 899 Batch   19/175   train_loss = 1.159\n",
      "Epoch 899 Batch   51/175   train_loss = 1.103\n",
      "Epoch 899 Batch   83/175   train_loss = 1.207\n",
      "Epoch 899 Batch  115/175   train_loss = 1.267\n",
      "Epoch 899 Batch  147/175   train_loss = 1.147\n",
      "Epoch 900 Batch    4/175   train_loss = 1.176\n",
      "Epoch 900 Batch   36/175   train_loss = 1.124\n",
      "Epoch 900 Batch   68/175   train_loss = 1.140\n",
      "Epoch 900 Batch  100/175   train_loss = 1.178\n",
      "Epoch 900 Batch  132/175   train_loss = 1.136\n",
      "Epoch 900 Batch  164/175   train_loss = 1.140\n",
      "Epoch 901 Batch   21/175   train_loss = 1.130\n",
      "Epoch 901 Batch   53/175   train_loss = 1.145\n",
      "Epoch 901 Batch   85/175   train_loss = 1.168\n",
      "Epoch 901 Batch  117/175   train_loss = 1.191\n",
      "Epoch 901 Batch  149/175   train_loss = 1.173\n",
      "Epoch 902 Batch    6/175   train_loss = 1.195\n",
      "Epoch 902 Batch   38/175   train_loss = 1.130\n",
      "Epoch 902 Batch   70/175   train_loss = 1.169\n",
      "Epoch 902 Batch  102/175   train_loss = 1.182\n",
      "Epoch 902 Batch  134/175   train_loss = 1.129\n",
      "Epoch 902 Batch  166/175   train_loss = 1.161\n",
      "Epoch 903 Batch   23/175   train_loss = 1.108\n",
      "Epoch 903 Batch   55/175   train_loss = 1.207\n",
      "Epoch 903 Batch   87/175   train_loss = 1.225\n",
      "Epoch 903 Batch  119/175   train_loss = 1.128\n",
      "Epoch 903 Batch  151/175   train_loss = 1.178\n",
      "Epoch 904 Batch    8/175   train_loss = 1.167\n",
      "Epoch 904 Batch   40/175   train_loss = 1.172\n",
      "Epoch 904 Batch   72/175   train_loss = 1.186\n",
      "Epoch 904 Batch  104/175   train_loss = 1.175\n",
      "Epoch 904 Batch  136/175   train_loss = 1.158\n",
      "Epoch 904 Batch  168/175   train_loss = 1.200\n",
      "Epoch 905 Batch   25/175   train_loss = 1.171\n",
      "Epoch 905 Batch   57/175   train_loss = 1.199\n",
      "Epoch 905 Batch   89/175   train_loss = 1.168\n",
      "Epoch 905 Batch  121/175   train_loss = 1.145\n",
      "Epoch 905 Batch  153/175   train_loss = 1.171\n",
      "Epoch 906 Batch   10/175   train_loss = 1.125\n",
      "Epoch 906 Batch   42/175   train_loss = 1.207\n",
      "Epoch 906 Batch   74/175   train_loss = 1.203\n",
      "Epoch 906 Batch  106/175   train_loss = 1.205\n",
      "Epoch 906 Batch  138/175   train_loss = 1.150\n",
      "Epoch 906 Batch  170/175   train_loss = 1.211\n",
      "Epoch 907 Batch   27/175   train_loss = 1.156\n",
      "Epoch 907 Batch   59/175   train_loss = 1.156\n",
      "Epoch 907 Batch   91/175   train_loss = 1.153\n",
      "Epoch 907 Batch  123/175   train_loss = 1.162\n",
      "Epoch 907 Batch  155/175   train_loss = 1.137\n",
      "Epoch 908 Batch   12/175   train_loss = 1.166\n",
      "Epoch 908 Batch   44/175   train_loss = 1.137\n",
      "Epoch 908 Batch   76/175   train_loss = 1.136\n",
      "Epoch 908 Batch  108/175   train_loss = 1.189\n",
      "Epoch 908 Batch  140/175   train_loss = 1.150\n",
      "Epoch 908 Batch  172/175   train_loss = 1.184\n",
      "Epoch 909 Batch   29/175   train_loss = 1.153\n",
      "Epoch 909 Batch   61/175   train_loss = 1.173\n",
      "Epoch 909 Batch   93/175   train_loss = 1.168\n",
      "Epoch 909 Batch  125/175   train_loss = 1.169\n",
      "Epoch 909 Batch  157/175   train_loss = 1.138\n",
      "Epoch 910 Batch   14/175   train_loss = 1.192\n",
      "Epoch 910 Batch   46/175   train_loss = 1.177\n",
      "Epoch 910 Batch   78/175   train_loss = 1.150\n",
      "Epoch 910 Batch  110/175   train_loss = 1.251\n",
      "Epoch 910 Batch  142/175   train_loss = 1.185\n",
      "Epoch 910 Batch  174/175   train_loss = 1.150\n",
      "Epoch 911 Batch   31/175   train_loss = 1.193\n",
      "Epoch 911 Batch   63/175   train_loss = 1.174\n",
      "Epoch 911 Batch   95/175   train_loss = 1.143\n",
      "Epoch 911 Batch  127/175   train_loss = 1.129\n",
      "Epoch 911 Batch  159/175   train_loss = 1.137\n",
      "Epoch 912 Batch   16/175   train_loss = 1.159\n",
      "Epoch 912 Batch   48/175   train_loss = 1.179\n",
      "Epoch 912 Batch   80/175   train_loss = 1.196\n",
      "Epoch 912 Batch  112/175   train_loss = 1.159\n",
      "Epoch 912 Batch  144/175   train_loss = 1.110\n",
      "Epoch 913 Batch    1/175   train_loss = 1.209\n",
      "Epoch 913 Batch   33/175   train_loss = 1.215\n",
      "Epoch 913 Batch   65/175   train_loss = 1.171\n",
      "Epoch 913 Batch   97/175   train_loss = 1.187\n",
      "Epoch 913 Batch  129/175   train_loss = 1.149\n",
      "Epoch 913 Batch  161/175   train_loss = 1.162\n",
      "Epoch 914 Batch   18/175   train_loss = 1.120\n",
      "Epoch 914 Batch   50/175   train_loss = 1.151\n",
      "Epoch 914 Batch   82/175   train_loss = 1.213\n",
      "Epoch 914 Batch  114/175   train_loss = 1.221\n",
      "Epoch 914 Batch  146/175   train_loss = 1.151\n",
      "Epoch 915 Batch    3/175   train_loss = 1.192\n",
      "Epoch 915 Batch   35/175   train_loss = 1.145\n",
      "Epoch 915 Batch   67/175   train_loss = 1.123\n",
      "Epoch 915 Batch   99/175   train_loss = 1.235\n",
      "Epoch 915 Batch  131/175   train_loss = 1.145\n",
      "Epoch 915 Batch  163/175   train_loss = 1.155\n",
      "Epoch 916 Batch   20/175   train_loss = 1.140\n",
      "Epoch 916 Batch   52/175   train_loss = 1.104\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 916 Batch   84/175   train_loss = 1.191\n",
      "Epoch 916 Batch  116/175   train_loss = 1.214\n",
      "Epoch 916 Batch  148/175   train_loss = 1.145\n",
      "Epoch 917 Batch    5/175   train_loss = 1.152\n",
      "Epoch 917 Batch   37/175   train_loss = 1.133\n",
      "Epoch 917 Batch   69/175   train_loss = 1.214\n",
      "Epoch 917 Batch  101/175   train_loss = 1.215\n",
      "Epoch 917 Batch  133/175   train_loss = 1.093\n",
      "Epoch 917 Batch  165/175   train_loss = 1.176\n",
      "Epoch 918 Batch   22/175   train_loss = 1.122\n",
      "Epoch 918 Batch   54/175   train_loss = 1.192\n",
      "Epoch 918 Batch   86/175   train_loss = 1.237\n",
      "Epoch 918 Batch  118/175   train_loss = 1.216\n",
      "Epoch 918 Batch  150/175   train_loss = 1.179\n",
      "Epoch 919 Batch    7/175   train_loss = 1.212\n",
      "Epoch 919 Batch   39/175   train_loss = 1.138\n",
      "Epoch 919 Batch   71/175   train_loss = 1.211\n",
      "Epoch 919 Batch  103/175   train_loss = 1.181\n",
      "Epoch 919 Batch  135/175   train_loss = 1.150\n",
      "Epoch 919 Batch  167/175   train_loss = 1.236\n",
      "Epoch 920 Batch   24/175   train_loss = 1.120\n",
      "Epoch 920 Batch   56/175   train_loss = 1.208\n",
      "Epoch 920 Batch   88/175   train_loss = 1.217\n",
      "Epoch 920 Batch  120/175   train_loss = 1.154\n",
      "Epoch 920 Batch  152/175   train_loss = 1.144\n",
      "Epoch 921 Batch    9/175   train_loss = 1.201\n",
      "Epoch 921 Batch   41/175   train_loss = 1.212\n",
      "Epoch 921 Batch   73/175   train_loss = 1.190\n",
      "Epoch 921 Batch  105/175   train_loss = 1.212\n",
      "Epoch 921 Batch  137/175   train_loss = 1.157\n",
      "Epoch 921 Batch  169/175   train_loss = 1.200\n",
      "Epoch 922 Batch   26/175   train_loss = 1.172\n",
      "Epoch 922 Batch   58/175   train_loss = 1.217\n",
      "Epoch 922 Batch   90/175   train_loss = 1.191\n",
      "Epoch 922 Batch  122/175   train_loss = 1.148\n",
      "Epoch 922 Batch  154/175   train_loss = 1.150\n",
      "Epoch 923 Batch   11/175   train_loss = 1.154\n",
      "Epoch 923 Batch   43/175   train_loss = 1.167\n",
      "Epoch 923 Batch   75/175   train_loss = 1.147\n",
      "Epoch 923 Batch  107/175   train_loss = 1.234\n",
      "Epoch 923 Batch  139/175   train_loss = 1.122\n",
      "Epoch 923 Batch  171/175   train_loss = 1.219\n",
      "Epoch 924 Batch   28/175   train_loss = 1.142\n",
      "Epoch 924 Batch   60/175   train_loss = 1.173\n",
      "Epoch 924 Batch   92/175   train_loss = 1.146\n",
      "Epoch 924 Batch  124/175   train_loss = 1.172\n",
      "Epoch 924 Batch  156/175   train_loss = 1.222\n",
      "Epoch 925 Batch   13/175   train_loss = 1.178\n",
      "Epoch 925 Batch   45/175   train_loss = 1.172\n",
      "Epoch 925 Batch   77/175   train_loss = 1.178\n",
      "Epoch 925 Batch  109/175   train_loss = 1.197\n",
      "Epoch 925 Batch  141/175   train_loss = 1.141\n",
      "Epoch 925 Batch  173/175   train_loss = 1.156\n",
      "Epoch 926 Batch   30/175   train_loss = 1.227\n",
      "Epoch 926 Batch   62/175   train_loss = 1.205\n",
      "Epoch 926 Batch   94/175   train_loss = 1.164\n",
      "Epoch 926 Batch  126/175   train_loss = 1.180\n",
      "Epoch 926 Batch  158/175   train_loss = 1.159\n",
      "Epoch 927 Batch   15/175   train_loss = 1.208\n",
      "Epoch 927 Batch   47/175   train_loss = 1.183\n",
      "Epoch 927 Batch   79/175   train_loss = 1.213\n",
      "Epoch 927 Batch  111/175   train_loss = 1.214\n",
      "Epoch 927 Batch  143/175   train_loss = 1.138\n",
      "Epoch 928 Batch    0/175   train_loss = 1.181\n",
      "Epoch 928 Batch   32/175   train_loss = 1.178\n",
      "Epoch 928 Batch   64/175   train_loss = 1.206\n",
      "Epoch 928 Batch   96/175   train_loss = 1.203\n",
      "Epoch 928 Batch  128/175   train_loss = 1.121\n",
      "Epoch 928 Batch  160/175   train_loss = 1.162\n",
      "Epoch 929 Batch   17/175   train_loss = 1.133\n",
      "Epoch 929 Batch   49/175   train_loss = 1.166\n",
      "Epoch 929 Batch   81/175   train_loss = 1.162\n",
      "Epoch 929 Batch  113/175   train_loss = 1.168\n",
      "Epoch 929 Batch  145/175   train_loss = 1.124\n",
      "Epoch 930 Batch    2/175   train_loss = 1.174\n",
      "Epoch 930 Batch   34/175   train_loss = 1.194\n",
      "Epoch 930 Batch   66/175   train_loss = 1.209\n",
      "Epoch 930 Batch   98/175   train_loss = 1.205\n",
      "Epoch 930 Batch  130/175   train_loss = 1.205\n",
      "Epoch 930 Batch  162/175   train_loss = 1.155\n",
      "Epoch 931 Batch   19/175   train_loss = 1.186\n",
      "Epoch 931 Batch   51/175   train_loss = 1.118\n",
      "Epoch 931 Batch   83/175   train_loss = 1.228\n",
      "Epoch 931 Batch  115/175   train_loss = 1.299\n",
      "Epoch 931 Batch  147/175   train_loss = 1.147\n",
      "Epoch 932 Batch    4/175   train_loss = 1.179\n",
      "Epoch 932 Batch   36/175   train_loss = 1.133\n",
      "Epoch 932 Batch   68/175   train_loss = 1.158\n",
      "Epoch 932 Batch  100/175   train_loss = 1.169\n",
      "Epoch 932 Batch  132/175   train_loss = 1.139\n",
      "Epoch 932 Batch  164/175   train_loss = 1.139\n",
      "Epoch 933 Batch   21/175   train_loss = 1.133\n",
      "Epoch 933 Batch   53/175   train_loss = 1.146\n",
      "Epoch 933 Batch   85/175   train_loss = 1.185\n",
      "Epoch 933 Batch  117/175   train_loss = 1.201\n",
      "Epoch 933 Batch  149/175   train_loss = 1.190\n",
      "Epoch 934 Batch    6/175   train_loss = 1.215\n",
      "Epoch 934 Batch   38/175   train_loss = 1.145\n",
      "Epoch 934 Batch   70/175   train_loss = 1.189\n",
      "Epoch 934 Batch  102/175   train_loss = 1.184\n",
      "Epoch 934 Batch  134/175   train_loss = 1.136\n",
      "Epoch 934 Batch  166/175   train_loss = 1.171\n",
      "Epoch 935 Batch   23/175   train_loss = 1.123\n",
      "Epoch 935 Batch   55/175   train_loss = 1.213\n",
      "Epoch 935 Batch   87/175   train_loss = 1.220\n",
      "Epoch 935 Batch  119/175   train_loss = 1.149\n",
      "Epoch 935 Batch  151/175   train_loss = 1.193\n",
      "Epoch 936 Batch    8/175   train_loss = 1.177\n",
      "Epoch 936 Batch   40/175   train_loss = 1.153\n",
      "Epoch 936 Batch   72/175   train_loss = 1.182\n",
      "Epoch 936 Batch  104/175   train_loss = 1.184\n",
      "Epoch 936 Batch  136/175   train_loss = 1.144\n",
      "Epoch 936 Batch  168/175   train_loss = 1.187\n",
      "Epoch 937 Batch   25/175   train_loss = 1.180\n",
      "Epoch 937 Batch   57/175   train_loss = 1.196\n",
      "Epoch 937 Batch   89/175   train_loss = 1.166\n",
      "Epoch 937 Batch  121/175   train_loss = 1.150\n",
      "Epoch 937 Batch  153/175   train_loss = 1.174\n",
      "Epoch 938 Batch   10/175   train_loss = 1.138\n",
      "Epoch 938 Batch   42/175   train_loss = 1.217\n",
      "Epoch 938 Batch   74/175   train_loss = 1.203\n",
      "Epoch 938 Batch  106/175   train_loss = 1.219\n",
      "Epoch 938 Batch  138/175   train_loss = 1.165\n",
      "Epoch 938 Batch  170/175   train_loss = 1.197\n",
      "Epoch 939 Batch   27/175   train_loss = 1.155\n",
      "Epoch 939 Batch   59/175   train_loss = 1.163\n",
      "Epoch 939 Batch   91/175   train_loss = 1.158\n",
      "Epoch 939 Batch  123/175   train_loss = 1.154\n",
      "Epoch 939 Batch  155/175   train_loss = 1.171\n",
      "Epoch 940 Batch   12/175   train_loss = 1.169\n",
      "Epoch 940 Batch   44/175   train_loss = 1.147\n",
      "Epoch 940 Batch   76/175   train_loss = 1.136\n",
      "Epoch 940 Batch  108/175   train_loss = 1.193\n",
      "Epoch 940 Batch  140/175   train_loss = 1.151\n",
      "Epoch 940 Batch  172/175   train_loss = 1.192\n",
      "Epoch 941 Batch   29/175   train_loss = 1.160\n",
      "Epoch 941 Batch   61/175   train_loss = 1.184\n",
      "Epoch 941 Batch   93/175   train_loss = 1.176\n",
      "Epoch 941 Batch  125/175   train_loss = 1.200\n",
      "Epoch 941 Batch  157/175   train_loss = 1.157\n",
      "Epoch 942 Batch   14/175   train_loss = 1.191\n",
      "Epoch 942 Batch   46/175   train_loss = 1.173\n",
      "Epoch 942 Batch   78/175   train_loss = 1.148\n",
      "Epoch 942 Batch  110/175   train_loss = 1.252\n",
      "Epoch 942 Batch  142/175   train_loss = 1.199\n",
      "Epoch 942 Batch  174/175   train_loss = 1.162\n",
      "Epoch 943 Batch   31/175   train_loss = 1.197\n",
      "Epoch 943 Batch   63/175   train_loss = 1.181\n",
      "Epoch 943 Batch   95/175   train_loss = 1.159\n",
      "Epoch 943 Batch  127/175   train_loss = 1.126\n",
      "Epoch 943 Batch  159/175   train_loss = 1.127\n",
      "Epoch 944 Batch   16/175   train_loss = 1.175\n",
      "Epoch 944 Batch   48/175   train_loss = 1.173\n",
      "Epoch 944 Batch   80/175   train_loss = 1.214\n",
      "Epoch 944 Batch  112/175   train_loss = 1.172\n",
      "Epoch 944 Batch  144/175   train_loss = 1.135\n",
      "Epoch 945 Batch    1/175   train_loss = 1.200\n",
      "Epoch 945 Batch   33/175   train_loss = 1.217\n",
      "Epoch 945 Batch   65/175   train_loss = 1.179\n",
      "Epoch 945 Batch   97/175   train_loss = 1.171\n",
      "Epoch 945 Batch  129/175   train_loss = 1.156\n",
      "Epoch 945 Batch  161/175   train_loss = 1.174\n",
      "Epoch 946 Batch   18/175   train_loss = 1.146\n",
      "Epoch 946 Batch   50/175   train_loss = 1.174\n",
      "Epoch 946 Batch   82/175   train_loss = 1.213\n",
      "Epoch 946 Batch  114/175   train_loss = 1.229\n",
      "Epoch 946 Batch  146/175   train_loss = 1.158\n",
      "Epoch 947 Batch    3/175   train_loss = 1.196\n",
      "Epoch 947 Batch   35/175   train_loss = 1.166\n",
      "Epoch 947 Batch   67/175   train_loss = 1.148\n",
      "Epoch 947 Batch   99/175   train_loss = 1.249\n",
      "Epoch 947 Batch  131/175   train_loss = 1.166\n",
      "Epoch 947 Batch  163/175   train_loss = 1.168\n",
      "Epoch 948 Batch   20/175   train_loss = 1.144\n",
      "Epoch 948 Batch   52/175   train_loss = 1.107\n",
      "Epoch 948 Batch   84/175   train_loss = 1.171\n",
      "Epoch 948 Batch  116/175   train_loss = 1.209\n",
      "Epoch 948 Batch  148/175   train_loss = 1.162\n",
      "Epoch 949 Batch    5/175   train_loss = 1.169\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 949 Batch   37/175   train_loss = 1.129\n",
      "Epoch 949 Batch   69/175   train_loss = 1.185\n",
      "Epoch 949 Batch  101/175   train_loss = 1.209\n",
      "Epoch 949 Batch  133/175   train_loss = 1.089\n",
      "Epoch 949 Batch  165/175   train_loss = 1.163\n",
      "Epoch 950 Batch   22/175   train_loss = 1.107\n",
      "Epoch 950 Batch   54/175   train_loss = 1.192\n",
      "Epoch 950 Batch   86/175   train_loss = 1.234\n",
      "Epoch 950 Batch  118/175   train_loss = 1.224\n",
      "Epoch 950 Batch  150/175   train_loss = 1.183\n",
      "Epoch 951 Batch    7/175   train_loss = 1.214\n",
      "Epoch 951 Batch   39/175   train_loss = 1.130\n",
      "Epoch 951 Batch   71/175   train_loss = 1.205\n",
      "Epoch 951 Batch  103/175   train_loss = 1.169\n",
      "Epoch 951 Batch  135/175   train_loss = 1.136\n",
      "Epoch 951 Batch  167/175   train_loss = 1.228\n",
      "Epoch 952 Batch   24/175   train_loss = 1.125\n",
      "Epoch 952 Batch   56/175   train_loss = 1.195\n",
      "Epoch 952 Batch   88/175   train_loss = 1.203\n",
      "Epoch 952 Batch  120/175   train_loss = 1.150\n",
      "Epoch 952 Batch  152/175   train_loss = 1.162\n",
      "Epoch 953 Batch    9/175   train_loss = 1.202\n",
      "Epoch 953 Batch   41/175   train_loss = 1.201\n",
      "Epoch 953 Batch   73/175   train_loss = 1.193\n",
      "Epoch 953 Batch  105/175   train_loss = 1.197\n",
      "Epoch 953 Batch  137/175   train_loss = 1.142\n",
      "Epoch 953 Batch  169/175   train_loss = 1.195\n",
      "Epoch 954 Batch   26/175   train_loss = 1.182\n",
      "Epoch 954 Batch   58/175   train_loss = 1.213\n",
      "Epoch 954 Batch   90/175   train_loss = 1.210\n",
      "Epoch 954 Batch  122/175   train_loss = 1.174\n",
      "Epoch 954 Batch  154/175   train_loss = 1.156\n",
      "Epoch 955 Batch   11/175   train_loss = 1.153\n",
      "Epoch 955 Batch   43/175   train_loss = 1.164\n",
      "Epoch 955 Batch   75/175   train_loss = 1.145\n",
      "Epoch 955 Batch  107/175   train_loss = 1.234\n",
      "Epoch 955 Batch  139/175   train_loss = 1.129\n",
      "Epoch 955 Batch  171/175   train_loss = 1.219\n",
      "Epoch 956 Batch   28/175   train_loss = 1.157\n",
      "Epoch 956 Batch   60/175   train_loss = 1.160\n",
      "Epoch 956 Batch   92/175   train_loss = 1.140\n",
      "Epoch 956 Batch  124/175   train_loss = 1.176\n",
      "Epoch 956 Batch  156/175   train_loss = 1.205\n",
      "Epoch 957 Batch   13/175   train_loss = 1.183\n",
      "Epoch 957 Batch   45/175   train_loss = 1.168\n",
      "Epoch 957 Batch   77/175   train_loss = 1.154\n",
      "Epoch 957 Batch  109/175   train_loss = 1.230\n",
      "Epoch 957 Batch  141/175   train_loss = 1.149\n",
      "Epoch 957 Batch  173/175   train_loss = 1.147\n",
      "Epoch 958 Batch   30/175   train_loss = 1.226\n",
      "Epoch 958 Batch   62/175   train_loss = 1.196\n",
      "Epoch 958 Batch   94/175   train_loss = 1.166\n",
      "Epoch 958 Batch  126/175   train_loss = 1.172\n",
      "Epoch 958 Batch  158/175   train_loss = 1.170\n",
      "Epoch 959 Batch   15/175   train_loss = 1.206\n",
      "Epoch 959 Batch   47/175   train_loss = 1.200\n",
      "Epoch 959 Batch   79/175   train_loss = 1.208\n",
      "Epoch 959 Batch  111/175   train_loss = 1.229\n",
      "Epoch 959 Batch  143/175   train_loss = 1.162\n",
      "Epoch 960 Batch    0/175   train_loss = 1.204\n",
      "Epoch 960 Batch   32/175   train_loss = 1.201\n",
      "Epoch 960 Batch   64/175   train_loss = 1.232\n",
      "Epoch 960 Batch   96/175   train_loss = 1.210\n",
      "Epoch 960 Batch  128/175   train_loss = 1.154\n",
      "Epoch 960 Batch  160/175   train_loss = 1.184\n",
      "Epoch 961 Batch   17/175   train_loss = 1.151\n",
      "Epoch 961 Batch   49/175   train_loss = 1.186\n",
      "Epoch 961 Batch   81/175   train_loss = 1.151\n",
      "Epoch 961 Batch  113/175   train_loss = 1.191\n",
      "Epoch 961 Batch  145/175   train_loss = 1.139\n",
      "Epoch 962 Batch    2/175   train_loss = 1.194\n",
      "Epoch 962 Batch   34/175   train_loss = 1.203\n",
      "Epoch 962 Batch   66/175   train_loss = 1.223\n",
      "Epoch 962 Batch   98/175   train_loss = 1.196\n",
      "Epoch 962 Batch  130/175   train_loss = 1.207\n",
      "Epoch 962 Batch  162/175   train_loss = 1.183\n",
      "Epoch 963 Batch   19/175   train_loss = 1.179\n",
      "Epoch 963 Batch   51/175   train_loss = 1.127\n",
      "Epoch 963 Batch   83/175   train_loss = 1.211\n",
      "Epoch 963 Batch  115/175   train_loss = 1.303\n",
      "Epoch 963 Batch  147/175   train_loss = 1.151\n",
      "Epoch 964 Batch    4/175   train_loss = 1.165\n",
      "Epoch 964 Batch   36/175   train_loss = 1.124\n",
      "Epoch 964 Batch   68/175   train_loss = 1.178\n",
      "Epoch 964 Batch  100/175   train_loss = 1.186\n",
      "Epoch 964 Batch  132/175   train_loss = 1.141\n",
      "Epoch 964 Batch  164/175   train_loss = 1.141\n",
      "Epoch 965 Batch   21/175   train_loss = 1.140\n",
      "Epoch 965 Batch   53/175   train_loss = 1.151\n",
      "Epoch 965 Batch   85/175   train_loss = 1.180\n",
      "Epoch 965 Batch  117/175   train_loss = 1.196\n",
      "Epoch 965 Batch  149/175   train_loss = 1.196\n",
      "Epoch 966 Batch    6/175   train_loss = 1.196\n",
      "Epoch 966 Batch   38/175   train_loss = 1.150\n",
      "Epoch 966 Batch   70/175   train_loss = 1.179\n",
      "Epoch 966 Batch  102/175   train_loss = 1.189\n",
      "Epoch 966 Batch  134/175   train_loss = 1.131\n",
      "Epoch 966 Batch  166/175   train_loss = 1.189\n",
      "Epoch 967 Batch   23/175   train_loss = 1.128\n",
      "Epoch 967 Batch   55/175   train_loss = 1.232\n",
      "Epoch 967 Batch   87/175   train_loss = 1.262\n",
      "Epoch 967 Batch  119/175   train_loss = 1.171\n",
      "Epoch 967 Batch  151/175   train_loss = 1.189\n",
      "Epoch 968 Batch    8/175   train_loss = 1.180\n",
      "Epoch 968 Batch   40/175   train_loss = 1.170\n",
      "Epoch 968 Batch   72/175   train_loss = 1.191\n",
      "Epoch 968 Batch  104/175   train_loss = 1.182\n",
      "Epoch 968 Batch  136/175   train_loss = 1.151\n",
      "Epoch 968 Batch  168/175   train_loss = 1.199\n",
      "Epoch 969 Batch   25/175   train_loss = 1.178\n",
      "Epoch 969 Batch   57/175   train_loss = 1.231\n",
      "Epoch 969 Batch   89/175   train_loss = 1.161\n",
      "Epoch 969 Batch  121/175   train_loss = 1.162\n",
      "Epoch 969 Batch  153/175   train_loss = 1.171\n",
      "Epoch 970 Batch   10/175   train_loss = 1.135\n",
      "Epoch 970 Batch   42/175   train_loss = 1.218\n",
      "Epoch 970 Batch   74/175   train_loss = 1.219\n",
      "Epoch 970 Batch  106/175   train_loss = 1.230\n",
      "Epoch 970 Batch  138/175   train_loss = 1.179\n",
      "Epoch 970 Batch  170/175   train_loss = 1.219\n",
      "Epoch 971 Batch   27/175   train_loss = 1.155\n",
      "Epoch 971 Batch   59/175   train_loss = 1.163\n",
      "Epoch 971 Batch   91/175   train_loss = 1.160\n",
      "Epoch 971 Batch  123/175   train_loss = 1.156\n",
      "Epoch 971 Batch  155/175   train_loss = 1.143\n",
      "Epoch 972 Batch   12/175   train_loss = 1.167\n",
      "Epoch 972 Batch   44/175   train_loss = 1.167\n",
      "Epoch 972 Batch   76/175   train_loss = 1.145\n",
      "Epoch 972 Batch  108/175   train_loss = 1.189\n",
      "Epoch 972 Batch  140/175   train_loss = 1.187\n",
      "Epoch 972 Batch  172/175   train_loss = 1.205\n",
      "Epoch 973 Batch   29/175   train_loss = 1.175\n",
      "Epoch 973 Batch   61/175   train_loss = 1.196\n",
      "Epoch 973 Batch   93/175   train_loss = 1.184\n",
      "Epoch 973 Batch  125/175   train_loss = 1.199\n",
      "Epoch 973 Batch  157/175   train_loss = 1.179\n",
      "Epoch 974 Batch   14/175   train_loss = 1.196\n",
      "Epoch 974 Batch   46/175   train_loss = 1.164\n",
      "Epoch 974 Batch   78/175   train_loss = 1.151\n",
      "Epoch 974 Batch  110/175   train_loss = 1.251\n",
      "Epoch 974 Batch  142/175   train_loss = 1.188\n",
      "Epoch 974 Batch  174/175   train_loss = 1.155\n",
      "Epoch 975 Batch   31/175   train_loss = 1.189\n",
      "Epoch 975 Batch   63/175   train_loss = 1.182\n",
      "Epoch 975 Batch   95/175   train_loss = 1.133\n",
      "Epoch 975 Batch  127/175   train_loss = 1.136\n",
      "Epoch 975 Batch  159/175   train_loss = 1.168\n",
      "Epoch 976 Batch   16/175   train_loss = 1.173\n",
      "Epoch 976 Batch   48/175   train_loss = 1.177\n",
      "Epoch 976 Batch   80/175   train_loss = 1.187\n",
      "Epoch 976 Batch  112/175   train_loss = 1.182\n",
      "Epoch 976 Batch  144/175   train_loss = 1.121\n",
      "Epoch 977 Batch    1/175   train_loss = 1.203\n",
      "Epoch 977 Batch   33/175   train_loss = 1.256\n",
      "Epoch 977 Batch   65/175   train_loss = 1.170\n",
      "Epoch 977 Batch   97/175   train_loss = 1.180\n",
      "Epoch 977 Batch  129/175   train_loss = 1.174\n",
      "Epoch 977 Batch  161/175   train_loss = 1.171\n",
      "Epoch 978 Batch   18/175   train_loss = 1.128\n",
      "Epoch 978 Batch   50/175   train_loss = 1.182\n",
      "Epoch 978 Batch   82/175   train_loss = 1.218\n",
      "Epoch 978 Batch  114/175   train_loss = 1.222\n",
      "Epoch 978 Batch  146/175   train_loss = 1.159\n",
      "Epoch 979 Batch    3/175   train_loss = 1.210\n",
      "Epoch 979 Batch   35/175   train_loss = 1.153\n",
      "Epoch 979 Batch   67/175   train_loss = 1.139\n",
      "Epoch 979 Batch   99/175   train_loss = 1.233\n",
      "Epoch 979 Batch  131/175   train_loss = 1.173\n",
      "Epoch 979 Batch  163/175   train_loss = 1.162\n",
      "Epoch 980 Batch   20/175   train_loss = 1.159\n",
      "Epoch 980 Batch   52/175   train_loss = 1.112\n",
      "Epoch 980 Batch   84/175   train_loss = 1.174\n",
      "Epoch 980 Batch  116/175   train_loss = 1.219\n",
      "Epoch 980 Batch  148/175   train_loss = 1.183\n",
      "Epoch 981 Batch    5/175   train_loss = 1.178\n",
      "Epoch 981 Batch   37/175   train_loss = 1.143\n",
      "Epoch 981 Batch   69/175   train_loss = 1.201\n",
      "Epoch 981 Batch  101/175   train_loss = 1.242\n",
      "Epoch 981 Batch  133/175   train_loss = 1.090\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 981 Batch  165/175   train_loss = 1.188\n",
      "Epoch 982 Batch   22/175   train_loss = 1.131\n",
      "Epoch 982 Batch   54/175   train_loss = 1.208\n",
      "Epoch 982 Batch   86/175   train_loss = 1.259\n",
      "Epoch 982 Batch  118/175   train_loss = 1.215\n",
      "Epoch 982 Batch  150/175   train_loss = 1.179\n",
      "Epoch 983 Batch    7/175   train_loss = 1.206\n",
      "Epoch 983 Batch   39/175   train_loss = 1.122\n",
      "Epoch 983 Batch   71/175   train_loss = 1.216\n",
      "Epoch 983 Batch  103/175   train_loss = 1.188\n",
      "Epoch 983 Batch  135/175   train_loss = 1.128\n",
      "Epoch 983 Batch  167/175   train_loss = 1.230\n",
      "Epoch 984 Batch   24/175   train_loss = 1.116\n",
      "Epoch 984 Batch   56/175   train_loss = 1.204\n",
      "Epoch 984 Batch   88/175   train_loss = 1.213\n",
      "Epoch 984 Batch  120/175   train_loss = 1.171\n",
      "Epoch 984 Batch  152/175   train_loss = 1.158\n",
      "Epoch 985 Batch    9/175   train_loss = 1.197\n",
      "Epoch 985 Batch   41/175   train_loss = 1.203\n",
      "Epoch 985 Batch   73/175   train_loss = 1.202\n",
      "Epoch 985 Batch  105/175   train_loss = 1.215\n",
      "Epoch 985 Batch  137/175   train_loss = 1.149\n",
      "Epoch 985 Batch  169/175   train_loss = 1.193\n",
      "Epoch 986 Batch   26/175   train_loss = 1.173\n",
      "Epoch 986 Batch   58/175   train_loss = 1.226\n",
      "Epoch 986 Batch   90/175   train_loss = 1.203\n",
      "Epoch 986 Batch  122/175   train_loss = 1.175\n",
      "Epoch 986 Batch  154/175   train_loss = 1.172\n",
      "Epoch 987 Batch   11/175   train_loss = 1.157\n",
      "Epoch 987 Batch   43/175   train_loss = 1.166\n",
      "Epoch 987 Batch   75/175   train_loss = 1.147\n",
      "Epoch 987 Batch  107/175   train_loss = 1.223\n",
      "Epoch 987 Batch  139/175   train_loss = 1.135\n",
      "Epoch 987 Batch  171/175   train_loss = 1.226\n",
      "Epoch 988 Batch   28/175   train_loss = 1.167\n",
      "Epoch 988 Batch   60/175   train_loss = 1.158\n",
      "Epoch 988 Batch   92/175   train_loss = 1.143\n",
      "Epoch 988 Batch  124/175   train_loss = 1.174\n",
      "Epoch 988 Batch  156/175   train_loss = 1.219\n",
      "Epoch 989 Batch   13/175   train_loss = 1.172\n",
      "Epoch 989 Batch   45/175   train_loss = 1.179\n",
      "Epoch 989 Batch   77/175   train_loss = 1.178\n",
      "Epoch 989 Batch  109/175   train_loss = 1.214\n",
      "Epoch 989 Batch  141/175   train_loss = 1.130\n",
      "Epoch 989 Batch  173/175   train_loss = 1.164\n",
      "Epoch 990 Batch   30/175   train_loss = 1.203\n",
      "Epoch 990 Batch   62/175   train_loss = 1.199\n",
      "Epoch 990 Batch   94/175   train_loss = 1.167\n",
      "Epoch 990 Batch  126/175   train_loss = 1.191\n",
      "Epoch 990 Batch  158/175   train_loss = 1.166\n",
      "Epoch 991 Batch   15/175   train_loss = 1.215\n",
      "Epoch 991 Batch   47/175   train_loss = 1.190\n",
      "Epoch 991 Batch   79/175   train_loss = 1.204\n",
      "Epoch 991 Batch  111/175   train_loss = 1.242\n",
      "Epoch 991 Batch  143/175   train_loss = 1.160\n",
      "Epoch 992 Batch    0/175   train_loss = 1.191\n",
      "Epoch 992 Batch   32/175   train_loss = 1.254\n",
      "Epoch 992 Batch   64/175   train_loss = 1.212\n",
      "Epoch 992 Batch   96/175   train_loss = 1.186\n",
      "Epoch 992 Batch  128/175   train_loss = 1.135\n",
      "Epoch 992 Batch  160/175   train_loss = 1.185\n",
      "Epoch 993 Batch   17/175   train_loss = 1.149\n",
      "Epoch 993 Batch   49/175   train_loss = 1.178\n",
      "Epoch 993 Batch   81/175   train_loss = 1.150\n",
      "Epoch 993 Batch  113/175   train_loss = 1.180\n",
      "Epoch 993 Batch  145/175   train_loss = 1.122\n",
      "Epoch 994 Batch    2/175   train_loss = 1.174\n",
      "Epoch 994 Batch   34/175   train_loss = 1.190\n",
      "Epoch 994 Batch   66/175   train_loss = 1.227\n",
      "Epoch 994 Batch   98/175   train_loss = 1.219\n",
      "Epoch 994 Batch  130/175   train_loss = 1.209\n",
      "Epoch 994 Batch  162/175   train_loss = 1.158\n",
      "Epoch 995 Batch   19/175   train_loss = 1.164\n",
      "Epoch 995 Batch   51/175   train_loss = 1.125\n",
      "Epoch 995 Batch   83/175   train_loss = 1.210\n",
      "Epoch 995 Batch  115/175   train_loss = 1.284\n",
      "Epoch 995 Batch  147/175   train_loss = 1.153\n",
      "Epoch 996 Batch    4/175   train_loss = 1.188\n",
      "Epoch 996 Batch   36/175   train_loss = 1.145\n",
      "Epoch 996 Batch   68/175   train_loss = 1.173\n",
      "Epoch 996 Batch  100/175   train_loss = 1.182\n",
      "Epoch 996 Batch  132/175   train_loss = 1.157\n",
      "Epoch 996 Batch  164/175   train_loss = 1.151\n",
      "Epoch 997 Batch   21/175   train_loss = 1.147\n",
      "Epoch 997 Batch   53/175   train_loss = 1.142\n",
      "Epoch 997 Batch   85/175   train_loss = 1.179\n",
      "Epoch 997 Batch  117/175   train_loss = 1.196\n",
      "Epoch 997 Batch  149/175   train_loss = 1.188\n",
      "Epoch 998 Batch    6/175   train_loss = 1.207\n",
      "Epoch 998 Batch   38/175   train_loss = 1.152\n",
      "Epoch 998 Batch   70/175   train_loss = 1.185\n",
      "Epoch 998 Batch  102/175   train_loss = 1.179\n",
      "Epoch 998 Batch  134/175   train_loss = 1.149\n",
      "Epoch 998 Batch  166/175   train_loss = 1.198\n",
      "Epoch 999 Batch   23/175   train_loss = 1.125\n",
      "Epoch 999 Batch   55/175   train_loss = 1.244\n",
      "Epoch 999 Batch   87/175   train_loss = 1.246\n",
      "Epoch 999 Batch  119/175   train_loss = 1.168\n",
      "Epoch 999 Batch  151/175   train_loss = 1.199\n",
      "Epoch 1000 Batch    8/175   train_loss = 1.175\n",
      "Epoch 1000 Batch   40/175   train_loss = 1.157\n",
      "Epoch 1000 Batch   72/175   train_loss = 1.209\n",
      "Epoch 1000 Batch  104/175   train_loss = 1.187\n",
      "Epoch 1000 Batch  136/175   train_loss = 1.153\n",
      "Epoch 1000 Batch  168/175   train_loss = 1.204\n",
      "Epoch 1001 Batch   25/175   train_loss = 1.173\n",
      "Epoch 1001 Batch   57/175   train_loss = 1.226\n",
      "Epoch 1001 Batch   89/175   train_loss = 1.173\n",
      "Epoch 1001 Batch  121/175   train_loss = 1.153\n",
      "Epoch 1001 Batch  153/175   train_loss = 1.176\n",
      "Epoch 1002 Batch   10/175   train_loss = 1.144\n",
      "Epoch 1002 Batch   42/175   train_loss = 1.215\n",
      "Epoch 1002 Batch   74/175   train_loss = 1.199\n",
      "Epoch 1002 Batch  106/175   train_loss = 1.222\n",
      "Epoch 1002 Batch  138/175   train_loss = 1.167\n",
      "Epoch 1002 Batch  170/175   train_loss = 1.239\n",
      "Epoch 1003 Batch   27/175   train_loss = 1.162\n",
      "Epoch 1003 Batch   59/175   train_loss = 1.169\n",
      "Epoch 1003 Batch   91/175   train_loss = 1.162\n",
      "Epoch 1003 Batch  123/175   train_loss = 1.160\n",
      "Epoch 1003 Batch  155/175   train_loss = 1.170\n",
      "Epoch 1004 Batch   12/175   train_loss = 1.170\n",
      "Epoch 1004 Batch   44/175   train_loss = 1.164\n",
      "Epoch 1004 Batch   76/175   train_loss = 1.143\n",
      "Epoch 1004 Batch  108/175   train_loss = 1.185\n",
      "Epoch 1004 Batch  140/175   train_loss = 1.157\n",
      "Epoch 1004 Batch  172/175   train_loss = 1.195\n",
      "Epoch 1005 Batch   29/175   train_loss = 1.166\n",
      "Epoch 1005 Batch   61/175   train_loss = 1.189\n",
      "Epoch 1005 Batch   93/175   train_loss = 1.180\n",
      "Epoch 1005 Batch  125/175   train_loss = 1.178\n",
      "Epoch 1005 Batch  157/175   train_loss = 1.160\n",
      "Epoch 1006 Batch   14/175   train_loss = 1.193\n",
      "Epoch 1006 Batch   46/175   train_loss = 1.184\n",
      "Epoch 1006 Batch   78/175   train_loss = 1.141\n",
      "Epoch 1006 Batch  110/175   train_loss = 1.269\n",
      "Epoch 1006 Batch  142/175   train_loss = 1.185\n",
      "Epoch 1006 Batch  174/175   train_loss = 1.145\n",
      "Epoch 1007 Batch   31/175   train_loss = 1.197\n",
      "Epoch 1007 Batch   63/175   train_loss = 1.183\n",
      "Epoch 1007 Batch   95/175   train_loss = 1.150\n",
      "Epoch 1007 Batch  127/175   train_loss = 1.138\n",
      "Epoch 1007 Batch  159/175   train_loss = 1.156\n",
      "Epoch 1008 Batch   16/175   train_loss = 1.188\n",
      "Epoch 1008 Batch   48/175   train_loss = 1.170\n",
      "Epoch 1008 Batch   80/175   train_loss = 1.194\n",
      "Epoch 1008 Batch  112/175   train_loss = 1.179\n",
      "Epoch 1008 Batch  144/175   train_loss = 1.131\n",
      "Epoch 1009 Batch    1/175   train_loss = 1.200\n",
      "Epoch 1009 Batch   33/175   train_loss = 1.247\n",
      "Epoch 1009 Batch   65/175   train_loss = 1.198\n",
      "Epoch 1009 Batch   97/175   train_loss = 1.176\n",
      "Epoch 1009 Batch  129/175   train_loss = 1.171\n",
      "Epoch 1009 Batch  161/175   train_loss = 1.165\n",
      "Epoch 1010 Batch   18/175   train_loss = 1.147\n",
      "Epoch 1010 Batch   50/175   train_loss = 1.188\n",
      "Epoch 1010 Batch   82/175   train_loss = 1.216\n",
      "Epoch 1010 Batch  114/175   train_loss = 1.220\n",
      "Epoch 1010 Batch  146/175   train_loss = 1.166\n",
      "Epoch 1011 Batch    3/175   train_loss = 1.215\n",
      "Epoch 1011 Batch   35/175   train_loss = 1.168\n",
      "Epoch 1011 Batch   67/175   train_loss = 1.157\n",
      "Epoch 1011 Batch   99/175   train_loss = 1.239\n",
      "Epoch 1011 Batch  131/175   train_loss = 1.168\n",
      "Epoch 1011 Batch  163/175   train_loss = 1.177\n",
      "Epoch 1012 Batch   20/175   train_loss = 1.160\n",
      "Epoch 1012 Batch   52/175   train_loss = 1.128\n",
      "Epoch 1012 Batch   84/175   train_loss = 1.194\n",
      "Epoch 1012 Batch  116/175   train_loss = 1.228\n",
      "Epoch 1012 Batch  148/175   train_loss = 1.186\n",
      "Epoch 1013 Batch    5/175   train_loss = 1.177\n",
      "Epoch 1013 Batch   37/175   train_loss = 1.160\n",
      "Epoch 1013 Batch   69/175   train_loss = 1.216\n",
      "Epoch 1013 Batch  101/175   train_loss = 1.224\n",
      "Epoch 1013 Batch  133/175   train_loss = 1.095\n",
      "Epoch 1013 Batch  165/175   train_loss = 1.158\n",
      "Epoch 1014 Batch   22/175   train_loss = 1.126\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1014 Batch   54/175   train_loss = 1.195\n",
      "Epoch 1014 Batch   86/175   train_loss = 1.257\n",
      "Epoch 1014 Batch  118/175   train_loss = 1.234\n",
      "Epoch 1014 Batch  150/175   train_loss = 1.185\n",
      "Epoch 1015 Batch    7/175   train_loss = 1.198\n",
      "Epoch 1015 Batch   39/175   train_loss = 1.130\n",
      "Epoch 1015 Batch   71/175   train_loss = 1.203\n",
      "Epoch 1015 Batch  103/175   train_loss = 1.177\n",
      "Epoch 1015 Batch  135/175   train_loss = 1.160\n",
      "Epoch 1015 Batch  167/175   train_loss = 1.211\n",
      "Epoch 1016 Batch   24/175   train_loss = 1.127\n",
      "Epoch 1016 Batch   56/175   train_loss = 1.193\n",
      "Epoch 1016 Batch   88/175   train_loss = 1.204\n",
      "Epoch 1016 Batch  120/175   train_loss = 1.159\n",
      "Epoch 1016 Batch  152/175   train_loss = 1.152\n",
      "Epoch 1017 Batch    9/175   train_loss = 1.203\n",
      "Epoch 1017 Batch   41/175   train_loss = 1.218\n",
      "Epoch 1017 Batch   73/175   train_loss = 1.207\n",
      "Epoch 1017 Batch  105/175   train_loss = 1.214\n",
      "Epoch 1017 Batch  137/175   train_loss = 1.139\n",
      "Epoch 1017 Batch  169/175   train_loss = 1.187\n",
      "Epoch 1018 Batch   26/175   train_loss = 1.173\n",
      "Epoch 1018 Batch   58/175   train_loss = 1.201\n",
      "Epoch 1018 Batch   90/175   train_loss = 1.183\n",
      "Epoch 1018 Batch  122/175   train_loss = 1.171\n",
      "Epoch 1018 Batch  154/175   train_loss = 1.171\n",
      "Epoch 1019 Batch   11/175   train_loss = 1.151\n",
      "Epoch 1019 Batch   43/175   train_loss = 1.186\n",
      "Epoch 1019 Batch   75/175   train_loss = 1.168\n",
      "Epoch 1019 Batch  107/175   train_loss = 1.235\n",
      "Epoch 1019 Batch  139/175   train_loss = 1.119\n",
      "Epoch 1019 Batch  171/175   train_loss = 1.224\n",
      "Epoch 1020 Batch   28/175   train_loss = 1.184\n",
      "Epoch 1020 Batch   60/175   train_loss = 1.173\n",
      "Epoch 1020 Batch   92/175   train_loss = 1.147\n",
      "Epoch 1020 Batch  124/175   train_loss = 1.184\n",
      "Epoch 1020 Batch  156/175   train_loss = 1.215\n",
      "Epoch 1021 Batch   13/175   train_loss = 1.183\n",
      "Epoch 1021 Batch   45/175   train_loss = 1.177\n",
      "Epoch 1021 Batch   77/175   train_loss = 1.173\n",
      "Epoch 1021 Batch  109/175   train_loss = 1.215\n",
      "Epoch 1021 Batch  141/175   train_loss = 1.154\n",
      "Epoch 1021 Batch  173/175   train_loss = 1.148\n",
      "Epoch 1022 Batch   30/175   train_loss = 1.221\n",
      "Epoch 1022 Batch   62/175   train_loss = 1.241\n",
      "Epoch 1022 Batch   94/175   train_loss = 1.180\n",
      "Epoch 1022 Batch  126/175   train_loss = 1.191\n",
      "Epoch 1022 Batch  158/175   train_loss = 1.173\n",
      "Epoch 1023 Batch   15/175   train_loss = 1.213\n",
      "Epoch 1023 Batch   47/175   train_loss = 1.190\n",
      "Epoch 1023 Batch   79/175   train_loss = 1.219\n",
      "Epoch 1023 Batch  111/175   train_loss = 1.224\n",
      "Epoch 1023 Batch  143/175   train_loss = 1.172\n",
      "Epoch 1024 Batch    0/175   train_loss = 1.206\n",
      "Epoch 1024 Batch   32/175   train_loss = 1.222\n",
      "Epoch 1024 Batch   64/175   train_loss = 1.211\n",
      "Epoch 1024 Batch   96/175   train_loss = 1.207\n",
      "Epoch 1024 Batch  128/175   train_loss = 1.125\n",
      "Epoch 1024 Batch  160/175   train_loss = 1.155\n",
      "Epoch 1025 Batch   17/175   train_loss = 1.154\n",
      "Epoch 1025 Batch   49/175   train_loss = 1.203\n",
      "Epoch 1025 Batch   81/175   train_loss = 1.159\n",
      "Epoch 1025 Batch  113/175   train_loss = 1.184\n",
      "Epoch 1025 Batch  145/175   train_loss = 1.141\n",
      "Epoch 1026 Batch    2/175   train_loss = 1.177\n",
      "Epoch 1026 Batch   34/175   train_loss = 1.216\n",
      "Epoch 1026 Batch   66/175   train_loss = 1.222\n",
      "Epoch 1026 Batch   98/175   train_loss = 1.199\n",
      "Epoch 1026 Batch  130/175   train_loss = 1.209\n",
      "Epoch 1026 Batch  162/175   train_loss = 1.170\n",
      "Epoch 1027 Batch   19/175   train_loss = 1.180\n",
      "Epoch 1027 Batch   51/175   train_loss = 1.132\n",
      "Epoch 1027 Batch   83/175   train_loss = 1.215\n",
      "Epoch 1027 Batch  115/175   train_loss = 1.312\n",
      "Epoch 1027 Batch  147/175   train_loss = 1.148\n",
      "Epoch 1028 Batch    4/175   train_loss = 1.204\n",
      "Epoch 1028 Batch   36/175   train_loss = 1.136\n",
      "Epoch 1028 Batch   68/175   train_loss = 1.170\n",
      "Epoch 1028 Batch  100/175   train_loss = 1.171\n",
      "Epoch 1028 Batch  132/175   train_loss = 1.152\n",
      "Epoch 1028 Batch  164/175   train_loss = 1.141\n",
      "Epoch 1029 Batch   21/175   train_loss = 1.125\n",
      "Epoch 1029 Batch   53/175   train_loss = 1.148\n",
      "Epoch 1029 Batch   85/175   train_loss = 1.173\n",
      "Epoch 1029 Batch  117/175   train_loss = 1.207\n",
      "Epoch 1029 Batch  149/175   train_loss = 1.178\n",
      "Epoch 1030 Batch    6/175   train_loss = 1.209\n",
      "Epoch 1030 Batch   38/175   train_loss = 1.126\n",
      "Epoch 1030 Batch   70/175   train_loss = 1.170\n",
      "Epoch 1030 Batch  102/175   train_loss = 1.191\n",
      "Epoch 1030 Batch  134/175   train_loss = 1.151\n",
      "Epoch 1030 Batch  166/175   train_loss = 1.195\n",
      "Epoch 1031 Batch   23/175   train_loss = 1.126\n",
      "Epoch 1031 Batch   55/175   train_loss = 1.240\n",
      "Epoch 1031 Batch   87/175   train_loss = 1.246\n",
      "Epoch 1031 Batch  119/175   train_loss = 1.168\n",
      "Epoch 1031 Batch  151/175   train_loss = 1.196\n",
      "Epoch 1032 Batch    8/175   train_loss = 1.192\n",
      "Epoch 1032 Batch   40/175   train_loss = 1.175\n",
      "Epoch 1032 Batch   72/175   train_loss = 1.198\n",
      "Epoch 1032 Batch  104/175   train_loss = 1.200\n",
      "Epoch 1032 Batch  136/175   train_loss = 1.153\n",
      "Epoch 1032 Batch  168/175   train_loss = 1.224\n",
      "Epoch 1033 Batch   25/175   train_loss = 1.218\n",
      "Epoch 1033 Batch   57/175   train_loss = 1.235\n",
      "Epoch 1033 Batch   89/175   train_loss = 1.194\n",
      "Epoch 1033 Batch  121/175   train_loss = 1.162\n",
      "Epoch 1033 Batch  153/175   train_loss = 1.188\n",
      "Epoch 1034 Batch   10/175   train_loss = 1.146\n",
      "Epoch 1034 Batch   42/175   train_loss = 1.227\n",
      "Epoch 1034 Batch   74/175   train_loss = 1.221\n",
      "Epoch 1034 Batch  106/175   train_loss = 1.223\n",
      "Epoch 1034 Batch  138/175   train_loss = 1.165\n",
      "Epoch 1034 Batch  170/175   train_loss = 1.216\n",
      "Epoch 1035 Batch   27/175   train_loss = 1.176\n",
      "Epoch 1035 Batch   59/175   train_loss = 1.176\n",
      "Epoch 1035 Batch   91/175   train_loss = 1.153\n",
      "Epoch 1035 Batch  123/175   train_loss = 1.171\n",
      "Epoch 1035 Batch  155/175   train_loss = 1.158\n",
      "Epoch 1036 Batch   12/175   train_loss = 1.190\n",
      "Epoch 1036 Batch   44/175   train_loss = 1.156\n",
      "Epoch 1036 Batch   76/175   train_loss = 1.147\n",
      "Epoch 1036 Batch  108/175   train_loss = 1.193\n",
      "Epoch 1036 Batch  140/175   train_loss = 1.164\n",
      "Epoch 1036 Batch  172/175   train_loss = 1.204\n",
      "Epoch 1037 Batch   29/175   train_loss = 1.158\n",
      "Epoch 1037 Batch   61/175   train_loss = 1.205\n",
      "Epoch 1037 Batch   93/175   train_loss = 1.203\n",
      "Epoch 1037 Batch  125/175   train_loss = 1.192\n",
      "Epoch 1037 Batch  157/175   train_loss = 1.182\n",
      "Epoch 1038 Batch   14/175   train_loss = 1.210\n",
      "Epoch 1038 Batch   46/175   train_loss = 1.198\n",
      "Epoch 1038 Batch   78/175   train_loss = 1.161\n",
      "Epoch 1038 Batch  110/175   train_loss = 1.272\n",
      "Epoch 1038 Batch  142/175   train_loss = 1.201\n",
      "Epoch 1038 Batch  174/175   train_loss = 1.159\n",
      "Epoch 1039 Batch   31/175   train_loss = 1.222\n",
      "Epoch 1039 Batch   63/175   train_loss = 1.182\n",
      "Epoch 1039 Batch   95/175   train_loss = 1.160\n",
      "Epoch 1039 Batch  127/175   train_loss = 1.139\n",
      "Epoch 1039 Batch  159/175   train_loss = 1.148\n",
      "Epoch 1040 Batch   16/175   train_loss = 1.168\n",
      "Epoch 1040 Batch   48/175   train_loss = 1.181\n",
      "Epoch 1040 Batch   80/175   train_loss = 1.206\n",
      "Epoch 1040 Batch  112/175   train_loss = 1.184\n",
      "Epoch 1040 Batch  144/175   train_loss = 1.114\n",
      "Epoch 1041 Batch    1/175   train_loss = 1.222\n",
      "Epoch 1041 Batch   33/175   train_loss = 1.234\n",
      "Epoch 1041 Batch   65/175   train_loss = 1.170\n",
      "Epoch 1041 Batch   97/175   train_loss = 1.194\n",
      "Epoch 1041 Batch  129/175   train_loss = 1.176\n",
      "Epoch 1041 Batch  161/175   train_loss = 1.179\n",
      "Epoch 1042 Batch   18/175   train_loss = 1.150\n",
      "Epoch 1042 Batch   50/175   train_loss = 1.180\n",
      "Epoch 1042 Batch   82/175   train_loss = 1.216\n",
      "Epoch 1042 Batch  114/175   train_loss = 1.242\n",
      "Epoch 1042 Batch  146/175   train_loss = 1.188\n",
      "Epoch 1043 Batch    3/175   train_loss = 1.232\n",
      "Epoch 1043 Batch   35/175   train_loss = 1.165\n",
      "Epoch 1043 Batch   67/175   train_loss = 1.135\n",
      "Epoch 1043 Batch   99/175   train_loss = 1.252\n",
      "Epoch 1043 Batch  131/175   train_loss = 1.173\n",
      "Epoch 1043 Batch  163/175   train_loss = 1.181\n",
      "Epoch 1044 Batch   20/175   train_loss = 1.166\n",
      "Epoch 1044 Batch   52/175   train_loss = 1.101\n",
      "Epoch 1044 Batch   84/175   train_loss = 1.178\n",
      "Epoch 1044 Batch  116/175   train_loss = 1.212\n",
      "Epoch 1044 Batch  148/175   train_loss = 1.163\n",
      "Epoch 1045 Batch    5/175   train_loss = 1.176\n",
      "Epoch 1045 Batch   37/175   train_loss = 1.152\n",
      "Epoch 1045 Batch   69/175   train_loss = 1.209\n",
      "Epoch 1045 Batch  101/175   train_loss = 1.237\n",
      "Epoch 1045 Batch  133/175   train_loss = 1.089\n",
      "Epoch 1045 Batch  165/175   train_loss = 1.176\n",
      "Epoch 1046 Batch   22/175   train_loss = 1.105\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1046 Batch   54/175   train_loss = 1.206\n",
      "Epoch 1046 Batch   86/175   train_loss = 1.246\n",
      "Epoch 1046 Batch  118/175   train_loss = 1.230\n",
      "Epoch 1046 Batch  150/175   train_loss = 1.179\n",
      "Epoch 1047 Batch    7/175   train_loss = 1.201\n",
      "Epoch 1047 Batch   39/175   train_loss = 1.131\n",
      "Epoch 1047 Batch   71/175   train_loss = 1.206\n",
      "Epoch 1047 Batch  103/175   train_loss = 1.177\n",
      "Epoch 1047 Batch  135/175   train_loss = 1.143\n",
      "Epoch 1047 Batch  167/175   train_loss = 1.225\n",
      "Epoch 1048 Batch   24/175   train_loss = 1.122\n",
      "Epoch 1048 Batch   56/175   train_loss = 1.189\n",
      "Epoch 1048 Batch   88/175   train_loss = 1.214\n",
      "Epoch 1048 Batch  120/175   train_loss = 1.174\n",
      "Epoch 1048 Batch  152/175   train_loss = 1.156\n",
      "Epoch 1049 Batch    9/175   train_loss = 1.189\n",
      "Epoch 1049 Batch   41/175   train_loss = 1.219\n",
      "Epoch 1049 Batch   73/175   train_loss = 1.217\n",
      "Epoch 1049 Batch  105/175   train_loss = 1.221\n",
      "Epoch 1049 Batch  137/175   train_loss = 1.160\n",
      "Epoch 1049 Batch  169/175   train_loss = 1.201\n",
      "Epoch 1050 Batch   26/175   train_loss = 1.185\n",
      "Epoch 1050 Batch   58/175   train_loss = 1.218\n",
      "Epoch 1050 Batch   90/175   train_loss = 1.176\n",
      "Epoch 1050 Batch  122/175   train_loss = 1.151\n",
      "Epoch 1050 Batch  154/175   train_loss = 1.151\n",
      "Epoch 1051 Batch   11/175   train_loss = 1.169\n",
      "Epoch 1051 Batch   43/175   train_loss = 1.176\n",
      "Epoch 1051 Batch   75/175   train_loss = 1.164\n",
      "Epoch 1051 Batch  107/175   train_loss = 1.248\n",
      "Epoch 1051 Batch  139/175   train_loss = 1.138\n",
      "Epoch 1051 Batch  171/175   train_loss = 1.237\n",
      "Epoch 1052 Batch   28/175   train_loss = 1.173\n",
      "Epoch 1052 Batch   60/175   train_loss = 1.165\n",
      "Epoch 1052 Batch   92/175   train_loss = 1.144\n",
      "Epoch 1052 Batch  124/175   train_loss = 1.190\n",
      "Epoch 1052 Batch  156/175   train_loss = 1.221\n",
      "Epoch 1053 Batch   13/175   train_loss = 1.184\n",
      "Epoch 1053 Batch   45/175   train_loss = 1.179\n",
      "Epoch 1053 Batch   77/175   train_loss = 1.166\n",
      "Epoch 1053 Batch  109/175   train_loss = 1.217\n",
      "Epoch 1053 Batch  141/175   train_loss = 1.141\n",
      "Epoch 1053 Batch  173/175   train_loss = 1.158\n",
      "Epoch 1054 Batch   30/175   train_loss = 1.209\n",
      "Epoch 1054 Batch   62/175   train_loss = 1.203\n",
      "Epoch 1054 Batch   94/175   train_loss = 1.162\n",
      "Epoch 1054 Batch  126/175   train_loss = 1.204\n",
      "Epoch 1054 Batch  158/175   train_loss = 1.184\n",
      "Epoch 1055 Batch   15/175   train_loss = 1.222\n",
      "Epoch 1055 Batch   47/175   train_loss = 1.205\n",
      "Epoch 1055 Batch   79/175   train_loss = 1.216\n",
      "Epoch 1055 Batch  111/175   train_loss = 1.227\n",
      "Epoch 1055 Batch  143/175   train_loss = 1.156\n",
      "Epoch 1056 Batch    0/175   train_loss = 1.182\n",
      "Epoch 1056 Batch   32/175   train_loss = 1.213\n",
      "Epoch 1056 Batch   64/175   train_loss = 1.215\n",
      "Epoch 1056 Batch   96/175   train_loss = 1.193\n",
      "Epoch 1056 Batch  128/175   train_loss = 1.133\n",
      "Epoch 1056 Batch  160/175   train_loss = 1.178\n",
      "Epoch 1057 Batch   17/175   train_loss = 1.149\n",
      "Epoch 1057 Batch   49/175   train_loss = 1.186\n",
      "Epoch 1057 Batch   81/175   train_loss = 1.149\n",
      "Epoch 1057 Batch  113/175   train_loss = 1.171\n",
      "Epoch 1057 Batch  145/175   train_loss = 1.142\n",
      "Epoch 1058 Batch    2/175   train_loss = 1.185\n",
      "Epoch 1058 Batch   34/175   train_loss = 1.197\n",
      "Epoch 1058 Batch   66/175   train_loss = 1.217\n",
      "Epoch 1058 Batch   98/175   train_loss = 1.201\n",
      "Epoch 1058 Batch  130/175   train_loss = 1.193\n",
      "Epoch 1058 Batch  162/175   train_loss = 1.180\n",
      "Epoch 1059 Batch   19/175   train_loss = 1.162\n",
      "Epoch 1059 Batch   51/175   train_loss = 1.121\n",
      "Epoch 1059 Batch   83/175   train_loss = 1.222\n",
      "Epoch 1059 Batch  115/175   train_loss = 1.280\n",
      "Epoch 1059 Batch  147/175   train_loss = 1.150\n",
      "Epoch 1060 Batch    4/175   train_loss = 1.191\n",
      "Epoch 1060 Batch   36/175   train_loss = 1.117\n",
      "Epoch 1060 Batch   68/175   train_loss = 1.170\n",
      "Epoch 1060 Batch  100/175   train_loss = 1.165\n",
      "Epoch 1060 Batch  132/175   train_loss = 1.147\n",
      "Epoch 1060 Batch  164/175   train_loss = 1.154\n",
      "Epoch 1061 Batch   21/175   train_loss = 1.138\n",
      "Epoch 1061 Batch   53/175   train_loss = 1.178\n",
      "Epoch 1061 Batch   85/175   train_loss = 1.205\n",
      "Epoch 1061 Batch  117/175   train_loss = 1.200\n",
      "Epoch 1061 Batch  149/175   train_loss = 1.201\n",
      "Epoch 1062 Batch    6/175   train_loss = 1.208\n",
      "Epoch 1062 Batch   38/175   train_loss = 1.140\n",
      "Epoch 1062 Batch   70/175   train_loss = 1.188\n",
      "Epoch 1062 Batch  102/175   train_loss = 1.201\n",
      "Epoch 1062 Batch  134/175   train_loss = 1.148\n",
      "Epoch 1062 Batch  166/175   train_loss = 1.193\n",
      "Epoch 1063 Batch   23/175   train_loss = 1.128\n",
      "Epoch 1063 Batch   55/175   train_loss = 1.225\n",
      "Epoch 1063 Batch   87/175   train_loss = 1.246\n",
      "Epoch 1063 Batch  119/175   train_loss = 1.178\n",
      "Epoch 1063 Batch  151/175   train_loss = 1.199\n",
      "Epoch 1064 Batch    8/175   train_loss = 1.209\n",
      "Epoch 1064 Batch   40/175   train_loss = 1.175\n",
      "Epoch 1064 Batch   72/175   train_loss = 1.212\n",
      "Epoch 1064 Batch  104/175   train_loss = 1.208\n",
      "Epoch 1064 Batch  136/175   train_loss = 1.150\n",
      "Epoch 1064 Batch  168/175   train_loss = 1.213\n",
      "Epoch 1065 Batch   25/175   train_loss = 1.220\n",
      "Epoch 1065 Batch   57/175   train_loss = 1.230\n",
      "Epoch 1065 Batch   89/175   train_loss = 1.185\n",
      "Epoch 1065 Batch  121/175   train_loss = 1.157\n",
      "Epoch 1065 Batch  153/175   train_loss = 1.197\n",
      "Epoch 1066 Batch   10/175   train_loss = 1.156\n",
      "Epoch 1066 Batch   42/175   train_loss = 1.230\n",
      "Epoch 1066 Batch   74/175   train_loss = 1.220\n",
      "Epoch 1066 Batch  106/175   train_loss = 1.224\n",
      "Epoch 1066 Batch  138/175   train_loss = 1.197\n",
      "Epoch 1066 Batch  170/175   train_loss = 1.233\n",
      "Epoch 1067 Batch   27/175   train_loss = 1.170\n",
      "Epoch 1067 Batch   59/175   train_loss = 1.174\n",
      "Epoch 1067 Batch   91/175   train_loss = 1.169\n",
      "Epoch 1067 Batch  123/175   train_loss = 1.167\n",
      "Epoch 1067 Batch  155/175   train_loss = 1.151\n",
      "Epoch 1068 Batch   12/175   train_loss = 1.180\n",
      "Epoch 1068 Batch   44/175   train_loss = 1.154\n",
      "Epoch 1068 Batch   76/175   train_loss = 1.149\n",
      "Epoch 1068 Batch  108/175   train_loss = 1.181\n",
      "Epoch 1068 Batch  140/175   train_loss = 1.150\n",
      "Epoch 1068 Batch  172/175   train_loss = 1.216\n",
      "Epoch 1069 Batch   29/175   train_loss = 1.180\n",
      "Epoch 1069 Batch   61/175   train_loss = 1.195\n",
      "Epoch 1069 Batch   93/175   train_loss = 1.198\n",
      "Epoch 1069 Batch  125/175   train_loss = 1.181\n",
      "Epoch 1069 Batch  157/175   train_loss = 1.180\n",
      "Epoch 1070 Batch   14/175   train_loss = 1.191\n",
      "Epoch 1070 Batch   46/175   train_loss = 1.181\n",
      "Epoch 1070 Batch   78/175   train_loss = 1.152\n",
      "Epoch 1070 Batch  110/175   train_loss = 1.249\n",
      "Epoch 1070 Batch  142/175   train_loss = 1.197\n",
      "Epoch 1070 Batch  174/175   train_loss = 1.178\n",
      "Epoch 1071 Batch   31/175   train_loss = 1.197\n",
      "Epoch 1071 Batch   63/175   train_loss = 1.190\n",
      "Epoch 1071 Batch   95/175   train_loss = 1.138\n",
      "Epoch 1071 Batch  127/175   train_loss = 1.153\n",
      "Epoch 1071 Batch  159/175   train_loss = 1.140\n",
      "Epoch 1072 Batch   16/175   train_loss = 1.203\n",
      "Epoch 1072 Batch   48/175   train_loss = 1.186\n",
      "Epoch 1072 Batch   80/175   train_loss = 1.207\n",
      "Epoch 1072 Batch  112/175   train_loss = 1.192\n",
      "Epoch 1072 Batch  144/175   train_loss = 1.115\n",
      "Epoch 1073 Batch    1/175   train_loss = 1.226\n",
      "Epoch 1073 Batch   33/175   train_loss = 1.237\n",
      "Epoch 1073 Batch   65/175   train_loss = 1.177\n",
      "Epoch 1073 Batch   97/175   train_loss = 1.180\n",
      "Epoch 1073 Batch  129/175   train_loss = 1.168\n",
      "Epoch 1073 Batch  161/175   train_loss = 1.155\n",
      "Epoch 1074 Batch   18/175   train_loss = 1.141\n",
      "Epoch 1074 Batch   50/175   train_loss = 1.141\n",
      "Epoch 1074 Batch   82/175   train_loss = 1.222\n",
      "Epoch 1074 Batch  114/175   train_loss = 1.208\n",
      "Epoch 1074 Batch  146/175   train_loss = 1.179\n",
      "Epoch 1075 Batch    3/175   train_loss = 1.226\n",
      "Epoch 1075 Batch   35/175   train_loss = 1.156\n",
      "Epoch 1075 Batch   67/175   train_loss = 1.150\n",
      "Epoch 1075 Batch   99/175   train_loss = 1.242\n",
      "Epoch 1075 Batch  131/175   train_loss = 1.182\n",
      "Epoch 1075 Batch  163/175   train_loss = 1.174\n",
      "Epoch 1076 Batch   20/175   train_loss = 1.176\n",
      "Epoch 1076 Batch   52/175   train_loss = 1.102\n",
      "Epoch 1076 Batch   84/175   train_loss = 1.192\n",
      "Epoch 1076 Batch  116/175   train_loss = 1.205\n",
      "Epoch 1076 Batch  148/175   train_loss = 1.166\n",
      "Epoch 1077 Batch    5/175   train_loss = 1.178\n",
      "Epoch 1077 Batch   37/175   train_loss = 1.163\n",
      "Epoch 1077 Batch   69/175   train_loss = 1.199\n",
      "Epoch 1077 Batch  101/175   train_loss = 1.230\n",
      "Epoch 1077 Batch  133/175   train_loss = 1.107\n",
      "Epoch 1077 Batch  165/175   train_loss = 1.175\n",
      "Epoch 1078 Batch   22/175   train_loss = 1.134\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1078 Batch   54/175   train_loss = 1.242\n",
      "Epoch 1078 Batch   86/175   train_loss = 1.243\n",
      "Epoch 1078 Batch  118/175   train_loss = 1.221\n",
      "Epoch 1078 Batch  150/175   train_loss = 1.170\n",
      "Epoch 1079 Batch    7/175   train_loss = 1.206\n",
      "Epoch 1079 Batch   39/175   train_loss = 1.135\n",
      "Epoch 1079 Batch   71/175   train_loss = 1.220\n",
      "Epoch 1079 Batch  103/175   train_loss = 1.196\n",
      "Epoch 1079 Batch  135/175   train_loss = 1.148\n",
      "Epoch 1079 Batch  167/175   train_loss = 1.218\n",
      "Epoch 1080 Batch   24/175   train_loss = 1.125\n",
      "Epoch 1080 Batch   56/175   train_loss = 1.176\n",
      "Epoch 1080 Batch   88/175   train_loss = 1.231\n",
      "Epoch 1080 Batch  120/175   train_loss = 1.167\n",
      "Epoch 1080 Batch  152/175   train_loss = 1.142\n",
      "Epoch 1081 Batch    9/175   train_loss = 1.204\n",
      "Epoch 1081 Batch   41/175   train_loss = 1.200\n",
      "Epoch 1081 Batch   73/175   train_loss = 1.192\n",
      "Epoch 1081 Batch  105/175   train_loss = 1.206\n",
      "Epoch 1081 Batch  137/175   train_loss = 1.138\n",
      "Epoch 1081 Batch  169/175   train_loss = 1.196\n",
      "Epoch 1082 Batch   26/175   train_loss = 1.172\n",
      "Epoch 1082 Batch   58/175   train_loss = 1.227\n",
      "Epoch 1082 Batch   90/175   train_loss = 1.173\n",
      "Epoch 1082 Batch  122/175   train_loss = 1.169\n",
      "Epoch 1082 Batch  154/175   train_loss = 1.154\n",
      "Epoch 1083 Batch   11/175   train_loss = 1.153\n",
      "Epoch 1083 Batch   43/175   train_loss = 1.164\n",
      "Epoch 1083 Batch   75/175   train_loss = 1.160\n",
      "Epoch 1083 Batch  107/175   train_loss = 1.223\n",
      "Epoch 1083 Batch  139/175   train_loss = 1.138\n",
      "Epoch 1083 Batch  171/175   train_loss = 1.235\n",
      "Epoch 1084 Batch   28/175   train_loss = 1.164\n",
      "Epoch 1084 Batch   60/175   train_loss = 1.164\n",
      "Epoch 1084 Batch   92/175   train_loss = 1.160\n",
      "Epoch 1084 Batch  124/175   train_loss = 1.161\n",
      "Epoch 1084 Batch  156/175   train_loss = 1.205\n",
      "Epoch 1085 Batch   13/175   train_loss = 1.172\n",
      "Epoch 1085 Batch   45/175   train_loss = 1.166\n",
      "Epoch 1085 Batch   77/175   train_loss = 1.191\n",
      "Epoch 1085 Batch  109/175   train_loss = 1.217\n",
      "Epoch 1085 Batch  141/175   train_loss = 1.136\n",
      "Epoch 1085 Batch  173/175   train_loss = 1.176\n",
      "Epoch 1086 Batch   30/175   train_loss = 1.237\n",
      "Epoch 1086 Batch   62/175   train_loss = 1.203\n",
      "Epoch 1086 Batch   94/175   train_loss = 1.170\n",
      "Epoch 1086 Batch  126/175   train_loss = 1.201\n",
      "Epoch 1086 Batch  158/175   train_loss = 1.178\n",
      "Epoch 1087 Batch   15/175   train_loss = 1.225\n",
      "Epoch 1087 Batch   47/175   train_loss = 1.207\n",
      "Epoch 1087 Batch   79/175   train_loss = 1.219\n",
      "Epoch 1087 Batch  111/175   train_loss = 1.242\n",
      "Epoch 1087 Batch  143/175   train_loss = 1.153\n",
      "Epoch 1088 Batch    0/175   train_loss = 1.189\n",
      "Epoch 1088 Batch   32/175   train_loss = 1.211\n",
      "Epoch 1088 Batch   64/175   train_loss = 1.224\n",
      "Epoch 1088 Batch   96/175   train_loss = 1.220\n",
      "Epoch 1088 Batch  128/175   train_loss = 1.125\n",
      "Epoch 1088 Batch  160/175   train_loss = 1.176\n",
      "Epoch 1089 Batch   17/175   train_loss = 1.159\n",
      "Epoch 1089 Batch   49/175   train_loss = 1.220\n",
      "Epoch 1089 Batch   81/175   train_loss = 1.182\n",
      "Epoch 1089 Batch  113/175   train_loss = 1.183\n",
      "Epoch 1089 Batch  145/175   train_loss = 1.158\n",
      "Epoch 1090 Batch    2/175   train_loss = 1.177\n",
      "Epoch 1090 Batch   34/175   train_loss = 1.217\n",
      "Epoch 1090 Batch   66/175   train_loss = 1.230\n",
      "Epoch 1090 Batch   98/175   train_loss = 1.217\n",
      "Epoch 1090 Batch  130/175   train_loss = 1.214\n",
      "Epoch 1090 Batch  162/175   train_loss = 1.188\n",
      "Epoch 1091 Batch   19/175   train_loss = 1.185\n",
      "Epoch 1091 Batch   51/175   train_loss = 1.147\n",
      "Epoch 1091 Batch   83/175   train_loss = 1.242\n",
      "Epoch 1091 Batch  115/175   train_loss = 1.299\n",
      "Epoch 1091 Batch  147/175   train_loss = 1.156\n",
      "Epoch 1092 Batch    4/175   train_loss = 1.213\n",
      "Epoch 1092 Batch   36/175   train_loss = 1.144\n",
      "Epoch 1092 Batch   68/175   train_loss = 1.191\n",
      "Epoch 1092 Batch  100/175   train_loss = 1.211\n",
      "Epoch 1092 Batch  132/175   train_loss = 1.144\n",
      "Epoch 1092 Batch  164/175   train_loss = 1.148\n",
      "Epoch 1093 Batch   21/175   train_loss = 1.137\n",
      "Epoch 1093 Batch   53/175   train_loss = 1.151\n",
      "Epoch 1093 Batch   85/175   train_loss = 1.192\n",
      "Epoch 1093 Batch  117/175   train_loss = 1.191\n",
      "Epoch 1093 Batch  149/175   train_loss = 1.195\n",
      "Epoch 1094 Batch    6/175   train_loss = 1.200\n",
      "Epoch 1094 Batch   38/175   train_loss = 1.143\n",
      "Epoch 1094 Batch   70/175   train_loss = 1.183\n",
      "Epoch 1094 Batch  102/175   train_loss = 1.192\n",
      "Epoch 1094 Batch  134/175   train_loss = 1.151\n",
      "Epoch 1094 Batch  166/175   train_loss = 1.207\n",
      "Epoch 1095 Batch   23/175   train_loss = 1.118\n",
      "Epoch 1095 Batch   55/175   train_loss = 1.221\n",
      "Epoch 1095 Batch   87/175   train_loss = 1.255\n",
      "Epoch 1095 Batch  119/175   train_loss = 1.182\n",
      "Epoch 1095 Batch  151/175   train_loss = 1.194\n",
      "Epoch 1096 Batch    8/175   train_loss = 1.194\n",
      "Epoch 1096 Batch   40/175   train_loss = 1.175\n",
      "Epoch 1096 Batch   72/175   train_loss = 1.198\n",
      "Epoch 1096 Batch  104/175   train_loss = 1.189\n",
      "Epoch 1096 Batch  136/175   train_loss = 1.149\n",
      "Epoch 1096 Batch  168/175   train_loss = 1.212\n",
      "Epoch 1097 Batch   25/175   train_loss = 1.207\n",
      "Epoch 1097 Batch   57/175   train_loss = 1.235\n",
      "Epoch 1097 Batch   89/175   train_loss = 1.187\n",
      "Epoch 1097 Batch  121/175   train_loss = 1.153\n",
      "Epoch 1097 Batch  153/175   train_loss = 1.168\n",
      "Epoch 1098 Batch   10/175   train_loss = 1.143\n",
      "Epoch 1098 Batch   42/175   train_loss = 1.223\n",
      "Epoch 1098 Batch   74/175   train_loss = 1.225\n",
      "Epoch 1098 Batch  106/175   train_loss = 1.209\n",
      "Epoch 1098 Batch  138/175   train_loss = 1.166\n",
      "Epoch 1098 Batch  170/175   train_loss = 1.216\n",
      "Epoch 1099 Batch   27/175   train_loss = 1.164\n",
      "Epoch 1099 Batch   59/175   train_loss = 1.146\n",
      "Epoch 1099 Batch   91/175   train_loss = 1.173\n",
      "Epoch 1099 Batch  123/175   train_loss = 1.166\n",
      "Epoch 1099 Batch  155/175   train_loss = 1.150\n",
      "Epoch 1100 Batch   12/175   train_loss = 1.177\n",
      "Epoch 1100 Batch   44/175   train_loss = 1.156\n",
      "Epoch 1100 Batch   76/175   train_loss = 1.148\n",
      "Epoch 1100 Batch  108/175   train_loss = 1.201\n",
      "Epoch 1100 Batch  140/175   train_loss = 1.156\n",
      "Epoch 1100 Batch  172/175   train_loss = 1.204\n",
      "Epoch 1101 Batch   29/175   train_loss = 1.182\n",
      "Epoch 1101 Batch   61/175   train_loss = 1.203\n",
      "Epoch 1101 Batch   93/175   train_loss = 1.189\n",
      "Epoch 1101 Batch  125/175   train_loss = 1.202\n",
      "Epoch 1101 Batch  157/175   train_loss = 1.159\n",
      "Epoch 1102 Batch   14/175   train_loss = 1.200\n",
      "Epoch 1102 Batch   46/175   train_loss = 1.173\n",
      "Epoch 1102 Batch   78/175   train_loss = 1.135\n",
      "Epoch 1102 Batch  110/175   train_loss = 1.267\n",
      "Epoch 1102 Batch  142/175   train_loss = 1.194\n",
      "Epoch 1102 Batch  174/175   train_loss = 1.155\n",
      "Epoch 1103 Batch   31/175   train_loss = 1.206\n",
      "Epoch 1103 Batch   63/175   train_loss = 1.192\n",
      "Epoch 1103 Batch   95/175   train_loss = 1.126\n",
      "Epoch 1103 Batch  127/175   train_loss = 1.126\n",
      "Epoch 1103 Batch  159/175   train_loss = 1.140\n",
      "Epoch 1104 Batch   16/175   train_loss = 1.153\n",
      "Epoch 1104 Batch   48/175   train_loss = 1.163\n",
      "Epoch 1104 Batch   80/175   train_loss = 1.192\n",
      "Epoch 1104 Batch  112/175   train_loss = 1.198\n",
      "Epoch 1104 Batch  144/175   train_loss = 1.134\n",
      "Epoch 1105 Batch    1/175   train_loss = 1.220\n",
      "Epoch 1105 Batch   33/175   train_loss = 1.219\n",
      "Epoch 1105 Batch   65/175   train_loss = 1.183\n",
      "Epoch 1105 Batch   97/175   train_loss = 1.197\n",
      "Epoch 1105 Batch  129/175   train_loss = 1.184\n",
      "Epoch 1105 Batch  161/175   train_loss = 1.177\n",
      "Epoch 1106 Batch   18/175   train_loss = 1.160\n",
      "Epoch 1106 Batch   50/175   train_loss = 1.179\n",
      "Epoch 1106 Batch   82/175   train_loss = 1.226\n",
      "Epoch 1106 Batch  114/175   train_loss = 1.227\n",
      "Epoch 1106 Batch  146/175   train_loss = 1.187\n",
      "Epoch 1107 Batch    3/175   train_loss = 1.242\n",
      "Epoch 1107 Batch   35/175   train_loss = 1.177\n",
      "Epoch 1107 Batch   67/175   train_loss = 1.160\n",
      "Epoch 1107 Batch   99/175   train_loss = 1.248\n",
      "Epoch 1107 Batch  131/175   train_loss = 1.183\n",
      "Epoch 1107 Batch  163/175   train_loss = 1.180\n",
      "Epoch 1108 Batch   20/175   train_loss = 1.184\n",
      "Epoch 1108 Batch   52/175   train_loss = 1.115\n",
      "Epoch 1108 Batch   84/175   train_loss = 1.182\n",
      "Epoch 1108 Batch  116/175   train_loss = 1.204\n",
      "Epoch 1108 Batch  148/175   train_loss = 1.179\n",
      "Epoch 1109 Batch    5/175   train_loss = 1.193\n",
      "Epoch 1109 Batch   37/175   train_loss = 1.169\n",
      "Epoch 1109 Batch   69/175   train_loss = 1.194\n",
      "Epoch 1109 Batch  101/175   train_loss = 1.240\n",
      "Epoch 1109 Batch  133/175   train_loss = 1.115\n",
      "Epoch 1109 Batch  165/175   train_loss = 1.197\n",
      "Epoch 1110 Batch   22/175   train_loss = 1.131\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1110 Batch   54/175   train_loss = 1.238\n",
      "Epoch 1110 Batch   86/175   train_loss = 1.236\n",
      "Epoch 1110 Batch  118/175   train_loss = 1.233\n",
      "Epoch 1110 Batch  150/175   train_loss = 1.191\n",
      "Epoch 1111 Batch    7/175   train_loss = 1.206\n",
      "Epoch 1111 Batch   39/175   train_loss = 1.147\n",
      "Epoch 1111 Batch   71/175   train_loss = 1.230\n",
      "Epoch 1111 Batch  103/175   train_loss = 1.185\n",
      "Epoch 1111 Batch  135/175   train_loss = 1.161\n",
      "Epoch 1111 Batch  167/175   train_loss = 1.214\n",
      "Epoch 1112 Batch   24/175   train_loss = 1.133\n",
      "Epoch 1112 Batch   56/175   train_loss = 1.220\n",
      "Epoch 1112 Batch   88/175   train_loss = 1.231\n",
      "Epoch 1112 Batch  120/175   train_loss = 1.167\n",
      "Epoch 1112 Batch  152/175   train_loss = 1.142\n",
      "Epoch 1113 Batch    9/175   train_loss = 1.210\n",
      "Epoch 1113 Batch   41/175   train_loss = 1.233\n",
      "Epoch 1113 Batch   73/175   train_loss = 1.194\n",
      "Epoch 1113 Batch  105/175   train_loss = 1.220\n",
      "Epoch 1113 Batch  137/175   train_loss = 1.153\n",
      "Epoch 1113 Batch  169/175   train_loss = 1.194\n",
      "Epoch 1114 Batch   26/175   train_loss = 1.188\n",
      "Epoch 1114 Batch   58/175   train_loss = 1.223\n",
      "Epoch 1114 Batch   90/175   train_loss = 1.188\n",
      "Epoch 1114 Batch  122/175   train_loss = 1.155\n",
      "Epoch 1114 Batch  154/175   train_loss = 1.170\n",
      "Epoch 1115 Batch   11/175   train_loss = 1.150\n",
      "Epoch 1115 Batch   43/175   train_loss = 1.181\n",
      "Epoch 1115 Batch   75/175   train_loss = 1.164\n",
      "Epoch 1115 Batch  107/175   train_loss = 1.212\n",
      "Epoch 1115 Batch  139/175   train_loss = 1.138\n",
      "Epoch 1115 Batch  171/175   train_loss = 1.233\n",
      "Epoch 1116 Batch   28/175   train_loss = 1.164\n",
      "Epoch 1116 Batch   60/175   train_loss = 1.182\n",
      "Epoch 1116 Batch   92/175   train_loss = 1.147\n",
      "Epoch 1116 Batch  124/175   train_loss = 1.176\n",
      "Epoch 1116 Batch  156/175   train_loss = 1.224\n",
      "Epoch 1117 Batch   13/175   train_loss = 1.177\n",
      "Epoch 1117 Batch   45/175   train_loss = 1.157\n",
      "Epoch 1117 Batch   77/175   train_loss = 1.177\n",
      "Epoch 1117 Batch  109/175   train_loss = 1.231\n",
      "Epoch 1117 Batch  141/175   train_loss = 1.142\n",
      "Epoch 1117 Batch  173/175   train_loss = 1.166\n",
      "Epoch 1118 Batch   30/175   train_loss = 1.251\n",
      "Epoch 1118 Batch   62/175   train_loss = 1.209\n",
      "Epoch 1118 Batch   94/175   train_loss = 1.184\n",
      "Epoch 1118 Batch  126/175   train_loss = 1.206\n",
      "Epoch 1118 Batch  158/175   train_loss = 1.190\n",
      "Epoch 1119 Batch   15/175   train_loss = 1.241\n",
      "Epoch 1119 Batch   47/175   train_loss = 1.218\n",
      "Epoch 1119 Batch   79/175   train_loss = 1.223\n",
      "Epoch 1119 Batch  111/175   train_loss = 1.246\n",
      "Epoch 1119 Batch  143/175   train_loss = 1.155\n",
      "Epoch 1120 Batch    0/175   train_loss = 1.198\n",
      "Epoch 1120 Batch   32/175   train_loss = 1.201\n",
      "Epoch 1120 Batch   64/175   train_loss = 1.221\n",
      "Epoch 1120 Batch   96/175   train_loss = 1.189\n",
      "Epoch 1120 Batch  128/175   train_loss = 1.123\n",
      "Epoch 1120 Batch  160/175   train_loss = 1.169\n",
      "Epoch 1121 Batch   17/175   train_loss = 1.158\n",
      "Epoch 1121 Batch   49/175   train_loss = 1.198\n",
      "Epoch 1121 Batch   81/175   train_loss = 1.160\n",
      "Epoch 1121 Batch  113/175   train_loss = 1.190\n",
      "Epoch 1121 Batch  145/175   train_loss = 1.141\n",
      "Epoch 1122 Batch    2/175   train_loss = 1.187\n",
      "Epoch 1122 Batch   34/175   train_loss = 1.230\n",
      "Epoch 1122 Batch   66/175   train_loss = 1.214\n",
      "Epoch 1122 Batch   98/175   train_loss = 1.215\n",
      "Epoch 1122 Batch  130/175   train_loss = 1.213\n",
      "Epoch 1122 Batch  162/175   train_loss = 1.188\n",
      "Epoch 1123 Batch   19/175   train_loss = 1.180\n",
      "Epoch 1123 Batch   51/175   train_loss = 1.139\n",
      "Epoch 1123 Batch   83/175   train_loss = 1.223\n",
      "Epoch 1123 Batch  115/175   train_loss = 1.317\n",
      "Epoch 1123 Batch  147/175   train_loss = 1.154\n",
      "Epoch 1124 Batch    4/175   train_loss = 1.197\n",
      "Epoch 1124 Batch   36/175   train_loss = 1.142\n",
      "Epoch 1124 Batch   68/175   train_loss = 1.170\n",
      "Epoch 1124 Batch  100/175   train_loss = 1.193\n",
      "Epoch 1124 Batch  132/175   train_loss = 1.151\n",
      "Epoch 1124 Batch  164/175   train_loss = 1.140\n",
      "Epoch 1125 Batch   21/175   train_loss = 1.141\n",
      "Epoch 1125 Batch   53/175   train_loss = 1.156\n",
      "Epoch 1125 Batch   85/175   train_loss = 1.202\n",
      "Epoch 1125 Batch  117/175   train_loss = 1.189\n",
      "Epoch 1125 Batch  149/175   train_loss = 1.183\n",
      "Epoch 1126 Batch    6/175   train_loss = 1.213\n",
      "Epoch 1126 Batch   38/175   train_loss = 1.145\n",
      "Epoch 1126 Batch   70/175   train_loss = 1.183\n",
      "Epoch 1126 Batch  102/175   train_loss = 1.202\n",
      "Epoch 1126 Batch  134/175   train_loss = 1.152\n",
      "Epoch 1126 Batch  166/175   train_loss = 1.195\n",
      "Epoch 1127 Batch   23/175   train_loss = 1.132\n",
      "Epoch 1127 Batch   55/175   train_loss = 1.248\n",
      "Epoch 1127 Batch   87/175   train_loss = 1.264\n",
      "Epoch 1127 Batch  119/175   train_loss = 1.186\n",
      "Epoch 1127 Batch  151/175   train_loss = 1.188\n",
      "Epoch 1128 Batch    8/175   train_loss = 1.183\n",
      "Epoch 1128 Batch   40/175   train_loss = 1.168\n",
      "Epoch 1128 Batch   72/175   train_loss = 1.200\n",
      "Epoch 1128 Batch  104/175   train_loss = 1.222\n",
      "Epoch 1128 Batch  136/175   train_loss = 1.159\n",
      "Epoch 1128 Batch  168/175   train_loss = 1.198\n",
      "Epoch 1129 Batch   25/175   train_loss = 1.201\n",
      "Epoch 1129 Batch   57/175   train_loss = 1.212\n",
      "Epoch 1129 Batch   89/175   train_loss = 1.173\n",
      "Epoch 1129 Batch  121/175   train_loss = 1.155\n",
      "Epoch 1129 Batch  153/175   train_loss = 1.178\n",
      "Epoch 1130 Batch   10/175   train_loss = 1.149\n",
      "Epoch 1130 Batch   42/175   train_loss = 1.248\n",
      "Epoch 1130 Batch   74/175   train_loss = 1.219\n",
      "Epoch 1130 Batch  106/175   train_loss = 1.224\n",
      "Epoch 1130 Batch  138/175   train_loss = 1.168\n",
      "Epoch 1130 Batch  170/175   train_loss = 1.252\n",
      "Epoch 1131 Batch   27/175   train_loss = 1.176\n",
      "Epoch 1131 Batch   59/175   train_loss = 1.163\n",
      "Epoch 1131 Batch   91/175   train_loss = 1.182\n",
      "Epoch 1131 Batch  123/175   train_loss = 1.165\n",
      "Epoch 1131 Batch  155/175   train_loss = 1.145\n",
      "Epoch 1132 Batch   12/175   train_loss = 1.182\n",
      "Epoch 1132 Batch   44/175   train_loss = 1.180\n",
      "Epoch 1132 Batch   76/175   train_loss = 1.156\n",
      "Epoch 1132 Batch  108/175   train_loss = 1.196\n",
      "Epoch 1132 Batch  140/175   train_loss = 1.168\n",
      "Epoch 1132 Batch  172/175   train_loss = 1.218\n",
      "Epoch 1133 Batch   29/175   train_loss = 1.166\n",
      "Epoch 1133 Batch   61/175   train_loss = 1.206\n",
      "Epoch 1133 Batch   93/175   train_loss = 1.186\n",
      "Epoch 1133 Batch  125/175   train_loss = 1.216\n",
      "Epoch 1133 Batch  157/175   train_loss = 1.169\n",
      "Epoch 1134 Batch   14/175   train_loss = 1.206\n",
      "Epoch 1134 Batch   46/175   train_loss = 1.184\n",
      "Epoch 1134 Batch   78/175   train_loss = 1.167\n",
      "Epoch 1134 Batch  110/175   train_loss = 1.269\n",
      "Epoch 1134 Batch  142/175   train_loss = 1.193\n",
      "Epoch 1134 Batch  174/175   train_loss = 1.182\n",
      "Epoch 1135 Batch   31/175   train_loss = 1.215\n",
      "Epoch 1135 Batch   63/175   train_loss = 1.202\n",
      "Epoch 1135 Batch   95/175   train_loss = 1.153\n",
      "Epoch 1135 Batch  127/175   train_loss = 1.149\n",
      "Epoch 1135 Batch  159/175   train_loss = 1.138\n",
      "Epoch 1136 Batch   16/175   train_loss = 1.165\n",
      "Epoch 1136 Batch   48/175   train_loss = 1.178\n",
      "Epoch 1136 Batch   80/175   train_loss = 1.197\n",
      "Epoch 1136 Batch  112/175   train_loss = 1.193\n",
      "Epoch 1136 Batch  144/175   train_loss = 1.131\n",
      "Epoch 1137 Batch    1/175   train_loss = 1.230\n",
      "Epoch 1137 Batch   33/175   train_loss = 1.229\n",
      "Epoch 1137 Batch   65/175   train_loss = 1.198\n",
      "Epoch 1137 Batch   97/175   train_loss = 1.178\n",
      "Epoch 1137 Batch  129/175   train_loss = 1.181\n",
      "Epoch 1137 Batch  161/175   train_loss = 1.167\n",
      "Epoch 1138 Batch   18/175   train_loss = 1.149\n",
      "Epoch 1138 Batch   50/175   train_loss = 1.159\n",
      "Epoch 1138 Batch   82/175   train_loss = 1.221\n",
      "Epoch 1138 Batch  114/175   train_loss = 1.228\n",
      "Epoch 1138 Batch  146/175   train_loss = 1.188\n",
      "Epoch 1139 Batch    3/175   train_loss = 1.217\n",
      "Epoch 1139 Batch   35/175   train_loss = 1.157\n",
      "Epoch 1139 Batch   67/175   train_loss = 1.160\n",
      "Epoch 1139 Batch   99/175   train_loss = 1.240\n",
      "Epoch 1139 Batch  131/175   train_loss = 1.170\n",
      "Epoch 1139 Batch  163/175   train_loss = 1.186\n",
      "Epoch 1140 Batch   20/175   train_loss = 1.167\n",
      "Epoch 1140 Batch   52/175   train_loss = 1.115\n",
      "Epoch 1140 Batch   84/175   train_loss = 1.184\n",
      "Epoch 1140 Batch  116/175   train_loss = 1.216\n",
      "Epoch 1140 Batch  148/175   train_loss = 1.177\n",
      "Epoch 1141 Batch    5/175   train_loss = 1.185\n",
      "Epoch 1141 Batch   37/175   train_loss = 1.157\n",
      "Epoch 1141 Batch   69/175   train_loss = 1.196\n",
      "Epoch 1141 Batch  101/175   train_loss = 1.239\n",
      "Epoch 1141 Batch  133/175   train_loss = 1.084\n",
      "Epoch 1141 Batch  165/175   train_loss = 1.177\n",
      "Epoch 1142 Batch   22/175   train_loss = 1.111\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1142 Batch   54/175   train_loss = 1.229\n",
      "Epoch 1142 Batch   86/175   train_loss = 1.250\n",
      "Epoch 1142 Batch  118/175   train_loss = 1.235\n",
      "Epoch 1142 Batch  150/175   train_loss = 1.173\n",
      "Epoch 1143 Batch    7/175   train_loss = 1.205\n",
      "Epoch 1143 Batch   39/175   train_loss = 1.142\n",
      "Epoch 1143 Batch   71/175   train_loss = 1.215\n",
      "Epoch 1143 Batch  103/175   train_loss = 1.199\n",
      "Epoch 1143 Batch  135/175   train_loss = 1.161\n",
      "Epoch 1143 Batch  167/175   train_loss = 1.215\n",
      "Epoch 1144 Batch   24/175   train_loss = 1.146\n",
      "Epoch 1144 Batch   56/175   train_loss = 1.200\n",
      "Epoch 1144 Batch   88/175   train_loss = 1.224\n",
      "Epoch 1144 Batch  120/175   train_loss = 1.181\n",
      "Epoch 1144 Batch  152/175   train_loss = 1.153\n",
      "Epoch 1145 Batch    9/175   train_loss = 1.207\n",
      "Epoch 1145 Batch   41/175   train_loss = 1.199\n",
      "Epoch 1145 Batch   73/175   train_loss = 1.203\n",
      "Epoch 1145 Batch  105/175   train_loss = 1.210\n",
      "Epoch 1145 Batch  137/175   train_loss = 1.152\n",
      "Epoch 1145 Batch  169/175   train_loss = 1.220\n",
      "Epoch 1146 Batch   26/175   train_loss = 1.191\n",
      "Epoch 1146 Batch   58/175   train_loss = 1.227\n",
      "Epoch 1146 Batch   90/175   train_loss = 1.205\n",
      "Epoch 1146 Batch  122/175   train_loss = 1.182\n",
      "Epoch 1146 Batch  154/175   train_loss = 1.170\n",
      "Epoch 1147 Batch   11/175   train_loss = 1.190\n",
      "Epoch 1147 Batch   43/175   train_loss = 1.189\n",
      "Epoch 1147 Batch   75/175   train_loss = 1.186\n",
      "Epoch 1147 Batch  107/175   train_loss = 1.227\n",
      "Epoch 1147 Batch  139/175   train_loss = 1.140\n",
      "Epoch 1147 Batch  171/175   train_loss = 1.245\n",
      "Epoch 1148 Batch   28/175   train_loss = 1.182\n",
      "Epoch 1148 Batch   60/175   train_loss = 1.190\n",
      "Epoch 1148 Batch   92/175   train_loss = 1.175\n",
      "Epoch 1148 Batch  124/175   train_loss = 1.173\n",
      "Epoch 1148 Batch  156/175   train_loss = 1.219\n",
      "Epoch 1149 Batch   13/175   train_loss = 1.187\n",
      "Epoch 1149 Batch   45/175   train_loss = 1.198\n",
      "Epoch 1149 Batch   77/175   train_loss = 1.200\n",
      "Epoch 1149 Batch  109/175   train_loss = 1.242\n",
      "Epoch 1149 Batch  141/175   train_loss = 1.151\n",
      "Epoch 1149 Batch  173/175   train_loss = 1.164\n",
      "Epoch 1150 Batch   30/175   train_loss = 1.245\n",
      "Epoch 1150 Batch   62/175   train_loss = 1.248\n",
      "Epoch 1150 Batch   94/175   train_loss = 1.180\n",
      "Epoch 1150 Batch  126/175   train_loss = 1.212\n",
      "Epoch 1150 Batch  158/175   train_loss = 1.195\n",
      "Epoch 1151 Batch   15/175   train_loss = 1.230\n",
      "Epoch 1151 Batch   47/175   train_loss = 1.224\n",
      "Epoch 1151 Batch   79/175   train_loss = 1.233\n",
      "Epoch 1151 Batch  111/175   train_loss = 1.227\n",
      "Epoch 1151 Batch  143/175   train_loss = 1.164\n",
      "Epoch 1152 Batch    0/175   train_loss = 1.209\n",
      "Epoch 1152 Batch   32/175   train_loss = 1.218\n",
      "Epoch 1152 Batch   64/175   train_loss = 1.206\n",
      "Epoch 1152 Batch   96/175   train_loss = 1.229\n",
      "Epoch 1152 Batch  128/175   train_loss = 1.150\n",
      "Epoch 1152 Batch  160/175   train_loss = 1.177\n",
      "Epoch 1153 Batch   17/175   train_loss = 1.145\n",
      "Epoch 1153 Batch   49/175   train_loss = 1.188\n",
      "Epoch 1153 Batch   81/175   train_loss = 1.163\n",
      "Epoch 1153 Batch  113/175   train_loss = 1.203\n",
      "Epoch 1153 Batch  145/175   train_loss = 1.141\n",
      "Epoch 1154 Batch    2/175   train_loss = 1.196\n",
      "Epoch 1154 Batch   34/175   train_loss = 1.211\n",
      "Epoch 1154 Batch   66/175   train_loss = 1.221\n",
      "Epoch 1154 Batch   98/175   train_loss = 1.210\n",
      "Epoch 1154 Batch  130/175   train_loss = 1.213\n",
      "Epoch 1154 Batch  162/175   train_loss = 1.166\n",
      "Epoch 1155 Batch   19/175   train_loss = 1.166\n",
      "Epoch 1155 Batch   51/175   train_loss = 1.136\n",
      "Epoch 1155 Batch   83/175   train_loss = 1.244\n",
      "Epoch 1155 Batch  115/175   train_loss = 1.292\n",
      "Epoch 1155 Batch  147/175   train_loss = 1.166\n",
      "Epoch 1156 Batch    4/175   train_loss = 1.206\n",
      "Epoch 1156 Batch   36/175   train_loss = 1.141\n",
      "Epoch 1156 Batch   68/175   train_loss = 1.170\n",
      "Epoch 1156 Batch  100/175   train_loss = 1.186\n",
      "Epoch 1156 Batch  132/175   train_loss = 1.171\n",
      "Epoch 1156 Batch  164/175   train_loss = 1.155\n",
      "Epoch 1157 Batch   21/175   train_loss = 1.145\n",
      "Epoch 1157 Batch   53/175   train_loss = 1.150\n",
      "Epoch 1157 Batch   85/175   train_loss = 1.234\n",
      "Epoch 1157 Batch  117/175   train_loss = 1.225\n",
      "Epoch 1157 Batch  149/175   train_loss = 1.193\n",
      "Epoch 1158 Batch    6/175   train_loss = 1.202\n",
      "Epoch 1158 Batch   38/175   train_loss = 1.148\n",
      "Epoch 1158 Batch   70/175   train_loss = 1.201\n",
      "Epoch 1158 Batch  102/175   train_loss = 1.195\n",
      "Epoch 1158 Batch  134/175   train_loss = 1.163\n",
      "Epoch 1158 Batch  166/175   train_loss = 1.205\n",
      "Epoch 1159 Batch   23/175   train_loss = 1.141\n",
      "Epoch 1159 Batch   55/175   train_loss = 1.227\n",
      "Epoch 1159 Batch   87/175   train_loss = 1.282\n",
      "Epoch 1159 Batch  119/175   train_loss = 1.200\n",
      "Epoch 1159 Batch  151/175   train_loss = 1.205\n",
      "Epoch 1160 Batch    8/175   train_loss = 1.199\n",
      "Epoch 1160 Batch   40/175   train_loss = 1.164\n",
      "Epoch 1160 Batch   72/175   train_loss = 1.202\n",
      "Epoch 1160 Batch  104/175   train_loss = 1.218\n",
      "Epoch 1160 Batch  136/175   train_loss = 1.163\n",
      "Epoch 1160 Batch  168/175   train_loss = 1.214\n",
      "Epoch 1161 Batch   25/175   train_loss = 1.199\n",
      "Epoch 1161 Batch   57/175   train_loss = 1.232\n",
      "Epoch 1161 Batch   89/175   train_loss = 1.200\n",
      "Epoch 1161 Batch  121/175   train_loss = 1.163\n",
      "Epoch 1161 Batch  153/175   train_loss = 1.197\n",
      "Epoch 1162 Batch   10/175   train_loss = 1.152\n",
      "Epoch 1162 Batch   42/175   train_loss = 1.248\n",
      "Epoch 1162 Batch   74/175   train_loss = 1.229\n",
      "Epoch 1162 Batch  106/175   train_loss = 1.236\n",
      "Epoch 1162 Batch  138/175   train_loss = 1.175\n",
      "Epoch 1162 Batch  170/175   train_loss = 1.227\n",
      "Epoch 1163 Batch   27/175   train_loss = 1.192\n",
      "Epoch 1163 Batch   59/175   train_loss = 1.173\n",
      "Epoch 1163 Batch   91/175   train_loss = 1.174\n",
      "Epoch 1163 Batch  123/175   train_loss = 1.173\n",
      "Epoch 1163 Batch  155/175   train_loss = 1.145\n",
      "Epoch 1164 Batch   12/175   train_loss = 1.194\n",
      "Epoch 1164 Batch   44/175   train_loss = 1.165\n",
      "Epoch 1164 Batch   76/175   train_loss = 1.172\n",
      "Epoch 1164 Batch  108/175   train_loss = 1.203\n",
      "Epoch 1164 Batch  140/175   train_loss = 1.170\n",
      "Epoch 1164 Batch  172/175   train_loss = 1.215\n",
      "Epoch 1165 Batch   29/175   train_loss = 1.179\n",
      "Epoch 1165 Batch   61/175   train_loss = 1.215\n",
      "Epoch 1165 Batch   93/175   train_loss = 1.203\n",
      "Epoch 1165 Batch  125/175   train_loss = 1.221\n",
      "Epoch 1165 Batch  157/175   train_loss = 1.182\n",
      "Epoch 1166 Batch   14/175   train_loss = 1.196\n",
      "Epoch 1166 Batch   46/175   train_loss = 1.182\n",
      "Epoch 1166 Batch   78/175   train_loss = 1.160\n",
      "Epoch 1166 Batch  110/175   train_loss = 1.262\n",
      "Epoch 1166 Batch  142/175   train_loss = 1.193\n",
      "Epoch 1166 Batch  174/175   train_loss = 1.192\n",
      "Epoch 1167 Batch   31/175   train_loss = 1.218\n",
      "Epoch 1167 Batch   63/175   train_loss = 1.198\n",
      "Epoch 1167 Batch   95/175   train_loss = 1.168\n",
      "Epoch 1167 Batch  127/175   train_loss = 1.139\n",
      "Epoch 1167 Batch  159/175   train_loss = 1.156\n",
      "Epoch 1168 Batch   16/175   train_loss = 1.181\n",
      "Epoch 1168 Batch   48/175   train_loss = 1.185\n",
      "Epoch 1168 Batch   80/175   train_loss = 1.190\n",
      "Epoch 1168 Batch  112/175   train_loss = 1.194\n",
      "Epoch 1168 Batch  144/175   train_loss = 1.137\n",
      "Epoch 1169 Batch    1/175   train_loss = 1.223\n",
      "Epoch 1169 Batch   33/175   train_loss = 1.235\n",
      "Epoch 1169 Batch   65/175   train_loss = 1.176\n",
      "Epoch 1169 Batch   97/175   train_loss = 1.180\n",
      "Epoch 1169 Batch  129/175   train_loss = 1.167\n",
      "Epoch 1169 Batch  161/175   train_loss = 1.170\n",
      "Epoch 1170 Batch   18/175   train_loss = 1.152\n",
      "Epoch 1170 Batch   50/175   train_loss = 1.174\n",
      "Epoch 1170 Batch   82/175   train_loss = 1.233\n",
      "Epoch 1170 Batch  114/175   train_loss = 1.243\n",
      "Epoch 1170 Batch  146/175   train_loss = 1.195\n",
      "Epoch 1171 Batch    3/175   train_loss = 1.219\n",
      "Epoch 1171 Batch   35/175   train_loss = 1.190\n",
      "Epoch 1171 Batch   67/175   train_loss = 1.168\n",
      "Epoch 1171 Batch   99/175   train_loss = 1.250\n",
      "Epoch 1171 Batch  131/175   train_loss = 1.196\n",
      "Epoch 1171 Batch  163/175   train_loss = 1.184\n",
      "Epoch 1172 Batch   20/175   train_loss = 1.173\n",
      "Epoch 1172 Batch   52/175   train_loss = 1.105\n",
      "Epoch 1172 Batch   84/175   train_loss = 1.190\n",
      "Epoch 1172 Batch  116/175   train_loss = 1.215\n",
      "Epoch 1172 Batch  148/175   train_loss = 1.193\n",
      "Epoch 1173 Batch    5/175   train_loss = 1.185\n",
      "Epoch 1173 Batch   37/175   train_loss = 1.163\n",
      "Epoch 1173 Batch   69/175   train_loss = 1.202\n",
      "Epoch 1173 Batch  101/175   train_loss = 1.244\n",
      "Epoch 1173 Batch  133/175   train_loss = 1.101\n",
      "Epoch 1173 Batch  165/175   train_loss = 1.196\n",
      "Epoch 1174 Batch   22/175   train_loss = 1.151\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1174 Batch   54/175   train_loss = 1.234\n",
      "Epoch 1174 Batch   86/175   train_loss = 1.249\n",
      "Epoch 1174 Batch  118/175   train_loss = 1.255\n",
      "Epoch 1174 Batch  150/175   train_loss = 1.192\n",
      "Epoch 1175 Batch    7/175   train_loss = 1.208\n",
      "Epoch 1175 Batch   39/175   train_loss = 1.158\n",
      "Epoch 1175 Batch   71/175   train_loss = 1.239\n",
      "Epoch 1175 Batch  103/175   train_loss = 1.205\n",
      "Epoch 1175 Batch  135/175   train_loss = 1.183\n",
      "Epoch 1175 Batch  167/175   train_loss = 1.248\n",
      "Epoch 1176 Batch   24/175   train_loss = 1.142\n",
      "Epoch 1176 Batch   56/175   train_loss = 1.220\n",
      "Epoch 1176 Batch   88/175   train_loss = 1.216\n",
      "Epoch 1176 Batch  120/175   train_loss = 1.223\n",
      "Epoch 1176 Batch  152/175   train_loss = 1.194\n",
      "Epoch 1177 Batch    9/175   train_loss = 1.214\n",
      "Epoch 1177 Batch   41/175   train_loss = 1.230\n",
      "Epoch 1177 Batch   73/175   train_loss = 1.216\n",
      "Epoch 1177 Batch  105/175   train_loss = 1.263\n",
      "Epoch 1177 Batch  137/175   train_loss = 1.163\n",
      "Epoch 1177 Batch  169/175   train_loss = 1.223\n",
      "Epoch 1178 Batch   26/175   train_loss = 1.186\n",
      "Epoch 1178 Batch   58/175   train_loss = 1.221\n",
      "Epoch 1178 Batch   90/175   train_loss = 1.214\n",
      "Epoch 1178 Batch  122/175   train_loss = 1.192\n",
      "Epoch 1178 Batch  154/175   train_loss = 1.178\n",
      "Epoch 1179 Batch   11/175   train_loss = 1.178\n",
      "Epoch 1179 Batch   43/175   train_loss = 1.184\n",
      "Epoch 1179 Batch   75/175   train_loss = 1.179\n",
      "Epoch 1179 Batch  107/175   train_loss = 1.258\n",
      "Epoch 1179 Batch  139/175   train_loss = 1.150\n",
      "Epoch 1179 Batch  171/175   train_loss = 1.247\n",
      "Epoch 1180 Batch   28/175   train_loss = 1.180\n",
      "Epoch 1180 Batch   60/175   train_loss = 1.179\n",
      "Epoch 1180 Batch   92/175   train_loss = 1.172\n",
      "Epoch 1180 Batch  124/175   train_loss = 1.181\n",
      "Epoch 1180 Batch  156/175   train_loss = 1.217\n",
      "Epoch 1181 Batch   13/175   train_loss = 1.198\n",
      "Epoch 1181 Batch   45/175   train_loss = 1.192\n",
      "Epoch 1181 Batch   77/175   train_loss = 1.174\n",
      "Epoch 1181 Batch  109/175   train_loss = 1.216\n",
      "Epoch 1181 Batch  141/175   train_loss = 1.163\n",
      "Epoch 1181 Batch  173/175   train_loss = 1.154\n",
      "Epoch 1182 Batch   30/175   train_loss = 1.234\n",
      "Epoch 1182 Batch   62/175   train_loss = 1.220\n",
      "Epoch 1182 Batch   94/175   train_loss = 1.184\n",
      "Epoch 1182 Batch  126/175   train_loss = 1.207\n",
      "Epoch 1182 Batch  158/175   train_loss = 1.190\n",
      "Epoch 1183 Batch   15/175   train_loss = 1.251\n",
      "Epoch 1183 Batch   47/175   train_loss = 1.218\n",
      "Epoch 1183 Batch   79/175   train_loss = 1.241\n",
      "Epoch 1183 Batch  111/175   train_loss = 1.231\n",
      "Epoch 1183 Batch  143/175   train_loss = 1.176\n",
      "Epoch 1184 Batch    0/175   train_loss = 1.210\n",
      "Epoch 1184 Batch   32/175   train_loss = 1.217\n",
      "Epoch 1184 Batch   64/175   train_loss = 1.228\n",
      "Epoch 1184 Batch   96/175   train_loss = 1.216\n",
      "Epoch 1184 Batch  128/175   train_loss = 1.147\n",
      "Epoch 1184 Batch  160/175   train_loss = 1.183\n",
      "Epoch 1185 Batch   17/175   train_loss = 1.164\n",
      "Epoch 1185 Batch   49/175   train_loss = 1.205\n",
      "Epoch 1185 Batch   81/175   train_loss = 1.171\n",
      "Epoch 1185 Batch  113/175   train_loss = 1.203\n",
      "Epoch 1185 Batch  145/175   train_loss = 1.150\n",
      "Epoch 1186 Batch    2/175   train_loss = 1.185\n",
      "Epoch 1186 Batch   34/175   train_loss = 1.211\n",
      "Epoch 1186 Batch   66/175   train_loss = 1.225\n",
      "Epoch 1186 Batch   98/175   train_loss = 1.218\n",
      "Epoch 1186 Batch  130/175   train_loss = 1.206\n",
      "Epoch 1186 Batch  162/175   train_loss = 1.178\n",
      "Epoch 1187 Batch   19/175   train_loss = 1.190\n",
      "Epoch 1187 Batch   51/175   train_loss = 1.133\n",
      "Epoch 1187 Batch   83/175   train_loss = 1.256\n",
      "Epoch 1187 Batch  115/175   train_loss = 1.305\n",
      "Epoch 1187 Batch  147/175   train_loss = 1.163\n",
      "Epoch 1188 Batch    4/175   train_loss = 1.196\n",
      "Epoch 1188 Batch   36/175   train_loss = 1.150\n",
      "Epoch 1188 Batch   68/175   train_loss = 1.179\n",
      "Epoch 1188 Batch  100/175   train_loss = 1.174\n",
      "Epoch 1188 Batch  132/175   train_loss = 1.186\n",
      "Epoch 1188 Batch  164/175   train_loss = 1.154\n",
      "Epoch 1189 Batch   21/175   train_loss = 1.150\n",
      "Epoch 1189 Batch   53/175   train_loss = 1.146\n",
      "Epoch 1189 Batch   85/175   train_loss = 1.189\n",
      "Epoch 1189 Batch  117/175   train_loss = 1.206\n",
      "Epoch 1189 Batch  149/175   train_loss = 1.201\n",
      "Epoch 1190 Batch    6/175   train_loss = 1.202\n",
      "Epoch 1190 Batch   38/175   train_loss = 1.150\n",
      "Epoch 1190 Batch   70/175   train_loss = 1.175\n",
      "Epoch 1190 Batch  102/175   train_loss = 1.192\n",
      "Epoch 1190 Batch  134/175   train_loss = 1.146\n",
      "Epoch 1190 Batch  166/175   train_loss = 1.194\n",
      "Epoch 1191 Batch   23/175   train_loss = 1.151\n",
      "Epoch 1191 Batch   55/175   train_loss = 1.248\n",
      "Epoch 1191 Batch   87/175   train_loss = 1.267\n",
      "Epoch 1191 Batch  119/175   train_loss = 1.195\n",
      "Epoch 1191 Batch  151/175   train_loss = 1.209\n",
      "Epoch 1192 Batch    8/175   train_loss = 1.198\n",
      "Epoch 1192 Batch   40/175   train_loss = 1.174\n",
      "Epoch 1192 Batch   72/175   train_loss = 1.203\n",
      "Epoch 1192 Batch  104/175   train_loss = 1.206\n",
      "Epoch 1192 Batch  136/175   train_loss = 1.161\n",
      "Epoch 1192 Batch  168/175   train_loss = 1.213\n",
      "Epoch 1193 Batch   25/175   train_loss = 1.197\n",
      "Epoch 1193 Batch   57/175   train_loss = 1.222\n",
      "Epoch 1193 Batch   89/175   train_loss = 1.184\n",
      "Epoch 1193 Batch  121/175   train_loss = 1.179\n",
      "Epoch 1193 Batch  153/175   train_loss = 1.198\n",
      "Epoch 1194 Batch   10/175   train_loss = 1.144\n",
      "Epoch 1194 Batch   42/175   train_loss = 1.234\n",
      "Epoch 1194 Batch   74/175   train_loss = 1.223\n",
      "Epoch 1194 Batch  106/175   train_loss = 1.230\n",
      "Epoch 1194 Batch  138/175   train_loss = 1.158\n",
      "Epoch 1194 Batch  170/175   train_loss = 1.216\n",
      "Epoch 1195 Batch   27/175   train_loss = 1.168\n",
      "Epoch 1195 Batch   59/175   train_loss = 1.163\n",
      "Epoch 1195 Batch   91/175   train_loss = 1.177\n",
      "Epoch 1195 Batch  123/175   train_loss = 1.169\n",
      "Epoch 1195 Batch  155/175   train_loss = 1.165\n",
      "Epoch 1196 Batch   12/175   train_loss = 1.186\n",
      "Epoch 1196 Batch   44/175   train_loss = 1.194\n",
      "Epoch 1196 Batch   76/175   train_loss = 1.163\n",
      "Epoch 1196 Batch  108/175   train_loss = 1.213\n",
      "Epoch 1196 Batch  140/175   train_loss = 1.177\n",
      "Epoch 1196 Batch  172/175   train_loss = 1.218\n",
      "Epoch 1197 Batch   29/175   train_loss = 1.193\n",
      "Epoch 1197 Batch   61/175   train_loss = 1.209\n",
      "Epoch 1197 Batch   93/175   train_loss = 1.195\n",
      "Epoch 1197 Batch  125/175   train_loss = 1.201\n",
      "Epoch 1197 Batch  157/175   train_loss = 1.198\n",
      "Epoch 1198 Batch   14/175   train_loss = 1.214\n",
      "Epoch 1198 Batch   46/175   train_loss = 1.197\n",
      "Epoch 1198 Batch   78/175   train_loss = 1.154\n",
      "Epoch 1198 Batch  110/175   train_loss = 1.281\n",
      "Epoch 1198 Batch  142/175   train_loss = 1.203\n",
      "Epoch 1198 Batch  174/175   train_loss = 1.162\n",
      "Epoch 1199 Batch   31/175   train_loss = 1.204\n",
      "Epoch 1199 Batch   63/175   train_loss = 1.209\n",
      "Epoch 1199 Batch   95/175   train_loss = 1.163\n",
      "Epoch 1199 Batch  127/175   train_loss = 1.159\n",
      "Epoch 1199 Batch  159/175   train_loss = 1.151\n",
      "Epoch 1200 Batch   16/175   train_loss = 1.176\n",
      "Epoch 1200 Batch   48/175   train_loss = 1.184\n",
      "Epoch 1200 Batch   80/175   train_loss = 1.198\n",
      "Epoch 1200 Batch  112/175   train_loss = 1.199\n",
      "Epoch 1200 Batch  144/175   train_loss = 1.135\n",
      "Epoch 1201 Batch    1/175   train_loss = 1.232\n",
      "Epoch 1201 Batch   33/175   train_loss = 1.228\n",
      "Epoch 1201 Batch   65/175   train_loss = 1.175\n",
      "Epoch 1201 Batch   97/175   train_loss = 1.198\n",
      "Epoch 1201 Batch  129/175   train_loss = 1.188\n",
      "Epoch 1201 Batch  161/175   train_loss = 1.181\n",
      "Epoch 1202 Batch   18/175   train_loss = 1.140\n",
      "Epoch 1202 Batch   50/175   train_loss = 1.172\n",
      "Epoch 1202 Batch   82/175   train_loss = 1.215\n",
      "Epoch 1202 Batch  114/175   train_loss = 1.231\n",
      "Epoch 1202 Batch  146/175   train_loss = 1.173\n",
      "Epoch 1203 Batch    3/175   train_loss = 1.221\n",
      "Epoch 1203 Batch   35/175   train_loss = 1.171\n",
      "Epoch 1203 Batch   67/175   train_loss = 1.142\n",
      "Epoch 1203 Batch   99/175   train_loss = 1.239\n",
      "Epoch 1203 Batch  131/175   train_loss = 1.172\n",
      "Epoch 1203 Batch  163/175   train_loss = 1.176\n",
      "Epoch 1204 Batch   20/175   train_loss = 1.190\n",
      "Epoch 1204 Batch   52/175   train_loss = 1.119\n",
      "Epoch 1204 Batch   84/175   train_loss = 1.181\n",
      "Epoch 1204 Batch  116/175   train_loss = 1.220\n",
      "Epoch 1204 Batch  148/175   train_loss = 1.182\n",
      "Epoch 1205 Batch    5/175   train_loss = 1.175\n",
      "Epoch 1205 Batch   37/175   train_loss = 1.159\n",
      "Epoch 1205 Batch   69/175   train_loss = 1.212\n",
      "Epoch 1205 Batch  101/175   train_loss = 1.240\n",
      "Epoch 1205 Batch  133/175   train_loss = 1.097\n",
      "Epoch 1205 Batch  165/175   train_loss = 1.181\n",
      "Epoch 1206 Batch   22/175   train_loss = 1.128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1206 Batch   54/175   train_loss = 1.220\n",
      "Epoch 1206 Batch   86/175   train_loss = 1.234\n",
      "Epoch 1206 Batch  118/175   train_loss = 1.244\n",
      "Epoch 1206 Batch  150/175   train_loss = 1.200\n",
      "Epoch 1207 Batch    7/175   train_loss = 1.220\n",
      "Epoch 1207 Batch   39/175   train_loss = 1.144\n",
      "Epoch 1207 Batch   71/175   train_loss = 1.209\n",
      "Epoch 1207 Batch  103/175   train_loss = 1.194\n",
      "Epoch 1207 Batch  135/175   train_loss = 1.152\n",
      "Epoch 1207 Batch  167/175   train_loss = 1.231\n",
      "Epoch 1208 Batch   24/175   train_loss = 1.140\n",
      "Epoch 1208 Batch   56/175   train_loss = 1.223\n",
      "Epoch 1208 Batch   88/175   train_loss = 1.215\n",
      "Epoch 1208 Batch  120/175   train_loss = 1.158\n",
      "Epoch 1208 Batch  152/175   train_loss = 1.167\n",
      "Epoch 1209 Batch    9/175   train_loss = 1.198\n",
      "Epoch 1209 Batch   41/175   train_loss = 1.211\n",
      "Epoch 1209 Batch   73/175   train_loss = 1.202\n",
      "Epoch 1209 Batch  105/175   train_loss = 1.230\n",
      "Epoch 1209 Batch  137/175   train_loss = 1.141\n",
      "Epoch 1209 Batch  169/175   train_loss = 1.208\n",
      "Epoch 1210 Batch   26/175   train_loss = 1.193\n",
      "Epoch 1210 Batch   58/175   train_loss = 1.212\n",
      "Epoch 1210 Batch   90/175   train_loss = 1.218\n",
      "Epoch 1210 Batch  122/175   train_loss = 1.187\n",
      "Epoch 1210 Batch  154/175   train_loss = 1.158\n",
      "Epoch 1211 Batch   11/175   train_loss = 1.155\n",
      "Epoch 1211 Batch   43/175   train_loss = 1.172\n",
      "Epoch 1211 Batch   75/175   train_loss = 1.179\n",
      "Epoch 1211 Batch  107/175   train_loss = 1.239\n",
      "Epoch 1211 Batch  139/175   train_loss = 1.140\n",
      "Epoch 1211 Batch  171/175   train_loss = 1.243\n",
      "Epoch 1212 Batch   28/175   train_loss = 1.170\n",
      "Epoch 1212 Batch   60/175   train_loss = 1.164\n",
      "Epoch 1212 Batch   92/175   train_loss = 1.158\n",
      "Epoch 1212 Batch  124/175   train_loss = 1.193\n",
      "Epoch 1212 Batch  156/175   train_loss = 1.235\n",
      "Epoch 1213 Batch   13/175   train_loss = 1.187\n",
      "Epoch 1213 Batch   45/175   train_loss = 1.184\n",
      "Epoch 1213 Batch   77/175   train_loss = 1.167\n",
      "Epoch 1213 Batch  109/175   train_loss = 1.231\n",
      "Epoch 1213 Batch  141/175   train_loss = 1.146\n",
      "Epoch 1213 Batch  173/175   train_loss = 1.164\n",
      "Epoch 1214 Batch   30/175   train_loss = 1.230\n",
      "Epoch 1214 Batch   62/175   train_loss = 1.199\n",
      "Epoch 1214 Batch   94/175   train_loss = 1.186\n",
      "Epoch 1214 Batch  126/175   train_loss = 1.206\n",
      "Epoch 1214 Batch  158/175   train_loss = 1.157\n",
      "Epoch 1215 Batch   15/175   train_loss = 1.231\n",
      "Epoch 1215 Batch   47/175   train_loss = 1.197\n",
      "Epoch 1215 Batch   79/175   train_loss = 1.220\n",
      "Epoch 1215 Batch  111/175   train_loss = 1.224\n",
      "Epoch 1215 Batch  143/175   train_loss = 1.153\n",
      "Epoch 1216 Batch    0/175   train_loss = 1.195\n",
      "Epoch 1216 Batch   32/175   train_loss = 1.197\n",
      "Epoch 1216 Batch   64/175   train_loss = 1.199\n",
      "Epoch 1216 Batch   96/175   train_loss = 1.207\n",
      "Epoch 1216 Batch  128/175   train_loss = 1.128\n",
      "Epoch 1216 Batch  160/175   train_loss = 1.154\n",
      "Epoch 1217 Batch   17/175   train_loss = 1.155\n",
      "Epoch 1217 Batch   49/175   train_loss = 1.180\n",
      "Epoch 1217 Batch   81/175   train_loss = 1.169\n",
      "Epoch 1217 Batch  113/175   train_loss = 1.215\n",
      "Epoch 1217 Batch  145/175   train_loss = 1.136\n",
      "Epoch 1218 Batch    2/175   train_loss = 1.195\n",
      "Epoch 1218 Batch   34/175   train_loss = 1.211\n",
      "Epoch 1218 Batch   66/175   train_loss = 1.216\n",
      "Epoch 1218 Batch   98/175   train_loss = 1.203\n",
      "Epoch 1218 Batch  130/175   train_loss = 1.208\n",
      "Epoch 1218 Batch  162/175   train_loss = 1.187\n",
      "Epoch 1219 Batch   19/175   train_loss = 1.192\n",
      "Epoch 1219 Batch   51/175   train_loss = 1.134\n",
      "Epoch 1219 Batch   83/175   train_loss = 1.240\n",
      "Epoch 1219 Batch  115/175   train_loss = 1.298\n",
      "Epoch 1219 Batch  147/175   train_loss = 1.151\n",
      "Epoch 1220 Batch    4/175   train_loss = 1.208\n",
      "Epoch 1220 Batch   36/175   train_loss = 1.145\n",
      "Epoch 1220 Batch   68/175   train_loss = 1.184\n",
      "Epoch 1220 Batch  100/175   train_loss = 1.196\n",
      "Epoch 1220 Batch  132/175   train_loss = 1.157\n",
      "Epoch 1220 Batch  164/175   train_loss = 1.154\n",
      "Epoch 1221 Batch   21/175   train_loss = 1.172\n",
      "Epoch 1221 Batch   53/175   train_loss = 1.163\n",
      "Epoch 1221 Batch   85/175   train_loss = 1.201\n",
      "Epoch 1221 Batch  117/175   train_loss = 1.203\n",
      "Epoch 1221 Batch  149/175   train_loss = 1.196\n",
      "Epoch 1222 Batch    6/175   train_loss = 1.207\n",
      "Epoch 1222 Batch   38/175   train_loss = 1.163\n",
      "Epoch 1222 Batch   70/175   train_loss = 1.194\n",
      "Epoch 1222 Batch  102/175   train_loss = 1.191\n",
      "Epoch 1222 Batch  134/175   train_loss = 1.172\n",
      "Epoch 1222 Batch  166/175   train_loss = 1.211\n",
      "Epoch 1223 Batch   23/175   train_loss = 1.146\n",
      "Epoch 1223 Batch   55/175   train_loss = 1.237\n",
      "Epoch 1223 Batch   87/175   train_loss = 1.257\n",
      "Epoch 1223 Batch  119/175   train_loss = 1.187\n",
      "Epoch 1223 Batch  151/175   train_loss = 1.203\n",
      "Epoch 1224 Batch    8/175   train_loss = 1.252\n",
      "Epoch 1224 Batch   40/175   train_loss = 1.183\n",
      "Epoch 1224 Batch   72/175   train_loss = 1.218\n",
      "Epoch 1224 Batch  104/175   train_loss = 1.221\n",
      "Epoch 1224 Batch  136/175   train_loss = 1.186\n",
      "Epoch 1224 Batch  168/175   train_loss = 1.224\n",
      "Epoch 1225 Batch   25/175   train_loss = 1.217\n",
      "Epoch 1225 Batch   57/175   train_loss = 1.252\n",
      "Epoch 1225 Batch   89/175   train_loss = 1.192\n",
      "Epoch 1225 Batch  121/175   train_loss = 1.189\n",
      "Epoch 1225 Batch  153/175   train_loss = 1.192\n",
      "Epoch 1226 Batch   10/175   train_loss = 1.153\n",
      "Epoch 1226 Batch   42/175   train_loss = 1.241\n",
      "Epoch 1226 Batch   74/175   train_loss = 1.232\n",
      "Epoch 1226 Batch  106/175   train_loss = 1.236\n",
      "Epoch 1226 Batch  138/175   train_loss = 1.184\n",
      "Epoch 1226 Batch  170/175   train_loss = 1.237\n",
      "Epoch 1227 Batch   27/175   train_loss = 1.185\n",
      "Epoch 1227 Batch   59/175   train_loss = 1.177\n",
      "Epoch 1227 Batch   91/175   train_loss = 1.165\n",
      "Epoch 1227 Batch  123/175   train_loss = 1.166\n",
      "Epoch 1227 Batch  155/175   train_loss = 1.176\n",
      "Epoch 1228 Batch   12/175   train_loss = 1.196\n",
      "Epoch 1228 Batch   44/175   train_loss = 1.173\n",
      "Epoch 1228 Batch   76/175   train_loss = 1.169\n",
      "Epoch 1228 Batch  108/175   train_loss = 1.206\n",
      "Epoch 1228 Batch  140/175   train_loss = 1.210\n",
      "Epoch 1228 Batch  172/175   train_loss = 1.216\n",
      "Epoch 1229 Batch   29/175   train_loss = 1.186\n",
      "Epoch 1229 Batch   61/175   train_loss = 1.204\n",
      "Epoch 1229 Batch   93/175   train_loss = 1.169\n",
      "Epoch 1229 Batch  125/175   train_loss = 1.202\n",
      "Epoch 1229 Batch  157/175   train_loss = 1.176\n",
      "Epoch 1230 Batch   14/175   train_loss = 1.212\n",
      "Epoch 1230 Batch   46/175   train_loss = 1.197\n",
      "Epoch 1230 Batch   78/175   train_loss = 1.169\n",
      "Epoch 1230 Batch  110/175   train_loss = 1.270\n",
      "Epoch 1230 Batch  142/175   train_loss = 1.211\n",
      "Epoch 1230 Batch  174/175   train_loss = 1.180\n",
      "Epoch 1231 Batch   31/175   train_loss = 1.215\n",
      "Epoch 1231 Batch   63/175   train_loss = 1.204\n",
      "Epoch 1231 Batch   95/175   train_loss = 1.176\n",
      "Epoch 1231 Batch  127/175   train_loss = 1.159\n",
      "Epoch 1231 Batch  159/175   train_loss = 1.155\n",
      "Epoch 1232 Batch   16/175   train_loss = 1.165\n",
      "Epoch 1232 Batch   48/175   train_loss = 1.188\n",
      "Epoch 1232 Batch   80/175   train_loss = 1.201\n",
      "Epoch 1232 Batch  112/175   train_loss = 1.188\n",
      "Epoch 1232 Batch  144/175   train_loss = 1.135\n",
      "Epoch 1233 Batch    1/175   train_loss = 1.225\n",
      "Epoch 1233 Batch   33/175   train_loss = 1.236\n",
      "Epoch 1233 Batch   65/175   train_loss = 1.181\n",
      "Epoch 1233 Batch   97/175   train_loss = 1.188\n",
      "Epoch 1233 Batch  129/175   train_loss = 1.185\n",
      "Epoch 1233 Batch  161/175   train_loss = 1.186\n",
      "Epoch 1234 Batch   18/175   train_loss = 1.149\n",
      "Epoch 1234 Batch   50/175   train_loss = 1.156\n",
      "Epoch 1234 Batch   82/175   train_loss = 1.225\n",
      "Epoch 1234 Batch  114/175   train_loss = 1.241\n",
      "Epoch 1234 Batch  146/175   train_loss = 1.181\n",
      "Epoch 1235 Batch    3/175   train_loss = 1.234\n",
      "Epoch 1235 Batch   35/175   train_loss = 1.175\n",
      "Epoch 1235 Batch   67/175   train_loss = 1.165\n",
      "Epoch 1235 Batch   99/175   train_loss = 1.237\n",
      "Epoch 1235 Batch  131/175   train_loss = 1.183\n",
      "Epoch 1235 Batch  163/175   train_loss = 1.209\n",
      "Epoch 1236 Batch   20/175   train_loss = 1.186\n",
      "Epoch 1236 Batch   52/175   train_loss = 1.127\n",
      "Epoch 1236 Batch   84/175   train_loss = 1.192\n",
      "Epoch 1236 Batch  116/175   train_loss = 1.233\n",
      "Epoch 1236 Batch  148/175   train_loss = 1.195\n",
      "Epoch 1237 Batch    5/175   train_loss = 1.182\n",
      "Epoch 1237 Batch   37/175   train_loss = 1.169\n",
      "Epoch 1237 Batch   69/175   train_loss = 1.215\n",
      "Epoch 1237 Batch  101/175   train_loss = 1.249\n",
      "Epoch 1237 Batch  133/175   train_loss = 1.095\n",
      "Epoch 1237 Batch  165/175   train_loss = 1.200\n",
      "Epoch 1238 Batch   22/175   train_loss = 1.144\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1238 Batch   54/175   train_loss = 1.241\n",
      "Epoch 1238 Batch   86/175   train_loss = 1.251\n",
      "Epoch 1238 Batch  118/175   train_loss = 1.252\n",
      "Epoch 1238 Batch  150/175   train_loss = 1.206\n",
      "Epoch 1239 Batch    7/175   train_loss = 1.221\n",
      "Epoch 1239 Batch   39/175   train_loss = 1.151\n",
      "Epoch 1239 Batch   71/175   train_loss = 1.224\n",
      "Epoch 1239 Batch  103/175   train_loss = 1.220\n",
      "Epoch 1239 Batch  135/175   train_loss = 1.175\n",
      "Epoch 1239 Batch  167/175   train_loss = 1.248\n",
      "Epoch 1240 Batch   24/175   train_loss = 1.144\n",
      "Epoch 1240 Batch   56/175   train_loss = 1.216\n",
      "Epoch 1240 Batch   88/175   train_loss = 1.253\n",
      "Epoch 1240 Batch  120/175   train_loss = 1.184\n",
      "Epoch 1240 Batch  152/175   train_loss = 1.226\n",
      "Epoch 1241 Batch    9/175   train_loss = 1.282\n",
      "Epoch 1241 Batch   41/175   train_loss = 1.229\n",
      "Epoch 1241 Batch   73/175   train_loss = 1.223\n",
      "Epoch 1241 Batch  105/175   train_loss = 1.231\n",
      "Epoch 1241 Batch  137/175   train_loss = 1.180\n",
      "Epoch 1241 Batch  169/175   train_loss = 1.228\n",
      "Epoch 1242 Batch   26/175   train_loss = 1.211\n",
      "Epoch 1242 Batch   58/175   train_loss = 1.237\n",
      "Epoch 1242 Batch   90/175   train_loss = 1.221\n",
      "Epoch 1242 Batch  122/175   train_loss = 1.188\n",
      "Epoch 1242 Batch  154/175   train_loss = 1.194\n",
      "Epoch 1243 Batch   11/175   train_loss = 1.184\n",
      "Epoch 1243 Batch   43/175   train_loss = 1.197\n",
      "Epoch 1243 Batch   75/175   train_loss = 1.179\n",
      "Epoch 1243 Batch  107/175   train_loss = 1.257\n",
      "Epoch 1243 Batch  139/175   train_loss = 1.146\n",
      "Epoch 1243 Batch  171/175   train_loss = 1.236\n",
      "Epoch 1244 Batch   28/175   train_loss = 1.189\n",
      "Epoch 1244 Batch   60/175   train_loss = 1.189\n",
      "Epoch 1244 Batch   92/175   train_loss = 1.158\n",
      "Epoch 1244 Batch  124/175   train_loss = 1.188\n",
      "Epoch 1244 Batch  156/175   train_loss = 1.226\n",
      "Epoch 1245 Batch   13/175   train_loss = 1.194\n",
      "Epoch 1245 Batch   45/175   train_loss = 1.192\n",
      "Epoch 1245 Batch   77/175   train_loss = 1.182\n",
      "Epoch 1245 Batch  109/175   train_loss = 1.246\n",
      "Epoch 1245 Batch  141/175   train_loss = 1.175\n",
      "Epoch 1245 Batch  173/175   train_loss = 1.143\n",
      "Epoch 1246 Batch   30/175   train_loss = 1.236\n",
      "Epoch 1246 Batch   62/175   train_loss = 1.201\n",
      "Epoch 1246 Batch   94/175   train_loss = 1.172\n",
      "Epoch 1246 Batch  126/175   train_loss = 1.190\n",
      "Epoch 1246 Batch  158/175   train_loss = 1.168\n",
      "Epoch 1247 Batch   15/175   train_loss = 1.223\n",
      "Epoch 1247 Batch   47/175   train_loss = 1.200\n",
      "Epoch 1247 Batch   79/175   train_loss = 1.232\n",
      "Epoch 1247 Batch  111/175   train_loss = 1.233\n",
      "Epoch 1247 Batch  143/175   train_loss = 1.154\n",
      "Epoch 1248 Batch    0/175   train_loss = 1.187\n",
      "Epoch 1248 Batch   32/175   train_loss = 1.198\n",
      "Epoch 1248 Batch   64/175   train_loss = 1.236\n",
      "Epoch 1248 Batch   96/175   train_loss = 1.204\n",
      "Epoch 1248 Batch  128/175   train_loss = 1.139\n",
      "Epoch 1248 Batch  160/175   train_loss = 1.196\n",
      "Epoch 1249 Batch   17/175   train_loss = 1.170\n",
      "Epoch 1249 Batch   49/175   train_loss = 1.222\n",
      "Epoch 1249 Batch   81/175   train_loss = 1.178\n",
      "Epoch 1249 Batch  113/175   train_loss = 1.211\n",
      "Epoch 1249 Batch  145/175   train_loss = 1.163\n",
      "Epoch 1250 Batch    2/175   train_loss = 1.184\n",
      "Epoch 1250 Batch   34/175   train_loss = 1.240\n",
      "Epoch 1250 Batch   66/175   train_loss = 1.232\n",
      "Epoch 1250 Batch   98/175   train_loss = 1.213\n",
      "Epoch 1250 Batch  130/175   train_loss = 1.227\n",
      "Epoch 1250 Batch  162/175   train_loss = 1.177\n",
      "Epoch 1251 Batch   19/175   train_loss = 1.193\n",
      "Epoch 1251 Batch   51/175   train_loss = 1.143\n",
      "Epoch 1251 Batch   83/175   train_loss = 1.238\n",
      "Epoch 1251 Batch  115/175   train_loss = 1.284\n",
      "Epoch 1251 Batch  147/175   train_loss = 1.154\n",
      "Epoch 1252 Batch    4/175   train_loss = 1.206\n",
      "Epoch 1252 Batch   36/175   train_loss = 1.151\n",
      "Epoch 1252 Batch   68/175   train_loss = 1.163\n",
      "Epoch 1252 Batch  100/175   train_loss = 1.195\n",
      "Epoch 1252 Batch  132/175   train_loss = 1.174\n",
      "Epoch 1252 Batch  164/175   train_loss = 1.162\n",
      "Epoch 1253 Batch   21/175   train_loss = 1.150\n",
      "Epoch 1253 Batch   53/175   train_loss = 1.159\n",
      "Epoch 1253 Batch   85/175   train_loss = 1.196\n",
      "Epoch 1253 Batch  117/175   train_loss = 1.204\n",
      "Epoch 1253 Batch  149/175   train_loss = 1.191\n",
      "Epoch 1254 Batch    6/175   train_loss = 1.224\n",
      "Epoch 1254 Batch   38/175   train_loss = 1.170\n",
      "Epoch 1254 Batch   70/175   train_loss = 1.208\n",
      "Epoch 1254 Batch  102/175   train_loss = 1.210\n",
      "Epoch 1254 Batch  134/175   train_loss = 1.171\n",
      "Epoch 1254 Batch  166/175   train_loss = 1.200\n",
      "Epoch 1255 Batch   23/175   train_loss = 1.148\n",
      "Epoch 1255 Batch   55/175   train_loss = 1.223\n",
      "Epoch 1255 Batch   87/175   train_loss = 1.254\n",
      "Epoch 1255 Batch  119/175   train_loss = 1.172\n",
      "Epoch 1255 Batch  151/175   train_loss = 1.209\n",
      "Epoch 1256 Batch    8/175   train_loss = 1.215\n",
      "Epoch 1256 Batch   40/175   train_loss = 1.174\n",
      "Epoch 1256 Batch   72/175   train_loss = 1.201\n",
      "Epoch 1256 Batch  104/175   train_loss = 1.215\n",
      "Epoch 1256 Batch  136/175   train_loss = 1.180\n",
      "Epoch 1256 Batch  168/175   train_loss = 1.209\n",
      "Epoch 1257 Batch   25/175   train_loss = 1.196\n",
      "Epoch 1257 Batch   57/175   train_loss = 1.228\n",
      "Epoch 1257 Batch   89/175   train_loss = 1.189\n",
      "Epoch 1257 Batch  121/175   train_loss = 1.189\n",
      "Epoch 1257 Batch  153/175   train_loss = 1.174\n",
      "Epoch 1258 Batch   10/175   train_loss = 1.145\n",
      "Epoch 1258 Batch   42/175   train_loss = 1.225\n",
      "Epoch 1258 Batch   74/175   train_loss = 1.218\n",
      "Epoch 1258 Batch  106/175   train_loss = 1.237\n",
      "Epoch 1258 Batch  138/175   train_loss = 1.186\n",
      "Epoch 1258 Batch  170/175   train_loss = 1.215\n",
      "Epoch 1259 Batch   27/175   train_loss = 1.172\n",
      "Epoch 1259 Batch   59/175   train_loss = 1.183\n",
      "Epoch 1259 Batch   91/175   train_loss = 1.182\n",
      "Epoch 1259 Batch  123/175   train_loss = 1.187\n",
      "Epoch 1259 Batch  155/175   train_loss = 1.177\n",
      "Epoch 1260 Batch   12/175   train_loss = 1.181\n",
      "Epoch 1260 Batch   44/175   train_loss = 1.182\n",
      "Epoch 1260 Batch   76/175   train_loss = 1.158\n",
      "Epoch 1260 Batch  108/175   train_loss = 1.202\n",
      "Epoch 1260 Batch  140/175   train_loss = 1.197\n",
      "Epoch 1260 Batch  172/175   train_loss = 1.223\n",
      "Epoch 1261 Batch   29/175   train_loss = 1.190\n",
      "Epoch 1261 Batch   61/175   train_loss = 1.239\n",
      "Epoch 1261 Batch   93/175   train_loss = 1.191\n",
      "Epoch 1261 Batch  125/175   train_loss = 1.211\n",
      "Epoch 1261 Batch  157/175   train_loss = 1.175\n",
      "Epoch 1262 Batch   14/175   train_loss = 1.214\n",
      "Epoch 1262 Batch   46/175   train_loss = 1.198\n",
      "Epoch 1262 Batch   78/175   train_loss = 1.175\n",
      "Epoch 1262 Batch  110/175   train_loss = 1.268\n",
      "Epoch 1262 Batch  142/175   train_loss = 1.199\n",
      "Epoch 1262 Batch  174/175   train_loss = 1.183\n",
      "Epoch 1263 Batch   31/175   train_loss = 1.207\n",
      "Epoch 1263 Batch   63/175   train_loss = 1.200\n",
      "Epoch 1263 Batch   95/175   train_loss = 1.157\n",
      "Epoch 1263 Batch  127/175   train_loss = 1.167\n",
      "Epoch 1263 Batch  159/175   train_loss = 1.147\n",
      "Epoch 1264 Batch   16/175   train_loss = 1.169\n",
      "Epoch 1264 Batch   48/175   train_loss = 1.185\n",
      "Epoch 1264 Batch   80/175   train_loss = 1.214\n",
      "Epoch 1264 Batch  112/175   train_loss = 1.184\n",
      "Epoch 1264 Batch  144/175   train_loss = 1.133\n",
      "Epoch 1265 Batch    1/175   train_loss = 1.231\n",
      "Epoch 1265 Batch   33/175   train_loss = 1.237\n",
      "Epoch 1265 Batch   65/175   train_loss = 1.175\n",
      "Epoch 1265 Batch   97/175   train_loss = 1.189\n",
      "Epoch 1265 Batch  129/175   train_loss = 1.187\n",
      "Epoch 1265 Batch  161/175   train_loss = 1.183\n",
      "Epoch 1266 Batch   18/175   train_loss = 1.148\n",
      "Epoch 1266 Batch   50/175   train_loss = 1.174\n",
      "Epoch 1266 Batch   82/175   train_loss = 1.215\n",
      "Epoch 1266 Batch  114/175   train_loss = 1.238\n",
      "Epoch 1266 Batch  146/175   train_loss = 1.199\n",
      "Epoch 1267 Batch    3/175   train_loss = 1.219\n",
      "Epoch 1267 Batch   35/175   train_loss = 1.189\n",
      "Epoch 1267 Batch   67/175   train_loss = 1.161\n",
      "Epoch 1267 Batch   99/175   train_loss = 1.229\n",
      "Epoch 1267 Batch  131/175   train_loss = 1.188\n",
      "Epoch 1267 Batch  163/175   train_loss = 1.180\n",
      "Epoch 1268 Batch   20/175   train_loss = 1.161\n",
      "Epoch 1268 Batch   52/175   train_loss = 1.129\n",
      "Epoch 1268 Batch   84/175   train_loss = 1.184\n",
      "Epoch 1268 Batch  116/175   train_loss = 1.237\n",
      "Epoch 1268 Batch  148/175   train_loss = 1.192\n",
      "Epoch 1269 Batch    5/175   train_loss = 1.180\n",
      "Epoch 1269 Batch   37/175   train_loss = 1.165\n",
      "Epoch 1269 Batch   69/175   train_loss = 1.186\n",
      "Epoch 1269 Batch  101/175   train_loss = 1.230\n",
      "Epoch 1269 Batch  133/175   train_loss = 1.093\n",
      "Epoch 1269 Batch  165/175   train_loss = 1.195\n",
      "Epoch 1270 Batch   22/175   train_loss = 1.121\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1270 Batch   54/175   train_loss = 1.217\n",
      "Epoch 1270 Batch   86/175   train_loss = 1.241\n",
      "Epoch 1270 Batch  118/175   train_loss = 1.232\n",
      "Epoch 1270 Batch  150/175   train_loss = 1.188\n",
      "Epoch 1271 Batch    7/175   train_loss = 1.213\n",
      "Epoch 1271 Batch   39/175   train_loss = 1.146\n",
      "Epoch 1271 Batch   71/175   train_loss = 1.233\n",
      "Epoch 1271 Batch  103/175   train_loss = 1.182\n",
      "Epoch 1271 Batch  135/175   train_loss = 1.163\n",
      "Epoch 1271 Batch  167/175   train_loss = 1.228\n",
      "Epoch 1272 Batch   24/175   train_loss = 1.144\n",
      "Epoch 1272 Batch   56/175   train_loss = 1.198\n",
      "Epoch 1272 Batch   88/175   train_loss = 1.237\n",
      "Epoch 1272 Batch  120/175   train_loss = 1.179\n",
      "Epoch 1272 Batch  152/175   train_loss = 1.177\n",
      "Epoch 1273 Batch    9/175   train_loss = 1.197\n",
      "Epoch 1273 Batch   41/175   train_loss = 1.211\n",
      "Epoch 1273 Batch   73/175   train_loss = 1.188\n",
      "Epoch 1273 Batch  105/175   train_loss = 1.231\n",
      "Epoch 1273 Batch  137/175   train_loss = 1.144\n",
      "Epoch 1273 Batch  169/175   train_loss = 1.202\n",
      "Epoch 1274 Batch   26/175   train_loss = 1.189\n",
      "Epoch 1274 Batch   58/175   train_loss = 1.225\n",
      "Epoch 1274 Batch   90/175   train_loss = 1.195\n",
      "Epoch 1274 Batch  122/175   train_loss = 1.179\n",
      "Epoch 1274 Batch  154/175   train_loss = 1.169\n",
      "Epoch 1275 Batch   11/175   train_loss = 1.159\n",
      "Epoch 1275 Batch   43/175   train_loss = 1.197\n",
      "Epoch 1275 Batch   75/175   train_loss = 1.147\n",
      "Epoch 1275 Batch  107/175   train_loss = 1.240\n",
      "Epoch 1275 Batch  139/175   train_loss = 1.151\n",
      "Epoch 1275 Batch  171/175   train_loss = 1.241\n",
      "Epoch 1276 Batch   28/175   train_loss = 1.189\n",
      "Epoch 1276 Batch   60/175   train_loss = 1.206\n",
      "Epoch 1276 Batch   92/175   train_loss = 1.157\n",
      "Epoch 1276 Batch  124/175   train_loss = 1.185\n",
      "Epoch 1276 Batch  156/175   train_loss = 1.229\n",
      "Epoch 1277 Batch   13/175   train_loss = 1.202\n",
      "Epoch 1277 Batch   45/175   train_loss = 1.187\n",
      "Epoch 1277 Batch   77/175   train_loss = 1.178\n",
      "Epoch 1277 Batch  109/175   train_loss = 1.234\n",
      "Epoch 1277 Batch  141/175   train_loss = 1.159\n",
      "Epoch 1277 Batch  173/175   train_loss = 1.158\n",
      "Epoch 1278 Batch   30/175   train_loss = 1.247\n",
      "Epoch 1278 Batch   62/175   train_loss = 1.216\n",
      "Epoch 1278 Batch   94/175   train_loss = 1.191\n",
      "Epoch 1278 Batch  126/175   train_loss = 1.198\n",
      "Epoch 1278 Batch  158/175   train_loss = 1.185\n",
      "Epoch 1279 Batch   15/175   train_loss = 1.252\n",
      "Epoch 1279 Batch   47/175   train_loss = 1.223\n",
      "Epoch 1279 Batch   79/175   train_loss = 1.248\n",
      "Epoch 1279 Batch  111/175   train_loss = 1.256\n",
      "Epoch 1279 Batch  143/175   train_loss = 1.173\n",
      "Epoch 1280 Batch    0/175   train_loss = 1.209\n",
      "Epoch 1280 Batch   32/175   train_loss = 1.200\n",
      "Epoch 1280 Batch   64/175   train_loss = 1.225\n",
      "Epoch 1280 Batch   96/175   train_loss = 1.223\n",
      "Epoch 1280 Batch  128/175   train_loss = 1.145\n",
      "Epoch 1280 Batch  160/175   train_loss = 1.193\n",
      "Epoch 1281 Batch   17/175   train_loss = 1.178\n",
      "Epoch 1281 Batch   49/175   train_loss = 1.207\n",
      "Epoch 1281 Batch   81/175   train_loss = 1.163\n",
      "Epoch 1281 Batch  113/175   train_loss = 1.200\n",
      "Epoch 1281 Batch  145/175   train_loss = 1.159\n",
      "Epoch 1282 Batch    2/175   train_loss = 1.184\n",
      "Epoch 1282 Batch   34/175   train_loss = 1.200\n",
      "Epoch 1282 Batch   66/175   train_loss = 1.227\n",
      "Epoch 1282 Batch   98/175   train_loss = 1.216\n",
      "Epoch 1282 Batch  130/175   train_loss = 1.215\n",
      "Epoch 1282 Batch  162/175   train_loss = 1.186\n",
      "Epoch 1283 Batch   19/175   train_loss = 1.195\n",
      "Epoch 1283 Batch   51/175   train_loss = 1.143\n",
      "Epoch 1283 Batch   83/175   train_loss = 1.236\n",
      "Epoch 1283 Batch  115/175   train_loss = 1.314\n",
      "Epoch 1283 Batch  147/175   train_loss = 1.146\n",
      "Epoch 1284 Batch    4/175   train_loss = 1.206\n",
      "Epoch 1284 Batch   36/175   train_loss = 1.144\n",
      "Epoch 1284 Batch   68/175   train_loss = 1.182\n",
      "Epoch 1284 Batch  100/175   train_loss = 1.196\n",
      "Epoch 1284 Batch  132/175   train_loss = 1.179\n",
      "Epoch 1284 Batch  164/175   train_loss = 1.169\n",
      "Epoch 1285 Batch   21/175   train_loss = 1.137\n",
      "Epoch 1285 Batch   53/175   train_loss = 1.155\n",
      "Epoch 1285 Batch   85/175   train_loss = 1.207\n",
      "Epoch 1285 Batch  117/175   train_loss = 1.202\n",
      "Epoch 1285 Batch  149/175   train_loss = 1.211\n",
      "Epoch 1286 Batch    6/175   train_loss = 1.234\n",
      "Epoch 1286 Batch   38/175   train_loss = 1.165\n",
      "Epoch 1286 Batch   70/175   train_loss = 1.193\n",
      "Epoch 1286 Batch  102/175   train_loss = 1.188\n",
      "Epoch 1286 Batch  134/175   train_loss = 1.152\n",
      "Epoch 1286 Batch  166/175   train_loss = 1.182\n",
      "Epoch 1287 Batch   23/175   train_loss = 1.128\n",
      "Epoch 1287 Batch   55/175   train_loss = 1.231\n",
      "Epoch 1287 Batch   87/175   train_loss = 1.254\n",
      "Epoch 1287 Batch  119/175   train_loss = 1.174\n",
      "Epoch 1287 Batch  151/175   train_loss = 1.196\n",
      "Epoch 1288 Batch    8/175   train_loss = 1.195\n",
      "Epoch 1288 Batch   40/175   train_loss = 1.181\n",
      "Epoch 1288 Batch   72/175   train_loss = 1.227\n",
      "Epoch 1288 Batch  104/175   train_loss = 1.208\n",
      "Epoch 1288 Batch  136/175   train_loss = 1.187\n",
      "Epoch 1288 Batch  168/175   train_loss = 1.200\n",
      "Epoch 1289 Batch   25/175   train_loss = 1.207\n",
      "Epoch 1289 Batch   57/175   train_loss = 1.222\n",
      "Epoch 1289 Batch   89/175   train_loss = 1.182\n",
      "Epoch 1289 Batch  121/175   train_loss = 1.181\n",
      "Epoch 1289 Batch  153/175   train_loss = 1.190\n",
      "Epoch 1290 Batch   10/175   train_loss = 1.153\n",
      "Epoch 1290 Batch   42/175   train_loss = 1.223\n",
      "Epoch 1290 Batch   74/175   train_loss = 1.227\n",
      "Epoch 1290 Batch  106/175   train_loss = 1.244\n",
      "Epoch 1290 Batch  138/175   train_loss = 1.184\n",
      "Epoch 1290 Batch  170/175   train_loss = 1.216\n",
      "Epoch 1291 Batch   27/175   train_loss = 1.165\n",
      "Epoch 1291 Batch   59/175   train_loss = 1.165\n",
      "Epoch 1291 Batch   91/175   train_loss = 1.159\n",
      "Epoch 1291 Batch  123/175   train_loss = 1.171\n",
      "Epoch 1291 Batch  155/175   train_loss = 1.145\n",
      "Epoch 1292 Batch   12/175   train_loss = 1.175\n",
      "Epoch 1292 Batch   44/175   train_loss = 1.176\n",
      "Epoch 1292 Batch   76/175   train_loss = 1.176\n",
      "Epoch 1292 Batch  108/175   train_loss = 1.195\n",
      "Epoch 1292 Batch  140/175   train_loss = 1.163\n",
      "Epoch 1292 Batch  172/175   train_loss = 1.222\n",
      "Epoch 1293 Batch   29/175   train_loss = 1.178\n",
      "Epoch 1293 Batch   61/175   train_loss = 1.206\n",
      "Epoch 1293 Batch   93/175   train_loss = 1.205\n",
      "Epoch 1293 Batch  125/175   train_loss = 1.200\n",
      "Epoch 1293 Batch  157/175   train_loss = 1.165\n",
      "Epoch 1294 Batch   14/175   train_loss = 1.206\n",
      "Epoch 1294 Batch   46/175   train_loss = 1.190\n",
      "Epoch 1294 Batch   78/175   train_loss = 1.163\n",
      "Epoch 1294 Batch  110/175   train_loss = 1.251\n",
      "Epoch 1294 Batch  142/175   train_loss = 1.194\n",
      "Epoch 1294 Batch  174/175   train_loss = 1.183\n",
      "Epoch 1295 Batch   31/175   train_loss = 1.221\n",
      "Epoch 1295 Batch   63/175   train_loss = 1.191\n",
      "Epoch 1295 Batch   95/175   train_loss = 1.148\n",
      "Epoch 1295 Batch  127/175   train_loss = 1.160\n",
      "Epoch 1295 Batch  159/175   train_loss = 1.161\n",
      "Epoch 1296 Batch   16/175   train_loss = 1.184\n",
      "Epoch 1296 Batch   48/175   train_loss = 1.196\n",
      "Epoch 1296 Batch   80/175   train_loss = 1.196\n",
      "Epoch 1296 Batch  112/175   train_loss = 1.204\n",
      "Epoch 1296 Batch  144/175   train_loss = 1.124\n",
      "Epoch 1297 Batch    1/175   train_loss = 1.223\n",
      "Epoch 1297 Batch   33/175   train_loss = 1.227\n",
      "Epoch 1297 Batch   65/175   train_loss = 1.188\n",
      "Epoch 1297 Batch   97/175   train_loss = 1.205\n",
      "Epoch 1297 Batch  129/175   train_loss = 1.183\n",
      "Epoch 1297 Batch  161/175   train_loss = 1.171\n",
      "Epoch 1298 Batch   18/175   train_loss = 1.152\n",
      "Epoch 1298 Batch   50/175   train_loss = 1.171\n",
      "Epoch 1298 Batch   82/175   train_loss = 1.225\n",
      "Epoch 1298 Batch  114/175   train_loss = 1.244\n",
      "Epoch 1298 Batch  146/175   train_loss = 1.196\n",
      "Epoch 1299 Batch    3/175   train_loss = 1.217\n",
      "Epoch 1299 Batch   35/175   train_loss = 1.189\n",
      "Epoch 1299 Batch   67/175   train_loss = 1.162\n",
      "Epoch 1299 Batch   99/175   train_loss = 1.237\n",
      "Epoch 1299 Batch  131/175   train_loss = 1.175\n",
      "Epoch 1299 Batch  163/175   train_loss = 1.197\n",
      "Epoch 1300 Batch   20/175   train_loss = 1.180\n",
      "Epoch 1300 Batch   52/175   train_loss = 1.130\n",
      "Epoch 1300 Batch   84/175   train_loss = 1.191\n",
      "Epoch 1300 Batch  116/175   train_loss = 1.222\n",
      "Epoch 1300 Batch  148/175   train_loss = 1.198\n",
      "Epoch 1301 Batch    5/175   train_loss = 1.186\n",
      "Epoch 1301 Batch   37/175   train_loss = 1.173\n",
      "Epoch 1301 Batch   69/175   train_loss = 1.191\n",
      "Epoch 1301 Batch  101/175   train_loss = 1.238\n",
      "Epoch 1301 Batch  133/175   train_loss = 1.097\n",
      "Epoch 1301 Batch  165/175   train_loss = 1.190\n",
      "Epoch 1302 Batch   22/175   train_loss = 1.110\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1302 Batch   54/175   train_loss = 1.230\n",
      "Epoch 1302 Batch   86/175   train_loss = 1.240\n",
      "Epoch 1302 Batch  118/175   train_loss = 1.232\n",
      "Epoch 1302 Batch  150/175   train_loss = 1.177\n",
      "Epoch 1303 Batch    7/175   train_loss = 1.212\n",
      "Epoch 1303 Batch   39/175   train_loss = 1.135\n",
      "Epoch 1303 Batch   71/175   train_loss = 1.231\n",
      "Epoch 1303 Batch  103/175   train_loss = 1.193\n",
      "Epoch 1303 Batch  135/175   train_loss = 1.152\n",
      "Epoch 1303 Batch  167/175   train_loss = 1.243\n",
      "Epoch 1304 Batch   24/175   train_loss = 1.133\n",
      "Epoch 1304 Batch   56/175   train_loss = 1.188\n",
      "Epoch 1304 Batch   88/175   train_loss = 1.226\n",
      "Epoch 1304 Batch  120/175   train_loss = 1.178\n",
      "Epoch 1304 Batch  152/175   train_loss = 1.161\n",
      "Epoch 1305 Batch    9/175   train_loss = 1.216\n",
      "Epoch 1305 Batch   41/175   train_loss = 1.197\n",
      "Epoch 1305 Batch   73/175   train_loss = 1.200\n",
      "Epoch 1305 Batch  105/175   train_loss = 1.231\n",
      "Epoch 1305 Batch  137/175   train_loss = 1.139\n",
      "Epoch 1305 Batch  169/175   train_loss = 1.213\n",
      "Epoch 1306 Batch   26/175   train_loss = 1.191\n",
      "Epoch 1306 Batch   58/175   train_loss = 1.225\n",
      "Epoch 1306 Batch   90/175   train_loss = 1.195\n",
      "Epoch 1306 Batch  122/175   train_loss = 1.184\n",
      "Epoch 1306 Batch  154/175   train_loss = 1.159\n",
      "Epoch 1307 Batch   11/175   train_loss = 1.160\n",
      "Epoch 1307 Batch   43/175   train_loss = 1.194\n",
      "Epoch 1307 Batch   75/175   train_loss = 1.169\n",
      "Epoch 1307 Batch  107/175   train_loss = 1.237\n",
      "Epoch 1307 Batch  139/175   train_loss = 1.142\n",
      "Epoch 1307 Batch  171/175   train_loss = 1.240\n",
      "Epoch 1308 Batch   28/175   train_loss = 1.184\n",
      "Epoch 1308 Batch   60/175   train_loss = 1.175\n",
      "Epoch 1308 Batch   92/175   train_loss = 1.186\n",
      "Epoch 1308 Batch  124/175   train_loss = 1.196\n",
      "Epoch 1308 Batch  156/175   train_loss = 1.224\n",
      "Epoch 1309 Batch   13/175   train_loss = 1.178\n",
      "Epoch 1309 Batch   45/175   train_loss = 1.184\n",
      "Epoch 1309 Batch   77/175   train_loss = 1.183\n",
      "Epoch 1309 Batch  109/175   train_loss = 1.238\n",
      "Epoch 1309 Batch  141/175   train_loss = 1.137\n",
      "Epoch 1309 Batch  173/175   train_loss = 1.154\n",
      "Epoch 1310 Batch   30/175   train_loss = 1.229\n",
      "Epoch 1310 Batch   62/175   train_loss = 1.208\n",
      "Epoch 1310 Batch   94/175   train_loss = 1.176\n",
      "Epoch 1310 Batch  126/175   train_loss = 1.197\n",
      "Epoch 1310 Batch  158/175   train_loss = 1.179\n",
      "Epoch 1311 Batch   15/175   train_loss = 1.228\n",
      "Epoch 1311 Batch   47/175   train_loss = 1.205\n",
      "Epoch 1311 Batch   79/175   train_loss = 1.223\n",
      "Epoch 1311 Batch  111/175   train_loss = 1.228\n",
      "Epoch 1311 Batch  143/175   train_loss = 1.163\n",
      "Epoch 1312 Batch    0/175   train_loss = 1.202\n",
      "Epoch 1312 Batch   32/175   train_loss = 1.219\n",
      "Epoch 1312 Batch   64/175   train_loss = 1.229\n",
      "Epoch 1312 Batch   96/175   train_loss = 1.217\n",
      "Epoch 1312 Batch  128/175   train_loss = 1.129\n",
      "Epoch 1312 Batch  160/175   train_loss = 1.174\n",
      "Epoch 1313 Batch   17/175   train_loss = 1.149\n",
      "Epoch 1313 Batch   49/175   train_loss = 1.214\n",
      "Epoch 1313 Batch   81/175   train_loss = 1.171\n",
      "Epoch 1313 Batch  113/175   train_loss = 1.227\n",
      "Epoch 1313 Batch  145/175   train_loss = 1.139\n",
      "Epoch 1314 Batch    2/175   train_loss = 1.183\n",
      "Epoch 1314 Batch   34/175   train_loss = 1.208\n",
      "Epoch 1314 Batch   66/175   train_loss = 1.226\n",
      "Epoch 1314 Batch   98/175   train_loss = 1.210\n",
      "Epoch 1314 Batch  130/175   train_loss = 1.209\n",
      "Epoch 1314 Batch  162/175   train_loss = 1.183\n",
      "Epoch 1315 Batch   19/175   train_loss = 1.182\n",
      "Epoch 1315 Batch   51/175   train_loss = 1.122\n",
      "Epoch 1315 Batch   83/175   train_loss = 1.236\n",
      "Epoch 1315 Batch  115/175   train_loss = 1.295\n",
      "Epoch 1315 Batch  147/175   train_loss = 1.164\n",
      "Epoch 1316 Batch    4/175   train_loss = 1.195\n",
      "Epoch 1316 Batch   36/175   train_loss = 1.152\n",
      "Epoch 1316 Batch   68/175   train_loss = 1.168\n",
      "Epoch 1316 Batch  100/175   train_loss = 1.187\n",
      "Epoch 1316 Batch  132/175   train_loss = 1.157\n",
      "Epoch 1316 Batch  164/175   train_loss = 1.143\n",
      "Epoch 1317 Batch   21/175   train_loss = 1.159\n",
      "Epoch 1317 Batch   53/175   train_loss = 1.151\n",
      "Epoch 1317 Batch   85/175   train_loss = 1.204\n",
      "Epoch 1317 Batch  117/175   train_loss = 1.204\n",
      "Epoch 1317 Batch  149/175   train_loss = 1.196\n",
      "Epoch 1318 Batch    6/175   train_loss = 1.204\n",
      "Epoch 1318 Batch   38/175   train_loss = 1.138\n",
      "Epoch 1318 Batch   70/175   train_loss = 1.183\n",
      "Epoch 1318 Batch  102/175   train_loss = 1.194\n",
      "Epoch 1318 Batch  134/175   train_loss = 1.144\n",
      "Epoch 1318 Batch  166/175   train_loss = 1.191\n",
      "Epoch 1319 Batch   23/175   train_loss = 1.141\n",
      "Epoch 1319 Batch   55/175   train_loss = 1.224\n",
      "Epoch 1319 Batch   87/175   train_loss = 1.245\n",
      "Epoch 1319 Batch  119/175   train_loss = 1.178\n",
      "Epoch 1319 Batch  151/175   train_loss = 1.190\n",
      "Epoch 1320 Batch    8/175   train_loss = 1.184\n",
      "Epoch 1320 Batch   40/175   train_loss = 1.175\n",
      "Epoch 1320 Batch   72/175   train_loss = 1.220\n",
      "Epoch 1320 Batch  104/175   train_loss = 1.221\n",
      "Epoch 1320 Batch  136/175   train_loss = 1.156\n",
      "Epoch 1320 Batch  168/175   train_loss = 1.201\n",
      "Epoch 1321 Batch   25/175   train_loss = 1.192\n",
      "Epoch 1321 Batch   57/175   train_loss = 1.214\n",
      "Epoch 1321 Batch   89/175   train_loss = 1.207\n",
      "Epoch 1321 Batch  121/175   train_loss = 1.177\n",
      "Epoch 1321 Batch  153/175   train_loss = 1.181\n",
      "Epoch 1322 Batch   10/175   train_loss = 1.134\n",
      "Epoch 1322 Batch   42/175   train_loss = 1.237\n",
      "Epoch 1322 Batch   74/175   train_loss = 1.224\n",
      "Epoch 1322 Batch  106/175   train_loss = 1.224\n",
      "Epoch 1322 Batch  138/175   train_loss = 1.179\n",
      "Epoch 1322 Batch  170/175   train_loss = 1.210\n",
      "Epoch 1323 Batch   27/175   train_loss = 1.178\n",
      "Epoch 1323 Batch   59/175   train_loss = 1.157\n",
      "Epoch 1323 Batch   91/175   train_loss = 1.184\n",
      "Epoch 1323 Batch  123/175   train_loss = 1.198\n",
      "Epoch 1323 Batch  155/175   train_loss = 1.164\n",
      "Epoch 1324 Batch   12/175   train_loss = 1.176\n",
      "Epoch 1324 Batch   44/175   train_loss = 1.187\n",
      "Epoch 1324 Batch   76/175   train_loss = 1.177\n",
      "Epoch 1324 Batch  108/175   train_loss = 1.196\n",
      "Epoch 1324 Batch  140/175   train_loss = 1.173\n",
      "Epoch 1324 Batch  172/175   train_loss = 1.224\n",
      "Epoch 1325 Batch   29/175   train_loss = 1.192\n",
      "Epoch 1325 Batch   61/175   train_loss = 1.220\n",
      "Epoch 1325 Batch   93/175   train_loss = 1.193\n",
      "Epoch 1325 Batch  125/175   train_loss = 1.219\n",
      "Epoch 1325 Batch  157/175   train_loss = 1.188\n",
      "Epoch 1326 Batch   14/175   train_loss = 1.217\n",
      "Epoch 1326 Batch   46/175   train_loss = 1.181\n",
      "Epoch 1326 Batch   78/175   train_loss = 1.158\n",
      "Epoch 1326 Batch  110/175   train_loss = 1.257\n",
      "Epoch 1326 Batch  142/175   train_loss = 1.201\n",
      "Epoch 1326 Batch  174/175   train_loss = 1.158\n",
      "Epoch 1327 Batch   31/175   train_loss = 1.205\n",
      "Epoch 1327 Batch   63/175   train_loss = 1.190\n",
      "Epoch 1327 Batch   95/175   train_loss = 1.146\n",
      "Epoch 1327 Batch  127/175   train_loss = 1.148\n",
      "Epoch 1327 Batch  159/175   train_loss = 1.157\n",
      "Epoch 1328 Batch   16/175   train_loss = 1.165\n",
      "Epoch 1328 Batch   48/175   train_loss = 1.190\n",
      "Epoch 1328 Batch   80/175   train_loss = 1.189\n",
      "Epoch 1328 Batch  112/175   train_loss = 1.218\n",
      "Epoch 1328 Batch  144/175   train_loss = 1.134\n",
      "Epoch 1329 Batch    1/175   train_loss = 1.227\n",
      "Epoch 1329 Batch   33/175   train_loss = 1.246\n",
      "Epoch 1329 Batch   65/175   train_loss = 1.180\n",
      "Epoch 1329 Batch   97/175   train_loss = 1.185\n",
      "Epoch 1329 Batch  129/175   train_loss = 1.177\n",
      "Epoch 1329 Batch  161/175   train_loss = 1.174\n",
      "Epoch 1330 Batch   18/175   train_loss = 1.140\n",
      "Epoch 1330 Batch   50/175   train_loss = 1.157\n",
      "Epoch 1330 Batch   82/175   train_loss = 1.229\n",
      "Epoch 1330 Batch  114/175   train_loss = 1.241\n",
      "Epoch 1330 Batch  146/175   train_loss = 1.192\n",
      "Epoch 1331 Batch    3/175   train_loss = 1.217\n",
      "Epoch 1331 Batch   35/175   train_loss = 1.193\n",
      "Epoch 1331 Batch   67/175   train_loss = 1.155\n",
      "Epoch 1331 Batch   99/175   train_loss = 1.239\n",
      "Epoch 1331 Batch  131/175   train_loss = 1.169\n",
      "Epoch 1331 Batch  163/175   train_loss = 1.172\n",
      "Epoch 1332 Batch   20/175   train_loss = 1.151\n",
      "Epoch 1332 Batch   52/175   train_loss = 1.121\n",
      "Epoch 1332 Batch   84/175   train_loss = 1.188\n",
      "Epoch 1332 Batch  116/175   train_loss = 1.221\n",
      "Epoch 1332 Batch  148/175   train_loss = 1.170\n",
      "Epoch 1333 Batch    5/175   train_loss = 1.179\n",
      "Epoch 1333 Batch   37/175   train_loss = 1.170\n",
      "Epoch 1333 Batch   69/175   train_loss = 1.201\n",
      "Epoch 1333 Batch  101/175   train_loss = 1.227\n",
      "Epoch 1333 Batch  133/175   train_loss = 1.095\n",
      "Epoch 1333 Batch  165/175   train_loss = 1.191\n",
      "Epoch 1334 Batch   22/175   train_loss = 1.120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1334 Batch   54/175   train_loss = 1.221\n",
      "Epoch 1334 Batch   86/175   train_loss = 1.241\n",
      "Epoch 1334 Batch  118/175   train_loss = 1.234\n",
      "Epoch 1334 Batch  150/175   train_loss = 1.183\n",
      "Epoch 1335 Batch    7/175   train_loss = 1.226\n",
      "Epoch 1335 Batch   39/175   train_loss = 1.142\n",
      "Epoch 1335 Batch   71/175   train_loss = 1.208\n",
      "Epoch 1335 Batch  103/175   train_loss = 1.189\n",
      "Epoch 1335 Batch  135/175   train_loss = 1.152\n",
      "Epoch 1335 Batch  167/175   train_loss = 1.232\n",
      "Epoch 1336 Batch   24/175   train_loss = 1.137\n",
      "Epoch 1336 Batch   56/175   train_loss = 1.195\n",
      "Epoch 1336 Batch   88/175   train_loss = 1.220\n",
      "Epoch 1336 Batch  120/175   train_loss = 1.173\n",
      "Epoch 1336 Batch  152/175   train_loss = 1.152\n",
      "Epoch 1337 Batch    9/175   train_loss = 1.212\n",
      "Epoch 1337 Batch   41/175   train_loss = 1.205\n",
      "Epoch 1337 Batch   73/175   train_loss = 1.206\n",
      "Epoch 1337 Batch  105/175   train_loss = 1.216\n",
      "Epoch 1337 Batch  137/175   train_loss = 1.145\n",
      "Epoch 1337 Batch  169/175   train_loss = 1.192\n",
      "Epoch 1338 Batch   26/175   train_loss = 1.180\n",
      "Epoch 1338 Batch   58/175   train_loss = 1.216\n",
      "Epoch 1338 Batch   90/175   train_loss = 1.206\n",
      "Epoch 1338 Batch  122/175   train_loss = 1.177\n",
      "Epoch 1338 Batch  154/175   train_loss = 1.174\n",
      "Epoch 1339 Batch   11/175   train_loss = 1.166\n",
      "Epoch 1339 Batch   43/175   train_loss = 1.194\n",
      "Epoch 1339 Batch   75/175   train_loss = 1.196\n",
      "Epoch 1339 Batch  107/175   train_loss = 1.252\n",
      "Epoch 1339 Batch  139/175   train_loss = 1.144\n",
      "Epoch 1339 Batch  171/175   train_loss = 1.238\n",
      "Epoch 1340 Batch   28/175   train_loss = 1.200\n",
      "Epoch 1340 Batch   60/175   train_loss = 1.177\n",
      "Epoch 1340 Batch   92/175   train_loss = 1.163\n",
      "Epoch 1340 Batch  124/175   train_loss = 1.189\n",
      "Epoch 1340 Batch  156/175   train_loss = 1.211\n",
      "Epoch 1341 Batch   13/175   train_loss = 1.200\n",
      "Epoch 1341 Batch   45/175   train_loss = 1.201\n",
      "Epoch 1341 Batch   77/175   train_loss = 1.202\n",
      "Epoch 1341 Batch  109/175   train_loss = 1.248\n",
      "Epoch 1341 Batch  141/175   train_loss = 1.151\n",
      "Epoch 1341 Batch  173/175   train_loss = 1.178\n",
      "Epoch 1342 Batch   30/175   train_loss = 1.240\n",
      "Epoch 1342 Batch   62/175   train_loss = 1.221\n",
      "Epoch 1342 Batch   94/175   train_loss = 1.185\n",
      "Epoch 1342 Batch  126/175   train_loss = 1.202\n",
      "Epoch 1342 Batch  158/175   train_loss = 1.187\n",
      "Epoch 1343 Batch   15/175   train_loss = 1.226\n",
      "Epoch 1343 Batch   47/175   train_loss = 1.223\n",
      "Epoch 1343 Batch   79/175   train_loss = 1.221\n",
      "Epoch 1343 Batch  111/175   train_loss = 1.243\n",
      "Epoch 1343 Batch  143/175   train_loss = 1.181\n",
      "Epoch 1344 Batch    0/175   train_loss = 1.205\n",
      "Epoch 1344 Batch   32/175   train_loss = 1.216\n",
      "Epoch 1344 Batch   64/175   train_loss = 1.226\n",
      "Epoch 1344 Batch   96/175   train_loss = 1.215\n",
      "Epoch 1344 Batch  128/175   train_loss = 1.135\n",
      "Epoch 1344 Batch  160/175   train_loss = 1.182\n",
      "Epoch 1345 Batch   17/175   train_loss = 1.164\n",
      "Epoch 1345 Batch   49/175   train_loss = 1.190\n",
      "Epoch 1345 Batch   81/175   train_loss = 1.179\n",
      "Epoch 1345 Batch  113/175   train_loss = 1.191\n",
      "Epoch 1345 Batch  145/175   train_loss = 1.144\n",
      "Epoch 1346 Batch    2/175   train_loss = 1.199\n",
      "Epoch 1346 Batch   34/175   train_loss = 1.215\n",
      "Epoch 1346 Batch   66/175   train_loss = 1.230\n",
      "Epoch 1346 Batch   98/175   train_loss = 1.229\n",
      "Epoch 1346 Batch  130/175   train_loss = 1.225\n",
      "Epoch 1346 Batch  162/175   train_loss = 1.177\n",
      "Epoch 1347 Batch   19/175   train_loss = 1.188\n",
      "Epoch 1347 Batch   51/175   train_loss = 1.146\n",
      "Epoch 1347 Batch   83/175   train_loss = 1.249\n",
      "Epoch 1347 Batch  115/175   train_loss = 1.322\n",
      "Epoch 1347 Batch  147/175   train_loss = 1.169\n",
      "Epoch 1348 Batch    4/175   train_loss = 1.227\n",
      "Epoch 1348 Batch   36/175   train_loss = 1.152\n",
      "Epoch 1348 Batch   68/175   train_loss = 1.185\n",
      "Epoch 1348 Batch  100/175   train_loss = 1.213\n",
      "Epoch 1348 Batch  132/175   train_loss = 1.160\n",
      "Epoch 1348 Batch  164/175   train_loss = 1.162\n",
      "Epoch 1349 Batch   21/175   train_loss = 1.167\n",
      "Epoch 1349 Batch   53/175   train_loss = 1.147\n",
      "Epoch 1349 Batch   85/175   train_loss = 1.228\n",
      "Epoch 1349 Batch  117/175   train_loss = 1.219\n",
      "Epoch 1349 Batch  149/175   train_loss = 1.209\n",
      "Epoch 1350 Batch    6/175   train_loss = 1.222\n",
      "Epoch 1350 Batch   38/175   train_loss = 1.148\n",
      "Epoch 1350 Batch   70/175   train_loss = 1.194\n",
      "Epoch 1350 Batch  102/175   train_loss = 1.201\n",
      "Epoch 1350 Batch  134/175   train_loss = 1.164\n",
      "Epoch 1350 Batch  166/175   train_loss = 1.198\n",
      "Epoch 1351 Batch   23/175   train_loss = 1.140\n",
      "Epoch 1351 Batch   55/175   train_loss = 1.249\n",
      "Epoch 1351 Batch   87/175   train_loss = 1.266\n",
      "Epoch 1351 Batch  119/175   train_loss = 1.177\n",
      "Epoch 1351 Batch  151/175   train_loss = 1.197\n",
      "Epoch 1352 Batch    8/175   train_loss = 1.201\n",
      "Epoch 1352 Batch   40/175   train_loss = 1.201\n",
      "Epoch 1352 Batch   72/175   train_loss = 1.262\n",
      "Epoch 1352 Batch  104/175   train_loss = 1.215\n",
      "Epoch 1352 Batch  136/175   train_loss = 1.182\n",
      "Epoch 1352 Batch  168/175   train_loss = 1.212\n",
      "Epoch 1353 Batch   25/175   train_loss = 1.210\n",
      "Epoch 1353 Batch   57/175   train_loss = 1.241\n",
      "Epoch 1353 Batch   89/175   train_loss = 1.189\n",
      "Epoch 1353 Batch  121/175   train_loss = 1.189\n",
      "Epoch 1353 Batch  153/175   train_loss = 1.193\n",
      "Epoch 1354 Batch   10/175   train_loss = 1.147\n",
      "Epoch 1354 Batch   42/175   train_loss = 1.243\n",
      "Epoch 1354 Batch   74/175   train_loss = 1.230\n",
      "Epoch 1354 Batch  106/175   train_loss = 1.244\n",
      "Epoch 1354 Batch  138/175   train_loss = 1.182\n",
      "Epoch 1354 Batch  170/175   train_loss = 1.235\n",
      "Epoch 1355 Batch   27/175   train_loss = 1.177\n",
      "Epoch 1355 Batch   59/175   train_loss = 1.181\n",
      "Epoch 1355 Batch   91/175   train_loss = 1.195\n",
      "Epoch 1355 Batch  123/175   train_loss = 1.191\n",
      "Epoch 1355 Batch  155/175   train_loss = 1.147\n",
      "Epoch 1356 Batch   12/175   train_loss = 1.190\n",
      "Epoch 1356 Batch   44/175   train_loss = 1.188\n",
      "Epoch 1356 Batch   76/175   train_loss = 1.181\n",
      "Epoch 1356 Batch  108/175   train_loss = 1.198\n",
      "Epoch 1356 Batch  140/175   train_loss = 1.165\n",
      "Epoch 1356 Batch  172/175   train_loss = 1.219\n",
      "Epoch 1357 Batch   29/175   train_loss = 1.185\n",
      "Epoch 1357 Batch   61/175   train_loss = 1.213\n",
      "Epoch 1357 Batch   93/175   train_loss = 1.198\n",
      "Epoch 1357 Batch  125/175   train_loss = 1.210\n",
      "Epoch 1357 Batch  157/175   train_loss = 1.169\n",
      "Epoch 1358 Batch   14/175   train_loss = 1.205\n",
      "Epoch 1358 Batch   46/175   train_loss = 1.188\n",
      "Epoch 1358 Batch   78/175   train_loss = 1.168\n",
      "Epoch 1358 Batch  110/175   train_loss = 1.275\n",
      "Epoch 1358 Batch  142/175   train_loss = 1.201\n",
      "Epoch 1358 Batch  174/175   train_loss = 1.173\n",
      "Epoch 1359 Batch   31/175   train_loss = 1.195\n",
      "Epoch 1359 Batch   63/175   train_loss = 1.201\n",
      "Epoch 1359 Batch   95/175   train_loss = 1.175\n",
      "Epoch 1359 Batch  127/175   train_loss = 1.153\n",
      "Epoch 1359 Batch  159/175   train_loss = 1.169\n",
      "Epoch 1360 Batch   16/175   train_loss = 1.169\n",
      "Epoch 1360 Batch   48/175   train_loss = 1.191\n",
      "Epoch 1360 Batch   80/175   train_loss = 1.188\n",
      "Epoch 1360 Batch  112/175   train_loss = 1.185\n",
      "Epoch 1360 Batch  144/175   train_loss = 1.113\n",
      "Epoch 1361 Batch    1/175   train_loss = 1.228\n",
      "Epoch 1361 Batch   33/175   train_loss = 1.238\n",
      "Epoch 1361 Batch   65/175   train_loss = 1.164\n",
      "Epoch 1361 Batch   97/175   train_loss = 1.184\n",
      "Epoch 1361 Batch  129/175   train_loss = 1.176\n",
      "Epoch 1361 Batch  161/175   train_loss = 1.166\n",
      "Epoch 1362 Batch   18/175   train_loss = 1.150\n",
      "Epoch 1362 Batch   50/175   train_loss = 1.152\n",
      "Epoch 1362 Batch   82/175   train_loss = 1.246\n",
      "Epoch 1362 Batch  114/175   train_loss = 1.254\n",
      "Epoch 1362 Batch  146/175   train_loss = 1.189\n",
      "Epoch 1363 Batch    3/175   train_loss = 1.218\n",
      "Epoch 1363 Batch   35/175   train_loss = 1.161\n",
      "Epoch 1363 Batch   67/175   train_loss = 1.140\n",
      "Epoch 1363 Batch   99/175   train_loss = 1.243\n",
      "Epoch 1363 Batch  131/175   train_loss = 1.186\n",
      "Epoch 1363 Batch  163/175   train_loss = 1.181\n",
      "Epoch 1364 Batch   20/175   train_loss = 1.174\n",
      "Epoch 1364 Batch   52/175   train_loss = 1.130\n",
      "Epoch 1364 Batch   84/175   train_loss = 1.178\n",
      "Epoch 1364 Batch  116/175   train_loss = 1.239\n",
      "Epoch 1364 Batch  148/175   train_loss = 1.184\n",
      "Epoch 1365 Batch    5/175   train_loss = 1.193\n",
      "Epoch 1365 Batch   37/175   train_loss = 1.172\n",
      "Epoch 1365 Batch   69/175   train_loss = 1.192\n",
      "Epoch 1365 Batch  101/175   train_loss = 1.250\n",
      "Epoch 1365 Batch  133/175   train_loss = 1.100\n",
      "Epoch 1365 Batch  165/175   train_loss = 1.186\n",
      "Epoch 1366 Batch   22/175   train_loss = 1.134\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1366 Batch   54/175   train_loss = 1.215\n",
      "Epoch 1366 Batch   86/175   train_loss = 1.257\n",
      "Epoch 1366 Batch  118/175   train_loss = 1.248\n",
      "Epoch 1366 Batch  150/175   train_loss = 1.179\n",
      "Epoch 1367 Batch    7/175   train_loss = 1.216\n",
      "Epoch 1367 Batch   39/175   train_loss = 1.147\n",
      "Epoch 1367 Batch   71/175   train_loss = 1.223\n",
      "Epoch 1367 Batch  103/175   train_loss = 1.191\n",
      "Epoch 1367 Batch  135/175   train_loss = 1.174\n",
      "Epoch 1367 Batch  167/175   train_loss = 1.221\n",
      "Epoch 1368 Batch   24/175   train_loss = 1.150\n",
      "Epoch 1368 Batch   56/175   train_loss = 1.208\n",
      "Epoch 1368 Batch   88/175   train_loss = 1.247\n",
      "Epoch 1368 Batch  120/175   train_loss = 1.178\n",
      "Epoch 1368 Batch  152/175   train_loss = 1.170\n",
      "Epoch 1369 Batch    9/175   train_loss = 1.214\n",
      "Epoch 1369 Batch   41/175   train_loss = 1.198\n",
      "Epoch 1369 Batch   73/175   train_loss = 1.182\n",
      "Epoch 1369 Batch  105/175   train_loss = 1.219\n",
      "Epoch 1369 Batch  137/175   train_loss = 1.150\n",
      "Epoch 1369 Batch  169/175   train_loss = 1.203\n",
      "Epoch 1370 Batch   26/175   train_loss = 1.184\n",
      "Epoch 1370 Batch   58/175   train_loss = 1.222\n",
      "Epoch 1370 Batch   90/175   train_loss = 1.196\n",
      "Epoch 1370 Batch  122/175   train_loss = 1.173\n",
      "Epoch 1370 Batch  154/175   train_loss = 1.169\n",
      "Epoch 1371 Batch   11/175   train_loss = 1.168\n",
      "Epoch 1371 Batch   43/175   train_loss = 1.177\n",
      "Epoch 1371 Batch   75/175   train_loss = 1.173\n",
      "Epoch 1371 Batch  107/175   train_loss = 1.230\n",
      "Epoch 1371 Batch  139/175   train_loss = 1.141\n",
      "Epoch 1371 Batch  171/175   train_loss = 1.228\n",
      "Epoch 1372 Batch   28/175   train_loss = 1.169\n",
      "Epoch 1372 Batch   60/175   train_loss = 1.185\n",
      "Epoch 1372 Batch   92/175   train_loss = 1.180\n",
      "Epoch 1372 Batch  124/175   train_loss = 1.185\n",
      "Epoch 1372 Batch  156/175   train_loss = 1.220\n",
      "Epoch 1373 Batch   13/175   train_loss = 1.186\n",
      "Epoch 1373 Batch   45/175   train_loss = 1.183\n",
      "Epoch 1373 Batch   77/175   train_loss = 1.181\n",
      "Epoch 1373 Batch  109/175   train_loss = 1.255\n",
      "Epoch 1373 Batch  141/175   train_loss = 1.154\n",
      "Epoch 1373 Batch  173/175   train_loss = 1.178\n",
      "Epoch 1374 Batch   30/175   train_loss = 1.241\n",
      "Epoch 1374 Batch   62/175   train_loss = 1.219\n",
      "Epoch 1374 Batch   94/175   train_loss = 1.190\n",
      "Epoch 1374 Batch  126/175   train_loss = 1.206\n",
      "Epoch 1374 Batch  158/175   train_loss = 1.197\n",
      "Epoch 1375 Batch   15/175   train_loss = 1.245\n",
      "Epoch 1375 Batch   47/175   train_loss = 1.228\n",
      "Epoch 1375 Batch   79/175   train_loss = 1.249\n",
      "Epoch 1375 Batch  111/175   train_loss = 1.237\n",
      "Epoch 1375 Batch  143/175   train_loss = 1.168\n",
      "Epoch 1376 Batch    0/175   train_loss = 1.205\n",
      "Epoch 1376 Batch   32/175   train_loss = 1.215\n",
      "Epoch 1376 Batch   64/175   train_loss = 1.227\n",
      "Epoch 1376 Batch   96/175   train_loss = 1.223\n",
      "Epoch 1376 Batch  128/175   train_loss = 1.134\n",
      "Epoch 1376 Batch  160/175   train_loss = 1.193\n",
      "Epoch 1377 Batch   17/175   train_loss = 1.172\n",
      "Epoch 1377 Batch   49/175   train_loss = 1.212\n",
      "Epoch 1377 Batch   81/175   train_loss = 1.184\n",
      "Epoch 1377 Batch  113/175   train_loss = 1.215\n",
      "Epoch 1377 Batch  145/175   train_loss = 1.155\n",
      "Epoch 1378 Batch    2/175   train_loss = 1.194\n",
      "Epoch 1378 Batch   34/175   train_loss = 1.219\n",
      "Epoch 1378 Batch   66/175   train_loss = 1.231\n",
      "Epoch 1378 Batch   98/175   train_loss = 1.225\n",
      "Epoch 1378 Batch  130/175   train_loss = 1.219\n",
      "Epoch 1378 Batch  162/175   train_loss = 1.183\n",
      "Epoch 1379 Batch   19/175   train_loss = 1.208\n",
      "Epoch 1379 Batch   51/175   train_loss = 1.143\n",
      "Epoch 1379 Batch   83/175   train_loss = 1.250\n",
      "Epoch 1379 Batch  115/175   train_loss = 1.296\n",
      "Epoch 1379 Batch  147/175   train_loss = 1.173\n",
      "Epoch 1380 Batch    4/175   train_loss = 1.229\n",
      "Epoch 1380 Batch   36/175   train_loss = 1.162\n",
      "Epoch 1380 Batch   68/175   train_loss = 1.200\n",
      "Epoch 1380 Batch  100/175   train_loss = 1.233\n",
      "Epoch 1380 Batch  132/175   train_loss = 1.177\n",
      "Epoch 1380 Batch  164/175   train_loss = 1.159\n",
      "Epoch 1381 Batch   21/175   train_loss = 1.174\n",
      "Epoch 1381 Batch   53/175   train_loss = 1.172\n",
      "Epoch 1381 Batch   85/175   train_loss = 1.216\n",
      "Epoch 1381 Batch  117/175   train_loss = 1.217\n",
      "Epoch 1381 Batch  149/175   train_loss = 1.206\n",
      "Epoch 1382 Batch    6/175   train_loss = 1.213\n",
      "Epoch 1382 Batch   38/175   train_loss = 1.150\n",
      "Epoch 1382 Batch   70/175   train_loss = 1.191\n",
      "Epoch 1382 Batch  102/175   train_loss = 1.211\n",
      "Epoch 1382 Batch  134/175   train_loss = 1.166\n",
      "Epoch 1382 Batch  166/175   train_loss = 1.200\n",
      "Epoch 1383 Batch   23/175   train_loss = 1.152\n",
      "Epoch 1383 Batch   55/175   train_loss = 1.262\n",
      "Epoch 1383 Batch   87/175   train_loss = 1.301\n",
      "Epoch 1383 Batch  119/175   train_loss = 1.204\n",
      "Epoch 1383 Batch  151/175   train_loss = 1.200\n",
      "Epoch 1384 Batch    8/175   train_loss = 1.210\n",
      "Epoch 1384 Batch   40/175   train_loss = 1.191\n",
      "Epoch 1384 Batch   72/175   train_loss = 1.216\n",
      "Epoch 1384 Batch  104/175   train_loss = 1.231\n",
      "Epoch 1384 Batch  136/175   train_loss = 1.184\n",
      "Epoch 1384 Batch  168/175   train_loss = 1.218\n",
      "Epoch 1385 Batch   25/175   train_loss = 1.204\n",
      "Epoch 1385 Batch   57/175   train_loss = 1.226\n",
      "Epoch 1385 Batch   89/175   train_loss = 1.187\n",
      "Epoch 1385 Batch  121/175   train_loss = 1.180\n",
      "Epoch 1385 Batch  153/175   train_loss = 1.203\n",
      "Epoch 1386 Batch   10/175   train_loss = 1.151\n",
      "Epoch 1386 Batch   42/175   train_loss = 1.248\n",
      "Epoch 1386 Batch   74/175   train_loss = 1.248\n",
      "Epoch 1386 Batch  106/175   train_loss = 1.260\n",
      "Epoch 1386 Batch  138/175   train_loss = 1.204\n",
      "Epoch 1386 Batch  170/175   train_loss = 1.243\n",
      "Epoch 1387 Batch   27/175   train_loss = 1.187\n",
      "Epoch 1387 Batch   59/175   train_loss = 1.191\n",
      "Epoch 1387 Batch   91/175   train_loss = 1.193\n",
      "Epoch 1387 Batch  123/175   train_loss = 1.174\n",
      "Epoch 1387 Batch  155/175   train_loss = 1.186\n",
      "Epoch 1388 Batch   12/175   train_loss = 1.199\n",
      "Epoch 1388 Batch   44/175   train_loss = 1.193\n",
      "Epoch 1388 Batch   76/175   train_loss = 1.183\n",
      "Epoch 1388 Batch  108/175   train_loss = 1.205\n",
      "Epoch 1388 Batch  140/175   train_loss = 1.193\n",
      "Epoch 1388 Batch  172/175   train_loss = 1.228\n",
      "Epoch 1389 Batch   29/175   train_loss = 1.183\n",
      "Epoch 1389 Batch   61/175   train_loss = 1.228\n",
      "Epoch 1389 Batch   93/175   train_loss = 1.236\n",
      "Epoch 1389 Batch  125/175   train_loss = 1.215\n",
      "Epoch 1389 Batch  157/175   train_loss = 1.193\n",
      "Epoch 1390 Batch   14/175   train_loss = 1.235\n",
      "Epoch 1390 Batch   46/175   train_loss = 1.217\n",
      "Epoch 1390 Batch   78/175   train_loss = 1.194\n",
      "Epoch 1390 Batch  110/175   train_loss = 1.293\n",
      "Epoch 1390 Batch  142/175   train_loss = 1.212\n",
      "Epoch 1390 Batch  174/175   train_loss = 1.184\n",
      "Epoch 1391 Batch   31/175   train_loss = 1.228\n",
      "Epoch 1391 Batch   63/175   train_loss = 1.204\n",
      "Epoch 1391 Batch   95/175   train_loss = 1.161\n",
      "Epoch 1391 Batch  127/175   train_loss = 1.163\n",
      "Epoch 1391 Batch  159/175   train_loss = 1.165\n",
      "Epoch 1392 Batch   16/175   train_loss = 1.176\n",
      "Epoch 1392 Batch   48/175   train_loss = 1.209\n",
      "Epoch 1392 Batch   80/175   train_loss = 1.221\n",
      "Epoch 1392 Batch  112/175   train_loss = 1.201\n",
      "Epoch 1392 Batch  144/175   train_loss = 1.123\n",
      "Epoch 1393 Batch    1/175   train_loss = 1.249\n",
      "Epoch 1393 Batch   33/175   train_loss = 1.243\n",
      "Epoch 1393 Batch   65/175   train_loss = 1.172\n",
      "Epoch 1393 Batch   97/175   train_loss = 1.211\n",
      "Epoch 1393 Batch  129/175   train_loss = 1.191\n",
      "Epoch 1393 Batch  161/175   train_loss = 1.165\n",
      "Epoch 1394 Batch   18/175   train_loss = 1.145\n",
      "Epoch 1394 Batch   50/175   train_loss = 1.179\n",
      "Epoch 1394 Batch   82/175   train_loss = 1.244\n",
      "Epoch 1394 Batch  114/175   train_loss = 1.237\n",
      "Epoch 1394 Batch  146/175   train_loss = 1.182\n",
      "Epoch 1395 Batch    3/175   train_loss = 1.228\n",
      "Epoch 1395 Batch   35/175   train_loss = 1.199\n",
      "Epoch 1395 Batch   67/175   train_loss = 1.164\n",
      "Epoch 1395 Batch   99/175   train_loss = 1.260\n",
      "Epoch 1395 Batch  131/175   train_loss = 1.182\n",
      "Epoch 1395 Batch  163/175   train_loss = 1.201\n",
      "Epoch 1396 Batch   20/175   train_loss = 1.176\n",
      "Epoch 1396 Batch   52/175   train_loss = 1.148\n",
      "Epoch 1396 Batch   84/175   train_loss = 1.196\n",
      "Epoch 1396 Batch  116/175   train_loss = 1.244\n",
      "Epoch 1396 Batch  148/175   train_loss = 1.197\n",
      "Epoch 1397 Batch    5/175   train_loss = 1.181\n",
      "Epoch 1397 Batch   37/175   train_loss = 1.173\n",
      "Epoch 1397 Batch   69/175   train_loss = 1.207\n",
      "Epoch 1397 Batch  101/175   train_loss = 1.249\n",
      "Epoch 1397 Batch  133/175   train_loss = 1.093\n",
      "Epoch 1397 Batch  165/175   train_loss = 1.205\n",
      "Epoch 1398 Batch   22/175   train_loss = 1.137\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1398 Batch   54/175   train_loss = 1.232\n",
      "Epoch 1398 Batch   86/175   train_loss = 1.239\n",
      "Epoch 1398 Batch  118/175   train_loss = 1.235\n",
      "Epoch 1398 Batch  150/175   train_loss = 1.177\n",
      "Epoch 1399 Batch    7/175   train_loss = 1.209\n",
      "Epoch 1399 Batch   39/175   train_loss = 1.149\n",
      "Epoch 1399 Batch   71/175   train_loss = 1.205\n",
      "Epoch 1399 Batch  103/175   train_loss = 1.192\n",
      "Epoch 1399 Batch  135/175   train_loss = 1.171\n",
      "Epoch 1399 Batch  167/175   train_loss = 1.239\n",
      "Epoch 1400 Batch   24/175   train_loss = 1.137\n",
      "Epoch 1400 Batch   56/175   train_loss = 1.207\n",
      "Epoch 1400 Batch   88/175   train_loss = 1.234\n",
      "Epoch 1400 Batch  120/175   train_loss = 1.176\n",
      "Epoch 1400 Batch  152/175   train_loss = 1.166\n",
      "Epoch 1401 Batch    9/175   train_loss = 1.225\n",
      "Epoch 1401 Batch   41/175   train_loss = 1.228\n",
      "Epoch 1401 Batch   73/175   train_loss = 1.231\n",
      "Epoch 1401 Batch  105/175   train_loss = 1.261\n",
      "Epoch 1401 Batch  137/175   train_loss = 1.169\n",
      "Epoch 1401 Batch  169/175   train_loss = 1.212\n",
      "Epoch 1402 Batch   26/175   train_loss = 1.180\n",
      "Epoch 1402 Batch   58/175   train_loss = 1.235\n",
      "Epoch 1402 Batch   90/175   train_loss = 1.208\n",
      "Epoch 1402 Batch  122/175   train_loss = 1.182\n",
      "Epoch 1402 Batch  154/175   train_loss = 1.162\n",
      "Epoch 1403 Batch   11/175   train_loss = 1.173\n",
      "Epoch 1403 Batch   43/175   train_loss = 1.187\n",
      "Epoch 1403 Batch   75/175   train_loss = 1.181\n",
      "Epoch 1403 Batch  107/175   train_loss = 1.246\n",
      "Epoch 1403 Batch  139/175   train_loss = 1.153\n",
      "Epoch 1403 Batch  171/175   train_loss = 1.245\n",
      "Epoch 1404 Batch   28/175   train_loss = 1.181\n",
      "Epoch 1404 Batch   60/175   train_loss = 1.194\n",
      "Epoch 1404 Batch   92/175   train_loss = 1.169\n",
      "Epoch 1404 Batch  124/175   train_loss = 1.190\n",
      "Epoch 1404 Batch  156/175   train_loss = 1.232\n",
      "Epoch 1405 Batch   13/175   train_loss = 1.208\n",
      "Epoch 1405 Batch   45/175   train_loss = 1.179\n",
      "Epoch 1405 Batch   77/175   train_loss = 1.187\n",
      "Epoch 1405 Batch  109/175   train_loss = 1.262\n",
      "Epoch 1405 Batch  141/175   train_loss = 1.161\n",
      "Epoch 1405 Batch  173/175   train_loss = 1.173\n",
      "Epoch 1406 Batch   30/175   train_loss = 1.265\n",
      "Epoch 1406 Batch   62/175   train_loss = 1.220\n",
      "Epoch 1406 Batch   94/175   train_loss = 1.200\n",
      "Epoch 1406 Batch  126/175   train_loss = 1.206\n",
      "Epoch 1406 Batch  158/175   train_loss = 1.184\n",
      "Epoch 1407 Batch   15/175   train_loss = 1.233\n",
      "Epoch 1407 Batch   47/175   train_loss = 1.218\n",
      "Epoch 1407 Batch   79/175   train_loss = 1.234\n",
      "Epoch 1407 Batch  111/175   train_loss = 1.239\n",
      "Epoch 1407 Batch  143/175   train_loss = 1.175\n",
      "Epoch 1408 Batch    0/175   train_loss = 1.209\n",
      "Epoch 1408 Batch   32/175   train_loss = 1.213\n",
      "Epoch 1408 Batch   64/175   train_loss = 1.234\n",
      "Epoch 1408 Batch   96/175   train_loss = 1.220\n",
      "Epoch 1408 Batch  128/175   train_loss = 1.135\n",
      "Epoch 1408 Batch  160/175   train_loss = 1.182\n",
      "Epoch 1409 Batch   17/175   train_loss = 1.158\n",
      "Epoch 1409 Batch   49/175   train_loss = 1.218\n",
      "Epoch 1409 Batch   81/175   train_loss = 1.196\n",
      "Epoch 1409 Batch  113/175   train_loss = 1.224\n",
      "Epoch 1409 Batch  145/175   train_loss = 1.152\n",
      "Epoch 1410 Batch    2/175   train_loss = 1.200\n",
      "Epoch 1410 Batch   34/175   train_loss = 1.216\n",
      "Epoch 1410 Batch   66/175   train_loss = 1.232\n",
      "Epoch 1410 Batch   98/175   train_loss = 1.198\n",
      "Epoch 1410 Batch  130/175   train_loss = 1.214\n",
      "Epoch 1410 Batch  162/175   train_loss = 1.191\n",
      "Epoch 1411 Batch   19/175   train_loss = 1.192\n",
      "Epoch 1411 Batch   51/175   train_loss = 1.147\n",
      "Epoch 1411 Batch   83/175   train_loss = 1.240\n",
      "Epoch 1411 Batch  115/175   train_loss = 1.297\n",
      "Epoch 1411 Batch  147/175   train_loss = 1.170\n",
      "Epoch 1412 Batch    4/175   train_loss = 1.200\n",
      "Epoch 1412 Batch   36/175   train_loss = 1.160\n",
      "Epoch 1412 Batch   68/175   train_loss = 1.196\n",
      "Epoch 1412 Batch  100/175   train_loss = 1.216\n",
      "Epoch 1412 Batch  132/175   train_loss = 1.160\n",
      "Epoch 1412 Batch  164/175   train_loss = 1.156\n",
      "Epoch 1413 Batch   21/175   train_loss = 1.152\n",
      "Epoch 1413 Batch   53/175   train_loss = 1.154\n",
      "Epoch 1413 Batch   85/175   train_loss = 1.225\n",
      "Epoch 1413 Batch  117/175   train_loss = 1.204\n",
      "Epoch 1413 Batch  149/175   train_loss = 1.202\n",
      "Epoch 1414 Batch    6/175   train_loss = 1.204\n",
      "Epoch 1414 Batch   38/175   train_loss = 1.157\n",
      "Epoch 1414 Batch   70/175   train_loss = 1.184\n",
      "Epoch 1414 Batch  102/175   train_loss = 1.201\n",
      "Epoch 1414 Batch  134/175   train_loss = 1.144\n",
      "Epoch 1414 Batch  166/175   train_loss = 1.189\n",
      "Epoch 1415 Batch   23/175   train_loss = 1.135\n",
      "Epoch 1415 Batch   55/175   train_loss = 1.240\n",
      "Epoch 1415 Batch   87/175   train_loss = 1.266\n",
      "Epoch 1415 Batch  119/175   train_loss = 1.179\n",
      "Epoch 1415 Batch  151/175   train_loss = 1.194\n",
      "Epoch 1416 Batch    8/175   train_loss = 1.192\n",
      "Epoch 1416 Batch   40/175   train_loss = 1.173\n",
      "Epoch 1416 Batch   72/175   train_loss = 1.215\n",
      "Epoch 1416 Batch  104/175   train_loss = 1.200\n",
      "Epoch 1416 Batch  136/175   train_loss = 1.164\n",
      "Epoch 1416 Batch  168/175   train_loss = 1.203\n",
      "Epoch 1417 Batch   25/175   train_loss = 1.203\n",
      "Epoch 1417 Batch   57/175   train_loss = 1.252\n",
      "Epoch 1417 Batch   89/175   train_loss = 1.195\n",
      "Epoch 1417 Batch  121/175   train_loss = 1.178\n",
      "Epoch 1417 Batch  153/175   train_loss = 1.205\n",
      "Epoch 1418 Batch   10/175   train_loss = 1.155\n",
      "Epoch 1418 Batch   42/175   train_loss = 1.236\n",
      "Epoch 1418 Batch   74/175   train_loss = 1.241\n",
      "Epoch 1418 Batch  106/175   train_loss = 1.238\n",
      "Epoch 1418 Batch  138/175   train_loss = 1.195\n",
      "Epoch 1418 Batch  170/175   train_loss = 1.215\n",
      "Epoch 1419 Batch   27/175   train_loss = 1.179\n",
      "Epoch 1419 Batch   59/175   train_loss = 1.165\n",
      "Epoch 1419 Batch   91/175   train_loss = 1.170\n",
      "Epoch 1419 Batch  123/175   train_loss = 1.179\n",
      "Epoch 1419 Batch  155/175   train_loss = 1.167\n",
      "Epoch 1420 Batch   12/175   train_loss = 1.195\n",
      "Epoch 1420 Batch   44/175   train_loss = 1.165\n",
      "Epoch 1420 Batch   76/175   train_loss = 1.165\n",
      "Epoch 1420 Batch  108/175   train_loss = 1.209\n",
      "Epoch 1420 Batch  140/175   train_loss = 1.185\n",
      "Epoch 1420 Batch  172/175   train_loss = 1.235\n",
      "Epoch 1421 Batch   29/175   train_loss = 1.191\n",
      "Epoch 1421 Batch   61/175   train_loss = 1.215\n",
      "Epoch 1421 Batch   93/175   train_loss = 1.194\n",
      "Epoch 1421 Batch  125/175   train_loss = 1.209\n",
      "Epoch 1421 Batch  157/175   train_loss = 1.190\n",
      "Epoch 1422 Batch   14/175   train_loss = 1.218\n",
      "Epoch 1422 Batch   46/175   train_loss = 1.216\n",
      "Epoch 1422 Batch   78/175   train_loss = 1.170\n",
      "Epoch 1422 Batch  110/175   train_loss = 1.276\n",
      "Epoch 1422 Batch  142/175   train_loss = 1.221\n",
      "Epoch 1422 Batch  174/175   train_loss = 1.181\n",
      "Epoch 1423 Batch   31/175   train_loss = 1.227\n",
      "Epoch 1423 Batch   63/175   train_loss = 1.215\n",
      "Epoch 1423 Batch   95/175   train_loss = 1.172\n",
      "Epoch 1423 Batch  127/175   train_loss = 1.161\n",
      "Epoch 1423 Batch  159/175   train_loss = 1.157\n",
      "Epoch 1424 Batch   16/175   train_loss = 1.201\n",
      "Epoch 1424 Batch   48/175   train_loss = 1.199\n",
      "Epoch 1424 Batch   80/175   train_loss = 1.221\n",
      "Epoch 1424 Batch  112/175   train_loss = 1.190\n",
      "Epoch 1424 Batch  144/175   train_loss = 1.122\n",
      "Epoch 1425 Batch    1/175   train_loss = 1.229\n",
      "Epoch 1425 Batch   33/175   train_loss = 1.260\n",
      "Epoch 1425 Batch   65/175   train_loss = 1.194\n",
      "Epoch 1425 Batch   97/175   train_loss = 1.197\n",
      "Epoch 1425 Batch  129/175   train_loss = 1.196\n",
      "Epoch 1425 Batch  161/175   train_loss = 1.179\n",
      "Epoch 1426 Batch   18/175   train_loss = 1.165\n",
      "Epoch 1426 Batch   50/175   train_loss = 1.165\n",
      "Epoch 1426 Batch   82/175   train_loss = 1.255\n",
      "Epoch 1426 Batch  114/175   train_loss = 1.238\n",
      "Epoch 1426 Batch  146/175   train_loss = 1.189\n",
      "Epoch 1427 Batch    3/175   train_loss = 1.218\n",
      "Epoch 1427 Batch   35/175   train_loss = 1.196\n",
      "Epoch 1427 Batch   67/175   train_loss = 1.172\n",
      "Epoch 1427 Batch   99/175   train_loss = 1.258\n",
      "Epoch 1427 Batch  131/175   train_loss = 1.178\n",
      "Epoch 1427 Batch  163/175   train_loss = 1.197\n",
      "Epoch 1428 Batch   20/175   train_loss = 1.163\n",
      "Epoch 1428 Batch   52/175   train_loss = 1.134\n",
      "Epoch 1428 Batch   84/175   train_loss = 1.196\n",
      "Epoch 1428 Batch  116/175   train_loss = 1.233\n",
      "Epoch 1428 Batch  148/175   train_loss = 1.174\n",
      "Epoch 1429 Batch    5/175   train_loss = 1.179\n",
      "Epoch 1429 Batch   37/175   train_loss = 1.166\n",
      "Epoch 1429 Batch   69/175   train_loss = 1.213\n",
      "Epoch 1429 Batch  101/175   train_loss = 1.227\n",
      "Epoch 1429 Batch  133/175   train_loss = 1.098\n",
      "Epoch 1429 Batch  165/175   train_loss = 1.189\n",
      "Epoch 1430 Batch   22/175   train_loss = 1.129\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1430 Batch   54/175   train_loss = 1.220\n",
      "Epoch 1430 Batch   86/175   train_loss = 1.257\n",
      "Epoch 1430 Batch  118/175   train_loss = 1.241\n",
      "Epoch 1430 Batch  150/175   train_loss = 1.179\n",
      "Epoch 1431 Batch    7/175   train_loss = 1.213\n",
      "Epoch 1431 Batch   39/175   train_loss = 1.135\n",
      "Epoch 1431 Batch   71/175   train_loss = 1.207\n",
      "Epoch 1431 Batch  103/175   train_loss = 1.195\n",
      "Epoch 1431 Batch  135/175   train_loss = 1.149\n",
      "Epoch 1431 Batch  167/175   train_loss = 1.213\n",
      "Epoch 1432 Batch   24/175   train_loss = 1.148\n",
      "Epoch 1432 Batch   56/175   train_loss = 1.216\n",
      "Epoch 1432 Batch   88/175   train_loss = 1.253\n",
      "Epoch 1432 Batch  120/175   train_loss = 1.181\n",
      "Epoch 1432 Batch  152/175   train_loss = 1.166\n",
      "Epoch 1433 Batch    9/175   train_loss = 1.213\n",
      "Epoch 1433 Batch   41/175   train_loss = 1.226\n",
      "Epoch 1433 Batch   73/175   train_loss = 1.211\n",
      "Epoch 1433 Batch  105/175   train_loss = 1.213\n",
      "Epoch 1433 Batch  137/175   train_loss = 1.147\n",
      "Epoch 1433 Batch  169/175   train_loss = 1.210\n",
      "Epoch 1434 Batch   26/175   train_loss = 1.187\n",
      "Epoch 1434 Batch   58/175   train_loss = 1.236\n",
      "Epoch 1434 Batch   90/175   train_loss = 1.197\n",
      "Epoch 1434 Batch  122/175   train_loss = 1.195\n",
      "Epoch 1434 Batch  154/175   train_loss = 1.171\n",
      "Epoch 1435 Batch   11/175   train_loss = 1.168\n",
      "Epoch 1435 Batch   43/175   train_loss = 1.183\n",
      "Epoch 1435 Batch   75/175   train_loss = 1.155\n",
      "Epoch 1435 Batch  107/175   train_loss = 1.239\n",
      "Epoch 1435 Batch  139/175   train_loss = 1.128\n",
      "Epoch 1435 Batch  171/175   train_loss = 1.235\n",
      "Epoch 1436 Batch   28/175   train_loss = 1.170\n",
      "Epoch 1436 Batch   60/175   train_loss = 1.178\n",
      "Epoch 1436 Batch   92/175   train_loss = 1.173\n",
      "Epoch 1436 Batch  124/175   train_loss = 1.185\n",
      "Epoch 1436 Batch  156/175   train_loss = 1.237\n",
      "Epoch 1437 Batch   13/175   train_loss = 1.193\n",
      "Epoch 1437 Batch   45/175   train_loss = 1.187\n",
      "Epoch 1437 Batch   77/175   train_loss = 1.172\n",
      "Epoch 1437 Batch  109/175   train_loss = 1.230\n",
      "Epoch 1437 Batch  141/175   train_loss = 1.159\n",
      "Epoch 1437 Batch  173/175   train_loss = 1.174\n",
      "Epoch 1438 Batch   30/175   train_loss = 1.256\n",
      "Epoch 1438 Batch   62/175   train_loss = 1.211\n",
      "Epoch 1438 Batch   94/175   train_loss = 1.179\n",
      "Epoch 1438 Batch  126/175   train_loss = 1.204\n",
      "Epoch 1438 Batch  158/175   train_loss = 1.201\n",
      "Epoch 1439 Batch   15/175   train_loss = 1.252\n",
      "Epoch 1439 Batch   47/175   train_loss = 1.214\n",
      "Epoch 1439 Batch   79/175   train_loss = 1.244\n",
      "Epoch 1439 Batch  111/175   train_loss = 1.234\n",
      "Epoch 1439 Batch  143/175   train_loss = 1.171\n",
      "Epoch 1440 Batch    0/175   train_loss = 1.204\n",
      "Epoch 1440 Batch   32/175   train_loss = 1.216\n",
      "Epoch 1440 Batch   64/175   train_loss = 1.237\n",
      "Epoch 1440 Batch   96/175   train_loss = 1.243\n",
      "Epoch 1440 Batch  128/175   train_loss = 1.140\n",
      "Epoch 1440 Batch  160/175   train_loss = 1.207\n",
      "Epoch 1441 Batch   17/175   train_loss = 1.160\n",
      "Epoch 1441 Batch   49/175   train_loss = 1.239\n",
      "Epoch 1441 Batch   81/175   train_loss = 1.179\n",
      "Epoch 1441 Batch  113/175   train_loss = 1.220\n",
      "Epoch 1441 Batch  145/175   train_loss = 1.162\n",
      "Epoch 1442 Batch    2/175   train_loss = 1.209\n",
      "Epoch 1442 Batch   34/175   train_loss = 1.217\n",
      "Epoch 1442 Batch   66/175   train_loss = 1.223\n",
      "Epoch 1442 Batch   98/175   train_loss = 1.236\n",
      "Epoch 1442 Batch  130/175   train_loss = 1.227\n",
      "Epoch 1442 Batch  162/175   train_loss = 1.176\n",
      "Epoch 1443 Batch   19/175   train_loss = 1.183\n",
      "Epoch 1443 Batch   51/175   train_loss = 1.144\n",
      "Epoch 1443 Batch   83/175   train_loss = 1.238\n",
      "Epoch 1443 Batch  115/175   train_loss = 1.302\n",
      "Epoch 1443 Batch  147/175   train_loss = 1.179\n",
      "Epoch 1444 Batch    4/175   train_loss = 1.215\n",
      "Epoch 1444 Batch   36/175   train_loss = 1.146\n",
      "Epoch 1444 Batch   68/175   train_loss = 1.180\n",
      "Epoch 1444 Batch  100/175   train_loss = 1.209\n",
      "Epoch 1444 Batch  132/175   train_loss = 1.153\n",
      "Epoch 1444 Batch  164/175   train_loss = 1.161\n",
      "Epoch 1445 Batch   21/175   train_loss = 1.149\n",
      "Epoch 1445 Batch   53/175   train_loss = 1.153\n",
      "Epoch 1445 Batch   85/175   train_loss = 1.215\n",
      "Epoch 1445 Batch  117/175   train_loss = 1.214\n",
      "Epoch 1445 Batch  149/175   train_loss = 1.216\n",
      "Epoch 1446 Batch    6/175   train_loss = 1.211\n",
      "Epoch 1446 Batch   38/175   train_loss = 1.148\n",
      "Epoch 1446 Batch   70/175   train_loss = 1.177\n",
      "Epoch 1446 Batch  102/175   train_loss = 1.200\n",
      "Epoch 1446 Batch  134/175   train_loss = 1.156\n",
      "Epoch 1446 Batch  166/175   train_loss = 1.192\n",
      "Epoch 1447 Batch   23/175   train_loss = 1.136\n",
      "Epoch 1447 Batch   55/175   train_loss = 1.258\n",
      "Epoch 1447 Batch   87/175   train_loss = 1.271\n",
      "Epoch 1447 Batch  119/175   train_loss = 1.183\n",
      "Epoch 1447 Batch  151/175   train_loss = 1.208\n",
      "Epoch 1448 Batch    8/175   train_loss = 1.221\n",
      "Epoch 1448 Batch   40/175   train_loss = 1.189\n",
      "Epoch 1448 Batch   72/175   train_loss = 1.221\n",
      "Epoch 1448 Batch  104/175   train_loss = 1.213\n",
      "Epoch 1448 Batch  136/175   train_loss = 1.187\n",
      "Epoch 1448 Batch  168/175   train_loss = 1.216\n",
      "Epoch 1449 Batch   25/175   train_loss = 1.202\n",
      "Epoch 1449 Batch   57/175   train_loss = 1.239\n",
      "Epoch 1449 Batch   89/175   train_loss = 1.207\n",
      "Epoch 1449 Batch  121/175   train_loss = 1.197\n",
      "Epoch 1449 Batch  153/175   train_loss = 1.191\n",
      "Epoch 1450 Batch   10/175   train_loss = 1.157\n",
      "Epoch 1450 Batch   42/175   train_loss = 1.249\n",
      "Epoch 1450 Batch   74/175   train_loss = 1.246\n",
      "Epoch 1450 Batch  106/175   train_loss = 1.243\n",
      "Epoch 1450 Batch  138/175   train_loss = 1.178\n",
      "Epoch 1450 Batch  170/175   train_loss = 1.233\n",
      "Epoch 1451 Batch   27/175   train_loss = 1.186\n",
      "Epoch 1451 Batch   59/175   train_loss = 1.171\n",
      "Epoch 1451 Batch   91/175   train_loss = 1.185\n",
      "Epoch 1451 Batch  123/175   train_loss = 1.194\n",
      "Epoch 1451 Batch  155/175   train_loss = 1.165\n",
      "Epoch 1452 Batch   12/175   train_loss = 1.203\n",
      "Epoch 1452 Batch   44/175   train_loss = 1.209\n",
      "Epoch 1452 Batch   76/175   train_loss = 1.189\n",
      "Epoch 1452 Batch  108/175   train_loss = 1.215\n",
      "Epoch 1452 Batch  140/175   train_loss = 1.191\n",
      "Epoch 1452 Batch  172/175   train_loss = 1.234\n",
      "Epoch 1453 Batch   29/175   train_loss = 1.202\n",
      "Epoch 1453 Batch   61/175   train_loss = 1.237\n",
      "Epoch 1453 Batch   93/175   train_loss = 1.213\n",
      "Epoch 1453 Batch  125/175   train_loss = 1.213\n",
      "Epoch 1453 Batch  157/175   train_loss = 1.194\n",
      "Epoch 1454 Batch   14/175   train_loss = 1.225\n",
      "Epoch 1454 Batch   46/175   train_loss = 1.210\n",
      "Epoch 1454 Batch   78/175   train_loss = 1.183\n",
      "Epoch 1454 Batch  110/175   train_loss = 1.288\n",
      "Epoch 1454 Batch  142/175   train_loss = 1.227\n",
      "Epoch 1454 Batch  174/175   train_loss = 1.179\n",
      "Epoch 1455 Batch   31/175   train_loss = 1.218\n",
      "Epoch 1455 Batch   63/175   train_loss = 1.192\n",
      "Epoch 1455 Batch   95/175   train_loss = 1.181\n",
      "Epoch 1455 Batch  127/175   train_loss = 1.152\n",
      "Epoch 1455 Batch  159/175   train_loss = 1.176\n",
      "Epoch 1456 Batch   16/175   train_loss = 1.185\n",
      "Epoch 1456 Batch   48/175   train_loss = 1.223\n",
      "Epoch 1456 Batch   80/175   train_loss = 1.217\n",
      "Epoch 1456 Batch  112/175   train_loss = 1.192\n",
      "Epoch 1456 Batch  144/175   train_loss = 1.135\n",
      "Epoch 1457 Batch    1/175   train_loss = 1.276\n",
      "Epoch 1457 Batch   33/175   train_loss = 1.265\n",
      "Epoch 1457 Batch   65/175   train_loss = 1.196\n",
      "Epoch 1457 Batch   97/175   train_loss = 1.218\n",
      "Epoch 1457 Batch  129/175   train_loss = 1.185\n",
      "Epoch 1457 Batch  161/175   train_loss = 1.181\n",
      "Epoch 1458 Batch   18/175   train_loss = 1.169\n",
      "Epoch 1458 Batch   50/175   train_loss = 1.183\n",
      "Epoch 1458 Batch   82/175   train_loss = 1.236\n",
      "Epoch 1458 Batch  114/175   train_loss = 1.252\n",
      "Epoch 1458 Batch  146/175   train_loss = 1.204\n",
      "Epoch 1459 Batch    3/175   train_loss = 1.243\n",
      "Epoch 1459 Batch   35/175   train_loss = 1.194\n",
      "Epoch 1459 Batch   67/175   train_loss = 1.168\n",
      "Epoch 1459 Batch   99/175   train_loss = 1.243\n",
      "Epoch 1459 Batch  131/175   train_loss = 1.178\n",
      "Epoch 1459 Batch  163/175   train_loss = 1.202\n",
      "Epoch 1460 Batch   20/175   train_loss = 1.174\n",
      "Epoch 1460 Batch   52/175   train_loss = 1.133\n",
      "Epoch 1460 Batch   84/175   train_loss = 1.202\n",
      "Epoch 1460 Batch  116/175   train_loss = 1.246\n",
      "Epoch 1460 Batch  148/175   train_loss = 1.193\n",
      "Epoch 1461 Batch    5/175   train_loss = 1.191\n",
      "Epoch 1461 Batch   37/175   train_loss = 1.164\n",
      "Epoch 1461 Batch   69/175   train_loss = 1.195\n",
      "Epoch 1461 Batch  101/175   train_loss = 1.250\n",
      "Epoch 1461 Batch  133/175   train_loss = 1.120\n",
      "Epoch 1461 Batch  165/175   train_loss = 1.212\n",
      "Epoch 1462 Batch   22/175   train_loss = 1.135\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1462 Batch   54/175   train_loss = 1.216\n",
      "Epoch 1462 Batch   86/175   train_loss = 1.242\n",
      "Epoch 1462 Batch  118/175   train_loss = 1.251\n",
      "Epoch 1462 Batch  150/175   train_loss = 1.192\n",
      "Epoch 1463 Batch    7/175   train_loss = 1.239\n",
      "Epoch 1463 Batch   39/175   train_loss = 1.147\n",
      "Epoch 1463 Batch   71/175   train_loss = 1.234\n",
      "Epoch 1463 Batch  103/175   train_loss = 1.207\n",
      "Epoch 1463 Batch  135/175   train_loss = 1.165\n",
      "Epoch 1463 Batch  167/175   train_loss = 1.226\n",
      "Epoch 1464 Batch   24/175   train_loss = 1.156\n",
      "Epoch 1464 Batch   56/175   train_loss = 1.223\n",
      "Epoch 1464 Batch   88/175   train_loss = 1.234\n",
      "Epoch 1464 Batch  120/175   train_loss = 1.192\n",
      "Epoch 1464 Batch  152/175   train_loss = 1.169\n",
      "Epoch 1465 Batch    9/175   train_loss = 1.222\n",
      "Epoch 1465 Batch   41/175   train_loss = 1.222\n",
      "Epoch 1465 Batch   73/175   train_loss = 1.215\n",
      "Epoch 1465 Batch  105/175   train_loss = 1.223\n",
      "Epoch 1465 Batch  137/175   train_loss = 1.177\n",
      "Epoch 1465 Batch  169/175   train_loss = 1.205\n",
      "Epoch 1466 Batch   26/175   train_loss = 1.200\n",
      "Epoch 1466 Batch   58/175   train_loss = 1.244\n",
      "Epoch 1466 Batch   90/175   train_loss = 1.200\n",
      "Epoch 1466 Batch  122/175   train_loss = 1.190\n",
      "Epoch 1466 Batch  154/175   train_loss = 1.185\n",
      "Epoch 1467 Batch   11/175   train_loss = 1.177\n",
      "Epoch 1467 Batch   43/175   train_loss = 1.207\n",
      "Epoch 1467 Batch   75/175   train_loss = 1.175\n",
      "Epoch 1467 Batch  107/175   train_loss = 1.246\n",
      "Epoch 1467 Batch  139/175   train_loss = 1.139\n",
      "Epoch 1467 Batch  171/175   train_loss = 1.237\n",
      "Epoch 1468 Batch   28/175   train_loss = 1.187\n",
      "Epoch 1468 Batch   60/175   train_loss = 1.189\n",
      "Epoch 1468 Batch   92/175   train_loss = 1.174\n",
      "Epoch 1468 Batch  124/175   train_loss = 1.192\n",
      "Epoch 1468 Batch  156/175   train_loss = 1.229\n",
      "Epoch 1469 Batch   13/175   train_loss = 1.196\n",
      "Epoch 1469 Batch   45/175   train_loss = 1.168\n",
      "Epoch 1469 Batch   77/175   train_loss = 1.169\n",
      "Epoch 1469 Batch  109/175   train_loss = 1.257\n",
      "Epoch 1469 Batch  141/175   train_loss = 1.146\n",
      "Epoch 1469 Batch  173/175   train_loss = 1.165\n",
      "Epoch 1470 Batch   30/175   train_loss = 1.244\n",
      "Epoch 1470 Batch   62/175   train_loss = 1.210\n",
      "Epoch 1470 Batch   94/175   train_loss = 1.177\n",
      "Epoch 1470 Batch  126/175   train_loss = 1.204\n",
      "Epoch 1470 Batch  158/175   train_loss = 1.187\n",
      "Epoch 1471 Batch   15/175   train_loss = 1.230\n",
      "Epoch 1471 Batch   47/175   train_loss = 1.208\n",
      "Epoch 1471 Batch   79/175   train_loss = 1.233\n",
      "Epoch 1471 Batch  111/175   train_loss = 1.244\n",
      "Epoch 1471 Batch  143/175   train_loss = 1.172\n",
      "Epoch 1472 Batch    0/175   train_loss = 1.197\n",
      "Epoch 1472 Batch   32/175   train_loss = 1.214\n",
      "Epoch 1472 Batch   64/175   train_loss = 1.231\n",
      "Epoch 1472 Batch   96/175   train_loss = 1.233\n",
      "Epoch 1472 Batch  128/175   train_loss = 1.136\n",
      "Epoch 1472 Batch  160/175   train_loss = 1.206\n",
      "Epoch 1473 Batch   17/175   train_loss = 1.168\n",
      "Epoch 1473 Batch   49/175   train_loss = 1.218\n",
      "Epoch 1473 Batch   81/175   train_loss = 1.191\n",
      "Epoch 1473 Batch  113/175   train_loss = 1.227\n",
      "Epoch 1473 Batch  145/175   train_loss = 1.163\n",
      "Epoch 1474 Batch    2/175   train_loss = 1.204\n",
      "Epoch 1474 Batch   34/175   train_loss = 1.227\n",
      "Epoch 1474 Batch   66/175   train_loss = 1.255\n",
      "Epoch 1474 Batch   98/175   train_loss = 1.229\n",
      "Epoch 1474 Batch  130/175   train_loss = 1.222\n",
      "Epoch 1474 Batch  162/175   train_loss = 1.183\n",
      "Epoch 1475 Batch   19/175   train_loss = 1.197\n",
      "Epoch 1475 Batch   51/175   train_loss = 1.155\n",
      "Epoch 1475 Batch   83/175   train_loss = 1.243\n",
      "Epoch 1475 Batch  115/175   train_loss = 1.305\n",
      "Epoch 1475 Batch  147/175   train_loss = 1.183\n",
      "Epoch 1476 Batch    4/175   train_loss = 1.231\n",
      "Epoch 1476 Batch   36/175   train_loss = 1.158\n",
      "Epoch 1476 Batch   68/175   train_loss = 1.205\n",
      "Epoch 1476 Batch  100/175   train_loss = 1.209\n",
      "Epoch 1476 Batch  132/175   train_loss = 1.170\n",
      "Epoch 1476 Batch  164/175   train_loss = 1.166\n",
      "Epoch 1477 Batch   21/175   train_loss = 1.156\n",
      "Epoch 1477 Batch   53/175   train_loss = 1.173\n",
      "Epoch 1477 Batch   85/175   train_loss = 1.221\n",
      "Epoch 1477 Batch  117/175   train_loss = 1.212\n",
      "Epoch 1477 Batch  149/175   train_loss = 1.220\n",
      "Epoch 1478 Batch    6/175   train_loss = 1.214\n",
      "Epoch 1478 Batch   38/175   train_loss = 1.161\n",
      "Epoch 1478 Batch   70/175   train_loss = 1.208\n",
      "Epoch 1478 Batch  102/175   train_loss = 1.209\n",
      "Epoch 1478 Batch  134/175   train_loss = 1.159\n",
      "Epoch 1478 Batch  166/175   train_loss = 1.210\n",
      "Epoch 1479 Batch   23/175   train_loss = 1.167\n",
      "Epoch 1479 Batch   55/175   train_loss = 1.261\n",
      "Epoch 1479 Batch   87/175   train_loss = 1.277\n",
      "Epoch 1479 Batch  119/175   train_loss = 1.190\n",
      "Epoch 1479 Batch  151/175   train_loss = 1.210\n",
      "Epoch 1480 Batch    8/175   train_loss = 1.204\n",
      "Epoch 1480 Batch   40/175   train_loss = 1.188\n",
      "Epoch 1480 Batch   72/175   train_loss = 1.225\n",
      "Epoch 1480 Batch  104/175   train_loss = 1.216\n",
      "Epoch 1480 Batch  136/175   train_loss = 1.177\n",
      "Epoch 1480 Batch  168/175   train_loss = 1.221\n",
      "Epoch 1481 Batch   25/175   train_loss = 1.205\n",
      "Epoch 1481 Batch   57/175   train_loss = 1.237\n",
      "Epoch 1481 Batch   89/175   train_loss = 1.206\n",
      "Epoch 1481 Batch  121/175   train_loss = 1.168\n",
      "Epoch 1481 Batch  153/175   train_loss = 1.185\n",
      "Epoch 1482 Batch   10/175   train_loss = 1.160\n",
      "Epoch 1482 Batch   42/175   train_loss = 1.230\n",
      "Epoch 1482 Batch   74/175   train_loss = 1.237\n",
      "Epoch 1482 Batch  106/175   train_loss = 1.250\n",
      "Epoch 1482 Batch  138/175   train_loss = 1.202\n",
      "Epoch 1482 Batch  170/175   train_loss = 1.236\n",
      "Epoch 1483 Batch   27/175   train_loss = 1.195\n",
      "Epoch 1483 Batch   59/175   train_loss = 1.175\n",
      "Epoch 1483 Batch   91/175   train_loss = 1.181\n",
      "Epoch 1483 Batch  123/175   train_loss = 1.195\n",
      "Epoch 1483 Batch  155/175   train_loss = 1.162\n",
      "Epoch 1484 Batch   12/175   train_loss = 1.193\n",
      "Epoch 1484 Batch   44/175   train_loss = 1.172\n",
      "Epoch 1484 Batch   76/175   train_loss = 1.175\n",
      "Epoch 1484 Batch  108/175   train_loss = 1.195\n",
      "Epoch 1484 Batch  140/175   train_loss = 1.178\n",
      "Epoch 1484 Batch  172/175   train_loss = 1.229\n",
      "Epoch 1485 Batch   29/175   train_loss = 1.204\n",
      "Epoch 1485 Batch   61/175   train_loss = 1.223\n",
      "Epoch 1485 Batch   93/175   train_loss = 1.201\n",
      "Epoch 1485 Batch  125/175   train_loss = 1.210\n",
      "Epoch 1485 Batch  157/175   train_loss = 1.199\n",
      "Epoch 1486 Batch   14/175   train_loss = 1.213\n",
      "Epoch 1486 Batch   46/175   train_loss = 1.195\n",
      "Epoch 1486 Batch   78/175   train_loss = 1.191\n",
      "Epoch 1486 Batch  110/175   train_loss = 1.284\n",
      "Epoch 1486 Batch  142/175   train_loss = 1.216\n",
      "Epoch 1486 Batch  174/175   train_loss = 1.171\n",
      "Epoch 1487 Batch   31/175   train_loss = 1.215\n",
      "Epoch 1487 Batch   63/175   train_loss = 1.213\n",
      "Epoch 1487 Batch   95/175   train_loss = 1.176\n",
      "Epoch 1487 Batch  127/175   train_loss = 1.159\n",
      "Epoch 1487 Batch  159/175   train_loss = 1.175\n",
      "Epoch 1488 Batch   16/175   train_loss = 1.183\n",
      "Epoch 1488 Batch   48/175   train_loss = 1.211\n",
      "Epoch 1488 Batch   80/175   train_loss = 1.216\n",
      "Epoch 1488 Batch  112/175   train_loss = 1.178\n",
      "Epoch 1488 Batch  144/175   train_loss = 1.110\n",
      "Epoch 1489 Batch    1/175   train_loss = 1.253\n",
      "Epoch 1489 Batch   33/175   train_loss = 1.251\n",
      "Epoch 1489 Batch   65/175   train_loss = 1.181\n",
      "Epoch 1489 Batch   97/175   train_loss = 1.195\n",
      "Epoch 1489 Batch  129/175   train_loss = 1.185\n",
      "Epoch 1489 Batch  161/175   train_loss = 1.186\n",
      "Epoch 1490 Batch   18/175   train_loss = 1.166\n",
      "Epoch 1490 Batch   50/175   train_loss = 1.182\n",
      "Epoch 1490 Batch   82/175   train_loss = 1.245\n",
      "Epoch 1490 Batch  114/175   train_loss = 1.247\n",
      "Epoch 1490 Batch  146/175   train_loss = 1.185\n",
      "Epoch 1491 Batch    3/175   train_loss = 1.249\n",
      "Epoch 1491 Batch   35/175   train_loss = 1.205\n",
      "Epoch 1491 Batch   67/175   train_loss = 1.167\n",
      "Epoch 1491 Batch   99/175   train_loss = 1.239\n",
      "Epoch 1491 Batch  131/175   train_loss = 1.188\n",
      "Epoch 1491 Batch  163/175   train_loss = 1.209\n",
      "Epoch 1492 Batch   20/175   train_loss = 1.160\n",
      "Epoch 1492 Batch   52/175   train_loss = 1.137\n",
      "Epoch 1492 Batch   84/175   train_loss = 1.199\n",
      "Epoch 1492 Batch  116/175   train_loss = 1.243\n",
      "Epoch 1492 Batch  148/175   train_loss = 1.205\n",
      "Epoch 1493 Batch    5/175   train_loss = 1.178\n",
      "Epoch 1493 Batch   37/175   train_loss = 1.164\n",
      "Epoch 1493 Batch   69/175   train_loss = 1.207\n",
      "Epoch 1493 Batch  101/175   train_loss = 1.228\n",
      "Epoch 1493 Batch  133/175   train_loss = 1.102\n",
      "Epoch 1493 Batch  165/175   train_loss = 1.192\n",
      "Epoch 1494 Batch   22/175   train_loss = 1.140\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1494 Batch   54/175   train_loss = 1.226\n",
      "Epoch 1494 Batch   86/175   train_loss = 1.250\n",
      "Epoch 1494 Batch  118/175   train_loss = 1.247\n",
      "Epoch 1494 Batch  150/175   train_loss = 1.194\n",
      "Epoch 1495 Batch    7/175   train_loss = 1.220\n",
      "Epoch 1495 Batch   39/175   train_loss = 1.149\n",
      "Epoch 1495 Batch   71/175   train_loss = 1.223\n",
      "Epoch 1495 Batch  103/175   train_loss = 1.185\n",
      "Epoch 1495 Batch  135/175   train_loss = 1.161\n",
      "Epoch 1495 Batch  167/175   train_loss = 1.227\n",
      "Epoch 1496 Batch   24/175   train_loss = 1.156\n",
      "Epoch 1496 Batch   56/175   train_loss = 1.219\n",
      "Epoch 1496 Batch   88/175   train_loss = 1.264\n",
      "Epoch 1496 Batch  120/175   train_loss = 1.190\n",
      "Epoch 1496 Batch  152/175   train_loss = 1.177\n",
      "Epoch 1497 Batch    9/175   train_loss = 1.223\n",
      "Epoch 1497 Batch   41/175   train_loss = 1.222\n",
      "Epoch 1497 Batch   73/175   train_loss = 1.209\n",
      "Epoch 1497 Batch  105/175   train_loss = 1.219\n",
      "Epoch 1497 Batch  137/175   train_loss = 1.169\n",
      "Epoch 1497 Batch  169/175   train_loss = 1.204\n",
      "Epoch 1498 Batch   26/175   train_loss = 1.182\n",
      "Epoch 1498 Batch   58/175   train_loss = 1.233\n",
      "Epoch 1498 Batch   90/175   train_loss = 1.180\n",
      "Epoch 1498 Batch  122/175   train_loss = 1.156\n",
      "Epoch 1498 Batch  154/175   train_loss = 1.176\n",
      "Epoch 1499 Batch   11/175   train_loss = 1.159\n",
      "Epoch 1499 Batch   43/175   train_loss = 1.172\n",
      "Epoch 1499 Batch   75/175   train_loss = 1.169\n",
      "Epoch 1499 Batch  107/175   train_loss = 1.238\n",
      "Epoch 1499 Batch  139/175   train_loss = 1.143\n",
      "Epoch 1499 Batch  171/175   train_loss = 1.247\n",
      "Epoch 1500 Batch   28/175   train_loss = 1.170\n",
      "Epoch 1500 Batch   60/175   train_loss = 1.182\n",
      "Epoch 1500 Batch   92/175   train_loss = 1.153\n",
      "Epoch 1500 Batch  124/175   train_loss = 1.185\n",
      "Epoch 1500 Batch  156/175   train_loss = 1.234\n",
      "Epoch 1501 Batch   13/175   train_loss = 1.207\n",
      "Epoch 1501 Batch   45/175   train_loss = 1.184\n",
      "Epoch 1501 Batch   77/175   train_loss = 1.177\n",
      "Epoch 1501 Batch  109/175   train_loss = 1.233\n",
      "Epoch 1501 Batch  141/175   train_loss = 1.135\n",
      "Epoch 1501 Batch  173/175   train_loss = 1.160\n",
      "Epoch 1502 Batch   30/175   train_loss = 1.226\n",
      "Epoch 1502 Batch   62/175   train_loss = 1.213\n",
      "Epoch 1502 Batch   94/175   train_loss = 1.170\n",
      "Epoch 1502 Batch  126/175   train_loss = 1.203\n",
      "Epoch 1502 Batch  158/175   train_loss = 1.183\n",
      "Epoch 1503 Batch   15/175   train_loss = 1.236\n",
      "Epoch 1503 Batch   47/175   train_loss = 1.222\n",
      "Epoch 1503 Batch   79/175   train_loss = 1.236\n",
      "Epoch 1503 Batch  111/175   train_loss = 1.249\n",
      "Epoch 1503 Batch  143/175   train_loss = 1.186\n",
      "Epoch 1504 Batch    0/175   train_loss = 1.206\n",
      "Epoch 1504 Batch   32/175   train_loss = 1.229\n",
      "Epoch 1504 Batch   64/175   train_loss = 1.250\n",
      "Epoch 1504 Batch   96/175   train_loss = 1.252\n",
      "Epoch 1504 Batch  128/175   train_loss = 1.135\n",
      "Epoch 1504 Batch  160/175   train_loss = 1.199\n",
      "Epoch 1505 Batch   17/175   train_loss = 1.168\n",
      "Epoch 1505 Batch   49/175   train_loss = 1.227\n",
      "Epoch 1505 Batch   81/175   train_loss = 1.190\n",
      "Epoch 1505 Batch  113/175   train_loss = 1.219\n",
      "Epoch 1505 Batch  145/175   train_loss = 1.156\n",
      "Epoch 1506 Batch    2/175   train_loss = 1.208\n",
      "Epoch 1506 Batch   34/175   train_loss = 1.210\n",
      "Epoch 1506 Batch   66/175   train_loss = 1.245\n",
      "Epoch 1506 Batch   98/175   train_loss = 1.227\n",
      "Epoch 1506 Batch  130/175   train_loss = 1.246\n",
      "Epoch 1506 Batch  162/175   train_loss = 1.200\n",
      "Epoch 1507 Batch   19/175   train_loss = 1.198\n",
      "Epoch 1507 Batch   51/175   train_loss = 1.147\n",
      "Epoch 1507 Batch   83/175   train_loss = 1.247\n",
      "Epoch 1507 Batch  115/175   train_loss = 1.293\n",
      "Epoch 1507 Batch  147/175   train_loss = 1.168\n",
      "Epoch 1508 Batch    4/175   train_loss = 1.236\n",
      "Epoch 1508 Batch   36/175   train_loss = 1.166\n",
      "Epoch 1508 Batch   68/175   train_loss = 1.181\n",
      "Epoch 1508 Batch  100/175   train_loss = 1.212\n",
      "Epoch 1508 Batch  132/175   train_loss = 1.172\n",
      "Epoch 1508 Batch  164/175   train_loss = 1.183\n",
      "Epoch 1509 Batch   21/175   train_loss = 1.166\n",
      "Epoch 1509 Batch   53/175   train_loss = 1.172\n",
      "Epoch 1509 Batch   85/175   train_loss = 1.222\n",
      "Epoch 1509 Batch  117/175   train_loss = 1.209\n",
      "Epoch 1509 Batch  149/175   train_loss = 1.217\n",
      "Epoch 1510 Batch    6/175   train_loss = 1.215\n",
      "Epoch 1510 Batch   38/175   train_loss = 1.157\n",
      "Epoch 1510 Batch   70/175   train_loss = 1.205\n",
      "Epoch 1510 Batch  102/175   train_loss = 1.200\n",
      "Epoch 1510 Batch  134/175   train_loss = 1.156\n",
      "Epoch 1510 Batch  166/175   train_loss = 1.208\n",
      "Epoch 1511 Batch   23/175   train_loss = 1.130\n",
      "Epoch 1511 Batch   55/175   train_loss = 1.253\n",
      "Epoch 1511 Batch   87/175   train_loss = 1.260\n",
      "Epoch 1511 Batch  119/175   train_loss = 1.182\n",
      "Epoch 1511 Batch  151/175   train_loss = 1.230\n",
      "Epoch 1512 Batch    8/175   train_loss = 1.209\n",
      "Epoch 1512 Batch   40/175   train_loss = 1.193\n",
      "Epoch 1512 Batch   72/175   train_loss = 1.218\n",
      "Epoch 1512 Batch  104/175   train_loss = 1.217\n",
      "Epoch 1512 Batch  136/175   train_loss = 1.157\n",
      "Epoch 1512 Batch  168/175   train_loss = 1.230\n",
      "Epoch 1513 Batch   25/175   train_loss = 1.213\n",
      "Epoch 1513 Batch   57/175   train_loss = 1.241\n",
      "Epoch 1513 Batch   89/175   train_loss = 1.205\n",
      "Epoch 1513 Batch  121/175   train_loss = 1.160\n",
      "Epoch 1513 Batch  153/175   train_loss = 1.193\n",
      "Epoch 1514 Batch   10/175   train_loss = 1.144\n",
      "Epoch 1514 Batch   42/175   train_loss = 1.240\n",
      "Epoch 1514 Batch   74/175   train_loss = 1.225\n",
      "Epoch 1514 Batch  106/175   train_loss = 1.229\n",
      "Epoch 1514 Batch  138/175   train_loss = 1.187\n",
      "Epoch 1514 Batch  170/175   train_loss = 1.217\n",
      "Epoch 1515 Batch   27/175   train_loss = 1.212\n",
      "Epoch 1515 Batch   59/175   train_loss = 1.187\n",
      "Epoch 1515 Batch   91/175   train_loss = 1.179\n",
      "Epoch 1515 Batch  123/175   train_loss = 1.186\n",
      "Epoch 1515 Batch  155/175   train_loss = 1.169\n",
      "Epoch 1516 Batch   12/175   train_loss = 1.183\n",
      "Epoch 1516 Batch   44/175   train_loss = 1.173\n",
      "Epoch 1516 Batch   76/175   train_loss = 1.172\n",
      "Epoch 1516 Batch  108/175   train_loss = 1.193\n",
      "Epoch 1516 Batch  140/175   train_loss = 1.181\n",
      "Epoch 1516 Batch  172/175   train_loss = 1.229\n",
      "Epoch 1517 Batch   29/175   train_loss = 1.163\n",
      "Epoch 1517 Batch   61/175   train_loss = 1.227\n",
      "Epoch 1517 Batch   93/175   train_loss = 1.183\n",
      "Epoch 1517 Batch  125/175   train_loss = 1.209\n",
      "Epoch 1517 Batch  157/175   train_loss = 1.192\n",
      "Epoch 1518 Batch   14/175   train_loss = 1.234\n",
      "Epoch 1518 Batch   46/175   train_loss = 1.191\n",
      "Epoch 1518 Batch   78/175   train_loss = 1.188\n",
      "Epoch 1518 Batch  110/175   train_loss = 1.268\n",
      "Epoch 1518 Batch  142/175   train_loss = 1.199\n",
      "Epoch 1518 Batch  174/175   train_loss = 1.185\n",
      "Epoch 1519 Batch   31/175   train_loss = 1.210\n",
      "Epoch 1519 Batch   63/175   train_loss = 1.212\n",
      "Epoch 1519 Batch   95/175   train_loss = 1.166\n",
      "Epoch 1519 Batch  127/175   train_loss = 1.166\n",
      "Epoch 1519 Batch  159/175   train_loss = 1.174\n",
      "Epoch 1520 Batch   16/175   train_loss = 1.167\n",
      "Epoch 1520 Batch   48/175   train_loss = 1.220\n",
      "Epoch 1520 Batch   80/175   train_loss = 1.208\n",
      "Epoch 1520 Batch  112/175   train_loss = 1.196\n",
      "Epoch 1520 Batch  144/175   train_loss = 1.135\n",
      "Epoch 1521 Batch    1/175   train_loss = 1.254\n",
      "Epoch 1521 Batch   33/175   train_loss = 1.256\n",
      "Epoch 1521 Batch   65/175   train_loss = 1.202\n",
      "Epoch 1521 Batch   97/175   train_loss = 1.195\n",
      "Epoch 1521 Batch  129/175   train_loss = 1.196\n",
      "Epoch 1521 Batch  161/175   train_loss = 1.176\n",
      "Epoch 1522 Batch   18/175   train_loss = 1.162\n",
      "Epoch 1522 Batch   50/175   train_loss = 1.179\n",
      "Epoch 1522 Batch   82/175   train_loss = 1.230\n",
      "Epoch 1522 Batch  114/175   train_loss = 1.241\n",
      "Epoch 1522 Batch  146/175   train_loss = 1.190\n",
      "Epoch 1523 Batch    3/175   train_loss = 1.232\n",
      "Epoch 1523 Batch   35/175   train_loss = 1.206\n",
      "Epoch 1523 Batch   67/175   train_loss = 1.180\n",
      "Epoch 1523 Batch   99/175   train_loss = 1.276\n",
      "Epoch 1523 Batch  131/175   train_loss = 1.192\n",
      "Epoch 1523 Batch  163/175   train_loss = 1.217\n",
      "Epoch 1524 Batch   20/175   train_loss = 1.171\n",
      "Epoch 1524 Batch   52/175   train_loss = 1.146\n",
      "Epoch 1524 Batch   84/175   train_loss = 1.247\n",
      "Epoch 1524 Batch  116/175   train_loss = 1.259\n",
      "Epoch 1524 Batch  148/175   train_loss = 1.192\n",
      "Epoch 1525 Batch    5/175   train_loss = 1.186\n",
      "Epoch 1525 Batch   37/175   train_loss = 1.188\n",
      "Epoch 1525 Batch   69/175   train_loss = 1.234\n",
      "Epoch 1525 Batch  101/175   train_loss = 1.251\n",
      "Epoch 1525 Batch  133/175   train_loss = 1.142\n",
      "Epoch 1525 Batch  165/175   train_loss = 1.210\n",
      "Epoch 1526 Batch   22/175   train_loss = 1.140\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1526 Batch   54/175   train_loss = 1.233\n",
      "Epoch 1526 Batch   86/175   train_loss = 1.271\n",
      "Epoch 1526 Batch  118/175   train_loss = 1.238\n",
      "Epoch 1526 Batch  150/175   train_loss = 1.212\n",
      "Epoch 1527 Batch    7/175   train_loss = 1.217\n",
      "Epoch 1527 Batch   39/175   train_loss = 1.165\n",
      "Epoch 1527 Batch   71/175   train_loss = 1.228\n",
      "Epoch 1527 Batch  103/175   train_loss = 1.202\n",
      "Epoch 1527 Batch  135/175   train_loss = 1.168\n",
      "Epoch 1527 Batch  167/175   train_loss = 1.262\n",
      "Epoch 1528 Batch   24/175   train_loss = 1.154\n",
      "Epoch 1528 Batch   56/175   train_loss = 1.232\n",
      "Epoch 1528 Batch   88/175   train_loss = 1.258\n",
      "Epoch 1528 Batch  120/175   train_loss = 1.205\n",
      "Epoch 1528 Batch  152/175   train_loss = 1.189\n",
      "Epoch 1529 Batch    9/175   train_loss = 1.231\n",
      "Epoch 1529 Batch   41/175   train_loss = 1.233\n",
      "Epoch 1529 Batch   73/175   train_loss = 1.223\n",
      "Epoch 1529 Batch  105/175   train_loss = 1.236\n",
      "Epoch 1529 Batch  137/175   train_loss = 1.180\n",
      "Epoch 1529 Batch  169/175   train_loss = 1.216\n",
      "Epoch 1530 Batch   26/175   train_loss = 1.192\n",
      "Epoch 1530 Batch   58/175   train_loss = 1.238\n",
      "Epoch 1530 Batch   90/175   train_loss = 1.213\n",
      "Epoch 1530 Batch  122/175   train_loss = 1.185\n",
      "Epoch 1530 Batch  154/175   train_loss = 1.192\n",
      "Epoch 1531 Batch   11/175   train_loss = 1.168\n",
      "Epoch 1531 Batch   43/175   train_loss = 1.213\n",
      "Epoch 1531 Batch   75/175   train_loss = 1.164\n",
      "Epoch 1531 Batch  107/175   train_loss = 1.257\n",
      "Epoch 1531 Batch  139/175   train_loss = 1.149\n",
      "Epoch 1531 Batch  171/175   train_loss = 1.244\n",
      "Epoch 1532 Batch   28/175   train_loss = 1.181\n",
      "Epoch 1532 Batch   60/175   train_loss = 1.175\n",
      "Epoch 1532 Batch   92/175   train_loss = 1.154\n",
      "Epoch 1532 Batch  124/175   train_loss = 1.178\n",
      "Epoch 1532 Batch  156/175   train_loss = 1.223\n",
      "Epoch 1533 Batch   13/175   train_loss = 1.217\n",
      "Epoch 1533 Batch   45/175   train_loss = 1.188\n",
      "Epoch 1533 Batch   77/175   train_loss = 1.207\n",
      "Epoch 1533 Batch  109/175   train_loss = 1.244\n",
      "Epoch 1533 Batch  141/175   train_loss = 1.165\n",
      "Epoch 1533 Batch  173/175   train_loss = 1.161\n",
      "Epoch 1534 Batch   30/175   train_loss = 1.251\n",
      "Epoch 1534 Batch   62/175   train_loss = 1.230\n",
      "Epoch 1534 Batch   94/175   train_loss = 1.191\n",
      "Epoch 1534 Batch  126/175   train_loss = 1.213\n",
      "Epoch 1534 Batch  158/175   train_loss = 1.180\n",
      "Epoch 1535 Batch   15/175   train_loss = 1.207\n",
      "Epoch 1535 Batch   47/175   train_loss = 1.221\n",
      "Epoch 1535 Batch   79/175   train_loss = 1.228\n",
      "Epoch 1535 Batch  111/175   train_loss = 1.237\n",
      "Epoch 1535 Batch  143/175   train_loss = 1.165\n",
      "Epoch 1536 Batch    0/175   train_loss = 1.203\n",
      "Epoch 1536 Batch   32/175   train_loss = 1.212\n",
      "Epoch 1536 Batch   64/175   train_loss = 1.237\n",
      "Epoch 1536 Batch   96/175   train_loss = 1.224\n",
      "Epoch 1536 Batch  128/175   train_loss = 1.141\n",
      "Epoch 1536 Batch  160/175   train_loss = 1.214\n",
      "Epoch 1537 Batch   17/175   train_loss = 1.160\n",
      "Epoch 1537 Batch   49/175   train_loss = 1.236\n",
      "Epoch 1537 Batch   81/175   train_loss = 1.191\n",
      "Epoch 1537 Batch  113/175   train_loss = 1.206\n",
      "Epoch 1537 Batch  145/175   train_loss = 1.148\n",
      "Epoch 1538 Batch    2/175   train_loss = 1.198\n",
      "Epoch 1538 Batch   34/175   train_loss = 1.237\n",
      "Epoch 1538 Batch   66/175   train_loss = 1.247\n",
      "Epoch 1538 Batch   98/175   train_loss = 1.236\n",
      "Epoch 1538 Batch  130/175   train_loss = 1.230\n",
      "Epoch 1538 Batch  162/175   train_loss = 1.192\n",
      "Epoch 1539 Batch   19/175   train_loss = 1.193\n",
      "Epoch 1539 Batch   51/175   train_loss = 1.151\n",
      "Epoch 1539 Batch   83/175   train_loss = 1.244\n",
      "Epoch 1539 Batch  115/175   train_loss = 1.301\n",
      "Epoch 1539 Batch  147/175   train_loss = 1.160\n",
      "Epoch 1540 Batch    4/175   train_loss = 1.205\n",
      "Epoch 1540 Batch   36/175   train_loss = 1.164\n",
      "Epoch 1540 Batch   68/175   train_loss = 1.176\n",
      "Epoch 1540 Batch  100/175   train_loss = 1.205\n",
      "Epoch 1540 Batch  132/175   train_loss = 1.179\n",
      "Epoch 1540 Batch  164/175   train_loss = 1.166\n",
      "Epoch 1541 Batch   21/175   train_loss = 1.163\n",
      "Epoch 1541 Batch   53/175   train_loss = 1.144\n",
      "Epoch 1541 Batch   85/175   train_loss = 1.201\n",
      "Epoch 1541 Batch  117/175   train_loss = 1.211\n",
      "Epoch 1541 Batch  149/175   train_loss = 1.198\n",
      "Epoch 1542 Batch    6/175   train_loss = 1.201\n",
      "Epoch 1542 Batch   38/175   train_loss = 1.168\n",
      "Epoch 1542 Batch   70/175   train_loss = 1.187\n",
      "Epoch 1542 Batch  102/175   train_loss = 1.231\n",
      "Epoch 1542 Batch  134/175   train_loss = 1.157\n",
      "Epoch 1542 Batch  166/175   train_loss = 1.223\n",
      "Epoch 1543 Batch   23/175   train_loss = 1.162\n",
      "Epoch 1543 Batch   55/175   train_loss = 1.246\n",
      "Epoch 1543 Batch   87/175   train_loss = 1.256\n",
      "Epoch 1543 Batch  119/175   train_loss = 1.193\n",
      "Epoch 1543 Batch  151/175   train_loss = 1.214\n",
      "Epoch 1544 Batch    8/175   train_loss = 1.215\n",
      "Epoch 1544 Batch   40/175   train_loss = 1.183\n",
      "Epoch 1544 Batch   72/175   train_loss = 1.239\n",
      "Epoch 1544 Batch  104/175   train_loss = 1.235\n",
      "Epoch 1544 Batch  136/175   train_loss = 1.197\n",
      "Epoch 1544 Batch  168/175   train_loss = 1.230\n",
      "Epoch 1545 Batch   25/175   train_loss = 1.215\n",
      "Epoch 1545 Batch   57/175   train_loss = 1.249\n",
      "Epoch 1545 Batch   89/175   train_loss = 1.217\n",
      "Epoch 1545 Batch  121/175   train_loss = 1.179\n",
      "Epoch 1545 Batch  153/175   train_loss = 1.203\n",
      "Epoch 1546 Batch   10/175   train_loss = 1.159\n",
      "Epoch 1546 Batch   42/175   train_loss = 1.239\n",
      "Epoch 1546 Batch   74/175   train_loss = 1.257\n",
      "Epoch 1546 Batch  106/175   train_loss = 1.256\n",
      "Epoch 1546 Batch  138/175   train_loss = 1.204\n",
      "Epoch 1546 Batch  170/175   train_loss = 1.240\n",
      "Epoch 1547 Batch   27/175   train_loss = 1.186\n",
      "Epoch 1547 Batch   59/175   train_loss = 1.184\n",
      "Epoch 1547 Batch   91/175   train_loss = 1.189\n",
      "Epoch 1547 Batch  123/175   train_loss = 1.161\n",
      "Epoch 1547 Batch  155/175   train_loss = 1.162\n",
      "Epoch 1548 Batch   12/175   train_loss = 1.199\n",
      "Epoch 1548 Batch   44/175   train_loss = 1.173\n",
      "Epoch 1548 Batch   76/175   train_loss = 1.185\n",
      "Epoch 1548 Batch  108/175   train_loss = 1.207\n",
      "Epoch 1548 Batch  140/175   train_loss = 1.185\n",
      "Epoch 1548 Batch  172/175   train_loss = 1.205\n",
      "Epoch 1549 Batch   29/175   train_loss = 1.171\n",
      "Epoch 1549 Batch   61/175   train_loss = 1.215\n",
      "Epoch 1549 Batch   93/175   train_loss = 1.194\n",
      "Epoch 1549 Batch  125/175   train_loss = 1.201\n",
      "Epoch 1549 Batch  157/175   train_loss = 1.199\n",
      "Epoch 1550 Batch   14/175   train_loss = 1.224\n",
      "Epoch 1550 Batch   46/175   train_loss = 1.198\n",
      "Epoch 1550 Batch   78/175   train_loss = 1.162\n",
      "Epoch 1550 Batch  110/175   train_loss = 1.276\n",
      "Epoch 1550 Batch  142/175   train_loss = 1.225\n",
      "Epoch 1550 Batch  174/175   train_loss = 1.187\n",
      "Epoch 1551 Batch   31/175   train_loss = 1.208\n",
      "Epoch 1551 Batch   63/175   train_loss = 1.199\n",
      "Epoch 1551 Batch   95/175   train_loss = 1.159\n",
      "Epoch 1551 Batch  127/175   train_loss = 1.161\n",
      "Epoch 1551 Batch  159/175   train_loss = 1.161\n",
      "Epoch 1552 Batch   16/175   train_loss = 1.195\n",
      "Epoch 1552 Batch   48/175   train_loss = 1.216\n",
      "Epoch 1552 Batch   80/175   train_loss = 1.214\n",
      "Epoch 1552 Batch  112/175   train_loss = 1.173\n",
      "Epoch 1552 Batch  144/175   train_loss = 1.132\n",
      "Epoch 1553 Batch    1/175   train_loss = 1.226\n",
      "Epoch 1553 Batch   33/175   train_loss = 1.242\n",
      "Epoch 1553 Batch   65/175   train_loss = 1.216\n",
      "Epoch 1553 Batch   97/175   train_loss = 1.222\n",
      "Epoch 1553 Batch  129/175   train_loss = 1.193\n",
      "Epoch 1553 Batch  161/175   train_loss = 1.206\n",
      "Epoch 1554 Batch   18/175   train_loss = 1.185\n",
      "Epoch 1554 Batch   50/175   train_loss = 1.186\n",
      "Epoch 1554 Batch   82/175   train_loss = 1.261\n",
      "Epoch 1554 Batch  114/175   train_loss = 1.257\n",
      "Epoch 1554 Batch  146/175   train_loss = 1.202\n",
      "Epoch 1555 Batch    3/175   train_loss = 1.233\n",
      "Epoch 1555 Batch   35/175   train_loss = 1.199\n",
      "Epoch 1555 Batch   67/175   train_loss = 1.156\n",
      "Epoch 1555 Batch   99/175   train_loss = 1.243\n",
      "Epoch 1555 Batch  131/175   train_loss = 1.204\n",
      "Epoch 1555 Batch  163/175   train_loss = 1.210\n",
      "Epoch 1556 Batch   20/175   train_loss = 1.168\n",
      "Epoch 1556 Batch   52/175   train_loss = 1.142\n",
      "Epoch 1556 Batch   84/175   train_loss = 1.181\n",
      "Epoch 1556 Batch  116/175   train_loss = 1.236\n",
      "Epoch 1556 Batch  148/175   train_loss = 1.188\n",
      "Epoch 1557 Batch    5/175   train_loss = 1.190\n",
      "Epoch 1557 Batch   37/175   train_loss = 1.170\n",
      "Epoch 1557 Batch   69/175   train_loss = 1.205\n",
      "Epoch 1557 Batch  101/175   train_loss = 1.253\n",
      "Epoch 1557 Batch  133/175   train_loss = 1.096\n",
      "Epoch 1557 Batch  165/175   train_loss = 1.211\n",
      "Epoch 1558 Batch   22/175   train_loss = 1.130\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1558 Batch   54/175   train_loss = 1.229\n",
      "Epoch 1558 Batch   86/175   train_loss = 1.270\n",
      "Epoch 1558 Batch  118/175   train_loss = 1.249\n",
      "Epoch 1558 Batch  150/175   train_loss = 1.198\n",
      "Epoch 1559 Batch    7/175   train_loss = 1.212\n",
      "Epoch 1559 Batch   39/175   train_loss = 1.134\n",
      "Epoch 1559 Batch   71/175   train_loss = 1.212\n",
      "Epoch 1559 Batch  103/175   train_loss = 1.209\n",
      "Epoch 1559 Batch  135/175   train_loss = 1.176\n",
      "Epoch 1559 Batch  167/175   train_loss = 1.258\n",
      "Epoch 1560 Batch   24/175   train_loss = 1.145\n",
      "Epoch 1560 Batch   56/175   train_loss = 1.215\n",
      "Epoch 1560 Batch   88/175   train_loss = 1.233\n",
      "Epoch 1560 Batch  120/175   train_loss = 1.195\n",
      "Epoch 1560 Batch  152/175   train_loss = 1.164\n",
      "Epoch 1561 Batch    9/175   train_loss = 1.214\n",
      "Epoch 1561 Batch   41/175   train_loss = 1.218\n",
      "Epoch 1561 Batch   73/175   train_loss = 1.215\n",
      "Epoch 1561 Batch  105/175   train_loss = 1.226\n",
      "Epoch 1561 Batch  137/175   train_loss = 1.161\n",
      "Epoch 1561 Batch  169/175   train_loss = 1.220\n",
      "Epoch 1562 Batch   26/175   train_loss = 1.208\n",
      "Epoch 1562 Batch   58/175   train_loss = 1.246\n",
      "Epoch 1562 Batch   90/175   train_loss = 1.209\n",
      "Epoch 1562 Batch  122/175   train_loss = 1.184\n",
      "Epoch 1562 Batch  154/175   train_loss = 1.171\n",
      "Epoch 1563 Batch   11/175   train_loss = 1.173\n",
      "Epoch 1563 Batch   43/175   train_loss = 1.181\n",
      "Epoch 1563 Batch   75/175   train_loss = 1.175\n",
      "Epoch 1563 Batch  107/175   train_loss = 1.238\n",
      "Epoch 1563 Batch  139/175   train_loss = 1.151\n",
      "Epoch 1563 Batch  171/175   train_loss = 1.238\n",
      "Epoch 1564 Batch   28/175   train_loss = 1.182\n",
      "Epoch 1564 Batch   60/175   train_loss = 1.190\n",
      "Epoch 1564 Batch   92/175   train_loss = 1.175\n",
      "Epoch 1564 Batch  124/175   train_loss = 1.177\n",
      "Epoch 1564 Batch  156/175   train_loss = 1.243\n",
      "Epoch 1565 Batch   13/175   train_loss = 1.205\n",
      "Epoch 1565 Batch   45/175   train_loss = 1.191\n",
      "Epoch 1565 Batch   77/175   train_loss = 1.180\n",
      "Epoch 1565 Batch  109/175   train_loss = 1.237\n",
      "Epoch 1565 Batch  141/175   train_loss = 1.147\n",
      "Epoch 1565 Batch  173/175   train_loss = 1.160\n",
      "Epoch 1566 Batch   30/175   train_loss = 1.257\n",
      "Epoch 1566 Batch   62/175   train_loss = 1.209\n",
      "Epoch 1566 Batch   94/175   train_loss = 1.207\n",
      "Epoch 1566 Batch  126/175   train_loss = 1.215\n",
      "Epoch 1566 Batch  158/175   train_loss = 1.193\n",
      "Epoch 1567 Batch   15/175   train_loss = 1.242\n",
      "Epoch 1567 Batch   47/175   train_loss = 1.219\n",
      "Epoch 1567 Batch   79/175   train_loss = 1.231\n",
      "Epoch 1567 Batch  111/175   train_loss = 1.251\n",
      "Epoch 1567 Batch  143/175   train_loss = 1.176\n",
      "Epoch 1568 Batch    0/175   train_loss = 1.204\n",
      "Epoch 1568 Batch   32/175   train_loss = 1.209\n",
      "Epoch 1568 Batch   64/175   train_loss = 1.254\n",
      "Epoch 1568 Batch   96/175   train_loss = 1.245\n",
      "Epoch 1568 Batch  128/175   train_loss = 1.145\n",
      "Epoch 1568 Batch  160/175   train_loss = 1.192\n",
      "Epoch 1569 Batch   17/175   train_loss = 1.174\n",
      "Epoch 1569 Batch   49/175   train_loss = 1.212\n",
      "Epoch 1569 Batch   81/175   train_loss = 1.170\n",
      "Epoch 1569 Batch  113/175   train_loss = 1.220\n",
      "Epoch 1569 Batch  145/175   train_loss = 1.148\n",
      "Epoch 1570 Batch    2/175   train_loss = 1.184\n",
      "Epoch 1570 Batch   34/175   train_loss = 1.215\n",
      "Epoch 1570 Batch   66/175   train_loss = 1.242\n",
      "Epoch 1570 Batch   98/175   train_loss = 1.204\n",
      "Epoch 1570 Batch  130/175   train_loss = 1.218\n",
      "Epoch 1570 Batch  162/175   train_loss = 1.191\n",
      "Epoch 1571 Batch   19/175   train_loss = 1.222\n",
      "Epoch 1571 Batch   51/175   train_loss = 1.152\n",
      "Epoch 1571 Batch   83/175   train_loss = 1.248\n",
      "Epoch 1571 Batch  115/175   train_loss = 1.307\n",
      "Epoch 1571 Batch  147/175   train_loss = 1.145\n",
      "Epoch 1572 Batch    4/175   train_loss = 1.204\n",
      "Epoch 1572 Batch   36/175   train_loss = 1.161\n",
      "Epoch 1572 Batch   68/175   train_loss = 1.176\n",
      "Epoch 1572 Batch  100/175   train_loss = 1.196\n",
      "Epoch 1572 Batch  132/175   train_loss = 1.147\n",
      "Epoch 1572 Batch  164/175   train_loss = 1.153\n",
      "Epoch 1573 Batch   21/175   train_loss = 1.158\n",
      "Epoch 1573 Batch   53/175   train_loss = 1.161\n",
      "Epoch 1573 Batch   85/175   train_loss = 1.229\n",
      "Epoch 1573 Batch  117/175   train_loss = 1.217\n",
      "Epoch 1573 Batch  149/175   train_loss = 1.195\n",
      "Epoch 1574 Batch    6/175   train_loss = 1.201\n",
      "Epoch 1574 Batch   38/175   train_loss = 1.139\n",
      "Epoch 1574 Batch   70/175   train_loss = 1.189\n",
      "Epoch 1574 Batch  102/175   train_loss = 1.214\n",
      "Epoch 1574 Batch  134/175   train_loss = 1.171\n",
      "Epoch 1574 Batch  166/175   train_loss = 1.191\n",
      "Epoch 1575 Batch   23/175   train_loss = 1.132\n",
      "Epoch 1575 Batch   55/175   train_loss = 1.244\n",
      "Epoch 1575 Batch   87/175   train_loss = 1.252\n",
      "Epoch 1575 Batch  119/175   train_loss = 1.186\n",
      "Epoch 1575 Batch  151/175   train_loss = 1.194\n",
      "Epoch 1576 Batch    8/175   train_loss = 1.201\n",
      "Epoch 1576 Batch   40/175   train_loss = 1.154\n",
      "Epoch 1576 Batch   72/175   train_loss = 1.200\n",
      "Epoch 1576 Batch  104/175   train_loss = 1.202\n",
      "Epoch 1576 Batch  136/175   train_loss = 1.177\n",
      "Epoch 1576 Batch  168/175   train_loss = 1.216\n",
      "Epoch 1577 Batch   25/175   train_loss = 1.198\n",
      "Epoch 1577 Batch   57/175   train_loss = 1.225\n",
      "Epoch 1577 Batch   89/175   train_loss = 1.205\n",
      "Epoch 1577 Batch  121/175   train_loss = 1.203\n",
      "Epoch 1577 Batch  153/175   train_loss = 1.185\n",
      "Epoch 1578 Batch   10/175   train_loss = 1.189\n",
      "Epoch 1578 Batch   42/175   train_loss = 1.244\n",
      "Epoch 1578 Batch   74/175   train_loss = 1.249\n",
      "Epoch 1578 Batch  106/175   train_loss = 1.266\n",
      "Epoch 1578 Batch  138/175   train_loss = 1.180\n",
      "Epoch 1578 Batch  170/175   train_loss = 1.250\n",
      "Epoch 1579 Batch   27/175   train_loss = 1.177\n",
      "Epoch 1579 Batch   59/175   train_loss = 1.197\n",
      "Epoch 1579 Batch   91/175   train_loss = 1.193\n",
      "Epoch 1579 Batch  123/175   train_loss = 1.171\n",
      "Epoch 1579 Batch  155/175   train_loss = 1.180\n",
      "Epoch 1580 Batch   12/175   train_loss = 1.200\n",
      "Epoch 1580 Batch   44/175   train_loss = 1.185\n",
      "Epoch 1580 Batch   76/175   train_loss = 1.173\n",
      "Epoch 1580 Batch  108/175   train_loss = 1.212\n",
      "Epoch 1580 Batch  140/175   train_loss = 1.186\n",
      "Epoch 1580 Batch  172/175   train_loss = 1.212\n",
      "Epoch 1581 Batch   29/175   train_loss = 1.202\n",
      "Epoch 1581 Batch   61/175   train_loss = 1.230\n",
      "Epoch 1581 Batch   93/175   train_loss = 1.218\n",
      "Epoch 1581 Batch  125/175   train_loss = 1.234\n",
      "Epoch 1581 Batch  157/175   train_loss = 1.229\n",
      "Epoch 1582 Batch   14/175   train_loss = 1.245\n",
      "Epoch 1582 Batch   46/175   train_loss = 1.212\n",
      "Epoch 1582 Batch   78/175   train_loss = 1.197\n",
      "Epoch 1582 Batch  110/175   train_loss = 1.311\n",
      "Epoch 1582 Batch  142/175   train_loss = 1.220\n",
      "Epoch 1582 Batch  174/175   train_loss = 1.191\n",
      "Epoch 1583 Batch   31/175   train_loss = 1.219\n",
      "Epoch 1583 Batch   63/175   train_loss = 1.207\n",
      "Epoch 1583 Batch   95/175   train_loss = 1.193\n",
      "Epoch 1583 Batch  127/175   train_loss = 1.169\n",
      "Epoch 1583 Batch  159/175   train_loss = 1.183\n",
      "Epoch 1584 Batch   16/175   train_loss = 1.185\n",
      "Epoch 1584 Batch   48/175   train_loss = 1.239\n",
      "Epoch 1584 Batch   80/175   train_loss = 1.210\n",
      "Epoch 1584 Batch  112/175   train_loss = 1.208\n",
      "Epoch 1584 Batch  144/175   train_loss = 1.148\n",
      "Epoch 1585 Batch    1/175   train_loss = 1.248\n",
      "Epoch 1585 Batch   33/175   train_loss = 1.267\n",
      "Epoch 1585 Batch   65/175   train_loss = 1.207\n",
      "Epoch 1585 Batch   97/175   train_loss = 1.194\n",
      "Epoch 1585 Batch  129/175   train_loss = 1.199\n",
      "Epoch 1585 Batch  161/175   train_loss = 1.179\n",
      "Epoch 1586 Batch   18/175   train_loss = 1.160\n",
      "Epoch 1586 Batch   50/175   train_loss = 1.194\n",
      "Epoch 1586 Batch   82/175   train_loss = 1.249\n",
      "Epoch 1586 Batch  114/175   train_loss = 1.235\n",
      "Epoch 1586 Batch  146/175   train_loss = 1.215\n",
      "Epoch 1587 Batch    3/175   train_loss = 1.246\n",
      "Epoch 1587 Batch   35/175   train_loss = 1.209\n",
      "Epoch 1587 Batch   67/175   train_loss = 1.188\n",
      "Epoch 1587 Batch   99/175   train_loss = 1.264\n",
      "Epoch 1587 Batch  131/175   train_loss = 1.209\n",
      "Epoch 1587 Batch  163/175   train_loss = 1.194\n",
      "Epoch 1588 Batch   20/175   train_loss = 1.181\n",
      "Epoch 1588 Batch   52/175   train_loss = 1.168\n",
      "Epoch 1588 Batch   84/175   train_loss = 1.217\n",
      "Epoch 1588 Batch  116/175   train_loss = 1.240\n",
      "Epoch 1588 Batch  148/175   train_loss = 1.202\n",
      "Epoch 1589 Batch    5/175   train_loss = 1.195\n",
      "Epoch 1589 Batch   37/175   train_loss = 1.175\n",
      "Epoch 1589 Batch   69/175   train_loss = 1.236\n",
      "Epoch 1589 Batch  101/175   train_loss = 1.262\n",
      "Epoch 1589 Batch  133/175   train_loss = 1.119\n",
      "Epoch 1589 Batch  165/175   train_loss = 1.248\n",
      "Epoch 1590 Batch   22/175   train_loss = 1.157\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1590 Batch   54/175   train_loss = 1.253\n",
      "Epoch 1590 Batch   86/175   train_loss = 1.257\n",
      "Epoch 1590 Batch  118/175   train_loss = 1.269\n",
      "Epoch 1590 Batch  150/175   train_loss = 1.229\n",
      "Epoch 1591 Batch    7/175   train_loss = 1.226\n",
      "Epoch 1591 Batch   39/175   train_loss = 1.160\n",
      "Epoch 1591 Batch   71/175   train_loss = 1.234\n",
      "Epoch 1591 Batch  103/175   train_loss = 1.228\n",
      "Epoch 1591 Batch  135/175   train_loss = 1.187\n",
      "Epoch 1591 Batch  167/175   train_loss = 1.266\n",
      "Epoch 1592 Batch   24/175   train_loss = 1.166\n",
      "Epoch 1592 Batch   56/175   train_loss = 1.225\n",
      "Epoch 1592 Batch   88/175   train_loss = 1.240\n",
      "Epoch 1592 Batch  120/175   train_loss = 1.208\n",
      "Epoch 1592 Batch  152/175   train_loss = 1.185\n",
      "Epoch 1593 Batch    9/175   train_loss = 1.255\n",
      "Epoch 1593 Batch   41/175   train_loss = 1.249\n",
      "Epoch 1593 Batch   73/175   train_loss = 1.214\n",
      "Epoch 1593 Batch  105/175   train_loss = 1.233\n",
      "Epoch 1593 Batch  137/175   train_loss = 1.169\n",
      "Epoch 1593 Batch  169/175   train_loss = 1.238\n",
      "Epoch 1594 Batch   26/175   train_loss = 1.197\n",
      "Epoch 1594 Batch   58/175   train_loss = 1.245\n",
      "Epoch 1594 Batch   90/175   train_loss = 1.214\n",
      "Epoch 1594 Batch  122/175   train_loss = 1.174\n",
      "Epoch 1594 Batch  154/175   train_loss = 1.193\n",
      "Epoch 1595 Batch   11/175   train_loss = 1.173\n",
      "Epoch 1595 Batch   43/175   train_loss = 1.192\n",
      "Epoch 1595 Batch   75/175   train_loss = 1.179\n",
      "Epoch 1595 Batch  107/175   train_loss = 1.255\n",
      "Epoch 1595 Batch  139/175   train_loss = 1.157\n",
      "Epoch 1595 Batch  171/175   train_loss = 1.263\n",
      "Epoch 1596 Batch   28/175   train_loss = 1.183\n",
      "Epoch 1596 Batch   60/175   train_loss = 1.178\n",
      "Epoch 1596 Batch   92/175   train_loss = 1.158\n",
      "Epoch 1596 Batch  124/175   train_loss = 1.190\n",
      "Epoch 1596 Batch  156/175   train_loss = 1.249\n",
      "Epoch 1597 Batch   13/175   train_loss = 1.196\n",
      "Epoch 1597 Batch   45/175   train_loss = 1.206\n",
      "Epoch 1597 Batch   77/175   train_loss = 1.177\n",
      "Epoch 1597 Batch  109/175   train_loss = 1.224\n",
      "Epoch 1597 Batch  141/175   train_loss = 1.162\n",
      "Epoch 1597 Batch  173/175   train_loss = 1.177\n",
      "Epoch 1598 Batch   30/175   train_loss = 1.262\n",
      "Epoch 1598 Batch   62/175   train_loss = 1.233\n",
      "Epoch 1598 Batch   94/175   train_loss = 1.199\n",
      "Epoch 1598 Batch  126/175   train_loss = 1.215\n",
      "Epoch 1598 Batch  158/175   train_loss = 1.181\n",
      "Epoch 1599 Batch   15/175   train_loss = 1.252\n",
      "Epoch 1599 Batch   47/175   train_loss = 1.235\n",
      "Epoch 1599 Batch   79/175   train_loss = 1.254\n",
      "Epoch 1599 Batch  111/175   train_loss = 1.279\n",
      "Epoch 1599 Batch  143/175   train_loss = 1.189\n",
      "Epoch 1600 Batch    0/175   train_loss = 1.218\n",
      "Epoch 1600 Batch   32/175   train_loss = 1.256\n",
      "Epoch 1600 Batch   64/175   train_loss = 1.256\n",
      "Epoch 1600 Batch   96/175   train_loss = 1.251\n",
      "Epoch 1600 Batch  128/175   train_loss = 1.168\n",
      "Epoch 1600 Batch  160/175   train_loss = 1.214\n",
      "Epoch 1601 Batch   17/175   train_loss = 1.191\n",
      "Epoch 1601 Batch   49/175   train_loss = 1.233\n",
      "Epoch 1601 Batch   81/175   train_loss = 1.202\n",
      "Epoch 1601 Batch  113/175   train_loss = 1.232\n",
      "Epoch 1601 Batch  145/175   train_loss = 1.182\n",
      "Epoch 1602 Batch    2/175   train_loss = 1.193\n",
      "Epoch 1602 Batch   34/175   train_loss = 1.236\n",
      "Epoch 1602 Batch   66/175   train_loss = 1.250\n",
      "Epoch 1602 Batch   98/175   train_loss = 1.235\n",
      "Epoch 1602 Batch  130/175   train_loss = 1.255\n",
      "Epoch 1602 Batch  162/175   train_loss = 1.210\n",
      "Epoch 1603 Batch   19/175   train_loss = 1.218\n",
      "Epoch 1603 Batch   51/175   train_loss = 1.158\n",
      "Epoch 1603 Batch   83/175   train_loss = 1.284\n",
      "Epoch 1603 Batch  115/175   train_loss = 1.325\n",
      "Epoch 1603 Batch  147/175   train_loss = 1.151\n",
      "Epoch 1604 Batch    4/175   train_loss = 1.237\n",
      "Epoch 1604 Batch   36/175   train_loss = 1.169\n",
      "Epoch 1604 Batch   68/175   train_loss = 1.172\n",
      "Epoch 1604 Batch  100/175   train_loss = 1.219\n",
      "Epoch 1604 Batch  132/175   train_loss = 1.174\n",
      "Epoch 1604 Batch  164/175   train_loss = 1.182\n",
      "Epoch 1605 Batch   21/175   train_loss = 1.157\n",
      "Epoch 1605 Batch   53/175   train_loss = 1.187\n",
      "Epoch 1605 Batch   85/175   train_loss = 1.254\n",
      "Epoch 1605 Batch  117/175   train_loss = 1.221\n",
      "Epoch 1605 Batch  149/175   train_loss = 1.226\n",
      "Epoch 1606 Batch    6/175   train_loss = 1.206\n",
      "Epoch 1606 Batch   38/175   train_loss = 1.160\n",
      "Epoch 1606 Batch   70/175   train_loss = 1.215\n",
      "Epoch 1606 Batch  102/175   train_loss = 1.204\n",
      "Epoch 1606 Batch  134/175   train_loss = 1.168\n",
      "Epoch 1606 Batch  166/175   train_loss = 1.214\n",
      "Epoch 1607 Batch   23/175   train_loss = 1.144\n",
      "Epoch 1607 Batch   55/175   train_loss = 1.248\n",
      "Epoch 1607 Batch   87/175   train_loss = 1.283\n",
      "Epoch 1607 Batch  119/175   train_loss = 1.197\n",
      "Epoch 1607 Batch  151/175   train_loss = 1.204\n",
      "Epoch 1608 Batch    8/175   train_loss = 1.249\n",
      "Epoch 1608 Batch   40/175   train_loss = 1.205\n",
      "Epoch 1608 Batch   72/175   train_loss = 1.248\n",
      "Epoch 1608 Batch  104/175   train_loss = 1.220\n",
      "Epoch 1608 Batch  136/175   train_loss = 1.190\n",
      "Epoch 1608 Batch  168/175   train_loss = 1.225\n",
      "Epoch 1609 Batch   25/175   train_loss = 1.207\n",
      "Epoch 1609 Batch   57/175   train_loss = 1.245\n",
      "Epoch 1609 Batch   89/175   train_loss = 1.207\n",
      "Epoch 1609 Batch  121/175   train_loss = 1.193\n",
      "Epoch 1609 Batch  153/175   train_loss = 1.181\n",
      "Epoch 1610 Batch   10/175   train_loss = 1.159\n",
      "Epoch 1610 Batch   42/175   train_loss = 1.239\n",
      "Epoch 1610 Batch   74/175   train_loss = 1.246\n",
      "Epoch 1610 Batch  106/175   train_loss = 1.257\n",
      "Epoch 1610 Batch  138/175   train_loss = 1.180\n",
      "Epoch 1610 Batch  170/175   train_loss = 1.238\n",
      "Epoch 1611 Batch   27/175   train_loss = 1.180\n",
      "Epoch 1611 Batch   59/175   train_loss = 1.188\n",
      "Epoch 1611 Batch   91/175   train_loss = 1.191\n",
      "Epoch 1611 Batch  123/175   train_loss = 1.165\n",
      "Epoch 1611 Batch  155/175   train_loss = 1.151\n",
      "Epoch 1612 Batch   12/175   train_loss = 1.180\n",
      "Epoch 1612 Batch   44/175   train_loss = 1.180\n",
      "Epoch 1612 Batch   76/175   train_loss = 1.201\n",
      "Epoch 1612 Batch  108/175   train_loss = 1.248\n",
      "Epoch 1612 Batch  140/175   train_loss = 1.206\n",
      "Epoch 1612 Batch  172/175   train_loss = 1.237\n",
      "Epoch 1613 Batch   29/175   train_loss = 1.201\n",
      "Epoch 1613 Batch   61/175   train_loss = 1.231\n",
      "Epoch 1613 Batch   93/175   train_loss = 1.212\n",
      "Epoch 1613 Batch  125/175   train_loss = 1.228\n",
      "Epoch 1613 Batch  157/175   train_loss = 1.204\n",
      "Epoch 1614 Batch   14/175   train_loss = 1.219\n",
      "Epoch 1614 Batch   46/175   train_loss = 1.221\n",
      "Epoch 1614 Batch   78/175   train_loss = 1.206\n",
      "Epoch 1614 Batch  110/175   train_loss = 1.288\n",
      "Epoch 1614 Batch  142/175   train_loss = 1.236\n",
      "Epoch 1614 Batch  174/175   train_loss = 1.215\n",
      "Epoch 1615 Batch   31/175   train_loss = 1.214\n",
      "Epoch 1615 Batch   63/175   train_loss = 1.229\n",
      "Epoch 1615 Batch   95/175   train_loss = 1.177\n",
      "Epoch 1615 Batch  127/175   train_loss = 1.185\n",
      "Epoch 1615 Batch  159/175   train_loss = 1.174\n",
      "Epoch 1616 Batch   16/175   train_loss = 1.205\n",
      "Epoch 1616 Batch   48/175   train_loss = 1.222\n",
      "Epoch 1616 Batch   80/175   train_loss = 1.226\n",
      "Epoch 1616 Batch  112/175   train_loss = 1.192\n",
      "Epoch 1616 Batch  144/175   train_loss = 1.155\n",
      "Epoch 1617 Batch    1/175   train_loss = 1.254\n",
      "Epoch 1617 Batch   33/175   train_loss = 1.264\n",
      "Epoch 1617 Batch   65/175   train_loss = 1.194\n",
      "Epoch 1617 Batch   97/175   train_loss = 1.192\n",
      "Epoch 1617 Batch  129/175   train_loss = 1.194\n",
      "Epoch 1617 Batch  161/175   train_loss = 1.177\n",
      "Epoch 1618 Batch   18/175   train_loss = 1.161\n",
      "Epoch 1618 Batch   50/175   train_loss = 1.189\n",
      "Epoch 1618 Batch   82/175   train_loss = 1.230\n",
      "Epoch 1618 Batch  114/175   train_loss = 1.229\n",
      "Epoch 1618 Batch  146/175   train_loss = 1.192\n",
      "Epoch 1619 Batch    3/175   train_loss = 1.234\n",
      "Epoch 1619 Batch   35/175   train_loss = 1.198\n",
      "Epoch 1619 Batch   67/175   train_loss = 1.187\n",
      "Epoch 1619 Batch   99/175   train_loss = 1.262\n",
      "Epoch 1619 Batch  131/175   train_loss = 1.202\n",
      "Epoch 1619 Batch  163/175   train_loss = 1.191\n",
      "Epoch 1620 Batch   20/175   train_loss = 1.173\n",
      "Epoch 1620 Batch   52/175   train_loss = 1.131\n",
      "Epoch 1620 Batch   84/175   train_loss = 1.210\n",
      "Epoch 1620 Batch  116/175   train_loss = 1.253\n",
      "Epoch 1620 Batch  148/175   train_loss = 1.197\n",
      "Epoch 1621 Batch    5/175   train_loss = 1.186\n",
      "Epoch 1621 Batch   37/175   train_loss = 1.175\n",
      "Epoch 1621 Batch   69/175   train_loss = 1.208\n",
      "Epoch 1621 Batch  101/175   train_loss = 1.248\n",
      "Epoch 1621 Batch  133/175   train_loss = 1.125\n",
      "Epoch 1621 Batch  165/175   train_loss = 1.203\n",
      "Epoch 1622 Batch   22/175   train_loss = 1.140\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1622 Batch   54/175   train_loss = 1.220\n",
      "Epoch 1622 Batch   86/175   train_loss = 1.249\n",
      "Epoch 1622 Batch  118/175   train_loss = 1.263\n",
      "Epoch 1622 Batch  150/175   train_loss = 1.205\n",
      "Epoch 1623 Batch    7/175   train_loss = 1.216\n",
      "Epoch 1623 Batch   39/175   train_loss = 1.149\n",
      "Epoch 1623 Batch   71/175   train_loss = 1.211\n",
      "Epoch 1623 Batch  103/175   train_loss = 1.228\n",
      "Epoch 1623 Batch  135/175   train_loss = 1.190\n",
      "Epoch 1623 Batch  167/175   train_loss = 1.301\n",
      "Epoch 1624 Batch   24/175   train_loss = 1.170\n",
      "Epoch 1624 Batch   56/175   train_loss = 1.228\n",
      "Epoch 1624 Batch   88/175   train_loss = 1.260\n",
      "Epoch 1624 Batch  120/175   train_loss = 1.221\n",
      "Epoch 1624 Batch  152/175   train_loss = 1.178\n",
      "Epoch 1625 Batch    9/175   train_loss = 1.234\n",
      "Epoch 1625 Batch   41/175   train_loss = 1.268\n",
      "Epoch 1625 Batch   73/175   train_loss = 1.218\n",
      "Epoch 1625 Batch  105/175   train_loss = 1.248\n",
      "Epoch 1625 Batch  137/175   train_loss = 1.182\n",
      "Epoch 1625 Batch  169/175   train_loss = 1.233\n",
      "Epoch 1626 Batch   26/175   train_loss = 1.201\n",
      "Epoch 1626 Batch   58/175   train_loss = 1.246\n",
      "Epoch 1626 Batch   90/175   train_loss = 1.214\n",
      "Epoch 1626 Batch  122/175   train_loss = 1.192\n",
      "Epoch 1626 Batch  154/175   train_loss = 1.187\n",
      "Epoch 1627 Batch   11/175   train_loss = 1.206\n",
      "Epoch 1627 Batch   43/175   train_loss = 1.232\n",
      "Epoch 1627 Batch   75/175   train_loss = 1.193\n",
      "Epoch 1627 Batch  107/175   train_loss = 1.265\n",
      "Epoch 1627 Batch  139/175   train_loss = 1.163\n",
      "Epoch 1627 Batch  171/175   train_loss = 1.289\n",
      "Epoch 1628 Batch   28/175   train_loss = 1.193\n",
      "Epoch 1628 Batch   60/175   train_loss = 1.216\n",
      "Epoch 1628 Batch   92/175   train_loss = 1.189\n",
      "Epoch 1628 Batch  124/175   train_loss = 1.214\n",
      "Epoch 1628 Batch  156/175   train_loss = 1.250\n",
      "Epoch 1629 Batch   13/175   train_loss = 1.207\n",
      "Epoch 1629 Batch   45/175   train_loss = 1.200\n",
      "Epoch 1629 Batch   77/175   train_loss = 1.199\n",
      "Epoch 1629 Batch  109/175   train_loss = 1.267\n",
      "Epoch 1629 Batch  141/175   train_loss = 1.169\n",
      "Epoch 1629 Batch  173/175   train_loss = 1.210\n",
      "Epoch 1630 Batch   30/175   train_loss = 1.272\n",
      "Epoch 1630 Batch   62/175   train_loss = 1.248\n",
      "Epoch 1630 Batch   94/175   train_loss = 1.197\n",
      "Epoch 1630 Batch  126/175   train_loss = 1.219\n",
      "Epoch 1630 Batch  158/175   train_loss = 1.197\n",
      "Epoch 1631 Batch   15/175   train_loss = 1.238\n",
      "Epoch 1631 Batch   47/175   train_loss = 1.244\n",
      "Epoch 1631 Batch   79/175   train_loss = 1.233\n",
      "Epoch 1631 Batch  111/175   train_loss = 1.252\n",
      "Epoch 1631 Batch  143/175   train_loss = 1.186\n",
      "Epoch 1632 Batch    0/175   train_loss = 1.221\n",
      "Epoch 1632 Batch   32/175   train_loss = 1.232\n",
      "Epoch 1632 Batch   64/175   train_loss = 1.239\n",
      "Epoch 1632 Batch   96/175   train_loss = 1.258\n",
      "Epoch 1632 Batch  128/175   train_loss = 1.157\n",
      "Epoch 1632 Batch  160/175   train_loss = 1.216\n",
      "Epoch 1633 Batch   17/175   train_loss = 1.186\n",
      "Epoch 1633 Batch   49/175   train_loss = 1.224\n",
      "Epoch 1633 Batch   81/175   train_loss = 1.202\n",
      "Epoch 1633 Batch  113/175   train_loss = 1.231\n",
      "Epoch 1633 Batch  145/175   train_loss = 1.181\n",
      "Epoch 1634 Batch    2/175   train_loss = 1.199\n",
      "Epoch 1634 Batch   34/175   train_loss = 1.249\n",
      "Epoch 1634 Batch   66/175   train_loss = 1.250\n",
      "Epoch 1634 Batch   98/175   train_loss = 1.229\n",
      "Epoch 1634 Batch  130/175   train_loss = 1.234\n",
      "Epoch 1634 Batch  162/175   train_loss = 1.208\n",
      "Epoch 1635 Batch   19/175   train_loss = 1.204\n",
      "Epoch 1635 Batch   51/175   train_loss = 1.155\n",
      "Epoch 1635 Batch   83/175   train_loss = 1.276\n",
      "Epoch 1635 Batch  115/175   train_loss = 1.307\n",
      "Epoch 1635 Batch  147/175   train_loss = 1.172\n",
      "Epoch 1636 Batch    4/175   train_loss = 1.221\n",
      "Epoch 1636 Batch   36/175   train_loss = 1.182\n",
      "Epoch 1636 Batch   68/175   train_loss = 1.197\n",
      "Epoch 1636 Batch  100/175   train_loss = 1.214\n",
      "Epoch 1636 Batch  132/175   train_loss = 1.161\n",
      "Epoch 1636 Batch  164/175   train_loss = 1.169\n",
      "Epoch 1637 Batch   21/175   train_loss = 1.152\n",
      "Epoch 1637 Batch   53/175   train_loss = 1.186\n",
      "Epoch 1637 Batch   85/175   train_loss = 1.224\n",
      "Epoch 1637 Batch  117/175   train_loss = 1.230\n",
      "Epoch 1637 Batch  149/175   train_loss = 1.214\n",
      "Epoch 1638 Batch    6/175   train_loss = 1.232\n",
      "Epoch 1638 Batch   38/175   train_loss = 1.163\n",
      "Epoch 1638 Batch   70/175   train_loss = 1.222\n",
      "Epoch 1638 Batch  102/175   train_loss = 1.223\n",
      "Epoch 1638 Batch  134/175   train_loss = 1.186\n",
      "Epoch 1638 Batch  166/175   train_loss = 1.214\n",
      "Epoch 1639 Batch   23/175   train_loss = 1.168\n",
      "Epoch 1639 Batch   55/175   train_loss = 1.277\n",
      "Epoch 1639 Batch   87/175   train_loss = 1.294\n",
      "Epoch 1639 Batch  119/175   train_loss = 1.203\n",
      "Epoch 1639 Batch  151/175   train_loss = 1.215\n",
      "Epoch 1640 Batch    8/175   train_loss = 1.209\n",
      "Epoch 1640 Batch   40/175   train_loss = 1.199\n",
      "Epoch 1640 Batch   72/175   train_loss = 1.244\n",
      "Epoch 1640 Batch  104/175   train_loss = 1.232\n",
      "Epoch 1640 Batch  136/175   train_loss = 1.180\n",
      "Epoch 1640 Batch  168/175   train_loss = 1.223\n",
      "Epoch 1641 Batch   25/175   train_loss = 1.231\n",
      "Epoch 1641 Batch   57/175   train_loss = 1.273\n",
      "Epoch 1641 Batch   89/175   train_loss = 1.211\n",
      "Epoch 1641 Batch  121/175   train_loss = 1.189\n",
      "Epoch 1641 Batch  153/175   train_loss = 1.230\n",
      "Epoch 1642 Batch   10/175   train_loss = 1.169\n",
      "Epoch 1642 Batch   42/175   train_loss = 1.273\n",
      "Epoch 1642 Batch   74/175   train_loss = 1.256\n",
      "Epoch 1642 Batch  106/175   train_loss = 1.265\n",
      "Epoch 1642 Batch  138/175   train_loss = 1.197\n",
      "Epoch 1642 Batch  170/175   train_loss = 1.256\n",
      "Epoch 1643 Batch   27/175   train_loss = 1.199\n",
      "Epoch 1643 Batch   59/175   train_loss = 1.187\n",
      "Epoch 1643 Batch   91/175   train_loss = 1.195\n",
      "Epoch 1643 Batch  123/175   train_loss = 1.175\n",
      "Epoch 1643 Batch  155/175   train_loss = 1.164\n",
      "Epoch 1644 Batch   12/175   train_loss = 1.186\n",
      "Epoch 1644 Batch   44/175   train_loss = 1.199\n",
      "Epoch 1644 Batch   76/175   train_loss = 1.182\n",
      "Epoch 1644 Batch  108/175   train_loss = 1.214\n",
      "Epoch 1644 Batch  140/175   train_loss = 1.174\n",
      "Epoch 1644 Batch  172/175   train_loss = 1.228\n",
      "Epoch 1645 Batch   29/175   train_loss = 1.246\n",
      "Epoch 1645 Batch   61/175   train_loss = 1.243\n",
      "Epoch 1645 Batch   93/175   train_loss = 1.215\n",
      "Epoch 1645 Batch  125/175   train_loss = 1.225\n",
      "Epoch 1645 Batch  157/175   train_loss = 1.218\n",
      "Epoch 1646 Batch   14/175   train_loss = 1.238\n",
      "Epoch 1646 Batch   46/175   train_loss = 1.216\n",
      "Epoch 1646 Batch   78/175   train_loss = 1.196\n",
      "Epoch 1646 Batch  110/175   train_loss = 1.285\n",
      "Epoch 1646 Batch  142/175   train_loss = 1.221\n",
      "Epoch 1646 Batch  174/175   train_loss = 1.195\n",
      "Epoch 1647 Batch   31/175   train_loss = 1.218\n",
      "Epoch 1647 Batch   63/175   train_loss = 1.228\n",
      "Epoch 1647 Batch   95/175   train_loss = 1.179\n",
      "Epoch 1647 Batch  127/175   train_loss = 1.169\n",
      "Epoch 1647 Batch  159/175   train_loss = 1.165\n",
      "Epoch 1648 Batch   16/175   train_loss = 1.186\n",
      "Epoch 1648 Batch   48/175   train_loss = 1.207\n",
      "Epoch 1648 Batch   80/175   train_loss = 1.230\n",
      "Epoch 1648 Batch  112/175   train_loss = 1.197\n",
      "Epoch 1648 Batch  144/175   train_loss = 1.142\n",
      "Epoch 1649 Batch    1/175   train_loss = 1.249\n",
      "Epoch 1649 Batch   33/175   train_loss = 1.256\n",
      "Epoch 1649 Batch   65/175   train_loss = 1.207\n",
      "Epoch 1649 Batch   97/175   train_loss = 1.185\n",
      "Epoch 1649 Batch  129/175   train_loss = 1.191\n",
      "Epoch 1649 Batch  161/175   train_loss = 1.184\n",
      "Epoch 1650 Batch   18/175   train_loss = 1.178\n",
      "Epoch 1650 Batch   50/175   train_loss = 1.188\n",
      "Epoch 1650 Batch   82/175   train_loss = 1.247\n",
      "Epoch 1650 Batch  114/175   train_loss = 1.242\n",
      "Epoch 1650 Batch  146/175   train_loss = 1.194\n",
      "Epoch 1651 Batch    3/175   train_loss = 1.237\n",
      "Epoch 1651 Batch   35/175   train_loss = 1.215\n",
      "Epoch 1651 Batch   67/175   train_loss = 1.174\n",
      "Epoch 1651 Batch   99/175   train_loss = 1.259\n",
      "Epoch 1651 Batch  131/175   train_loss = 1.212\n",
      "Epoch 1651 Batch  163/175   train_loss = 1.201\n",
      "Epoch 1652 Batch   20/175   train_loss = 1.187\n",
      "Epoch 1652 Batch   52/175   train_loss = 1.150\n",
      "Epoch 1652 Batch   84/175   train_loss = 1.210\n",
      "Epoch 1652 Batch  116/175   train_loss = 1.247\n",
      "Epoch 1652 Batch  148/175   train_loss = 1.192\n",
      "Epoch 1653 Batch    5/175   train_loss = 1.199\n",
      "Epoch 1653 Batch   37/175   train_loss = 1.173\n",
      "Epoch 1653 Batch   69/175   train_loss = 1.217\n",
      "Epoch 1653 Batch  101/175   train_loss = 1.261\n",
      "Epoch 1653 Batch  133/175   train_loss = 1.112\n",
      "Epoch 1653 Batch  165/175   train_loss = 1.208\n",
      "Epoch 1654 Batch   22/175   train_loss = 1.151\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1654 Batch   54/175   train_loss = 1.224\n",
      "Epoch 1654 Batch   86/175   train_loss = 1.258\n",
      "Epoch 1654 Batch  118/175   train_loss = 1.273\n",
      "Epoch 1654 Batch  150/175   train_loss = 1.203\n",
      "Epoch 1655 Batch    7/175   train_loss = 1.217\n",
      "Epoch 1655 Batch   39/175   train_loss = 1.163\n",
      "Epoch 1655 Batch   71/175   train_loss = 1.252\n",
      "Epoch 1655 Batch  103/175   train_loss = 1.203\n",
      "Epoch 1655 Batch  135/175   train_loss = 1.170\n",
      "Epoch 1655 Batch  167/175   train_loss = 1.248\n",
      "Epoch 1656 Batch   24/175   train_loss = 1.174\n",
      "Epoch 1656 Batch   56/175   train_loss = 1.234\n",
      "Epoch 1656 Batch   88/175   train_loss = 1.251\n",
      "Epoch 1656 Batch  120/175   train_loss = 1.197\n",
      "Epoch 1656 Batch  152/175   train_loss = 1.175\n",
      "Epoch 1657 Batch    9/175   train_loss = 1.242\n",
      "Epoch 1657 Batch   41/175   train_loss = 1.238\n",
      "Epoch 1657 Batch   73/175   train_loss = 1.218\n",
      "Epoch 1657 Batch  105/175   train_loss = 1.236\n",
      "Epoch 1657 Batch  137/175   train_loss = 1.163\n",
      "Epoch 1657 Batch  169/175   train_loss = 1.209\n",
      "Epoch 1658 Batch   26/175   train_loss = 1.207\n",
      "Epoch 1658 Batch   58/175   train_loss = 1.232\n",
      "Epoch 1658 Batch   90/175   train_loss = 1.208\n",
      "Epoch 1658 Batch  122/175   train_loss = 1.185\n",
      "Epoch 1658 Batch  154/175   train_loss = 1.169\n",
      "Epoch 1659 Batch   11/175   train_loss = 1.178\n",
      "Epoch 1659 Batch   43/175   train_loss = 1.193\n",
      "Epoch 1659 Batch   75/175   train_loss = 1.192\n",
      "Epoch 1659 Batch  107/175   train_loss = 1.247\n",
      "Epoch 1659 Batch  139/175   train_loss = 1.150\n",
      "Epoch 1659 Batch  171/175   train_loss = 1.250\n",
      "Epoch 1660 Batch   28/175   train_loss = 1.199\n",
      "Epoch 1660 Batch   60/175   train_loss = 1.192\n",
      "Epoch 1660 Batch   92/175   train_loss = 1.175\n",
      "Epoch 1660 Batch  124/175   train_loss = 1.200\n",
      "Epoch 1660 Batch  156/175   train_loss = 1.283\n",
      "Epoch 1661 Batch   13/175   train_loss = 1.224\n",
      "Epoch 1661 Batch   45/175   train_loss = 1.197\n",
      "Epoch 1661 Batch   77/175   train_loss = 1.190\n",
      "Epoch 1661 Batch  109/175   train_loss = 1.244\n",
      "Epoch 1661 Batch  141/175   train_loss = 1.165\n",
      "Epoch 1661 Batch  173/175   train_loss = 1.186\n",
      "Epoch 1662 Batch   30/175   train_loss = 1.263\n",
      "Epoch 1662 Batch   62/175   train_loss = 1.229\n",
      "Epoch 1662 Batch   94/175   train_loss = 1.300\n",
      "Epoch 1662 Batch  126/175   train_loss = 1.247\n",
      "Epoch 1662 Batch  158/175   train_loss = 1.220\n",
      "Epoch 1663 Batch   15/175   train_loss = 1.261\n",
      "Epoch 1663 Batch   47/175   train_loss = 1.239\n",
      "Epoch 1663 Batch   79/175   train_loss = 1.238\n",
      "Epoch 1663 Batch  111/175   train_loss = 1.295\n",
      "Epoch 1663 Batch  143/175   train_loss = 1.180\n",
      "Epoch 1664 Batch    0/175   train_loss = 1.216\n",
      "Epoch 1664 Batch   32/175   train_loss = 1.232\n",
      "Epoch 1664 Batch   64/175   train_loss = 1.245\n",
      "Epoch 1664 Batch   96/175   train_loss = 1.246\n",
      "Epoch 1664 Batch  128/175   train_loss = 1.162\n",
      "Epoch 1664 Batch  160/175   train_loss = 1.201\n",
      "Epoch 1665 Batch   17/175   train_loss = 1.183\n",
      "Epoch 1665 Batch   49/175   train_loss = 1.220\n",
      "Epoch 1665 Batch   81/175   train_loss = 1.186\n",
      "Epoch 1665 Batch  113/175   train_loss = 1.223\n",
      "Epoch 1665 Batch  145/175   train_loss = 1.167\n",
      "Epoch 1666 Batch    2/175   train_loss = 1.192\n",
      "Epoch 1666 Batch   34/175   train_loss = 1.222\n",
      "Epoch 1666 Batch   66/175   train_loss = 1.253\n",
      "Epoch 1666 Batch   98/175   train_loss = 1.219\n",
      "Epoch 1666 Batch  130/175   train_loss = 1.247\n",
      "Epoch 1666 Batch  162/175   train_loss = 1.200\n",
      "Epoch 1667 Batch   19/175   train_loss = 1.201\n",
      "Epoch 1667 Batch   51/175   train_loss = 1.150\n",
      "Epoch 1667 Batch   83/175   train_loss = 1.256\n",
      "Epoch 1667 Batch  115/175   train_loss = 1.305\n",
      "Epoch 1667 Batch  147/175   train_loss = 1.155\n",
      "Epoch 1668 Batch    4/175   train_loss = 1.216\n",
      "Epoch 1668 Batch   36/175   train_loss = 1.175\n",
      "Epoch 1668 Batch   68/175   train_loss = 1.190\n",
      "Epoch 1668 Batch  100/175   train_loss = 1.212\n",
      "Epoch 1668 Batch  132/175   train_loss = 1.206\n",
      "Epoch 1668 Batch  164/175   train_loss = 1.201\n",
      "Epoch 1669 Batch   21/175   train_loss = 1.179\n",
      "Epoch 1669 Batch   53/175   train_loss = 1.240\n",
      "Epoch 1669 Batch   85/175   train_loss = 1.266\n",
      "Epoch 1669 Batch  117/175   train_loss = 1.262\n",
      "Epoch 1669 Batch  149/175   train_loss = 1.226\n",
      "Epoch 1670 Batch    6/175   train_loss = 1.217\n",
      "Epoch 1670 Batch   38/175   train_loss = 1.167\n",
      "Epoch 1670 Batch   70/175   train_loss = 1.228\n",
      "Epoch 1670 Batch  102/175   train_loss = 1.252\n",
      "Epoch 1670 Batch  134/175   train_loss = 1.183\n",
      "Epoch 1670 Batch  166/175   train_loss = 1.239\n",
      "Epoch 1671 Batch   23/175   train_loss = 1.171\n",
      "Epoch 1671 Batch   55/175   train_loss = 1.269\n",
      "Epoch 1671 Batch   87/175   train_loss = 1.299\n",
      "Epoch 1671 Batch  119/175   train_loss = 1.222\n",
      "Epoch 1671 Batch  151/175   train_loss = 1.229\n",
      "Epoch 1672 Batch    8/175   train_loss = 1.237\n",
      "Epoch 1672 Batch   40/175   train_loss = 1.205\n",
      "Epoch 1672 Batch   72/175   train_loss = 1.258\n",
      "Epoch 1672 Batch  104/175   train_loss = 1.243\n",
      "Epoch 1672 Batch  136/175   train_loss = 1.192\n",
      "Epoch 1672 Batch  168/175   train_loss = 1.243\n",
      "Epoch 1673 Batch   25/175   train_loss = 1.236\n",
      "Epoch 1673 Batch   57/175   train_loss = 1.253\n",
      "Epoch 1673 Batch   89/175   train_loss = 1.227\n",
      "Epoch 1673 Batch  121/175   train_loss = 1.191\n",
      "Epoch 1673 Batch  153/175   train_loss = 1.192\n",
      "Epoch 1674 Batch   10/175   train_loss = 1.161\n",
      "Epoch 1674 Batch   42/175   train_loss = 1.243\n",
      "Epoch 1674 Batch   74/175   train_loss = 1.257\n",
      "Epoch 1674 Batch  106/175   train_loss = 1.252\n",
      "Epoch 1674 Batch  138/175   train_loss = 1.202\n",
      "Epoch 1674 Batch  170/175   train_loss = 1.248\n",
      "Epoch 1675 Batch   27/175   train_loss = 1.182\n",
      "Epoch 1675 Batch   59/175   train_loss = 1.184\n",
      "Epoch 1675 Batch   91/175   train_loss = 1.197\n",
      "Epoch 1675 Batch  123/175   train_loss = 1.199\n",
      "Epoch 1675 Batch  155/175   train_loss = 1.167\n",
      "Epoch 1676 Batch   12/175   train_loss = 1.187\n",
      "Epoch 1676 Batch   44/175   train_loss = 1.200\n",
      "Epoch 1676 Batch   76/175   train_loss = 1.193\n",
      "Epoch 1676 Batch  108/175   train_loss = 1.214\n",
      "Epoch 1676 Batch  140/175   train_loss = 1.183\n",
      "Epoch 1676 Batch  172/175   train_loss = 1.224\n",
      "Epoch 1677 Batch   29/175   train_loss = 1.215\n",
      "Epoch 1677 Batch   61/175   train_loss = 1.227\n",
      "Epoch 1677 Batch   93/175   train_loss = 1.193\n",
      "Epoch 1677 Batch  125/175   train_loss = 1.218\n",
      "Epoch 1677 Batch  157/175   train_loss = 1.213\n",
      "Epoch 1678 Batch   14/175   train_loss = 1.224\n",
      "Epoch 1678 Batch   46/175   train_loss = 1.203\n",
      "Epoch 1678 Batch   78/175   train_loss = 1.178\n",
      "Epoch 1678 Batch  110/175   train_loss = 1.303\n",
      "Epoch 1678 Batch  142/175   train_loss = 1.208\n",
      "Epoch 1678 Batch  174/175   train_loss = 1.208\n",
      "Epoch 1679 Batch   31/175   train_loss = 1.217\n",
      "Epoch 1679 Batch   63/175   train_loss = 1.205\n",
      "Epoch 1679 Batch   95/175   train_loss = 1.170\n",
      "Epoch 1679 Batch  127/175   train_loss = 1.189\n",
      "Epoch 1679 Batch  159/175   train_loss = 1.168\n",
      "Epoch 1680 Batch   16/175   train_loss = 1.183\n",
      "Epoch 1680 Batch   48/175   train_loss = 1.217\n",
      "Epoch 1680 Batch   80/175   train_loss = 1.210\n",
      "Epoch 1680 Batch  112/175   train_loss = 1.214\n",
      "Epoch 1680 Batch  144/175   train_loss = 1.133\n",
      "Epoch 1681 Batch    1/175   train_loss = 1.318\n",
      "Epoch 1681 Batch   33/175   train_loss = 1.272\n",
      "Epoch 1681 Batch   65/175   train_loss = 1.202\n",
      "Epoch 1681 Batch   97/175   train_loss = 1.185\n",
      "Epoch 1681 Batch  129/175   train_loss = 1.212\n",
      "Epoch 1681 Batch  161/175   train_loss = 1.170\n",
      "Epoch 1682 Batch   18/175   train_loss = 1.159\n",
      "Epoch 1682 Batch   50/175   train_loss = 1.207\n",
      "Epoch 1682 Batch   82/175   train_loss = 1.261\n",
      "Epoch 1682 Batch  114/175   train_loss = 1.250\n",
      "Epoch 1682 Batch  146/175   train_loss = 1.202\n",
      "Epoch 1683 Batch    3/175   train_loss = 1.237\n",
      "Epoch 1683 Batch   35/175   train_loss = 1.193\n",
      "Epoch 1683 Batch   67/175   train_loss = 1.172\n",
      "Epoch 1683 Batch   99/175   train_loss = 1.274\n",
      "Epoch 1683 Batch  131/175   train_loss = 1.204\n",
      "Epoch 1683 Batch  163/175   train_loss = 1.201\n",
      "Epoch 1684 Batch   20/175   train_loss = 1.178\n",
      "Epoch 1684 Batch   52/175   train_loss = 1.143\n",
      "Epoch 1684 Batch   84/175   train_loss = 1.210\n",
      "Epoch 1684 Batch  116/175   train_loss = 1.232\n",
      "Epoch 1684 Batch  148/175   train_loss = 1.195\n",
      "Epoch 1685 Batch    5/175   train_loss = 1.191\n",
      "Epoch 1685 Batch   37/175   train_loss = 1.185\n",
      "Epoch 1685 Batch   69/175   train_loss = 1.239\n",
      "Epoch 1685 Batch  101/175   train_loss = 1.255\n",
      "Epoch 1685 Batch  133/175   train_loss = 1.111\n",
      "Epoch 1685 Batch  165/175   train_loss = 1.184\n",
      "Epoch 1686 Batch   22/175   train_loss = 1.140\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1686 Batch   54/175   train_loss = 1.226\n",
      "Epoch 1686 Batch   86/175   train_loss = 1.252\n",
      "Epoch 1686 Batch  118/175   train_loss = 1.275\n",
      "Epoch 1686 Batch  150/175   train_loss = 1.205\n",
      "Epoch 1687 Batch    7/175   train_loss = 1.213\n",
      "Epoch 1687 Batch   39/175   train_loss = 1.146\n",
      "Epoch 1687 Batch   71/175   train_loss = 1.226\n",
      "Epoch 1687 Batch  103/175   train_loss = 1.203\n",
      "Epoch 1687 Batch  135/175   train_loss = 1.160\n",
      "Epoch 1687 Batch  167/175   train_loss = 1.251\n",
      "Epoch 1688 Batch   24/175   train_loss = 1.158\n",
      "Epoch 1688 Batch   56/175   train_loss = 1.216\n",
      "Epoch 1688 Batch   88/175   train_loss = 1.238\n",
      "Epoch 1688 Batch  120/175   train_loss = 1.174\n",
      "Epoch 1688 Batch  152/175   train_loss = 1.166\n",
      "Epoch 1689 Batch    9/175   train_loss = 1.221\n",
      "Epoch 1689 Batch   41/175   train_loss = 1.213\n",
      "Epoch 1689 Batch   73/175   train_loss = 1.223\n",
      "Epoch 1689 Batch  105/175   train_loss = 1.246\n",
      "Epoch 1689 Batch  137/175   train_loss = 1.179\n",
      "Epoch 1689 Batch  169/175   train_loss = 1.224\n",
      "Epoch 1690 Batch   26/175   train_loss = 1.200\n",
      "Epoch 1690 Batch   58/175   train_loss = 1.246\n",
      "Epoch 1690 Batch   90/175   train_loss = 1.203\n",
      "Epoch 1690 Batch  122/175   train_loss = 1.180\n",
      "Epoch 1690 Batch  154/175   train_loss = 1.171\n",
      "Epoch 1691 Batch   11/175   train_loss = 1.180\n",
      "Epoch 1691 Batch   43/175   train_loss = 1.184\n",
      "Epoch 1691 Batch   75/175   train_loss = 1.193\n",
      "Epoch 1691 Batch  107/175   train_loss = 1.250\n",
      "Epoch 1691 Batch  139/175   train_loss = 1.150\n",
      "Epoch 1691 Batch  171/175   train_loss = 1.265\n",
      "Epoch 1692 Batch   28/175   train_loss = 1.176\n",
      "Epoch 1692 Batch   60/175   train_loss = 1.202\n",
      "Epoch 1692 Batch   92/175   train_loss = 1.175\n",
      "Epoch 1692 Batch  124/175   train_loss = 1.207\n",
      "Epoch 1692 Batch  156/175   train_loss = 1.236\n",
      "Epoch 1693 Batch   13/175   train_loss = 1.203\n",
      "Epoch 1693 Batch   45/175   train_loss = 1.196\n",
      "Epoch 1693 Batch   77/175   train_loss = 1.185\n",
      "Epoch 1693 Batch  109/175   train_loss = 1.253\n",
      "Epoch 1693 Batch  141/175   train_loss = 1.164\n",
      "Epoch 1693 Batch  173/175   train_loss = 1.176\n",
      "Epoch 1694 Batch   30/175   train_loss = 1.256\n",
      "Epoch 1694 Batch   62/175   train_loss = 1.222\n",
      "Epoch 1694 Batch   94/175   train_loss = 1.182\n",
      "Epoch 1694 Batch  126/175   train_loss = 1.219\n",
      "Epoch 1694 Batch  158/175   train_loss = 1.196\n",
      "Epoch 1695 Batch   15/175   train_loss = 1.239\n",
      "Epoch 1695 Batch   47/175   train_loss = 1.212\n",
      "Epoch 1695 Batch   79/175   train_loss = 1.254\n",
      "Epoch 1695 Batch  111/175   train_loss = 1.248\n",
      "Epoch 1695 Batch  143/175   train_loss = 1.162\n",
      "Epoch 1696 Batch    0/175   train_loss = 1.209\n",
      "Epoch 1696 Batch   32/175   train_loss = 1.221\n",
      "Epoch 1696 Batch   64/175   train_loss = 1.248\n",
      "Epoch 1696 Batch   96/175   train_loss = 1.237\n",
      "Epoch 1696 Batch  128/175   train_loss = 1.137\n",
      "Epoch 1696 Batch  160/175   train_loss = 1.201\n",
      "Epoch 1697 Batch   17/175   train_loss = 1.175\n",
      "Epoch 1697 Batch   49/175   train_loss = 1.225\n",
      "Epoch 1697 Batch   81/175   train_loss = 1.191\n",
      "Epoch 1697 Batch  113/175   train_loss = 1.218\n",
      "Epoch 1697 Batch  145/175   train_loss = 1.176\n",
      "Epoch 1698 Batch    2/175   train_loss = 1.196\n",
      "Epoch 1698 Batch   34/175   train_loss = 1.233\n",
      "Epoch 1698 Batch   66/175   train_loss = 1.253\n",
      "Epoch 1698 Batch   98/175   train_loss = 1.232\n",
      "Epoch 1698 Batch  130/175   train_loss = 1.258\n",
      "Epoch 1698 Batch  162/175   train_loss = 1.208\n",
      "Epoch 1699 Batch   19/175   train_loss = 1.198\n",
      "Epoch 1699 Batch   51/175   train_loss = 1.164\n",
      "Epoch 1699 Batch   83/175   train_loss = 1.260\n",
      "Epoch 1699 Batch  115/175   train_loss = 1.307\n",
      "Epoch 1699 Batch  147/175   train_loss = 1.177\n",
      "Epoch 1700 Batch    4/175   train_loss = 1.210\n",
      "Epoch 1700 Batch   36/175   train_loss = 1.181\n",
      "Epoch 1700 Batch   68/175   train_loss = 1.195\n",
      "Epoch 1700 Batch  100/175   train_loss = 1.198\n",
      "Epoch 1700 Batch  132/175   train_loss = 1.185\n",
      "Epoch 1700 Batch  164/175   train_loss = 1.165\n",
      "Epoch 1701 Batch   21/175   train_loss = 1.176\n",
      "Epoch 1701 Batch   53/175   train_loss = 1.172\n",
      "Epoch 1701 Batch   85/175   train_loss = 1.240\n",
      "Epoch 1701 Batch  117/175   train_loss = 1.219\n",
      "Epoch 1701 Batch  149/175   train_loss = 1.209\n",
      "Epoch 1702 Batch    6/175   train_loss = 1.232\n",
      "Epoch 1702 Batch   38/175   train_loss = 1.149\n",
      "Epoch 1702 Batch   70/175   train_loss = 1.218\n",
      "Epoch 1702 Batch  102/175   train_loss = 1.222\n",
      "Epoch 1702 Batch  134/175   train_loss = 1.178\n",
      "Epoch 1702 Batch  166/175   train_loss = 1.221\n",
      "Epoch 1703 Batch   23/175   train_loss = 1.157\n",
      "Epoch 1703 Batch   55/175   train_loss = 1.268\n",
      "Epoch 1703 Batch   87/175   train_loss = 1.289\n",
      "Epoch 1703 Batch  119/175   train_loss = 1.224\n",
      "Epoch 1703 Batch  151/175   train_loss = 1.240\n",
      "Epoch 1704 Batch    8/175   train_loss = 1.210\n",
      "Epoch 1704 Batch   40/175   train_loss = 1.198\n",
      "Epoch 1704 Batch   72/175   train_loss = 1.238\n",
      "Epoch 1704 Batch  104/175   train_loss = 1.228\n",
      "Epoch 1704 Batch  136/175   train_loss = 1.179\n",
      "Epoch 1704 Batch  168/175   train_loss = 1.213\n",
      "Epoch 1705 Batch   25/175   train_loss = 1.224\n",
      "Epoch 1705 Batch   57/175   train_loss = 1.271\n",
      "Epoch 1705 Batch   89/175   train_loss = 1.233\n",
      "Epoch 1705 Batch  121/175   train_loss = 1.190\n",
      "Epoch 1705 Batch  153/175   train_loss = 1.215\n",
      "Epoch 1706 Batch   10/175   train_loss = 1.164\n",
      "Epoch 1706 Batch   42/175   train_loss = 1.253\n",
      "Epoch 1706 Batch   74/175   train_loss = 1.266\n",
      "Epoch 1706 Batch  106/175   train_loss = 1.255\n",
      "Epoch 1706 Batch  138/175   train_loss = 1.189\n",
      "Epoch 1706 Batch  170/175   train_loss = 1.248\n",
      "Epoch 1707 Batch   27/175   train_loss = 1.197\n",
      "Epoch 1707 Batch   59/175   train_loss = 1.177\n",
      "Epoch 1707 Batch   91/175   train_loss = 1.191\n",
      "Epoch 1707 Batch  123/175   train_loss = 1.196\n",
      "Epoch 1707 Batch  155/175   train_loss = 1.178\n",
      "Epoch 1708 Batch   12/175   train_loss = 1.230\n",
      "Epoch 1708 Batch   44/175   train_loss = 1.202\n",
      "Epoch 1708 Batch   76/175   train_loss = 1.189\n",
      "Epoch 1708 Batch  108/175   train_loss = 1.225\n",
      "Epoch 1708 Batch  140/175   train_loss = 1.196\n",
      "Epoch 1708 Batch  172/175   train_loss = 1.254\n",
      "Epoch 1709 Batch   29/175   train_loss = 1.200\n",
      "Epoch 1709 Batch   61/175   train_loss = 1.253\n",
      "Epoch 1709 Batch   93/175   train_loss = 1.204\n",
      "Epoch 1709 Batch  125/175   train_loss = 1.215\n",
      "Epoch 1709 Batch  157/175   train_loss = 1.241\n",
      "Epoch 1710 Batch   14/175   train_loss = 1.233\n",
      "Epoch 1710 Batch   46/175   train_loss = 1.225\n",
      "Epoch 1710 Batch   78/175   train_loss = 1.233\n",
      "Epoch 1710 Batch  110/175   train_loss = 1.303\n",
      "Epoch 1710 Batch  142/175   train_loss = 1.242\n",
      "Epoch 1710 Batch  174/175   train_loss = 1.225\n",
      "Epoch 1711 Batch   31/175   train_loss = 1.222\n",
      "Epoch 1711 Batch   63/175   train_loss = 1.225\n",
      "Epoch 1711 Batch   95/175   train_loss = 1.197\n",
      "Epoch 1711 Batch  127/175   train_loss = 1.171\n",
      "Epoch 1711 Batch  159/175   train_loss = 1.199\n",
      "Epoch 1712 Batch   16/175   train_loss = 1.215\n",
      "Epoch 1712 Batch   48/175   train_loss = 1.220\n",
      "Epoch 1712 Batch   80/175   train_loss = 1.228\n",
      "Epoch 1712 Batch  112/175   train_loss = 1.200\n",
      "Epoch 1712 Batch  144/175   train_loss = 1.138\n",
      "Epoch 1713 Batch    1/175   train_loss = 1.245\n",
      "Epoch 1713 Batch   33/175   train_loss = 1.253\n",
      "Epoch 1713 Batch   65/175   train_loss = 1.208\n",
      "Epoch 1713 Batch   97/175   train_loss = 1.194\n",
      "Epoch 1713 Batch  129/175   train_loss = 1.197\n",
      "Epoch 1713 Batch  161/175   train_loss = 1.183\n",
      "Epoch 1714 Batch   18/175   train_loss = 1.157\n",
      "Epoch 1714 Batch   50/175   train_loss = 1.206\n",
      "Epoch 1714 Batch   82/175   train_loss = 1.237\n",
      "Epoch 1714 Batch  114/175   train_loss = 1.243\n",
      "Epoch 1714 Batch  146/175   train_loss = 1.193\n",
      "Epoch 1715 Batch    3/175   train_loss = 1.244\n",
      "Epoch 1715 Batch   35/175   train_loss = 1.207\n",
      "Epoch 1715 Batch   67/175   train_loss = 1.168\n",
      "Epoch 1715 Batch   99/175   train_loss = 1.261\n",
      "Epoch 1715 Batch  131/175   train_loss = 1.193\n",
      "Epoch 1715 Batch  163/175   train_loss = 1.212\n",
      "Epoch 1716 Batch   20/175   train_loss = 1.176\n",
      "Epoch 1716 Batch   52/175   train_loss = 1.151\n",
      "Epoch 1716 Batch   84/175   train_loss = 1.206\n",
      "Epoch 1716 Batch  116/175   train_loss = 1.254\n",
      "Epoch 1716 Batch  148/175   train_loss = 1.209\n",
      "Epoch 1717 Batch    5/175   train_loss = 1.194\n",
      "Epoch 1717 Batch   37/175   train_loss = 1.179\n",
      "Epoch 1717 Batch   69/175   train_loss = 1.217\n",
      "Epoch 1717 Batch  101/175   train_loss = 1.236\n",
      "Epoch 1717 Batch  133/175   train_loss = 1.116\n",
      "Epoch 1717 Batch  165/175   train_loss = 1.213\n",
      "Epoch 1718 Batch   22/175   train_loss = 1.136\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1718 Batch   54/175   train_loss = 1.227\n",
      "Epoch 1718 Batch   86/175   train_loss = 1.284\n",
      "Epoch 1718 Batch  118/175   train_loss = 1.267\n",
      "Epoch 1718 Batch  150/175   train_loss = 1.215\n",
      "Epoch 1719 Batch    7/175   train_loss = 1.219\n",
      "Epoch 1719 Batch   39/175   train_loss = 1.154\n",
      "Epoch 1719 Batch   71/175   train_loss = 1.243\n",
      "Epoch 1719 Batch  103/175   train_loss = 1.193\n",
      "Epoch 1719 Batch  135/175   train_loss = 1.167\n",
      "Epoch 1719 Batch  167/175   train_loss = 1.258\n",
      "Epoch 1720 Batch   24/175   train_loss = 1.156\n",
      "Epoch 1720 Batch   56/175   train_loss = 1.234\n",
      "Epoch 1720 Batch   88/175   train_loss = 1.252\n",
      "Epoch 1720 Batch  120/175   train_loss = 1.186\n",
      "Epoch 1720 Batch  152/175   train_loss = 1.185\n",
      "Epoch 1721 Batch    9/175   train_loss = 1.226\n",
      "Epoch 1721 Batch   41/175   train_loss = 1.226\n",
      "Epoch 1721 Batch   73/175   train_loss = 1.219\n",
      "Epoch 1721 Batch  105/175   train_loss = 1.257\n",
      "Epoch 1721 Batch  137/175   train_loss = 1.162\n",
      "Epoch 1721 Batch  169/175   train_loss = 1.239\n",
      "Epoch 1722 Batch   26/175   train_loss = 1.205\n",
      "Epoch 1722 Batch   58/175   train_loss = 1.244\n",
      "Epoch 1722 Batch   90/175   train_loss = 1.208\n",
      "Epoch 1722 Batch  122/175   train_loss = 1.192\n",
      "Epoch 1722 Batch  154/175   train_loss = 1.166\n",
      "Epoch 1723 Batch   11/175   train_loss = 1.182\n",
      "Epoch 1723 Batch   43/175   train_loss = 1.197\n",
      "Epoch 1723 Batch   75/175   train_loss = 1.190\n",
      "Epoch 1723 Batch  107/175   train_loss = 1.261\n",
      "Epoch 1723 Batch  139/175   train_loss = 1.156\n",
      "Epoch 1723 Batch  171/175   train_loss = 1.272\n",
      "Epoch 1724 Batch   28/175   train_loss = 1.191\n",
      "Epoch 1724 Batch   60/175   train_loss = 1.206\n",
      "Epoch 1724 Batch   92/175   train_loss = 1.190\n",
      "Epoch 1724 Batch  124/175   train_loss = 1.199\n",
      "Epoch 1724 Batch  156/175   train_loss = 1.252\n",
      "Epoch 1725 Batch   13/175   train_loss = 1.211\n",
      "Epoch 1725 Batch   45/175   train_loss = 1.194\n",
      "Epoch 1725 Batch   77/175   train_loss = 1.191\n",
      "Epoch 1725 Batch  109/175   train_loss = 1.251\n",
      "Epoch 1725 Batch  141/175   train_loss = 1.174\n",
      "Epoch 1725 Batch  173/175   train_loss = 1.173\n",
      "Epoch 1726 Batch   30/175   train_loss = 1.241\n",
      "Epoch 1726 Batch   62/175   train_loss = 1.231\n",
      "Epoch 1726 Batch   94/175   train_loss = 1.208\n",
      "Epoch 1726 Batch  126/175   train_loss = 1.209\n",
      "Epoch 1726 Batch  158/175   train_loss = 1.196\n",
      "Epoch 1727 Batch   15/175   train_loss = 1.255\n",
      "Epoch 1727 Batch   47/175   train_loss = 1.222\n",
      "Epoch 1727 Batch   79/175   train_loss = 1.246\n",
      "Epoch 1727 Batch  111/175   train_loss = 1.264\n",
      "Epoch 1727 Batch  143/175   train_loss = 1.176\n",
      "Epoch 1728 Batch    0/175   train_loss = 1.209\n",
      "Epoch 1728 Batch   32/175   train_loss = 1.298\n",
      "Epoch 1728 Batch   64/175   train_loss = 1.245\n",
      "Epoch 1728 Batch   96/175   train_loss = 1.233\n",
      "Epoch 1728 Batch  128/175   train_loss = 1.147\n",
      "Epoch 1728 Batch  160/175   train_loss = 1.218\n",
      "Epoch 1729 Batch   17/175   train_loss = 1.210\n",
      "Epoch 1729 Batch   49/175   train_loss = 1.240\n",
      "Epoch 1729 Batch   81/175   train_loss = 1.197\n",
      "Epoch 1729 Batch  113/175   train_loss = 1.238\n",
      "Epoch 1729 Batch  145/175   train_loss = 1.173\n",
      "Epoch 1730 Batch    2/175   train_loss = 1.197\n",
      "Epoch 1730 Batch   34/175   train_loss = 1.231\n",
      "Epoch 1730 Batch   66/175   train_loss = 1.252\n",
      "Epoch 1730 Batch   98/175   train_loss = 1.251\n",
      "Epoch 1730 Batch  130/175   train_loss = 1.259\n",
      "Epoch 1730 Batch  162/175   train_loss = 1.219\n",
      "Epoch 1731 Batch   19/175   train_loss = 1.210\n",
      "Epoch 1731 Batch   51/175   train_loss = 1.156\n",
      "Epoch 1731 Batch   83/175   train_loss = 1.268\n",
      "Epoch 1731 Batch  115/175   train_loss = 1.311\n",
      "Epoch 1731 Batch  147/175   train_loss = 1.172\n",
      "Epoch 1732 Batch    4/175   train_loss = 1.239\n",
      "Epoch 1732 Batch   36/175   train_loss = 1.180\n",
      "Epoch 1732 Batch   68/175   train_loss = 1.197\n",
      "Epoch 1732 Batch  100/175   train_loss = 1.212\n",
      "Epoch 1732 Batch  132/175   train_loss = 1.187\n",
      "Epoch 1732 Batch  164/175   train_loss = 1.174\n",
      "Epoch 1733 Batch   21/175   train_loss = 1.157\n",
      "Epoch 1733 Batch   53/175   train_loss = 1.189\n",
      "Epoch 1733 Batch   85/175   train_loss = 1.226\n",
      "Epoch 1733 Batch  117/175   train_loss = 1.228\n",
      "Epoch 1733 Batch  149/175   train_loss = 1.231\n",
      "Epoch 1734 Batch    6/175   train_loss = 1.237\n",
      "Epoch 1734 Batch   38/175   train_loss = 1.167\n",
      "Epoch 1734 Batch   70/175   train_loss = 1.202\n",
      "Epoch 1734 Batch  102/175   train_loss = 1.232\n",
      "Epoch 1734 Batch  134/175   train_loss = 1.169\n",
      "Epoch 1734 Batch  166/175   train_loss = 1.224\n",
      "Epoch 1735 Batch   23/175   train_loss = 1.151\n",
      "Epoch 1735 Batch   55/175   train_loss = 1.256\n",
      "Epoch 1735 Batch   87/175   train_loss = 1.271\n",
      "Epoch 1735 Batch  119/175   train_loss = 1.203\n",
      "Epoch 1735 Batch  151/175   train_loss = 1.211\n",
      "Epoch 1736 Batch    8/175   train_loss = 1.202\n",
      "Epoch 1736 Batch   40/175   train_loss = 1.191\n",
      "Epoch 1736 Batch   72/175   train_loss = 1.232\n",
      "Epoch 1736 Batch  104/175   train_loss = 1.227\n",
      "Epoch 1736 Batch  136/175   train_loss = 1.185\n",
      "Epoch 1736 Batch  168/175   train_loss = 1.225\n",
      "Epoch 1737 Batch   25/175   train_loss = 1.231\n",
      "Epoch 1737 Batch   57/175   train_loss = 1.252\n",
      "Epoch 1737 Batch   89/175   train_loss = 1.215\n",
      "Epoch 1737 Batch  121/175   train_loss = 1.193\n",
      "Epoch 1737 Batch  153/175   train_loss = 1.208\n",
      "Epoch 1738 Batch   10/175   train_loss = 1.170\n",
      "Epoch 1738 Batch   42/175   train_loss = 1.243\n",
      "Epoch 1738 Batch   74/175   train_loss = 1.241\n",
      "Epoch 1738 Batch  106/175   train_loss = 1.240\n",
      "Epoch 1738 Batch  138/175   train_loss = 1.201\n",
      "Epoch 1738 Batch  170/175   train_loss = 1.253\n",
      "Epoch 1739 Batch   27/175   train_loss = 1.214\n",
      "Epoch 1739 Batch   59/175   train_loss = 1.182\n",
      "Epoch 1739 Batch   91/175   train_loss = 1.200\n",
      "Epoch 1739 Batch  123/175   train_loss = 1.207\n",
      "Epoch 1739 Batch  155/175   train_loss = 1.163\n",
      "Epoch 1740 Batch   12/175   train_loss = 1.190\n",
      "Epoch 1740 Batch   44/175   train_loss = 1.197\n",
      "Epoch 1740 Batch   76/175   train_loss = 1.185\n",
      "Epoch 1740 Batch  108/175   train_loss = 1.209\n",
      "Epoch 1740 Batch  140/175   train_loss = 1.188\n",
      "Epoch 1740 Batch  172/175   train_loss = 1.230\n",
      "Epoch 1741 Batch   29/175   train_loss = 1.200\n",
      "Epoch 1741 Batch   61/175   train_loss = 1.244\n",
      "Epoch 1741 Batch   93/175   train_loss = 1.207\n",
      "Epoch 1741 Batch  125/175   train_loss = 1.241\n",
      "Epoch 1741 Batch  157/175   train_loss = 1.207\n",
      "Epoch 1742 Batch   14/175   train_loss = 1.231\n",
      "Epoch 1742 Batch   46/175   train_loss = 1.217\n",
      "Epoch 1742 Batch   78/175   train_loss = 1.224\n",
      "Epoch 1742 Batch  110/175   train_loss = 1.293\n",
      "Epoch 1742 Batch  142/175   train_loss = 1.236\n",
      "Epoch 1742 Batch  174/175   train_loss = 1.215\n",
      "Epoch 1743 Batch   31/175   train_loss = 1.223\n",
      "Epoch 1743 Batch   63/175   train_loss = 1.219\n",
      "Epoch 1743 Batch   95/175   train_loss = 1.207\n",
      "Epoch 1743 Batch  127/175   train_loss = 1.193\n",
      "Epoch 1743 Batch  159/175   train_loss = 1.199\n",
      "Epoch 1744 Batch   16/175   train_loss = 1.223\n",
      "Epoch 1744 Batch   48/175   train_loss = 1.218\n",
      "Epoch 1744 Batch   80/175   train_loss = 1.225\n",
      "Epoch 1744 Batch  112/175   train_loss = 1.220\n",
      "Epoch 1744 Batch  144/175   train_loss = 1.164\n",
      "Epoch 1745 Batch    1/175   train_loss = 1.248\n",
      "Epoch 1745 Batch   33/175   train_loss = 1.256\n",
      "Epoch 1745 Batch   65/175   train_loss = 1.219\n",
      "Epoch 1745 Batch   97/175   train_loss = 1.205\n",
      "Epoch 1745 Batch  129/175   train_loss = 1.189\n",
      "Epoch 1745 Batch  161/175   train_loss = 1.178\n",
      "Epoch 1746 Batch   18/175   train_loss = 1.158\n",
      "Epoch 1746 Batch   50/175   train_loss = 1.200\n",
      "Epoch 1746 Batch   82/175   train_loss = 1.262\n",
      "Epoch 1746 Batch  114/175   train_loss = 1.262\n",
      "Epoch 1746 Batch  146/175   train_loss = 1.211\n",
      "Epoch 1747 Batch    3/175   train_loss = 1.252\n",
      "Epoch 1747 Batch   35/175   train_loss = 1.207\n",
      "Epoch 1747 Batch   67/175   train_loss = 1.167\n",
      "Epoch 1747 Batch   99/175   train_loss = 1.269\n",
      "Epoch 1747 Batch  131/175   train_loss = 1.211\n",
      "Epoch 1747 Batch  163/175   train_loss = 1.227\n",
      "Epoch 1748 Batch   20/175   train_loss = 1.185\n",
      "Epoch 1748 Batch   52/175   train_loss = 1.146\n",
      "Epoch 1748 Batch   84/175   train_loss = 1.219\n",
      "Epoch 1748 Batch  116/175   train_loss = 1.251\n",
      "Epoch 1748 Batch  148/175   train_loss = 1.261\n",
      "Epoch 1749 Batch    5/175   train_loss = 1.207\n",
      "Epoch 1749 Batch   37/175   train_loss = 1.195\n",
      "Epoch 1749 Batch   69/175   train_loss = 1.230\n",
      "Epoch 1749 Batch  101/175   train_loss = 1.278\n",
      "Epoch 1749 Batch  133/175   train_loss = 1.119\n",
      "Epoch 1749 Batch  165/175   train_loss = 1.230\n",
      "Epoch 1750 Batch   22/175   train_loss = 1.155\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1750 Batch   54/175   train_loss = 1.245\n",
      "Epoch 1750 Batch   86/175   train_loss = 1.261\n",
      "Epoch 1750 Batch  118/175   train_loss = 1.287\n",
      "Epoch 1750 Batch  150/175   train_loss = 1.218\n",
      "Epoch 1751 Batch    7/175   train_loss = 1.244\n",
      "Epoch 1751 Batch   39/175   train_loss = 1.194\n",
      "Epoch 1751 Batch   71/175   train_loss = 1.250\n",
      "Epoch 1751 Batch  103/175   train_loss = 1.222\n",
      "Epoch 1751 Batch  135/175   train_loss = 1.202\n",
      "Epoch 1751 Batch  167/175   train_loss = 1.279\n",
      "Epoch 1752 Batch   24/175   train_loss = 1.160\n",
      "Epoch 1752 Batch   56/175   train_loss = 1.243\n",
      "Epoch 1752 Batch   88/175   train_loss = 1.259\n",
      "Epoch 1752 Batch  120/175   train_loss = 1.208\n",
      "Epoch 1752 Batch  152/175   train_loss = 1.185\n",
      "Epoch 1753 Batch    9/175   train_loss = 1.231\n",
      "Epoch 1753 Batch   41/175   train_loss = 1.249\n",
      "Epoch 1753 Batch   73/175   train_loss = 1.230\n",
      "Epoch 1753 Batch  105/175   train_loss = 1.253\n",
      "Epoch 1753 Batch  137/175   train_loss = 1.180\n",
      "Epoch 1753 Batch  169/175   train_loss = 1.235\n",
      "Epoch 1754 Batch   26/175   train_loss = 1.222\n",
      "Epoch 1754 Batch   58/175   train_loss = 1.258\n",
      "Epoch 1754 Batch   90/175   train_loss = 1.215\n",
      "Epoch 1754 Batch  122/175   train_loss = 1.201\n",
      "Epoch 1754 Batch  154/175   train_loss = 1.175\n",
      "Epoch 1755 Batch   11/175   train_loss = 1.194\n",
      "Epoch 1755 Batch   43/175   train_loss = 1.202\n",
      "Epoch 1755 Batch   75/175   train_loss = 1.191\n",
      "Epoch 1755 Batch  107/175   train_loss = 1.266\n",
      "Epoch 1755 Batch  139/175   train_loss = 1.165\n",
      "Epoch 1755 Batch  171/175   train_loss = 1.292\n",
      "Epoch 1756 Batch   28/175   train_loss = 1.211\n",
      "Epoch 1756 Batch   60/175   train_loss = 1.214\n",
      "Epoch 1756 Batch   92/175   train_loss = 1.172\n",
      "Epoch 1756 Batch  124/175   train_loss = 1.227\n",
      "Epoch 1756 Batch  156/175   train_loss = 1.229\n",
      "Epoch 1757 Batch   13/175   train_loss = 1.201\n",
      "Epoch 1757 Batch   45/175   train_loss = 1.195\n",
      "Epoch 1757 Batch   77/175   train_loss = 1.183\n",
      "Epoch 1757 Batch  109/175   train_loss = 1.243\n",
      "Epoch 1757 Batch  141/175   train_loss = 1.165\n",
      "Epoch 1757 Batch  173/175   train_loss = 1.187\n",
      "Epoch 1758 Batch   30/175   train_loss = 1.277\n",
      "Epoch 1758 Batch   62/175   train_loss = 1.243\n",
      "Epoch 1758 Batch   94/175   train_loss = 1.208\n",
      "Epoch 1758 Batch  126/175   train_loss = 1.212\n",
      "Epoch 1758 Batch  158/175   train_loss = 1.198\n",
      "Epoch 1759 Batch   15/175   train_loss = 1.246\n",
      "Epoch 1759 Batch   47/175   train_loss = 1.216\n",
      "Epoch 1759 Batch   79/175   train_loss = 1.245\n",
      "Epoch 1759 Batch  111/175   train_loss = 1.263\n",
      "Epoch 1759 Batch  143/175   train_loss = 1.192\n",
      "Epoch 1760 Batch    0/175   train_loss = 1.218\n",
      "Epoch 1760 Batch   32/175   train_loss = 1.227\n",
      "Epoch 1760 Batch   64/175   train_loss = 1.244\n",
      "Epoch 1760 Batch   96/175   train_loss = 1.240\n",
      "Epoch 1760 Batch  128/175   train_loss = 1.151\n",
      "Epoch 1760 Batch  160/175   train_loss = 1.219\n",
      "Epoch 1761 Batch   17/175   train_loss = 1.184\n",
      "Epoch 1761 Batch   49/175   train_loss = 1.258\n",
      "Epoch 1761 Batch   81/175   train_loss = 1.209\n",
      "Epoch 1761 Batch  113/175   train_loss = 1.237\n",
      "Epoch 1761 Batch  145/175   train_loss = 1.194\n",
      "Epoch 1762 Batch    2/175   train_loss = 1.196\n",
      "Epoch 1762 Batch   34/175   train_loss = 1.224\n",
      "Epoch 1762 Batch   66/175   train_loss = 1.238\n",
      "Epoch 1762 Batch   98/175   train_loss = 1.232\n",
      "Epoch 1762 Batch  130/175   train_loss = 1.240\n",
      "Epoch 1762 Batch  162/175   train_loss = 1.217\n",
      "Epoch 1763 Batch   19/175   train_loss = 1.220\n",
      "Epoch 1763 Batch   51/175   train_loss = 1.165\n",
      "Epoch 1763 Batch   83/175   train_loss = 1.276\n",
      "Epoch 1763 Batch  115/175   train_loss = 1.327\n",
      "Epoch 1763 Batch  147/175   train_loss = 1.187\n",
      "Epoch 1764 Batch    4/175   train_loss = 1.220\n",
      "Epoch 1764 Batch   36/175   train_loss = 1.169\n",
      "Epoch 1764 Batch   68/175   train_loss = 1.205\n",
      "Epoch 1764 Batch  100/175   train_loss = 1.211\n",
      "Epoch 1764 Batch  132/175   train_loss = 1.180\n",
      "Epoch 1764 Batch  164/175   train_loss = 1.172\n",
      "Epoch 1765 Batch   21/175   train_loss = 1.156\n",
      "Epoch 1765 Batch   53/175   train_loss = 1.166\n",
      "Epoch 1765 Batch   85/175   train_loss = 1.231\n",
      "Epoch 1765 Batch  117/175   train_loss = 1.218\n",
      "Epoch 1765 Batch  149/175   train_loss = 1.219\n",
      "Epoch 1766 Batch    6/175   train_loss = 1.227\n",
      "Epoch 1766 Batch   38/175   train_loss = 1.159\n",
      "Epoch 1766 Batch   70/175   train_loss = 1.202\n",
      "Epoch 1766 Batch  102/175   train_loss = 1.201\n",
      "Epoch 1766 Batch  134/175   train_loss = 1.168\n",
      "Epoch 1766 Batch  166/175   train_loss = 1.214\n",
      "Epoch 1767 Batch   23/175   train_loss = 1.171\n",
      "Epoch 1767 Batch   55/175   train_loss = 1.254\n",
      "Epoch 1767 Batch   87/175   train_loss = 1.284\n",
      "Epoch 1767 Batch  119/175   train_loss = 1.184\n",
      "Epoch 1767 Batch  151/175   train_loss = 1.224\n",
      "Epoch 1768 Batch    8/175   train_loss = 1.207\n",
      "Epoch 1768 Batch   40/175   train_loss = 1.196\n",
      "Epoch 1768 Batch   72/175   train_loss = 1.225\n",
      "Epoch 1768 Batch  104/175   train_loss = 1.224\n",
      "Epoch 1768 Batch  136/175   train_loss = 1.185\n",
      "Epoch 1768 Batch  168/175   train_loss = 1.231\n",
      "Epoch 1769 Batch   25/175   train_loss = 1.227\n",
      "Epoch 1769 Batch   57/175   train_loss = 1.262\n",
      "Epoch 1769 Batch   89/175   train_loss = 1.230\n",
      "Epoch 1769 Batch  121/175   train_loss = 1.199\n",
      "Epoch 1769 Batch  153/175   train_loss = 1.213\n",
      "Epoch 1770 Batch   10/175   train_loss = 1.191\n",
      "Epoch 1770 Batch   42/175   train_loss = 1.242\n",
      "Epoch 1770 Batch   74/175   train_loss = 1.261\n",
      "Epoch 1770 Batch  106/175   train_loss = 1.278\n",
      "Epoch 1770 Batch  138/175   train_loss = 1.192\n",
      "Epoch 1770 Batch  170/175   train_loss = 1.248\n",
      "Epoch 1771 Batch   27/175   train_loss = 1.195\n",
      "Epoch 1771 Batch   59/175   train_loss = 1.188\n",
      "Epoch 1771 Batch   91/175   train_loss = 1.207\n",
      "Epoch 1771 Batch  123/175   train_loss = 1.185\n",
      "Epoch 1771 Batch  155/175   train_loss = 1.184\n",
      "Epoch 1772 Batch   12/175   train_loss = 1.197\n",
      "Epoch 1772 Batch   44/175   train_loss = 1.194\n",
      "Epoch 1772 Batch   76/175   train_loss = 1.174\n",
      "Epoch 1772 Batch  108/175   train_loss = 1.210\n",
      "Epoch 1772 Batch  140/175   train_loss = 1.179\n",
      "Epoch 1772 Batch  172/175   train_loss = 1.223\n",
      "Epoch 1773 Batch   29/175   train_loss = 1.199\n",
      "Epoch 1773 Batch   61/175   train_loss = 1.242\n",
      "Epoch 1773 Batch   93/175   train_loss = 1.206\n",
      "Epoch 1773 Batch  125/175   train_loss = 1.216\n",
      "Epoch 1773 Batch  157/175   train_loss = 1.218\n",
      "Epoch 1774 Batch   14/175   train_loss = 1.225\n",
      "Epoch 1774 Batch   46/175   train_loss = 1.224\n",
      "Epoch 1774 Batch   78/175   train_loss = 1.192\n",
      "Epoch 1774 Batch  110/175   train_loss = 1.290\n",
      "Epoch 1774 Batch  142/175   train_loss = 1.227\n",
      "Epoch 1774 Batch  174/175   train_loss = 1.188\n",
      "Epoch 1775 Batch   31/175   train_loss = 1.218\n",
      "Epoch 1775 Batch   63/175   train_loss = 1.217\n",
      "Epoch 1775 Batch   95/175   train_loss = 1.205\n",
      "Epoch 1775 Batch  127/175   train_loss = 1.185\n",
      "Epoch 1775 Batch  159/175   train_loss = 1.185\n",
      "Epoch 1776 Batch   16/175   train_loss = 1.202\n",
      "Epoch 1776 Batch   48/175   train_loss = 1.216\n",
      "Epoch 1776 Batch   80/175   train_loss = 1.234\n",
      "Epoch 1776 Batch  112/175   train_loss = 1.220\n",
      "Epoch 1776 Batch  144/175   train_loss = 1.139\n",
      "Epoch 1777 Batch    1/175   train_loss = 1.251\n",
      "Epoch 1777 Batch   33/175   train_loss = 1.252\n",
      "Epoch 1777 Batch   65/175   train_loss = 1.214\n",
      "Epoch 1777 Batch   97/175   train_loss = 1.215\n",
      "Epoch 1777 Batch  129/175   train_loss = 1.190\n",
      "Epoch 1777 Batch  161/175   train_loss = 1.177\n",
      "Epoch 1778 Batch   18/175   train_loss = 1.181\n",
      "Epoch 1778 Batch   50/175   train_loss = 1.221\n",
      "Epoch 1778 Batch   82/175   train_loss = 1.263\n",
      "Epoch 1778 Batch  114/175   train_loss = 1.324\n",
      "Epoch 1778 Batch  146/175   train_loss = 1.218\n",
      "Epoch 1779 Batch    3/175   train_loss = 1.278\n",
      "Epoch 1779 Batch   35/175   train_loss = 1.235\n",
      "Epoch 1779 Batch   67/175   train_loss = 1.185\n",
      "Epoch 1779 Batch   99/175   train_loss = 1.282\n",
      "Epoch 1779 Batch  131/175   train_loss = 1.215\n",
      "Epoch 1779 Batch  163/175   train_loss = 1.240\n",
      "Epoch 1780 Batch   20/175   train_loss = 1.202\n",
      "Epoch 1780 Batch   52/175   train_loss = 1.147\n",
      "Epoch 1780 Batch   84/175   train_loss = 1.213\n",
      "Epoch 1780 Batch  116/175   train_loss = 1.252\n",
      "Epoch 1780 Batch  148/175   train_loss = 1.205\n",
      "Epoch 1781 Batch    5/175   train_loss = 1.206\n",
      "Epoch 1781 Batch   37/175   train_loss = 1.254\n",
      "Epoch 1781 Batch   69/175   train_loss = 1.284\n",
      "Epoch 1781 Batch  101/175   train_loss = 1.289\n",
      "Epoch 1781 Batch  133/175   train_loss = 1.131\n",
      "Epoch 1781 Batch  165/175   train_loss = 1.217\n",
      "Epoch 1782 Batch   22/175   train_loss = 1.136\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1782 Batch   54/175   train_loss = 1.226\n",
      "Epoch 1782 Batch   86/175   train_loss = 1.281\n",
      "Epoch 1782 Batch  118/175   train_loss = 1.268\n",
      "Epoch 1782 Batch  150/175   train_loss = 1.215\n",
      "Epoch 1783 Batch    7/175   train_loss = 1.236\n",
      "Epoch 1783 Batch   39/175   train_loss = 1.175\n",
      "Epoch 1783 Batch   71/175   train_loss = 1.242\n",
      "Epoch 1783 Batch  103/175   train_loss = 1.217\n",
      "Epoch 1783 Batch  135/175   train_loss = 1.169\n",
      "Epoch 1783 Batch  167/175   train_loss = 1.261\n",
      "Epoch 1784 Batch   24/175   train_loss = 1.159\n",
      "Epoch 1784 Batch   56/175   train_loss = 1.251\n",
      "Epoch 1784 Batch   88/175   train_loss = 1.237\n",
      "Epoch 1784 Batch  120/175   train_loss = 1.198\n",
      "Epoch 1784 Batch  152/175   train_loss = 1.185\n",
      "Epoch 1785 Batch    9/175   train_loss = 1.227\n",
      "Epoch 1785 Batch   41/175   train_loss = 1.259\n",
      "Epoch 1785 Batch   73/175   train_loss = 1.224\n",
      "Epoch 1785 Batch  105/175   train_loss = 1.249\n",
      "Epoch 1785 Batch  137/175   train_loss = 1.175\n",
      "Epoch 1785 Batch  169/175   train_loss = 1.233\n",
      "Epoch 1786 Batch   26/175   train_loss = 1.215\n",
      "Epoch 1786 Batch   58/175   train_loss = 1.258\n",
      "Epoch 1786 Batch   90/175   train_loss = 1.221\n",
      "Epoch 1786 Batch  122/175   train_loss = 1.189\n",
      "Epoch 1786 Batch  154/175   train_loss = 1.186\n",
      "Epoch 1787 Batch   11/175   train_loss = 1.209\n",
      "Epoch 1787 Batch   43/175   train_loss = 1.223\n",
      "Epoch 1787 Batch   75/175   train_loss = 1.197\n",
      "Epoch 1787 Batch  107/175   train_loss = 1.243\n",
      "Epoch 1787 Batch  139/175   train_loss = 1.160\n",
      "Epoch 1787 Batch  171/175   train_loss = 1.286\n",
      "Epoch 1788 Batch   28/175   train_loss = 1.217\n",
      "Epoch 1788 Batch   60/175   train_loss = 1.209\n",
      "Epoch 1788 Batch   92/175   train_loss = 1.189\n",
      "Epoch 1788 Batch  124/175   train_loss = 1.218\n",
      "Epoch 1788 Batch  156/175   train_loss = 1.250\n",
      "Epoch 1789 Batch   13/175   train_loss = 1.218\n",
      "Epoch 1789 Batch   45/175   train_loss = 1.192\n",
      "Epoch 1789 Batch   77/175   train_loss = 1.191\n",
      "Epoch 1789 Batch  109/175   train_loss = 1.251\n",
      "Epoch 1789 Batch  141/175   train_loss = 1.171\n",
      "Epoch 1789 Batch  173/175   train_loss = 1.196\n",
      "Epoch 1790 Batch   30/175   train_loss = 1.266\n",
      "Epoch 1790 Batch   62/175   train_loss = 1.238\n",
      "Epoch 1790 Batch   94/175   train_loss = 1.239\n",
      "Epoch 1790 Batch  126/175   train_loss = 1.221\n",
      "Epoch 1790 Batch  158/175   train_loss = 1.203\n",
      "Epoch 1791 Batch   15/175   train_loss = 1.245\n",
      "Epoch 1791 Batch   47/175   train_loss = 1.234\n",
      "Epoch 1791 Batch   79/175   train_loss = 1.265\n",
      "Epoch 1791 Batch  111/175   train_loss = 1.248\n",
      "Epoch 1791 Batch  143/175   train_loss = 1.192\n",
      "Epoch 1792 Batch    0/175   train_loss = 1.219\n",
      "Epoch 1792 Batch   32/175   train_loss = 1.220\n",
      "Epoch 1792 Batch   64/175   train_loss = 1.262\n",
      "Epoch 1792 Batch   96/175   train_loss = 1.255\n",
      "Epoch 1792 Batch  128/175   train_loss = 1.167\n",
      "Epoch 1792 Batch  160/175   train_loss = 1.228\n",
      "Epoch 1793 Batch   17/175   train_loss = 1.198\n",
      "Epoch 1793 Batch   49/175   train_loss = 1.222\n",
      "Epoch 1793 Batch   81/175   train_loss = 1.223\n",
      "Epoch 1793 Batch  113/175   train_loss = 1.232\n",
      "Epoch 1793 Batch  145/175   train_loss = 1.189\n",
      "Epoch 1794 Batch    2/175   train_loss = 1.214\n",
      "Epoch 1794 Batch   34/175   train_loss = 1.263\n",
      "Epoch 1794 Batch   66/175   train_loss = 1.246\n",
      "Epoch 1794 Batch   98/175   train_loss = 1.257\n",
      "Epoch 1794 Batch  130/175   train_loss = 1.293\n",
      "Epoch 1794 Batch  162/175   train_loss = 1.241\n",
      "Epoch 1795 Batch   19/175   train_loss = 1.232\n",
      "Epoch 1795 Batch   51/175   train_loss = 1.187\n",
      "Epoch 1795 Batch   83/175   train_loss = 1.273\n",
      "Epoch 1795 Batch  115/175   train_loss = 1.340\n",
      "Epoch 1795 Batch  147/175   train_loss = 1.183\n",
      "Epoch 1796 Batch    4/175   train_loss = 1.225\n",
      "Epoch 1796 Batch   36/175   train_loss = 1.187\n",
      "Epoch 1796 Batch   68/175   train_loss = 1.200\n",
      "Epoch 1796 Batch  100/175   train_loss = 1.241\n",
      "Epoch 1796 Batch  132/175   train_loss = 1.200\n",
      "Epoch 1796 Batch  164/175   train_loss = 1.198\n",
      "Epoch 1797 Batch   21/175   train_loss = 1.188\n",
      "Epoch 1797 Batch   53/175   train_loss = 1.206\n",
      "Epoch 1797 Batch   85/175   train_loss = 1.236\n",
      "Epoch 1797 Batch  117/175   train_loss = 1.240\n",
      "Epoch 1797 Batch  149/175   train_loss = 1.239\n",
      "Epoch 1798 Batch    6/175   train_loss = 1.230\n",
      "Epoch 1798 Batch   38/175   train_loss = 1.184\n",
      "Epoch 1798 Batch   70/175   train_loss = 1.224\n",
      "Epoch 1798 Batch  102/175   train_loss = 1.219\n",
      "Epoch 1798 Batch  134/175   train_loss = 1.193\n",
      "Epoch 1798 Batch  166/175   train_loss = 1.246\n",
      "Epoch 1799 Batch   23/175   train_loss = 1.171\n",
      "Epoch 1799 Batch   55/175   train_loss = 1.277\n",
      "Epoch 1799 Batch   87/175   train_loss = 1.304\n",
      "Epoch 1799 Batch  119/175   train_loss = 1.205\n",
      "Epoch 1799 Batch  151/175   train_loss = 1.237\n",
      "Epoch 1800 Batch    8/175   train_loss = 1.229\n",
      "Epoch 1800 Batch   40/175   train_loss = 1.221\n",
      "Epoch 1800 Batch   72/175   train_loss = 1.259\n",
      "Epoch 1800 Batch  104/175   train_loss = 1.250\n",
      "Epoch 1800 Batch  136/175   train_loss = 1.209\n",
      "Epoch 1800 Batch  168/175   train_loss = 1.230\n",
      "Epoch 1801 Batch   25/175   train_loss = 1.229\n",
      "Epoch 1801 Batch   57/175   train_loss = 1.262\n",
      "Epoch 1801 Batch   89/175   train_loss = 1.225\n",
      "Epoch 1801 Batch  121/175   train_loss = 1.215\n",
      "Epoch 1801 Batch  153/175   train_loss = 1.245\n",
      "Epoch 1802 Batch   10/175   train_loss = 1.183\n",
      "Epoch 1802 Batch   42/175   train_loss = 1.259\n",
      "Epoch 1802 Batch   74/175   train_loss = 1.262\n",
      "Epoch 1802 Batch  106/175   train_loss = 1.281\n",
      "Epoch 1802 Batch  138/175   train_loss = 1.205\n",
      "Epoch 1802 Batch  170/175   train_loss = 1.250\n",
      "Epoch 1803 Batch   27/175   train_loss = 1.199\n",
      "Epoch 1803 Batch   59/175   train_loss = 1.204\n",
      "Epoch 1803 Batch   91/175   train_loss = 1.193\n",
      "Epoch 1803 Batch  123/175   train_loss = 1.194\n",
      "Epoch 1803 Batch  155/175   train_loss = 1.207\n",
      "Epoch 1804 Batch   12/175   train_loss = 1.207\n",
      "Epoch 1804 Batch   44/175   train_loss = 1.224\n",
      "Epoch 1804 Batch   76/175   train_loss = 1.214\n",
      "Epoch 1804 Batch  108/175   train_loss = 1.233\n",
      "Epoch 1804 Batch  140/175   train_loss = 1.187\n",
      "Epoch 1804 Batch  172/175   train_loss = 1.238\n",
      "Epoch 1805 Batch   29/175   train_loss = 1.232\n",
      "Epoch 1805 Batch   61/175   train_loss = 1.232\n",
      "Epoch 1805 Batch   93/175   train_loss = 1.223\n",
      "Epoch 1805 Batch  125/175   train_loss = 1.231\n",
      "Epoch 1805 Batch  157/175   train_loss = 1.219\n",
      "Epoch 1806 Batch   14/175   train_loss = 1.230\n",
      "Epoch 1806 Batch   46/175   train_loss = 1.222\n",
      "Epoch 1806 Batch   78/175   train_loss = 1.199\n",
      "Epoch 1806 Batch  110/175   train_loss = 1.295\n",
      "Epoch 1806 Batch  142/175   train_loss = 1.243\n",
      "Epoch 1806 Batch  174/175   train_loss = 1.201\n",
      "Epoch 1807 Batch   31/175   train_loss = 1.235\n",
      "Epoch 1807 Batch   63/175   train_loss = 1.291\n",
      "Epoch 1807 Batch   95/175   train_loss = 1.206\n",
      "Epoch 1807 Batch  127/175   train_loss = 1.198\n",
      "Epoch 1807 Batch  159/175   train_loss = 1.206\n",
      "Epoch 1808 Batch   16/175   train_loss = 1.221\n",
      "Epoch 1808 Batch   48/175   train_loss = 1.253\n",
      "Epoch 1808 Batch   80/175   train_loss = 1.242\n",
      "Epoch 1808 Batch  112/175   train_loss = 1.224\n",
      "Epoch 1808 Batch  144/175   train_loss = 1.142\n",
      "Epoch 1809 Batch    1/175   train_loss = 1.253\n",
      "Epoch 1809 Batch   33/175   train_loss = 1.245\n",
      "Epoch 1809 Batch   65/175   train_loss = 1.223\n",
      "Epoch 1809 Batch   97/175   train_loss = 1.221\n",
      "Epoch 1809 Batch  129/175   train_loss = 1.195\n",
      "Epoch 1809 Batch  161/175   train_loss = 1.201\n",
      "Epoch 1810 Batch   18/175   train_loss = 1.171\n",
      "Epoch 1810 Batch   50/175   train_loss = 1.195\n",
      "Epoch 1810 Batch   82/175   train_loss = 1.260\n",
      "Epoch 1810 Batch  114/175   train_loss = 1.258\n",
      "Epoch 1810 Batch  146/175   train_loss = 1.213\n",
      "Epoch 1811 Batch    3/175   train_loss = 1.240\n",
      "Epoch 1811 Batch   35/175   train_loss = 1.210\n",
      "Epoch 1811 Batch   67/175   train_loss = 1.179\n",
      "Epoch 1811 Batch   99/175   train_loss = 1.268\n",
      "Epoch 1811 Batch  131/175   train_loss = 1.219\n",
      "Epoch 1811 Batch  163/175   train_loss = 1.214\n",
      "Epoch 1812 Batch   20/175   train_loss = 1.180\n",
      "Epoch 1812 Batch   52/175   train_loss = 1.157\n",
      "Epoch 1812 Batch   84/175   train_loss = 1.223\n",
      "Epoch 1812 Batch  116/175   train_loss = 1.276\n",
      "Epoch 1812 Batch  148/175   train_loss = 1.221\n",
      "Epoch 1813 Batch    5/175   train_loss = 1.200\n",
      "Epoch 1813 Batch   37/175   train_loss = 1.210\n",
      "Epoch 1813 Batch   69/175   train_loss = 1.250\n",
      "Epoch 1813 Batch  101/175   train_loss = 1.274\n",
      "Epoch 1813 Batch  133/175   train_loss = 1.123\n",
      "Epoch 1813 Batch  165/175   train_loss = 1.226\n",
      "Epoch 1814 Batch   22/175   train_loss = 1.161\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1814 Batch   54/175   train_loss = 1.268\n",
      "Epoch 1814 Batch   86/175   train_loss = 1.295\n",
      "Epoch 1814 Batch  118/175   train_loss = 1.289\n",
      "Epoch 1814 Batch  150/175   train_loss = 1.216\n",
      "Epoch 1815 Batch    7/175   train_loss = 1.251\n",
      "Epoch 1815 Batch   39/175   train_loss = 1.166\n",
      "Epoch 1815 Batch   71/175   train_loss = 1.265\n",
      "Epoch 1815 Batch  103/175   train_loss = 1.231\n",
      "Epoch 1815 Batch  135/175   train_loss = 1.193\n",
      "Epoch 1815 Batch  167/175   train_loss = 1.288\n",
      "Epoch 1816 Batch   24/175   train_loss = 1.170\n",
      "Epoch 1816 Batch   56/175   train_loss = 1.242\n",
      "Epoch 1816 Batch   88/175   train_loss = 1.264\n",
      "Epoch 1816 Batch  120/175   train_loss = 1.212\n",
      "Epoch 1816 Batch  152/175   train_loss = 1.197\n",
      "Epoch 1817 Batch    9/175   train_loss = 1.259\n",
      "Epoch 1817 Batch   41/175   train_loss = 1.286\n",
      "Epoch 1817 Batch   73/175   train_loss = 1.243\n",
      "Epoch 1817 Batch  105/175   train_loss = 1.258\n",
      "Epoch 1817 Batch  137/175   train_loss = 1.170\n",
      "Epoch 1817 Batch  169/175   train_loss = 1.232\n",
      "Epoch 1818 Batch   26/175   train_loss = 1.222\n",
      "Epoch 1818 Batch   58/175   train_loss = 1.267\n",
      "Epoch 1818 Batch   90/175   train_loss = 1.227\n",
      "Epoch 1818 Batch  122/175   train_loss = 1.199\n",
      "Epoch 1818 Batch  154/175   train_loss = 1.299\n",
      "Epoch 1819 Batch   11/175   train_loss = 1.245\n",
      "Epoch 1819 Batch   43/175   train_loss = 1.250\n",
      "Epoch 1819 Batch   75/175   train_loss = 1.219\n",
      "Epoch 1819 Batch  107/175   train_loss = 1.283\n",
      "Epoch 1819 Batch  139/175   train_loss = 1.173\n",
      "Epoch 1819 Batch  171/175   train_loss = 1.299\n",
      "Epoch 1820 Batch   28/175   train_loss = 1.227\n",
      "Epoch 1820 Batch   60/175   train_loss = 1.234\n",
      "Epoch 1820 Batch   92/175   train_loss = 1.201\n",
      "Epoch 1820 Batch  124/175   train_loss = 1.224\n",
      "Epoch 1820 Batch  156/175   train_loss = 1.265\n",
      "Epoch 1821 Batch   13/175   train_loss = 1.233\n",
      "Epoch 1821 Batch   45/175   train_loss = 1.213\n",
      "Epoch 1821 Batch   77/175   train_loss = 1.206\n",
      "Epoch 1821 Batch  109/175   train_loss = 1.278\n",
      "Epoch 1821 Batch  141/175   train_loss = 1.168\n",
      "Epoch 1821 Batch  173/175   train_loss = 1.194\n",
      "Epoch 1822 Batch   30/175   train_loss = 1.272\n",
      "Epoch 1822 Batch   62/175   train_loss = 1.255\n",
      "Epoch 1822 Batch   94/175   train_loss = 1.211\n",
      "Epoch 1822 Batch  126/175   train_loss = 1.233\n",
      "Epoch 1822 Batch  158/175   train_loss = 1.208\n",
      "Epoch 1823 Batch   15/175   train_loss = 1.274\n",
      "Epoch 1823 Batch   47/175   train_loss = 1.232\n",
      "Epoch 1823 Batch   79/175   train_loss = 1.258\n",
      "Epoch 1823 Batch  111/175   train_loss = 1.290\n",
      "Epoch 1823 Batch  143/175   train_loss = 1.205\n",
      "Epoch 1824 Batch    0/175   train_loss = 1.221\n",
      "Epoch 1824 Batch   32/175   train_loss = 1.243\n",
      "Epoch 1824 Batch   64/175   train_loss = 1.269\n",
      "Epoch 1824 Batch   96/175   train_loss = 1.251\n",
      "Epoch 1824 Batch  128/175   train_loss = 1.177\n",
      "Epoch 1824 Batch  160/175   train_loss = 1.228\n",
      "Epoch 1825 Batch   17/175   train_loss = 1.201\n",
      "Epoch 1825 Batch   49/175   train_loss = 1.266\n",
      "Epoch 1825 Batch   81/175   train_loss = 1.220\n",
      "Epoch 1825 Batch  113/175   train_loss = 1.239\n",
      "Epoch 1825 Batch  145/175   train_loss = 1.175\n",
      "Epoch 1826 Batch    2/175   train_loss = 1.212\n",
      "Epoch 1826 Batch   34/175   train_loss = 1.262\n",
      "Epoch 1826 Batch   66/175   train_loss = 1.243\n",
      "Epoch 1826 Batch   98/175   train_loss = 1.254\n",
      "Epoch 1826 Batch  130/175   train_loss = 1.250\n",
      "Epoch 1826 Batch  162/175   train_loss = 1.211\n",
      "Epoch 1827 Batch   19/175   train_loss = 1.222\n",
      "Epoch 1827 Batch   51/175   train_loss = 1.178\n",
      "Epoch 1827 Batch   83/175   train_loss = 1.292\n",
      "Epoch 1827 Batch  115/175   train_loss = 1.323\n",
      "Epoch 1827 Batch  147/175   train_loss = 1.192\n",
      "Epoch 1828 Batch    4/175   train_loss = 1.233\n",
      "Epoch 1828 Batch   36/175   train_loss = 1.184\n",
      "Epoch 1828 Batch   68/175   train_loss = 1.199\n",
      "Epoch 1828 Batch  100/175   train_loss = 1.224\n",
      "Epoch 1828 Batch  132/175   train_loss = 1.202\n",
      "Epoch 1828 Batch  164/175   train_loss = 1.174\n",
      "Epoch 1829 Batch   21/175   train_loss = 1.182\n",
      "Epoch 1829 Batch   53/175   train_loss = 1.192\n",
      "Epoch 1829 Batch   85/175   train_loss = 1.245\n",
      "Epoch 1829 Batch  117/175   train_loss = 1.252\n",
      "Epoch 1829 Batch  149/175   train_loss = 1.231\n",
      "Epoch 1830 Batch    6/175   train_loss = 1.239\n",
      "Epoch 1830 Batch   38/175   train_loss = 1.155\n",
      "Epoch 1830 Batch   70/175   train_loss = 1.226\n",
      "Epoch 1830 Batch  102/175   train_loss = 1.249\n",
      "Epoch 1830 Batch  134/175   train_loss = 1.183\n",
      "Epoch 1830 Batch  166/175   train_loss = 1.251\n",
      "Epoch 1831 Batch   23/175   train_loss = 1.182\n",
      "Epoch 1831 Batch   55/175   train_loss = 1.293\n",
      "Epoch 1831 Batch   87/175   train_loss = 1.299\n",
      "Epoch 1831 Batch  119/175   train_loss = 1.236\n",
      "Epoch 1831 Batch  151/175   train_loss = 1.225\n",
      "Epoch 1832 Batch    8/175   train_loss = 1.226\n",
      "Epoch 1832 Batch   40/175   train_loss = 1.214\n",
      "Epoch 1832 Batch   72/175   train_loss = 1.246\n",
      "Epoch 1832 Batch  104/175   train_loss = 1.250\n",
      "Epoch 1832 Batch  136/175   train_loss = 1.201\n",
      "Epoch 1832 Batch  168/175   train_loss = 1.226\n",
      "Epoch 1833 Batch   25/175   train_loss = 1.217\n",
      "Epoch 1833 Batch   57/175   train_loss = 1.253\n",
      "Epoch 1833 Batch   89/175   train_loss = 1.227\n",
      "Epoch 1833 Batch  121/175   train_loss = 1.196\n",
      "Epoch 1833 Batch  153/175   train_loss = 1.234\n",
      "Epoch 1834 Batch   10/175   train_loss = 1.183\n",
      "Epoch 1834 Batch   42/175   train_loss = 1.251\n",
      "Epoch 1834 Batch   74/175   train_loss = 1.252\n",
      "Epoch 1834 Batch  106/175   train_loss = 1.247\n",
      "Epoch 1834 Batch  138/175   train_loss = 1.195\n",
      "Epoch 1834 Batch  170/175   train_loss = 1.242\n",
      "Epoch 1835 Batch   27/175   train_loss = 1.200\n",
      "Epoch 1835 Batch   59/175   train_loss = 1.210\n",
      "Epoch 1835 Batch   91/175   train_loss = 1.213\n",
      "Epoch 1835 Batch  123/175   train_loss = 1.177\n",
      "Epoch 1835 Batch  155/175   train_loss = 1.184\n",
      "Epoch 1836 Batch   12/175   train_loss = 1.218\n",
      "Epoch 1836 Batch   44/175   train_loss = 1.197\n",
      "Epoch 1836 Batch   76/175   train_loss = 1.194\n",
      "Epoch 1836 Batch  108/175   train_loss = 1.207\n",
      "Epoch 1836 Batch  140/175   train_loss = 1.192\n",
      "Epoch 1836 Batch  172/175   train_loss = 1.231\n",
      "Epoch 1837 Batch   29/175   train_loss = 1.221\n",
      "Epoch 1837 Batch   61/175   train_loss = 1.238\n",
      "Epoch 1837 Batch   93/175   train_loss = 1.199\n",
      "Epoch 1837 Batch  125/175   train_loss = 1.226\n",
      "Epoch 1837 Batch  157/175   train_loss = 1.210\n",
      "Epoch 1838 Batch   14/175   train_loss = 1.245\n",
      "Epoch 1838 Batch   46/175   train_loss = 1.220\n",
      "Epoch 1838 Batch   78/175   train_loss = 1.189\n",
      "Epoch 1838 Batch  110/175   train_loss = 1.286\n",
      "Epoch 1838 Batch  142/175   train_loss = 1.224\n",
      "Epoch 1838 Batch  174/175   train_loss = 1.203\n",
      "Epoch 1839 Batch   31/175   train_loss = 1.242\n",
      "Epoch 1839 Batch   63/175   train_loss = 1.230\n",
      "Epoch 1839 Batch   95/175   train_loss = 1.174\n",
      "Epoch 1839 Batch  127/175   train_loss = 1.182\n",
      "Epoch 1839 Batch  159/175   train_loss = 1.183\n",
      "Epoch 1840 Batch   16/175   train_loss = 1.190\n",
      "Epoch 1840 Batch   48/175   train_loss = 1.230\n",
      "Epoch 1840 Batch   80/175   train_loss = 1.241\n",
      "Epoch 1840 Batch  112/175   train_loss = 1.209\n",
      "Epoch 1840 Batch  144/175   train_loss = 1.150\n",
      "Epoch 1841 Batch    1/175   train_loss = 1.273\n",
      "Epoch 1841 Batch   33/175   train_loss = 1.248\n",
      "Epoch 1841 Batch   65/175   train_loss = 1.217\n",
      "Epoch 1841 Batch   97/175   train_loss = 1.189\n",
      "Epoch 1841 Batch  129/175   train_loss = 1.190\n",
      "Epoch 1841 Batch  161/175   train_loss = 1.184\n",
      "Epoch 1842 Batch   18/175   train_loss = 1.162\n",
      "Epoch 1842 Batch   50/175   train_loss = 1.192\n",
      "Epoch 1842 Batch   82/175   train_loss = 1.243\n",
      "Epoch 1842 Batch  114/175   train_loss = 1.259\n",
      "Epoch 1842 Batch  146/175   train_loss = 1.198\n",
      "Epoch 1843 Batch    3/175   train_loss = 1.229\n",
      "Epoch 1843 Batch   35/175   train_loss = 1.208\n",
      "Epoch 1843 Batch   67/175   train_loss = 1.172\n",
      "Epoch 1843 Batch   99/175   train_loss = 1.282\n",
      "Epoch 1843 Batch  131/175   train_loss = 1.205\n",
      "Epoch 1843 Batch  163/175   train_loss = 1.219\n",
      "Epoch 1844 Batch   20/175   train_loss = 1.191\n",
      "Epoch 1844 Batch   52/175   train_loss = 1.142\n",
      "Epoch 1844 Batch   84/175   train_loss = 1.221\n",
      "Epoch 1844 Batch  116/175   train_loss = 1.249\n",
      "Epoch 1844 Batch  148/175   train_loss = 1.228\n",
      "Epoch 1845 Batch    5/175   train_loss = 1.200\n",
      "Epoch 1845 Batch   37/175   train_loss = 1.191\n",
      "Epoch 1845 Batch   69/175   train_loss = 1.232\n",
      "Epoch 1845 Batch  101/175   train_loss = 1.261\n",
      "Epoch 1845 Batch  133/175   train_loss = 1.120\n",
      "Epoch 1845 Batch  165/175   train_loss = 1.221\n",
      "Epoch 1846 Batch   22/175   train_loss = 1.139\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1846 Batch   54/175   train_loss = 1.231\n",
      "Epoch 1846 Batch   86/175   train_loss = 1.264\n",
      "Epoch 1846 Batch  118/175   train_loss = 1.268\n",
      "Epoch 1846 Batch  150/175   train_loss = 1.211\n",
      "Epoch 1847 Batch    7/175   train_loss = 1.249\n",
      "Epoch 1847 Batch   39/175   train_loss = 1.200\n",
      "Epoch 1847 Batch   71/175   train_loss = 1.239\n",
      "Epoch 1847 Batch  103/175   train_loss = 1.235\n",
      "Epoch 1847 Batch  135/175   train_loss = 1.211\n",
      "Epoch 1847 Batch  167/175   train_loss = 1.288\n",
      "Epoch 1848 Batch   24/175   train_loss = 1.175\n",
      "Epoch 1848 Batch   56/175   train_loss = 1.255\n",
      "Epoch 1848 Batch   88/175   train_loss = 1.266\n",
      "Epoch 1848 Batch  120/175   train_loss = 1.227\n",
      "Epoch 1848 Batch  152/175   train_loss = 1.225\n",
      "Epoch 1849 Batch    9/175   train_loss = 1.262\n",
      "Epoch 1849 Batch   41/175   train_loss = 1.262\n",
      "Epoch 1849 Batch   73/175   train_loss = 1.244\n",
      "Epoch 1849 Batch  105/175   train_loss = 1.275\n",
      "Epoch 1849 Batch  137/175   train_loss = 1.217\n",
      "Epoch 1849 Batch  169/175   train_loss = 1.263\n",
      "Epoch 1850 Batch   26/175   train_loss = 1.240\n",
      "Epoch 1850 Batch   58/175   train_loss = 1.281\n",
      "Epoch 1850 Batch   90/175   train_loss = 1.233\n",
      "Epoch 1850 Batch  122/175   train_loss = 1.248\n",
      "Epoch 1850 Batch  154/175   train_loss = 1.193\n",
      "Epoch 1851 Batch   11/175   train_loss = 1.216\n",
      "Epoch 1851 Batch   43/175   train_loss = 1.229\n",
      "Epoch 1851 Batch   75/175   train_loss = 1.226\n",
      "Epoch 1851 Batch  107/175   train_loss = 1.268\n",
      "Epoch 1851 Batch  139/175   train_loss = 1.167\n",
      "Epoch 1851 Batch  171/175   train_loss = 1.294\n",
      "Epoch 1852 Batch   28/175   train_loss = 1.204\n",
      "Epoch 1852 Batch   60/175   train_loss = 1.219\n",
      "Epoch 1852 Batch   92/175   train_loss = 1.173\n",
      "Epoch 1852 Batch  124/175   train_loss = 1.212\n",
      "Epoch 1852 Batch  156/175   train_loss = 1.248\n",
      "Epoch 1853 Batch   13/175   train_loss = 1.243\n",
      "Epoch 1853 Batch   45/175   train_loss = 1.195\n",
      "Epoch 1853 Batch   77/175   train_loss = 1.208\n",
      "Epoch 1853 Batch  109/175   train_loss = 1.277\n",
      "Epoch 1853 Batch  141/175   train_loss = 1.176\n",
      "Epoch 1853 Batch  173/175   train_loss = 1.193\n",
      "Epoch 1854 Batch   30/175   train_loss = 1.267\n",
      "Epoch 1854 Batch   62/175   train_loss = 1.259\n",
      "Epoch 1854 Batch   94/175   train_loss = 1.203\n",
      "Epoch 1854 Batch  126/175   train_loss = 1.235\n",
      "Epoch 1854 Batch  158/175   train_loss = 1.225\n",
      "Epoch 1855 Batch   15/175   train_loss = 1.282\n",
      "Epoch 1855 Batch   47/175   train_loss = 1.245\n",
      "Epoch 1855 Batch   79/175   train_loss = 1.261\n",
      "Epoch 1855 Batch  111/175   train_loss = 1.281\n",
      "Epoch 1855 Batch  143/175   train_loss = 1.188\n",
      "Epoch 1856 Batch    0/175   train_loss = 1.234\n",
      "Epoch 1856 Batch   32/175   train_loss = 1.241\n",
      "Epoch 1856 Batch   64/175   train_loss = 1.260\n",
      "Epoch 1856 Batch   96/175   train_loss = 1.277\n",
      "Epoch 1856 Batch  128/175   train_loss = 1.163\n",
      "Epoch 1856 Batch  160/175   train_loss = 1.219\n",
      "Epoch 1857 Batch   17/175   train_loss = 1.214\n",
      "Epoch 1857 Batch   49/175   train_loss = 1.255\n",
      "Epoch 1857 Batch   81/175   train_loss = 1.215\n",
      "Epoch 1857 Batch  113/175   train_loss = 1.253\n",
      "Epoch 1857 Batch  145/175   train_loss = 1.204\n",
      "Epoch 1858 Batch    2/175   train_loss = 1.216\n",
      "Epoch 1858 Batch   34/175   train_loss = 1.233\n",
      "Epoch 1858 Batch   66/175   train_loss = 1.246\n",
      "Epoch 1858 Batch   98/175   train_loss = 1.258\n",
      "Epoch 1858 Batch  130/175   train_loss = 1.278\n",
      "Epoch 1858 Batch  162/175   train_loss = 1.223\n",
      "Epoch 1859 Batch   19/175   train_loss = 1.216\n",
      "Epoch 1859 Batch   51/175   train_loss = 1.186\n",
      "Epoch 1859 Batch   83/175   train_loss = 1.275\n",
      "Epoch 1859 Batch  115/175   train_loss = 1.312\n",
      "Epoch 1859 Batch  147/175   train_loss = 1.179\n",
      "Epoch 1860 Batch    4/175   train_loss = 1.233\n",
      "Epoch 1860 Batch   36/175   train_loss = 1.177\n",
      "Epoch 1860 Batch   68/175   train_loss = 1.214\n",
      "Epoch 1860 Batch  100/175   train_loss = 1.224\n",
      "Epoch 1860 Batch  132/175   train_loss = 1.199\n",
      "Epoch 1860 Batch  164/175   train_loss = 1.176\n",
      "Epoch 1861 Batch   21/175   train_loss = 1.176\n",
      "Epoch 1861 Batch   53/175   train_loss = 1.181\n",
      "Epoch 1861 Batch   85/175   train_loss = 1.236\n",
      "Epoch 1861 Batch  117/175   train_loss = 1.225\n",
      "Epoch 1861 Batch  149/175   train_loss = 1.232\n",
      "Epoch 1862 Batch    6/175   train_loss = 1.246\n",
      "Epoch 1862 Batch   38/175   train_loss = 1.158\n",
      "Epoch 1862 Batch   70/175   train_loss = 1.228\n",
      "Epoch 1862 Batch  102/175   train_loss = 1.218\n",
      "Epoch 1862 Batch  134/175   train_loss = 1.182\n",
      "Epoch 1862 Batch  166/175   train_loss = 1.223\n",
      "Epoch 1863 Batch   23/175   train_loss = 1.166\n",
      "Epoch 1863 Batch   55/175   train_loss = 1.263\n",
      "Epoch 1863 Batch   87/175   train_loss = 1.285\n",
      "Epoch 1863 Batch  119/175   train_loss = 1.208\n",
      "Epoch 1863 Batch  151/175   train_loss = 1.235\n",
      "Epoch 1864 Batch    8/175   train_loss = 1.220\n",
      "Epoch 1864 Batch   40/175   train_loss = 1.200\n",
      "Epoch 1864 Batch   72/175   train_loss = 1.246\n",
      "Epoch 1864 Batch  104/175   train_loss = 1.244\n",
      "Epoch 1864 Batch  136/175   train_loss = 1.217\n",
      "Epoch 1864 Batch  168/175   train_loss = 1.232\n",
      "Epoch 1865 Batch   25/175   train_loss = 1.248\n",
      "Epoch 1865 Batch   57/175   train_loss = 1.260\n",
      "Epoch 1865 Batch   89/175   train_loss = 1.222\n",
      "Epoch 1865 Batch  121/175   train_loss = 1.209\n",
      "Epoch 1865 Batch  153/175   train_loss = 1.215\n",
      "Epoch 1866 Batch   10/175   train_loss = 1.180\n",
      "Epoch 1866 Batch   42/175   train_loss = 1.267\n",
      "Epoch 1866 Batch   74/175   train_loss = 1.254\n",
      "Epoch 1866 Batch  106/175   train_loss = 1.265\n",
      "Epoch 1866 Batch  138/175   train_loss = 1.209\n",
      "Epoch 1866 Batch  170/175   train_loss = 1.255\n",
      "Epoch 1867 Batch   27/175   train_loss = 1.204\n",
      "Epoch 1867 Batch   59/175   train_loss = 1.196\n",
      "Epoch 1867 Batch   91/175   train_loss = 1.194\n",
      "Epoch 1867 Batch  123/175   train_loss = 1.214\n",
      "Epoch 1867 Batch  155/175   train_loss = 1.201\n",
      "Epoch 1868 Batch   12/175   train_loss = 1.199\n",
      "Epoch 1868 Batch   44/175   train_loss = 1.190\n",
      "Epoch 1868 Batch   76/175   train_loss = 1.198\n",
      "Epoch 1868 Batch  108/175   train_loss = 1.220\n",
      "Epoch 1868 Batch  140/175   train_loss = 1.210\n",
      "Epoch 1868 Batch  172/175   train_loss = 1.234\n",
      "Epoch 1869 Batch   29/175   train_loss = 1.210\n",
      "Epoch 1869 Batch   61/175   train_loss = 1.239\n",
      "Epoch 1869 Batch   93/175   train_loss = 1.225\n",
      "Epoch 1869 Batch  125/175   train_loss = 1.245\n",
      "Epoch 1869 Batch  157/175   train_loss = 1.254\n",
      "Epoch 1870 Batch   14/175   train_loss = 1.228\n",
      "Epoch 1870 Batch   46/175   train_loss = 1.211\n",
      "Epoch 1870 Batch   78/175   train_loss = 1.221\n",
      "Epoch 1870 Batch  110/175   train_loss = 1.285\n",
      "Epoch 1870 Batch  142/175   train_loss = 1.250\n",
      "Epoch 1870 Batch  174/175   train_loss = 1.241\n",
      "Epoch 1871 Batch   31/175   train_loss = 1.248\n",
      "Epoch 1871 Batch   63/175   train_loss = 1.222\n",
      "Epoch 1871 Batch   95/175   train_loss = 1.194\n",
      "Epoch 1871 Batch  127/175   train_loss = 1.191\n",
      "Epoch 1871 Batch  159/175   train_loss = 1.176\n",
      "Epoch 1872 Batch   16/175   train_loss = 1.209\n",
      "Epoch 1872 Batch   48/175   train_loss = 1.228\n",
      "Epoch 1872 Batch   80/175   train_loss = 1.217\n",
      "Epoch 1872 Batch  112/175   train_loss = 1.217\n",
      "Epoch 1872 Batch  144/175   train_loss = 1.146\n",
      "Epoch 1873 Batch    1/175   train_loss = 1.273\n",
      "Epoch 1873 Batch   33/175   train_loss = 1.266\n",
      "Epoch 1873 Batch   65/175   train_loss = 1.203\n",
      "Epoch 1873 Batch   97/175   train_loss = 1.197\n",
      "Epoch 1873 Batch  129/175   train_loss = 1.206\n",
      "Epoch 1873 Batch  161/175   train_loss = 1.205\n",
      "Epoch 1874 Batch   18/175   train_loss = 1.147\n",
      "Epoch 1874 Batch   50/175   train_loss = 1.184\n",
      "Epoch 1874 Batch   82/175   train_loss = 1.260\n",
      "Epoch 1874 Batch  114/175   train_loss = 1.250\n",
      "Epoch 1874 Batch  146/175   train_loss = 1.202\n",
      "Epoch 1875 Batch    3/175   train_loss = 1.241\n",
      "Epoch 1875 Batch   35/175   train_loss = 1.210\n",
      "Epoch 1875 Batch   67/175   train_loss = 1.170\n",
      "Epoch 1875 Batch   99/175   train_loss = 1.252\n",
      "Epoch 1875 Batch  131/175   train_loss = 1.209\n",
      "Epoch 1875 Batch  163/175   train_loss = 1.235\n",
      "Epoch 1876 Batch   20/175   train_loss = 1.183\n",
      "Epoch 1876 Batch   52/175   train_loss = 1.159\n",
      "Epoch 1876 Batch   84/175   train_loss = 1.226\n",
      "Epoch 1876 Batch  116/175   train_loss = 1.273\n",
      "Epoch 1876 Batch  148/175   train_loss = 1.217\n",
      "Epoch 1877 Batch    5/175   train_loss = 1.188\n",
      "Epoch 1877 Batch   37/175   train_loss = 1.201\n",
      "Epoch 1877 Batch   69/175   train_loss = 1.227\n",
      "Epoch 1877 Batch  101/175   train_loss = 1.256\n",
      "Epoch 1877 Batch  133/175   train_loss = 1.120\n",
      "Epoch 1877 Batch  165/175   train_loss = 1.225\n",
      "Epoch 1878 Batch   22/175   train_loss = 1.152\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1878 Batch   54/175   train_loss = 1.239\n",
      "Epoch 1878 Batch   86/175   train_loss = 1.277\n",
      "Epoch 1878 Batch  118/175   train_loss = 1.269\n",
      "Epoch 1878 Batch  150/175   train_loss = 1.227\n",
      "Epoch 1879 Batch    7/175   train_loss = 1.242\n",
      "Epoch 1879 Batch   39/175   train_loss = 1.165\n",
      "Epoch 1879 Batch   71/175   train_loss = 1.248\n",
      "Epoch 1879 Batch  103/175   train_loss = 1.231\n",
      "Epoch 1879 Batch  135/175   train_loss = 1.203\n",
      "Epoch 1879 Batch  167/175   train_loss = 1.264\n",
      "Epoch 1880 Batch   24/175   train_loss = 1.184\n",
      "Epoch 1880 Batch   56/175   train_loss = 1.245\n",
      "Epoch 1880 Batch   88/175   train_loss = 1.264\n",
      "Epoch 1880 Batch  120/175   train_loss = 1.217\n",
      "Epoch 1880 Batch  152/175   train_loss = 1.222\n",
      "Epoch 1881 Batch    9/175   train_loss = 1.241\n",
      "Epoch 1881 Batch   41/175   train_loss = 1.264\n",
      "Epoch 1881 Batch   73/175   train_loss = 1.253\n",
      "Epoch 1881 Batch  105/175   train_loss = 1.281\n",
      "Epoch 1881 Batch  137/175   train_loss = 1.208\n",
      "Epoch 1881 Batch  169/175   train_loss = 1.256\n",
      "Epoch 1882 Batch   26/175   train_loss = 1.241\n",
      "Epoch 1882 Batch   58/175   train_loss = 1.267\n",
      "Epoch 1882 Batch   90/175   train_loss = 1.282\n",
      "Epoch 1882 Batch  122/175   train_loss = 1.240\n",
      "Epoch 1882 Batch  154/175   train_loss = 1.208\n",
      "Epoch 1883 Batch   11/175   train_loss = 1.226\n",
      "Epoch 1883 Batch   43/175   train_loss = 1.231\n",
      "Epoch 1883 Batch   75/175   train_loss = 1.202\n",
      "Epoch 1883 Batch  107/175   train_loss = 1.285\n",
      "Epoch 1883 Batch  139/175   train_loss = 1.191\n",
      "Epoch 1883 Batch  171/175   train_loss = 1.291\n",
      "Epoch 1884 Batch   28/175   train_loss = 1.223\n",
      "Epoch 1884 Batch   60/175   train_loss = 1.236\n",
      "Epoch 1884 Batch   92/175   train_loss = 1.199\n",
      "Epoch 1884 Batch  124/175   train_loss = 1.230\n",
      "Epoch 1884 Batch  156/175   train_loss = 1.260\n",
      "Epoch 1885 Batch   13/175   train_loss = 1.214\n",
      "Epoch 1885 Batch   45/175   train_loss = 1.196\n",
      "Epoch 1885 Batch   77/175   train_loss = 1.190\n",
      "Epoch 1885 Batch  109/175   train_loss = 1.266\n",
      "Epoch 1885 Batch  141/175   train_loss = 1.180\n",
      "Epoch 1885 Batch  173/175   train_loss = 1.200\n",
      "Epoch 1886 Batch   30/175   train_loss = 1.252\n",
      "Epoch 1886 Batch   62/175   train_loss = 1.264\n",
      "Epoch 1886 Batch   94/175   train_loss = 1.217\n",
      "Epoch 1886 Batch  126/175   train_loss = 1.241\n",
      "Epoch 1886 Batch  158/175   train_loss = 1.223\n",
      "Epoch 1887 Batch   15/175   train_loss = 1.289\n",
      "Epoch 1887 Batch   47/175   train_loss = 1.254\n",
      "Epoch 1887 Batch   79/175   train_loss = 1.279\n",
      "Epoch 1887 Batch  111/175   train_loss = 1.291\n",
      "Epoch 1887 Batch  143/175   train_loss = 1.194\n",
      "Epoch 1888 Batch    0/175   train_loss = 1.211\n",
      "Epoch 1888 Batch   32/175   train_loss = 1.261\n",
      "Epoch 1888 Batch   64/175   train_loss = 1.272\n",
      "Epoch 1888 Batch   96/175   train_loss = 1.257\n",
      "Epoch 1888 Batch  128/175   train_loss = 1.155\n",
      "Epoch 1888 Batch  160/175   train_loss = 1.219\n",
      "Epoch 1889 Batch   17/175   train_loss = 1.185\n",
      "Epoch 1889 Batch   49/175   train_loss = 1.229\n",
      "Epoch 1889 Batch   81/175   train_loss = 1.191\n",
      "Epoch 1889 Batch  113/175   train_loss = 1.247\n",
      "Epoch 1889 Batch  145/175   train_loss = 1.187\n",
      "Epoch 1890 Batch    2/175   train_loss = 1.229\n",
      "Epoch 1890 Batch   34/175   train_loss = 1.251\n",
      "Epoch 1890 Batch   66/175   train_loss = 1.294\n",
      "Epoch 1890 Batch   98/175   train_loss = 1.263\n",
      "Epoch 1890 Batch  130/175   train_loss = 1.239\n",
      "Epoch 1890 Batch  162/175   train_loss = 1.229\n",
      "Epoch 1891 Batch   19/175   train_loss = 1.219\n",
      "Epoch 1891 Batch   51/175   train_loss = 1.164\n",
      "Epoch 1891 Batch   83/175   train_loss = 1.281\n",
      "Epoch 1891 Batch  115/175   train_loss = 1.315\n",
      "Epoch 1891 Batch  147/175   train_loss = 1.173\n",
      "Epoch 1892 Batch    4/175   train_loss = 1.240\n",
      "Epoch 1892 Batch   36/175   train_loss = 1.168\n",
      "Epoch 1892 Batch   68/175   train_loss = 1.185\n",
      "Epoch 1892 Batch  100/175   train_loss = 1.242\n",
      "Epoch 1892 Batch  132/175   train_loss = 1.191\n",
      "Epoch 1892 Batch  164/175   train_loss = 1.181\n",
      "Epoch 1893 Batch   21/175   train_loss = 1.175\n",
      "Epoch 1893 Batch   53/175   train_loss = 1.164\n",
      "Epoch 1893 Batch   85/175   train_loss = 1.235\n",
      "Epoch 1893 Batch  117/175   train_loss = 1.237\n",
      "Epoch 1893 Batch  149/175   train_loss = 1.243\n",
      "Epoch 1894 Batch    6/175   train_loss = 1.220\n",
      "Epoch 1894 Batch   38/175   train_loss = 1.168\n",
      "Epoch 1894 Batch   70/175   train_loss = 1.219\n",
      "Epoch 1894 Batch  102/175   train_loss = 1.224\n",
      "Epoch 1894 Batch  134/175   train_loss = 1.170\n",
      "Epoch 1894 Batch  166/175   train_loss = 1.213\n",
      "Epoch 1895 Batch   23/175   train_loss = 1.172\n",
      "Epoch 1895 Batch   55/175   train_loss = 1.263\n",
      "Epoch 1895 Batch   87/175   train_loss = 1.302\n",
      "Epoch 1895 Batch  119/175   train_loss = 1.211\n",
      "Epoch 1895 Batch  151/175   train_loss = 1.209\n",
      "Epoch 1896 Batch    8/175   train_loss = 1.210\n",
      "Epoch 1896 Batch   40/175   train_loss = 1.196\n",
      "Epoch 1896 Batch   72/175   train_loss = 1.214\n",
      "Epoch 1896 Batch  104/175   train_loss = 1.222\n",
      "Epoch 1896 Batch  136/175   train_loss = 1.201\n",
      "Epoch 1896 Batch  168/175   train_loss = 1.262\n",
      "Epoch 1897 Batch   25/175   train_loss = 1.242\n",
      "Epoch 1897 Batch   57/175   train_loss = 1.271\n",
      "Epoch 1897 Batch   89/175   train_loss = 1.235\n",
      "Epoch 1897 Batch  121/175   train_loss = 1.203\n",
      "Epoch 1897 Batch  153/175   train_loss = 1.218\n",
      "Epoch 1898 Batch   10/175   train_loss = 1.180\n",
      "Epoch 1898 Batch   42/175   train_loss = 1.278\n",
      "Epoch 1898 Batch   74/175   train_loss = 1.279\n",
      "Epoch 1898 Batch  106/175   train_loss = 1.296\n",
      "Epoch 1898 Batch  138/175   train_loss = 1.203\n",
      "Epoch 1898 Batch  170/175   train_loss = 1.260\n",
      "Epoch 1899 Batch   27/175   train_loss = 1.222\n",
      "Epoch 1899 Batch   59/175   train_loss = 1.193\n",
      "Epoch 1899 Batch   91/175   train_loss = 1.203\n",
      "Epoch 1899 Batch  123/175   train_loss = 1.215\n",
      "Epoch 1899 Batch  155/175   train_loss = 1.198\n",
      "Epoch 1900 Batch   12/175   train_loss = 1.218\n",
      "Epoch 1900 Batch   44/175   train_loss = 1.196\n",
      "Epoch 1900 Batch   76/175   train_loss = 1.217\n",
      "Epoch 1900 Batch  108/175   train_loss = 1.213\n",
      "Epoch 1900 Batch  140/175   train_loss = 1.177\n",
      "Epoch 1900 Batch  172/175   train_loss = 1.249\n",
      "Epoch 1901 Batch   29/175   train_loss = 1.205\n",
      "Epoch 1901 Batch   61/175   train_loss = 1.239\n",
      "Epoch 1901 Batch   93/175   train_loss = 1.261\n",
      "Epoch 1901 Batch  125/175   train_loss = 1.255\n",
      "Epoch 1901 Batch  157/175   train_loss = 1.230\n",
      "Epoch 1902 Batch   14/175   train_loss = 1.234\n",
      "Epoch 1902 Batch   46/175   train_loss = 1.222\n",
      "Epoch 1902 Batch   78/175   train_loss = 1.188\n",
      "Epoch 1902 Batch  110/175   train_loss = 1.303\n",
      "Epoch 1902 Batch  142/175   train_loss = 1.231\n",
      "Epoch 1902 Batch  174/175   train_loss = 1.220\n",
      "Epoch 1903 Batch   31/175   train_loss = 1.234\n",
      "Epoch 1903 Batch   63/175   train_loss = 1.237\n",
      "Epoch 1903 Batch   95/175   train_loss = 1.223\n",
      "Epoch 1903 Batch  127/175   train_loss = 1.183\n",
      "Epoch 1903 Batch  159/175   train_loss = 1.180\n",
      "Epoch 1904 Batch   16/175   train_loss = 1.204\n",
      "Epoch 1904 Batch   48/175   train_loss = 1.221\n",
      "Epoch 1904 Batch   80/175   train_loss = 1.259\n",
      "Epoch 1904 Batch  112/175   train_loss = 1.217\n",
      "Epoch 1904 Batch  144/175   train_loss = 1.171\n",
      "Epoch 1905 Batch    1/175   train_loss = 1.269\n",
      "Epoch 1905 Batch   33/175   train_loss = 1.263\n",
      "Epoch 1905 Batch   65/175   train_loss = 1.205\n",
      "Epoch 1905 Batch   97/175   train_loss = 1.218\n",
      "Epoch 1905 Batch  129/175   train_loss = 1.204\n",
      "Epoch 1905 Batch  161/175   train_loss = 1.203\n",
      "Epoch 1906 Batch   18/175   train_loss = 1.167\n",
      "Epoch 1906 Batch   50/175   train_loss = 1.197\n",
      "Epoch 1906 Batch   82/175   train_loss = 1.252\n",
      "Epoch 1906 Batch  114/175   train_loss = 1.262\n",
      "Epoch 1906 Batch  146/175   train_loss = 1.205\n",
      "Epoch 1907 Batch    3/175   train_loss = 1.244\n",
      "Epoch 1907 Batch   35/175   train_loss = 1.216\n",
      "Epoch 1907 Batch   67/175   train_loss = 1.170\n",
      "Epoch 1907 Batch   99/175   train_loss = 1.252\n",
      "Epoch 1907 Batch  131/175   train_loss = 1.217\n",
      "Epoch 1907 Batch  163/175   train_loss = 1.210\n",
      "Epoch 1908 Batch   20/175   train_loss = 1.183\n",
      "Epoch 1908 Batch   52/175   train_loss = 1.146\n",
      "Epoch 1908 Batch   84/175   train_loss = 1.238\n",
      "Epoch 1908 Batch  116/175   train_loss = 1.256\n",
      "Epoch 1908 Batch  148/175   train_loss = 1.194\n",
      "Epoch 1909 Batch    5/175   train_loss = 1.194\n",
      "Epoch 1909 Batch   37/175   train_loss = 1.179\n",
      "Epoch 1909 Batch   69/175   train_loss = 1.241\n",
      "Epoch 1909 Batch  101/175   train_loss = 1.249\n",
      "Epoch 1909 Batch  133/175   train_loss = 1.150\n",
      "Epoch 1909 Batch  165/175   train_loss = 1.211\n",
      "Epoch 1910 Batch   22/175   train_loss = 1.138\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1910 Batch   54/175   train_loss = 1.242\n",
      "Epoch 1910 Batch   86/175   train_loss = 1.283\n",
      "Epoch 1910 Batch  118/175   train_loss = 1.253\n",
      "Epoch 1910 Batch  150/175   train_loss = 1.209\n",
      "Epoch 1911 Batch    7/175   train_loss = 1.250\n",
      "Epoch 1911 Batch   39/175   train_loss = 1.167\n",
      "Epoch 1911 Batch   71/175   train_loss = 1.234\n",
      "Epoch 1911 Batch  103/175   train_loss = 1.216\n",
      "Epoch 1911 Batch  135/175   train_loss = 1.191\n",
      "Epoch 1911 Batch  167/175   train_loss = 1.266\n",
      "Epoch 1912 Batch   24/175   train_loss = 1.160\n",
      "Epoch 1912 Batch   56/175   train_loss = 1.241\n",
      "Epoch 1912 Batch   88/175   train_loss = 1.271\n",
      "Epoch 1912 Batch  120/175   train_loss = 1.200\n",
      "Epoch 1912 Batch  152/175   train_loss = 1.187\n",
      "Epoch 1913 Batch    9/175   train_loss = 1.226\n",
      "Epoch 1913 Batch   41/175   train_loss = 1.249\n",
      "Epoch 1913 Batch   73/175   train_loss = 1.235\n",
      "Epoch 1913 Batch  105/175   train_loss = 1.242\n",
      "Epoch 1913 Batch  137/175   train_loss = 1.181\n",
      "Epoch 1913 Batch  169/175   train_loss = 1.244\n",
      "Epoch 1914 Batch   26/175   train_loss = 1.232\n",
      "Epoch 1914 Batch   58/175   train_loss = 1.238\n",
      "Epoch 1914 Batch   90/175   train_loss = 1.204\n",
      "Epoch 1914 Batch  122/175   train_loss = 1.180\n",
      "Epoch 1914 Batch  154/175   train_loss = 1.160\n",
      "Epoch 1915 Batch   11/175   train_loss = 1.185\n",
      "Epoch 1915 Batch   43/175   train_loss = 1.203\n",
      "Epoch 1915 Batch   75/175   train_loss = 1.178\n",
      "Epoch 1915 Batch  107/175   train_loss = 1.226\n",
      "Epoch 1915 Batch  139/175   train_loss = 1.166\n",
      "Epoch 1915 Batch  171/175   train_loss = 1.258\n",
      "Epoch 1916 Batch   28/175   train_loss = 1.191\n",
      "Epoch 1916 Batch   60/175   train_loss = 1.207\n",
      "Epoch 1916 Batch   92/175   train_loss = 1.175\n",
      "Epoch 1916 Batch  124/175   train_loss = 1.200\n",
      "Epoch 1916 Batch  156/175   train_loss = 1.241\n",
      "Epoch 1917 Batch   13/175   train_loss = 1.200\n",
      "Epoch 1917 Batch   45/175   train_loss = 1.196\n",
      "Epoch 1917 Batch   77/175   train_loss = 1.192\n",
      "Epoch 1917 Batch  109/175   train_loss = 1.250\n",
      "Epoch 1917 Batch  141/175   train_loss = 1.153\n",
      "Epoch 1917 Batch  173/175   train_loss = 1.164\n",
      "Epoch 1918 Batch   30/175   train_loss = 1.260\n",
      "Epoch 1918 Batch   62/175   train_loss = 1.238\n",
      "Epoch 1918 Batch   94/175   train_loss = 1.213\n",
      "Epoch 1918 Batch  126/175   train_loss = 1.223\n",
      "Epoch 1918 Batch  158/175   train_loss = 1.182\n",
      "Epoch 1919 Batch   15/175   train_loss = 1.270\n",
      "Epoch 1919 Batch   47/175   train_loss = 1.208\n",
      "Epoch 1919 Batch   79/175   train_loss = 1.248\n",
      "Epoch 1919 Batch  111/175   train_loss = 1.288\n",
      "Epoch 1919 Batch  143/175   train_loss = 1.195\n",
      "Epoch 1920 Batch    0/175   train_loss = 1.209\n",
      "Epoch 1920 Batch   32/175   train_loss = 1.235\n",
      "Epoch 1920 Batch   64/175   train_loss = 1.252\n",
      "Epoch 1920 Batch   96/175   train_loss = 1.249\n",
      "Epoch 1920 Batch  128/175   train_loss = 1.168\n",
      "Epoch 1920 Batch  160/175   train_loss = 1.207\n",
      "Epoch 1921 Batch   17/175   train_loss = 1.195\n",
      "Epoch 1921 Batch   49/175   train_loss = 1.231\n",
      "Epoch 1921 Batch   81/175   train_loss = 1.183\n",
      "Epoch 1921 Batch  113/175   train_loss = 1.229\n",
      "Epoch 1921 Batch  145/175   train_loss = 1.172\n",
      "Epoch 1922 Batch    2/175   train_loss = 1.212\n",
      "Epoch 1922 Batch   34/175   train_loss = 1.257\n",
      "Epoch 1922 Batch   66/175   train_loss = 1.237\n",
      "Epoch 1922 Batch   98/175   train_loss = 1.238\n",
      "Epoch 1922 Batch  130/175   train_loss = 1.231\n",
      "Epoch 1922 Batch  162/175   train_loss = 1.222\n",
      "Epoch 1923 Batch   19/175   train_loss = 1.205\n",
      "Epoch 1923 Batch   51/175   train_loss = 1.167\n",
      "Epoch 1923 Batch   83/175   train_loss = 1.261\n",
      "Epoch 1923 Batch  115/175   train_loss = 1.294\n",
      "Epoch 1923 Batch  147/175   train_loss = 1.180\n",
      "Epoch 1924 Batch    4/175   train_loss = 1.235\n",
      "Epoch 1924 Batch   36/175   train_loss = 1.163\n",
      "Epoch 1924 Batch   68/175   train_loss = 1.200\n",
      "Epoch 1924 Batch  100/175   train_loss = 1.231\n",
      "Epoch 1924 Batch  132/175   train_loss = 1.196\n",
      "Epoch 1924 Batch  164/175   train_loss = 1.188\n",
      "Epoch 1925 Batch   21/175   train_loss = 1.181\n",
      "Epoch 1925 Batch   53/175   train_loss = 1.171\n",
      "Epoch 1925 Batch   85/175   train_loss = 1.240\n",
      "Epoch 1925 Batch  117/175   train_loss = 1.245\n",
      "Epoch 1925 Batch  149/175   train_loss = 1.251\n",
      "Epoch 1926 Batch    6/175   train_loss = 1.235\n",
      "Epoch 1926 Batch   38/175   train_loss = 1.172\n",
      "Epoch 1926 Batch   70/175   train_loss = 1.220\n",
      "Epoch 1926 Batch  102/175   train_loss = 1.219\n",
      "Epoch 1926 Batch  134/175   train_loss = 1.190\n",
      "Epoch 1926 Batch  166/175   train_loss = 1.211\n",
      "Epoch 1927 Batch   23/175   train_loss = 1.171\n",
      "Epoch 1927 Batch   55/175   train_loss = 1.270\n",
      "Epoch 1927 Batch   87/175   train_loss = 1.301\n",
      "Epoch 1927 Batch  119/175   train_loss = 1.223\n",
      "Epoch 1927 Batch  151/175   train_loss = 1.249\n",
      "Epoch 1928 Batch    8/175   train_loss = 1.237\n",
      "Epoch 1928 Batch   40/175   train_loss = 1.193\n",
      "Epoch 1928 Batch   72/175   train_loss = 1.221\n",
      "Epoch 1928 Batch  104/175   train_loss = 1.238\n",
      "Epoch 1928 Batch  136/175   train_loss = 1.194\n",
      "Epoch 1928 Batch  168/175   train_loss = 1.229\n",
      "Epoch 1929 Batch   25/175   train_loss = 1.216\n",
      "Epoch 1929 Batch   57/175   train_loss = 1.257\n",
      "Epoch 1929 Batch   89/175   train_loss = 1.218\n",
      "Epoch 1929 Batch  121/175   train_loss = 1.208\n",
      "Epoch 1929 Batch  153/175   train_loss = 1.214\n",
      "Epoch 1930 Batch   10/175   train_loss = 1.177\n",
      "Epoch 1930 Batch   42/175   train_loss = 1.257\n",
      "Epoch 1930 Batch   74/175   train_loss = 1.283\n",
      "Epoch 1930 Batch  106/175   train_loss = 1.267\n",
      "Epoch 1930 Batch  138/175   train_loss = 1.189\n",
      "Epoch 1930 Batch  170/175   train_loss = 1.266\n",
      "Epoch 1931 Batch   27/175   train_loss = 1.213\n",
      "Epoch 1931 Batch   59/175   train_loss = 1.189\n",
      "Epoch 1931 Batch   91/175   train_loss = 1.193\n",
      "Epoch 1931 Batch  123/175   train_loss = 1.206\n",
      "Epoch 1931 Batch  155/175   train_loss = 1.199\n",
      "Epoch 1932 Batch   12/175   train_loss = 1.229\n",
      "Epoch 1932 Batch   44/175   train_loss = 1.204\n",
      "Epoch 1932 Batch   76/175   train_loss = 1.194\n",
      "Epoch 1932 Batch  108/175   train_loss = 1.223\n",
      "Epoch 1932 Batch  140/175   train_loss = 1.169\n",
      "Epoch 1932 Batch  172/175   train_loss = 1.254\n",
      "Epoch 1933 Batch   29/175   train_loss = 1.197\n",
      "Epoch 1933 Batch   61/175   train_loss = 1.238\n",
      "Epoch 1933 Batch   93/175   train_loss = 1.218\n",
      "Epoch 1933 Batch  125/175   train_loss = 1.241\n",
      "Epoch 1933 Batch  157/175   train_loss = 1.210\n",
      "Epoch 1934 Batch   14/175   train_loss = 1.260\n",
      "Epoch 1934 Batch   46/175   train_loss = 1.241\n",
      "Epoch 1934 Batch   78/175   train_loss = 1.186\n",
      "Epoch 1934 Batch  110/175   train_loss = 1.309\n",
      "Epoch 1934 Batch  142/175   train_loss = 1.231\n",
      "Epoch 1934 Batch  174/175   train_loss = 1.206\n",
      "Epoch 1935 Batch   31/175   train_loss = 1.232\n",
      "Epoch 1935 Batch   63/175   train_loss = 1.233\n",
      "Epoch 1935 Batch   95/175   train_loss = 1.197\n",
      "Epoch 1935 Batch  127/175   train_loss = 1.176\n",
      "Epoch 1935 Batch  159/175   train_loss = 1.168\n",
      "Epoch 1936 Batch   16/175   train_loss = 1.219\n",
      "Epoch 1936 Batch   48/175   train_loss = 1.229\n",
      "Epoch 1936 Batch   80/175   train_loss = 1.223\n",
      "Epoch 1936 Batch  112/175   train_loss = 1.218\n",
      "Epoch 1936 Batch  144/175   train_loss = 1.141\n",
      "Epoch 1937 Batch    1/175   train_loss = 1.250\n",
      "Epoch 1937 Batch   33/175   train_loss = 1.253\n",
      "Epoch 1937 Batch   65/175   train_loss = 1.210\n",
      "Epoch 1937 Batch   97/175   train_loss = 1.197\n",
      "Epoch 1937 Batch  129/175   train_loss = 1.192\n",
      "Epoch 1937 Batch  161/175   train_loss = 1.207\n",
      "Epoch 1938 Batch   18/175   train_loss = 1.181\n",
      "Epoch 1938 Batch   50/175   train_loss = 1.207\n",
      "Epoch 1938 Batch   82/175   train_loss = 1.267\n",
      "Epoch 1938 Batch  114/175   train_loss = 1.276\n",
      "Epoch 1938 Batch  146/175   train_loss = 1.214\n",
      "Epoch 1939 Batch    3/175   train_loss = 1.236\n",
      "Epoch 1939 Batch   35/175   train_loss = 1.229\n",
      "Epoch 1939 Batch   67/175   train_loss = 1.189\n",
      "Epoch 1939 Batch   99/175   train_loss = 1.266\n",
      "Epoch 1939 Batch  131/175   train_loss = 1.234\n",
      "Epoch 1939 Batch  163/175   train_loss = 1.226\n",
      "Epoch 1940 Batch   20/175   train_loss = 1.221\n",
      "Epoch 1940 Batch   52/175   train_loss = 1.163\n",
      "Epoch 1940 Batch   84/175   train_loss = 1.219\n",
      "Epoch 1940 Batch  116/175   train_loss = 1.260\n",
      "Epoch 1940 Batch  148/175   train_loss = 1.208\n",
      "Epoch 1941 Batch    5/175   train_loss = 1.213\n",
      "Epoch 1941 Batch   37/175   train_loss = 1.234\n",
      "Epoch 1941 Batch   69/175   train_loss = 1.222\n",
      "Epoch 1941 Batch  101/175   train_loss = 1.259\n",
      "Epoch 1941 Batch  133/175   train_loss = 1.124\n",
      "Epoch 1941 Batch  165/175   train_loss = 1.214\n",
      "Epoch 1942 Batch   22/175   train_loss = 1.142\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1942 Batch   54/175   train_loss = 1.253\n",
      "Epoch 1942 Batch   86/175   train_loss = 1.281\n",
      "Epoch 1942 Batch  118/175   train_loss = 1.277\n",
      "Epoch 1942 Batch  150/175   train_loss = 1.217\n",
      "Epoch 1943 Batch    7/175   train_loss = 1.250\n",
      "Epoch 1943 Batch   39/175   train_loss = 1.182\n",
      "Epoch 1943 Batch   71/175   train_loss = 1.249\n",
      "Epoch 1943 Batch  103/175   train_loss = 1.215\n",
      "Epoch 1943 Batch  135/175   train_loss = 1.201\n",
      "Epoch 1943 Batch  167/175   train_loss = 1.300\n",
      "Epoch 1944 Batch   24/175   train_loss = 1.161\n",
      "Epoch 1944 Batch   56/175   train_loss = 1.246\n",
      "Epoch 1944 Batch   88/175   train_loss = 1.269\n",
      "Epoch 1944 Batch  120/175   train_loss = 1.213\n",
      "Epoch 1944 Batch  152/175   train_loss = 1.206\n",
      "Epoch 1945 Batch    9/175   train_loss = 1.245\n",
      "Epoch 1945 Batch   41/175   train_loss = 1.263\n",
      "Epoch 1945 Batch   73/175   train_loss = 1.256\n",
      "Epoch 1945 Batch  105/175   train_loss = 1.257\n",
      "Epoch 1945 Batch  137/175   train_loss = 1.198\n",
      "Epoch 1945 Batch  169/175   train_loss = 1.251\n",
      "Epoch 1946 Batch   26/175   train_loss = 1.223\n",
      "Epoch 1946 Batch   58/175   train_loss = 1.260\n",
      "Epoch 1946 Batch   90/175   train_loss = 1.230\n",
      "Epoch 1946 Batch  122/175   train_loss = 1.216\n",
      "Epoch 1946 Batch  154/175   train_loss = 1.180\n",
      "Epoch 1947 Batch   11/175   train_loss = 1.195\n",
      "Epoch 1947 Batch   43/175   train_loss = 1.233\n",
      "Epoch 1947 Batch   75/175   train_loss = 1.216\n",
      "Epoch 1947 Batch  107/175   train_loss = 1.253\n",
      "Epoch 1947 Batch  139/175   train_loss = 1.198\n",
      "Epoch 1947 Batch  171/175   train_loss = 1.272\n",
      "Epoch 1948 Batch   28/175   train_loss = 1.235\n",
      "Epoch 1948 Batch   60/175   train_loss = 1.211\n",
      "Epoch 1948 Batch   92/175   train_loss = 1.193\n",
      "Epoch 1948 Batch  124/175   train_loss = 1.220\n",
      "Epoch 1948 Batch  156/175   train_loss = 1.249\n",
      "Epoch 1949 Batch   13/175   train_loss = 1.232\n",
      "Epoch 1949 Batch   45/175   train_loss = 1.190\n",
      "Epoch 1949 Batch   77/175   train_loss = 1.196\n",
      "Epoch 1949 Batch  109/175   train_loss = 1.257\n",
      "Epoch 1949 Batch  141/175   train_loss = 1.167\n",
      "Epoch 1949 Batch  173/175   train_loss = 1.179\n",
      "Epoch 1950 Batch   30/175   train_loss = 1.307\n",
      "Epoch 1950 Batch   62/175   train_loss = 1.271\n",
      "Epoch 1950 Batch   94/175   train_loss = 1.207\n",
      "Epoch 1950 Batch  126/175   train_loss = 1.240\n",
      "Epoch 1950 Batch  158/175   train_loss = 1.199\n",
      "Epoch 1951 Batch   15/175   train_loss = 1.293\n",
      "Epoch 1951 Batch   47/175   train_loss = 1.227\n",
      "Epoch 1951 Batch   79/175   train_loss = 1.251\n",
      "Epoch 1951 Batch  111/175   train_loss = 1.270\n",
      "Epoch 1951 Batch  143/175   train_loss = 1.207\n",
      "Epoch 1952 Batch    0/175   train_loss = 1.230\n",
      "Epoch 1952 Batch   32/175   train_loss = 1.251\n",
      "Epoch 1952 Batch   64/175   train_loss = 1.257\n",
      "Epoch 1952 Batch   96/175   train_loss = 1.259\n",
      "Epoch 1952 Batch  128/175   train_loss = 1.171\n",
      "Epoch 1952 Batch  160/175   train_loss = 1.208\n",
      "Epoch 1953 Batch   17/175   train_loss = 1.194\n",
      "Epoch 1953 Batch   49/175   train_loss = 1.235\n",
      "Epoch 1953 Batch   81/175   train_loss = 1.207\n",
      "Epoch 1953 Batch  113/175   train_loss = 1.221\n",
      "Epoch 1953 Batch  145/175   train_loss = 1.171\n",
      "Epoch 1954 Batch    2/175   train_loss = 1.205\n",
      "Epoch 1954 Batch   34/175   train_loss = 1.256\n",
      "Epoch 1954 Batch   66/175   train_loss = 1.269\n",
      "Epoch 1954 Batch   98/175   train_loss = 1.249\n",
      "Epoch 1954 Batch  130/175   train_loss = 1.244\n",
      "Epoch 1954 Batch  162/175   train_loss = 1.244\n",
      "Epoch 1955 Batch   19/175   train_loss = 1.243\n",
      "Epoch 1955 Batch   51/175   train_loss = 1.171\n",
      "Epoch 1955 Batch   83/175   train_loss = 1.275\n",
      "Epoch 1955 Batch  115/175   train_loss = 1.337\n",
      "Epoch 1955 Batch  147/175   train_loss = 1.171\n",
      "Epoch 1956 Batch    4/175   train_loss = 1.240\n",
      "Epoch 1956 Batch   36/175   train_loss = 1.182\n",
      "Epoch 1956 Batch   68/175   train_loss = 1.199\n",
      "Epoch 1956 Batch  100/175   train_loss = 1.250\n",
      "Epoch 1956 Batch  132/175   train_loss = 1.209\n",
      "Epoch 1956 Batch  164/175   train_loss = 1.207\n",
      "Epoch 1957 Batch   21/175   train_loss = 1.190\n",
      "Epoch 1957 Batch   53/175   train_loss = 1.202\n",
      "Epoch 1957 Batch   85/175   train_loss = 1.278\n",
      "Epoch 1957 Batch  117/175   train_loss = 1.250\n",
      "Epoch 1957 Batch  149/175   train_loss = 1.252\n",
      "Epoch 1958 Batch    6/175   train_loss = 1.254\n",
      "Epoch 1958 Batch   38/175   train_loss = 1.190\n",
      "Epoch 1958 Batch   70/175   train_loss = 1.226\n",
      "Epoch 1958 Batch  102/175   train_loss = 1.210\n",
      "Epoch 1958 Batch  134/175   train_loss = 1.185\n",
      "Epoch 1958 Batch  166/175   train_loss = 1.222\n",
      "Epoch 1959 Batch   23/175   train_loss = 1.170\n",
      "Epoch 1959 Batch   55/175   train_loss = 1.277\n",
      "Epoch 1959 Batch   87/175   train_loss = 1.287\n",
      "Epoch 1959 Batch  119/175   train_loss = 1.217\n",
      "Epoch 1959 Batch  151/175   train_loss = 1.227\n",
      "Epoch 1960 Batch    8/175   train_loss = 1.227\n",
      "Epoch 1960 Batch   40/175   train_loss = 1.208\n",
      "Epoch 1960 Batch   72/175   train_loss = 1.247\n",
      "Epoch 1960 Batch  104/175   train_loss = 1.246\n",
      "Epoch 1960 Batch  136/175   train_loss = 1.212\n",
      "Epoch 1960 Batch  168/175   train_loss = 1.258\n",
      "Epoch 1961 Batch   25/175   train_loss = 1.227\n",
      "Epoch 1961 Batch   57/175   train_loss = 1.259\n",
      "Epoch 1961 Batch   89/175   train_loss = 1.238\n",
      "Epoch 1961 Batch  121/175   train_loss = 1.231\n",
      "Epoch 1961 Batch  153/175   train_loss = 1.215\n",
      "Epoch 1962 Batch   10/175   train_loss = 1.186\n",
      "Epoch 1962 Batch   42/175   train_loss = 1.263\n",
      "Epoch 1962 Batch   74/175   train_loss = 1.266\n",
      "Epoch 1962 Batch  106/175   train_loss = 1.248\n",
      "Epoch 1962 Batch  138/175   train_loss = 1.187\n",
      "Epoch 1962 Batch  170/175   train_loss = 1.243\n",
      "Epoch 1963 Batch   27/175   train_loss = 1.224\n",
      "Epoch 1963 Batch   59/175   train_loss = 1.230\n",
      "Epoch 1963 Batch   91/175   train_loss = 1.224\n",
      "Epoch 1963 Batch  123/175   train_loss = 1.213\n",
      "Epoch 1963 Batch  155/175   train_loss = 1.207\n",
      "Epoch 1964 Batch   12/175   train_loss = 1.222\n",
      "Epoch 1964 Batch   44/175   train_loss = 1.217\n",
      "Epoch 1964 Batch   76/175   train_loss = 1.197\n",
      "Epoch 1964 Batch  108/175   train_loss = 1.221\n",
      "Epoch 1964 Batch  140/175   train_loss = 1.216\n",
      "Epoch 1964 Batch  172/175   train_loss = 1.248\n",
      "Epoch 1965 Batch   29/175   train_loss = 1.231\n",
      "Epoch 1965 Batch   61/175   train_loss = 1.241\n",
      "Epoch 1965 Batch   93/175   train_loss = 1.219\n",
      "Epoch 1965 Batch  125/175   train_loss = 1.249\n",
      "Epoch 1965 Batch  157/175   train_loss = 1.239\n",
      "Epoch 1966 Batch   14/175   train_loss = 1.252\n",
      "Epoch 1966 Batch   46/175   train_loss = 1.224\n",
      "Epoch 1966 Batch   78/175   train_loss = 1.199\n",
      "Epoch 1966 Batch  110/175   train_loss = 1.283\n",
      "Epoch 1966 Batch  142/175   train_loss = 1.233\n",
      "Epoch 1966 Batch  174/175   train_loss = 1.225\n",
      "Epoch 1967 Batch   31/175   train_loss = 1.256\n",
      "Epoch 1967 Batch   63/175   train_loss = 1.242\n",
      "Epoch 1967 Batch   95/175   train_loss = 1.207\n",
      "Epoch 1967 Batch  127/175   train_loss = 1.178\n",
      "Epoch 1967 Batch  159/175   train_loss = 1.196\n",
      "Epoch 1968 Batch   16/175   train_loss = 1.225\n",
      "Epoch 1968 Batch   48/175   train_loss = 1.230\n",
      "Epoch 1968 Batch   80/175   train_loss = 1.227\n",
      "Epoch 1968 Batch  112/175   train_loss = 1.232\n",
      "Epoch 1968 Batch  144/175   train_loss = 1.149\n",
      "Epoch 1969 Batch    1/175   train_loss = 1.258\n",
      "Epoch 1969 Batch   33/175   train_loss = 1.278\n",
      "Epoch 1969 Batch   65/175   train_loss = 1.236\n",
      "Epoch 1969 Batch   97/175   train_loss = 1.209\n",
      "Epoch 1969 Batch  129/175   train_loss = 1.225\n",
      "Epoch 1969 Batch  161/175   train_loss = 1.195\n",
      "Epoch 1970 Batch   18/175   train_loss = 1.179\n",
      "Epoch 1970 Batch   50/175   train_loss = 1.208\n",
      "Epoch 1970 Batch   82/175   train_loss = 1.246\n",
      "Epoch 1970 Batch  114/175   train_loss = 1.284\n",
      "Epoch 1970 Batch  146/175   train_loss = 1.206\n",
      "Epoch 1971 Batch    3/175   train_loss = 1.261\n",
      "Epoch 1971 Batch   35/175   train_loss = 1.218\n",
      "Epoch 1971 Batch   67/175   train_loss = 1.199\n",
      "Epoch 1971 Batch   99/175   train_loss = 1.291\n",
      "Epoch 1971 Batch  131/175   train_loss = 1.209\n",
      "Epoch 1971 Batch  163/175   train_loss = 1.230\n",
      "Epoch 1972 Batch   20/175   train_loss = 1.212\n",
      "Epoch 1972 Batch   52/175   train_loss = 1.158\n",
      "Epoch 1972 Batch   84/175   train_loss = 1.232\n",
      "Epoch 1972 Batch  116/175   train_loss = 1.288\n",
      "Epoch 1972 Batch  148/175   train_loss = 1.215\n",
      "Epoch 1973 Batch    5/175   train_loss = 1.228\n",
      "Epoch 1973 Batch   37/175   train_loss = 1.209\n",
      "Epoch 1973 Batch   69/175   train_loss = 1.220\n",
      "Epoch 1973 Batch  101/175   train_loss = 1.280\n",
      "Epoch 1973 Batch  133/175   train_loss = 1.119\n",
      "Epoch 1973 Batch  165/175   train_loss = 1.215\n",
      "Epoch 1974 Batch   22/175   train_loss = 1.164\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1974 Batch   54/175   train_loss = 1.250\n",
      "Epoch 1974 Batch   86/175   train_loss = 1.288\n",
      "Epoch 1974 Batch  118/175   train_loss = 1.284\n",
      "Epoch 1974 Batch  150/175   train_loss = 1.232\n",
      "Epoch 1975 Batch    7/175   train_loss = 1.252\n",
      "Epoch 1975 Batch   39/175   train_loss = 1.173\n",
      "Epoch 1975 Batch   71/175   train_loss = 1.252\n",
      "Epoch 1975 Batch  103/175   train_loss = 1.235\n",
      "Epoch 1975 Batch  135/175   train_loss = 1.208\n",
      "Epoch 1975 Batch  167/175   train_loss = 1.274\n",
      "Epoch 1976 Batch   24/175   train_loss = 1.176\n",
      "Epoch 1976 Batch   56/175   train_loss = 1.265\n",
      "Epoch 1976 Batch   88/175   train_loss = 1.285\n",
      "Epoch 1976 Batch  120/175   train_loss = 1.245\n",
      "Epoch 1976 Batch  152/175   train_loss = 1.202\n",
      "Epoch 1977 Batch    9/175   train_loss = 1.262\n",
      "Epoch 1977 Batch   41/175   train_loss = 1.262\n",
      "Epoch 1977 Batch   73/175   train_loss = 1.230\n",
      "Epoch 1977 Batch  105/175   train_loss = 1.246\n",
      "Epoch 1977 Batch  137/175   train_loss = 1.187\n",
      "Epoch 1977 Batch  169/175   train_loss = 1.259\n",
      "Epoch 1978 Batch   26/175   train_loss = 1.227\n",
      "Epoch 1978 Batch   58/175   train_loss = 1.242\n",
      "Epoch 1978 Batch   90/175   train_loss = 1.227\n",
      "Epoch 1978 Batch  122/175   train_loss = 1.201\n",
      "Epoch 1978 Batch  154/175   train_loss = 1.195\n",
      "Epoch 1979 Batch   11/175   train_loss = 1.186\n",
      "Epoch 1979 Batch   43/175   train_loss = 1.210\n",
      "Epoch 1979 Batch   75/175   train_loss = 1.196\n",
      "Epoch 1979 Batch  107/175   train_loss = 1.257\n",
      "Epoch 1979 Batch  139/175   train_loss = 1.170\n",
      "Epoch 1979 Batch  171/175   train_loss = 1.264\n",
      "Epoch 1980 Batch   28/175   train_loss = 1.252\n",
      "Epoch 1980 Batch   60/175   train_loss = 1.256\n",
      "Epoch 1980 Batch   92/175   train_loss = 1.234\n",
      "Epoch 1980 Batch  124/175   train_loss = 1.244\n",
      "Epoch 1980 Batch  156/175   train_loss = 1.286\n",
      "Epoch 1981 Batch   13/175   train_loss = 1.267\n",
      "Epoch 1981 Batch   45/175   train_loss = 1.215\n",
      "Epoch 1981 Batch   77/175   train_loss = 1.233\n",
      "Epoch 1981 Batch  109/175   train_loss = 1.278\n",
      "Epoch 1981 Batch  141/175   train_loss = 1.184\n",
      "Epoch 1981 Batch  173/175   train_loss = 1.200\n",
      "Epoch 1982 Batch   30/175   train_loss = 1.312\n",
      "Epoch 1982 Batch   62/175   train_loss = 1.256\n",
      "Epoch 1982 Batch   94/175   train_loss = 1.214\n",
      "Epoch 1982 Batch  126/175   train_loss = 1.245\n",
      "Epoch 1982 Batch  158/175   train_loss = 1.208\n",
      "Epoch 1983 Batch   15/175   train_loss = 1.278\n",
      "Epoch 1983 Batch   47/175   train_loss = 1.238\n",
      "Epoch 1983 Batch   79/175   train_loss = 1.311\n",
      "Epoch 1983 Batch  111/175   train_loss = 1.301\n",
      "Epoch 1983 Batch  143/175   train_loss = 1.206\n",
      "Epoch 1984 Batch    0/175   train_loss = 1.227\n",
      "Epoch 1984 Batch   32/175   train_loss = 1.258\n",
      "Epoch 1984 Batch   64/175   train_loss = 1.260\n",
      "Epoch 1984 Batch   96/175   train_loss = 1.266\n",
      "Epoch 1984 Batch  128/175   train_loss = 1.195\n",
      "Epoch 1984 Batch  160/175   train_loss = 1.219\n",
      "Epoch 1985 Batch   17/175   train_loss = 1.200\n",
      "Epoch 1985 Batch   49/175   train_loss = 1.260\n",
      "Epoch 1985 Batch   81/175   train_loss = 1.213\n",
      "Epoch 1985 Batch  113/175   train_loss = 1.244\n",
      "Epoch 1985 Batch  145/175   train_loss = 1.170\n",
      "Epoch 1986 Batch    2/175   train_loss = 1.210\n",
      "Epoch 1986 Batch   34/175   train_loss = 1.249\n",
      "Epoch 1986 Batch   66/175   train_loss = 1.265\n",
      "Epoch 1986 Batch   98/175   train_loss = 1.272\n",
      "Epoch 1986 Batch  130/175   train_loss = 1.255\n",
      "Epoch 1986 Batch  162/175   train_loss = 1.227\n",
      "Epoch 1987 Batch   19/175   train_loss = 1.218\n",
      "Epoch 1987 Batch   51/175   train_loss = 1.173\n",
      "Epoch 1987 Batch   83/175   train_loss = 1.271\n",
      "Epoch 1987 Batch  115/175   train_loss = 1.337\n",
      "Epoch 1987 Batch  147/175   train_loss = 1.177\n",
      "Epoch 1988 Batch    4/175   train_loss = 1.235\n",
      "Epoch 1988 Batch   36/175   train_loss = 1.185\n",
      "Epoch 1988 Batch   68/175   train_loss = 1.209\n",
      "Epoch 1988 Batch  100/175   train_loss = 1.249\n",
      "Epoch 1988 Batch  132/175   train_loss = 1.194\n",
      "Epoch 1988 Batch  164/175   train_loss = 1.212\n",
      "Epoch 1989 Batch   21/175   train_loss = 1.222\n",
      "Epoch 1989 Batch   53/175   train_loss = 1.199\n",
      "Epoch 1989 Batch   85/175   train_loss = 1.243\n",
      "Epoch 1989 Batch  117/175   train_loss = 1.253\n",
      "Epoch 1989 Batch  149/175   train_loss = 1.243\n",
      "Epoch 1990 Batch    6/175   train_loss = 1.264\n",
      "Epoch 1990 Batch   38/175   train_loss = 1.181\n",
      "Epoch 1990 Batch   70/175   train_loss = 1.239\n",
      "Epoch 1990 Batch  102/175   train_loss = 1.252\n",
      "Epoch 1990 Batch  134/175   train_loss = 1.200\n",
      "Epoch 1990 Batch  166/175   train_loss = 1.231\n",
      "Epoch 1991 Batch   23/175   train_loss = 1.192\n",
      "Epoch 1991 Batch   55/175   train_loss = 1.288\n",
      "Epoch 1991 Batch   87/175   train_loss = 1.324\n",
      "Epoch 1991 Batch  119/175   train_loss = 1.207\n",
      "Epoch 1991 Batch  151/175   train_loss = 1.239\n",
      "Epoch 1992 Batch    8/175   train_loss = 1.233\n",
      "Epoch 1992 Batch   40/175   train_loss = 1.215\n",
      "Epoch 1992 Batch   72/175   train_loss = 1.231\n",
      "Epoch 1992 Batch  104/175   train_loss = 1.234\n",
      "Epoch 1992 Batch  136/175   train_loss = 1.208\n",
      "Epoch 1992 Batch  168/175   train_loss = 1.231\n",
      "Epoch 1993 Batch   25/175   train_loss = 1.219\n",
      "Epoch 1993 Batch   57/175   train_loss = 1.254\n",
      "Epoch 1993 Batch   89/175   train_loss = 1.223\n",
      "Epoch 1993 Batch  121/175   train_loss = 1.183\n",
      "Epoch 1993 Batch  153/175   train_loss = 1.219\n",
      "Epoch 1994 Batch   10/175   train_loss = 1.190\n",
      "Epoch 1994 Batch   42/175   train_loss = 1.269\n",
      "Epoch 1994 Batch   74/175   train_loss = 1.267\n",
      "Epoch 1994 Batch  106/175   train_loss = 1.251\n",
      "Epoch 1994 Batch  138/175   train_loss = 1.208\n",
      "Epoch 1994 Batch  170/175   train_loss = 1.254\n",
      "Epoch 1995 Batch   27/175   train_loss = 1.206\n",
      "Epoch 1995 Batch   59/175   train_loss = 1.204\n",
      "Epoch 1995 Batch   91/175   train_loss = 1.203\n",
      "Epoch 1995 Batch  123/175   train_loss = 1.218\n",
      "Epoch 1995 Batch  155/175   train_loss = 1.202\n",
      "Epoch 1996 Batch   12/175   train_loss = 1.211\n",
      "Epoch 1996 Batch   44/175   train_loss = 1.213\n",
      "Epoch 1996 Batch   76/175   train_loss = 1.186\n",
      "Epoch 1996 Batch  108/175   train_loss = 1.215\n",
      "Epoch 1996 Batch  140/175   train_loss = 1.200\n",
      "Epoch 1996 Batch  172/175   train_loss = 1.260\n",
      "Epoch 1997 Batch   29/175   train_loss = 1.230\n",
      "Epoch 1997 Batch   61/175   train_loss = 1.257\n",
      "Epoch 1997 Batch   93/175   train_loss = 1.215\n",
      "Epoch 1997 Batch  125/175   train_loss = 1.255\n",
      "Epoch 1997 Batch  157/175   train_loss = 1.249\n",
      "Epoch 1998 Batch   14/175   train_loss = 1.240\n",
      "Epoch 1998 Batch   46/175   train_loss = 1.214\n",
      "Epoch 1998 Batch   78/175   train_loss = 1.177\n",
      "Epoch 1998 Batch  110/175   train_loss = 1.305\n",
      "Epoch 1998 Batch  142/175   train_loss = 1.234\n",
      "Epoch 1998 Batch  174/175   train_loss = 1.226\n",
      "Epoch 1999 Batch   31/175   train_loss = 1.244\n",
      "Epoch 1999 Batch   63/175   train_loss = 1.233\n",
      "Epoch 1999 Batch   95/175   train_loss = 1.209\n",
      "Epoch 1999 Batch  127/175   train_loss = 1.184\n",
      "Epoch 1999 Batch  159/175   train_loss = 1.183\n",
      "Epoch 2000 Batch   16/175   train_loss = 1.189\n",
      "Epoch 2000 Batch   48/175   train_loss = 1.205\n",
      "Epoch 2000 Batch   80/175   train_loss = 1.227\n",
      "Epoch 2000 Batch  112/175   train_loss = 1.215\n",
      "Epoch 2000 Batch  144/175   train_loss = 1.136\n",
      "Epoch 2001 Batch    1/175   train_loss = 1.273\n",
      "Epoch 2001 Batch   33/175   train_loss = 1.260\n",
      "Epoch 2001 Batch   65/175   train_loss = 1.233\n",
      "Epoch 2001 Batch   97/175   train_loss = 1.207\n",
      "Epoch 2001 Batch  129/175   train_loss = 1.221\n",
      "Epoch 2001 Batch  161/175   train_loss = 1.198\n",
      "Epoch 2002 Batch   18/175   train_loss = 1.199\n",
      "Epoch 2002 Batch   50/175   train_loss = 1.191\n",
      "Epoch 2002 Batch   82/175   train_loss = 1.257\n",
      "Epoch 2002 Batch  114/175   train_loss = 1.269\n",
      "Epoch 2002 Batch  146/175   train_loss = 1.207\n",
      "Epoch 2003 Batch    3/175   train_loss = 1.239\n",
      "Epoch 2003 Batch   35/175   train_loss = 1.218\n",
      "Epoch 2003 Batch   67/175   train_loss = 1.189\n",
      "Epoch 2003 Batch   99/175   train_loss = 1.290\n",
      "Epoch 2003 Batch  131/175   train_loss = 1.212\n",
      "Epoch 2003 Batch  163/175   train_loss = 1.223\n",
      "Epoch 2004 Batch   20/175   train_loss = 1.203\n",
      "Epoch 2004 Batch   52/175   train_loss = 1.165\n",
      "Epoch 2004 Batch   84/175   train_loss = 1.217\n",
      "Epoch 2004 Batch  116/175   train_loss = 1.260\n",
      "Epoch 2004 Batch  148/175   train_loss = 1.216\n",
      "Epoch 2005 Batch    5/175   train_loss = 1.223\n",
      "Epoch 2005 Batch   37/175   train_loss = 1.198\n",
      "Epoch 2005 Batch   69/175   train_loss = 1.221\n",
      "Epoch 2005 Batch  101/175   train_loss = 1.274\n",
      "Epoch 2005 Batch  133/175   train_loss = 1.120\n",
      "Epoch 2005 Batch  165/175   train_loss = 1.226\n",
      "Epoch 2006 Batch   22/175   train_loss = 1.154\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2006 Batch   54/175   train_loss = 1.235\n",
      "Epoch 2006 Batch   86/175   train_loss = 1.302\n",
      "Epoch 2006 Batch  118/175   train_loss = 1.288\n",
      "Epoch 2006 Batch  150/175   train_loss = 1.222\n",
      "Epoch 2007 Batch    7/175   train_loss = 1.252\n",
      "Epoch 2007 Batch   39/175   train_loss = 1.178\n",
      "Epoch 2007 Batch   71/175   train_loss = 1.264\n",
      "Epoch 2007 Batch  103/175   train_loss = 1.217\n",
      "Epoch 2007 Batch  135/175   train_loss = 1.192\n",
      "Epoch 2007 Batch  167/175   train_loss = 1.266\n",
      "Epoch 2008 Batch   24/175   train_loss = 1.184\n",
      "Epoch 2008 Batch   56/175   train_loss = 1.244\n",
      "Epoch 2008 Batch   88/175   train_loss = 1.282\n",
      "Epoch 2008 Batch  120/175   train_loss = 1.208\n",
      "Epoch 2008 Batch  152/175   train_loss = 1.195\n",
      "Epoch 2009 Batch    9/175   train_loss = 1.260\n",
      "Epoch 2009 Batch   41/175   train_loss = 1.252\n",
      "Epoch 2009 Batch   73/175   train_loss = 1.241\n",
      "Epoch 2009 Batch  105/175   train_loss = 1.262\n",
      "Epoch 2009 Batch  137/175   train_loss = 1.207\n",
      "Epoch 2009 Batch  169/175   train_loss = 1.246\n",
      "Epoch 2010 Batch   26/175   train_loss = 1.223\n",
      "Epoch 2010 Batch   58/175   train_loss = 1.243\n",
      "Epoch 2010 Batch   90/175   train_loss = 1.233\n",
      "Epoch 2010 Batch  122/175   train_loss = 1.176\n",
      "Epoch 2010 Batch  154/175   train_loss = 1.206\n",
      "Epoch 2011 Batch   11/175   train_loss = 1.230\n",
      "Epoch 2011 Batch   43/175   train_loss = 1.240\n",
      "Epoch 2011 Batch   75/175   train_loss = 1.212\n",
      "Epoch 2011 Batch  107/175   train_loss = 1.290\n",
      "Epoch 2011 Batch  139/175   train_loss = 1.183\n",
      "Epoch 2011 Batch  171/175   train_loss = 1.258\n",
      "Epoch 2012 Batch   28/175   train_loss = 1.220\n",
      "Epoch 2012 Batch   60/175   train_loss = 1.213\n",
      "Epoch 2012 Batch   92/175   train_loss = 1.213\n",
      "Epoch 2012 Batch  124/175   train_loss = 1.226\n",
      "Epoch 2012 Batch  156/175   train_loss = 1.277\n",
      "Epoch 2013 Batch   13/175   train_loss = 1.231\n",
      "Epoch 2013 Batch   45/175   train_loss = 1.222\n",
      "Epoch 2013 Batch   77/175   train_loss = 1.202\n",
      "Epoch 2013 Batch  109/175   train_loss = 1.282\n",
      "Epoch 2013 Batch  141/175   train_loss = 1.209\n",
      "Epoch 2013 Batch  173/175   train_loss = 1.208\n",
      "Epoch 2014 Batch   30/175   train_loss = 1.290\n",
      "Epoch 2014 Batch   62/175   train_loss = 1.263\n",
      "Epoch 2014 Batch   94/175   train_loss = 1.211\n",
      "Epoch 2014 Batch  126/175   train_loss = 1.239\n",
      "Epoch 2014 Batch  158/175   train_loss = 1.210\n",
      "Epoch 2015 Batch   15/175   train_loss = 1.268\n",
      "Epoch 2015 Batch   47/175   train_loss = 1.232\n",
      "Epoch 2015 Batch   79/175   train_loss = 1.259\n",
      "Epoch 2015 Batch  111/175   train_loss = 1.296\n",
      "Epoch 2015 Batch  143/175   train_loss = 1.188\n",
      "Epoch 2016 Batch    0/175   train_loss = 1.221\n",
      "Epoch 2016 Batch   32/175   train_loss = 1.264\n",
      "Epoch 2016 Batch   64/175   train_loss = 1.425\n",
      "Epoch 2016 Batch   96/175   train_loss = 1.283\n",
      "Epoch 2016 Batch  128/175   train_loss = 1.170\n",
      "Epoch 2016 Batch  160/175   train_loss = 1.241\n",
      "Epoch 2017 Batch   17/175   train_loss = 1.211\n",
      "Epoch 2017 Batch   49/175   train_loss = 1.237\n",
      "Epoch 2017 Batch   81/175   train_loss = 1.200\n",
      "Epoch 2017 Batch  113/175   train_loss = 1.232\n",
      "Epoch 2017 Batch  145/175   train_loss = 1.172\n",
      "Epoch 2018 Batch    2/175   train_loss = 1.224\n",
      "Epoch 2018 Batch   34/175   train_loss = 1.250\n",
      "Epoch 2018 Batch   66/175   train_loss = 1.246\n",
      "Epoch 2018 Batch   98/175   train_loss = 1.250\n",
      "Epoch 2018 Batch  130/175   train_loss = 1.244\n",
      "Epoch 2018 Batch  162/175   train_loss = 1.215\n",
      "Epoch 2019 Batch   19/175   train_loss = 1.210\n",
      "Epoch 2019 Batch   51/175   train_loss = 1.191\n",
      "Epoch 2019 Batch   83/175   train_loss = 1.290\n",
      "Epoch 2019 Batch  115/175   train_loss = 1.322\n",
      "Epoch 2019 Batch  147/175   train_loss = 1.190\n",
      "Epoch 2020 Batch    4/175   train_loss = 1.239\n",
      "Epoch 2020 Batch   36/175   train_loss = 1.182\n",
      "Epoch 2020 Batch   68/175   train_loss = 1.192\n",
      "Epoch 2020 Batch  100/175   train_loss = 1.258\n",
      "Epoch 2020 Batch  132/175   train_loss = 1.199\n",
      "Epoch 2020 Batch  164/175   train_loss = 1.217\n",
      "Epoch 2021 Batch   21/175   train_loss = 1.188\n",
      "Epoch 2021 Batch   53/175   train_loss = 1.213\n",
      "Epoch 2021 Batch   85/175   train_loss = 1.248\n",
      "Epoch 2021 Batch  117/175   train_loss = 1.252\n",
      "Epoch 2021 Batch  149/175   train_loss = 1.270\n",
      "Epoch 2022 Batch    6/175   train_loss = 1.268\n",
      "Epoch 2022 Batch   38/175   train_loss = 1.184\n",
      "Epoch 2022 Batch   70/175   train_loss = 1.212\n",
      "Epoch 2022 Batch  102/175   train_loss = 1.223\n",
      "Epoch 2022 Batch  134/175   train_loss = 1.196\n",
      "Epoch 2022 Batch  166/175   train_loss = 1.232\n",
      "Epoch 2023 Batch   23/175   train_loss = 1.211\n",
      "Epoch 2023 Batch   55/175   train_loss = 1.284\n",
      "Epoch 2023 Batch   87/175   train_loss = 1.333\n",
      "Epoch 2023 Batch  119/175   train_loss = 1.226\n",
      "Epoch 2023 Batch  151/175   train_loss = 1.256\n",
      "Epoch 2024 Batch    8/175   train_loss = 1.263\n",
      "Epoch 2024 Batch   40/175   train_loss = 1.213\n",
      "Epoch 2024 Batch   72/175   train_loss = 1.248\n",
      "Epoch 2024 Batch  104/175   train_loss = 1.276\n",
      "Epoch 2024 Batch  136/175   train_loss = 1.209\n",
      "Epoch 2024 Batch  168/175   train_loss = 1.233\n",
      "Epoch 2025 Batch   25/175   train_loss = 1.248\n",
      "Epoch 2025 Batch   57/175   train_loss = 1.258\n",
      "Epoch 2025 Batch   89/175   train_loss = 1.237\n",
      "Epoch 2025 Batch  121/175   train_loss = 1.193\n",
      "Epoch 2025 Batch  153/175   train_loss = 1.234\n",
      "Epoch 2026 Batch   10/175   train_loss = 1.169\n",
      "Epoch 2026 Batch   42/175   train_loss = 1.256\n",
      "Epoch 2026 Batch   74/175   train_loss = 1.263\n",
      "Epoch 2026 Batch  106/175   train_loss = 1.254\n",
      "Epoch 2026 Batch  138/175   train_loss = 1.209\n",
      "Epoch 2026 Batch  170/175   train_loss = 1.246\n",
      "Epoch 2027 Batch   27/175   train_loss = 1.195\n",
      "Epoch 2027 Batch   59/175   train_loss = 1.193\n",
      "Epoch 2027 Batch   91/175   train_loss = 1.194\n",
      "Epoch 2027 Batch  123/175   train_loss = 1.211\n",
      "Epoch 2027 Batch  155/175   train_loss = 1.201\n",
      "Epoch 2028 Batch   12/175   train_loss = 1.208\n",
      "Epoch 2028 Batch   44/175   train_loss = 1.215\n",
      "Epoch 2028 Batch   76/175   train_loss = 1.204\n",
      "Epoch 2028 Batch  108/175   train_loss = 1.241\n",
      "Epoch 2028 Batch  140/175   train_loss = 1.194\n",
      "Epoch 2028 Batch  172/175   train_loss = 1.231\n",
      "Epoch 2029 Batch   29/175   train_loss = 1.210\n",
      "Epoch 2029 Batch   61/175   train_loss = 1.254\n",
      "Epoch 2029 Batch   93/175   train_loss = 1.209\n",
      "Epoch 2029 Batch  125/175   train_loss = 1.221\n",
      "Epoch 2029 Batch  157/175   train_loss = 1.268\n",
      "Epoch 2030 Batch   14/175   train_loss = 1.258\n",
      "Epoch 2030 Batch   46/175   train_loss = 1.210\n",
      "Epoch 2030 Batch   78/175   train_loss = 1.196\n",
      "Epoch 2030 Batch  110/175   train_loss = 1.310\n",
      "Epoch 2030 Batch  142/175   train_loss = 1.279\n",
      "Epoch 2030 Batch  174/175   train_loss = 1.231\n",
      "Epoch 2031 Batch   31/175   train_loss = 1.253\n",
      "Epoch 2031 Batch   63/175   train_loss = 1.242\n",
      "Epoch 2031 Batch   95/175   train_loss = 1.206\n",
      "Epoch 2031 Batch  127/175   train_loss = 1.183\n",
      "Epoch 2031 Batch  159/175   train_loss = 1.209\n",
      "Epoch 2032 Batch   16/175   train_loss = 1.210\n",
      "Epoch 2032 Batch   48/175   train_loss = 1.261\n",
      "Epoch 2032 Batch   80/175   train_loss = 1.232\n",
      "Epoch 2032 Batch  112/175   train_loss = 1.218\n",
      "Epoch 2032 Batch  144/175   train_loss = 1.177\n",
      "Epoch 2033 Batch    1/175   train_loss = 1.279\n",
      "Epoch 2033 Batch   33/175   train_loss = 1.271\n",
      "Epoch 2033 Batch   65/175   train_loss = 1.230\n",
      "Epoch 2033 Batch   97/175   train_loss = 1.227\n",
      "Epoch 2033 Batch  129/175   train_loss = 1.211\n",
      "Epoch 2033 Batch  161/175   train_loss = 1.199\n",
      "Epoch 2034 Batch   18/175   train_loss = 1.190\n",
      "Epoch 2034 Batch   50/175   train_loss = 1.187\n",
      "Epoch 2034 Batch   82/175   train_loss = 1.258\n",
      "Epoch 2034 Batch  114/175   train_loss = 1.273\n",
      "Epoch 2034 Batch  146/175   train_loss = 1.217\n",
      "Epoch 2035 Batch    3/175   train_loss = 1.267\n",
      "Epoch 2035 Batch   35/175   train_loss = 1.217\n",
      "Epoch 2035 Batch   67/175   train_loss = 1.182\n",
      "Epoch 2035 Batch   99/175   train_loss = 1.279\n",
      "Epoch 2035 Batch  131/175   train_loss = 1.229\n",
      "Epoch 2035 Batch  163/175   train_loss = 1.222\n",
      "Epoch 2036 Batch   20/175   train_loss = 1.192\n",
      "Epoch 2036 Batch   52/175   train_loss = 1.161\n",
      "Epoch 2036 Batch   84/175   train_loss = 1.212\n",
      "Epoch 2036 Batch  116/175   train_loss = 1.280\n",
      "Epoch 2036 Batch  148/175   train_loss = 1.206\n",
      "Epoch 2037 Batch    5/175   train_loss = 1.220\n",
      "Epoch 2037 Batch   37/175   train_loss = 1.214\n",
      "Epoch 2037 Batch   69/175   train_loss = 1.239\n",
      "Epoch 2037 Batch  101/175   train_loss = 1.273\n",
      "Epoch 2037 Batch  133/175   train_loss = 1.123\n",
      "Epoch 2037 Batch  165/175   train_loss = 1.248\n",
      "Epoch 2038 Batch   22/175   train_loss = 1.158\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2038 Batch   54/175   train_loss = 1.265\n",
      "Epoch 2038 Batch   86/175   train_loss = 1.286\n",
      "Epoch 2038 Batch  118/175   train_loss = 1.296\n",
      "Epoch 2038 Batch  150/175   train_loss = 1.218\n",
      "Epoch 2039 Batch    7/175   train_loss = 1.244\n",
      "Epoch 2039 Batch   39/175   train_loss = 1.191\n",
      "Epoch 2039 Batch   71/175   train_loss = 1.252\n",
      "Epoch 2039 Batch  103/175   train_loss = 1.237\n",
      "Epoch 2039 Batch  135/175   train_loss = 1.176\n",
      "Epoch 2039 Batch  167/175   train_loss = 1.269\n",
      "Epoch 2040 Batch   24/175   train_loss = 1.179\n",
      "Epoch 2040 Batch   56/175   train_loss = 1.253\n",
      "Epoch 2040 Batch   88/175   train_loss = 1.270\n",
      "Epoch 2040 Batch  120/175   train_loss = 1.211\n",
      "Epoch 2040 Batch  152/175   train_loss = 1.201\n",
      "Epoch 2041 Batch    9/175   train_loss = 1.254\n",
      "Epoch 2041 Batch   41/175   train_loss = 1.247\n",
      "Epoch 2041 Batch   73/175   train_loss = 1.239\n",
      "Epoch 2041 Batch  105/175   train_loss = 1.253\n",
      "Epoch 2041 Batch  137/175   train_loss = 1.179\n",
      "Epoch 2041 Batch  169/175   train_loss = 1.248\n",
      "Epoch 2042 Batch   26/175   train_loss = 1.206\n",
      "Epoch 2042 Batch   58/175   train_loss = 1.252\n",
      "Epoch 2042 Batch   90/175   train_loss = 1.225\n",
      "Epoch 2042 Batch  122/175   train_loss = 1.186\n",
      "Epoch 2042 Batch  154/175   train_loss = 1.175\n",
      "Epoch 2043 Batch   11/175   train_loss = 1.202\n",
      "Epoch 2043 Batch   43/175   train_loss = 1.212\n",
      "Epoch 2043 Batch   75/175   train_loss = 1.185\n",
      "Epoch 2043 Batch  107/175   train_loss = 1.255\n",
      "Epoch 2043 Batch  139/175   train_loss = 1.173\n",
      "Epoch 2043 Batch  171/175   train_loss = 1.256\n",
      "Epoch 2044 Batch   28/175   train_loss = 1.224\n",
      "Epoch 2044 Batch   60/175   train_loss = 1.216\n",
      "Epoch 2044 Batch   92/175   train_loss = 1.197\n",
      "Epoch 2044 Batch  124/175   train_loss = 1.231\n",
      "Epoch 2044 Batch  156/175   train_loss = 1.249\n",
      "Epoch 2045 Batch   13/175   train_loss = 1.213\n",
      "Epoch 2045 Batch   45/175   train_loss = 1.211\n",
      "Epoch 2045 Batch   77/175   train_loss = 1.224\n",
      "Epoch 2045 Batch  109/175   train_loss = 1.256\n",
      "Epoch 2045 Batch  141/175   train_loss = 1.182\n",
      "Epoch 2045 Batch  173/175   train_loss = 1.212\n",
      "Epoch 2046 Batch   30/175   train_loss = 1.292\n",
      "Epoch 2046 Batch   62/175   train_loss = 1.274\n",
      "Epoch 2046 Batch   94/175   train_loss = 1.230\n",
      "Epoch 2046 Batch  126/175   train_loss = 1.249\n",
      "Epoch 2046 Batch  158/175   train_loss = 1.206\n",
      "Epoch 2047 Batch   15/175   train_loss = 1.286\n",
      "Epoch 2047 Batch   47/175   train_loss = 1.254\n",
      "Epoch 2047 Batch   79/175   train_loss = 1.260\n",
      "Epoch 2047 Batch  111/175   train_loss = 1.282\n",
      "Epoch 2047 Batch  143/175   train_loss = 1.201\n",
      "Epoch 2048 Batch    0/175   train_loss = 1.225\n",
      "Epoch 2048 Batch   32/175   train_loss = 1.259\n",
      "Epoch 2048 Batch   64/175   train_loss = 1.285\n",
      "Epoch 2048 Batch   96/175   train_loss = 1.251\n",
      "Epoch 2048 Batch  128/175   train_loss = 1.171\n",
      "Epoch 2048 Batch  160/175   train_loss = 1.234\n",
      "Epoch 2049 Batch   17/175   train_loss = 1.217\n",
      "Epoch 2049 Batch   49/175   train_loss = 1.255\n",
      "Epoch 2049 Batch   81/175   train_loss = 1.216\n",
      "Epoch 2049 Batch  113/175   train_loss = 1.245\n",
      "Epoch 2049 Batch  145/175   train_loss = 1.173\n",
      "Epoch 2050 Batch    2/175   train_loss = 1.244\n",
      "Epoch 2050 Batch   34/175   train_loss = 1.242\n",
      "Epoch 2050 Batch   66/175   train_loss = 1.262\n",
      "Epoch 2050 Batch   98/175   train_loss = 1.277\n",
      "Epoch 2050 Batch  130/175   train_loss = 1.263\n",
      "Epoch 2050 Batch  162/175   train_loss = 1.251\n",
      "Epoch 2051 Batch   19/175   train_loss = 1.216\n",
      "Epoch 2051 Batch   51/175   train_loss = 1.180\n",
      "Epoch 2051 Batch   83/175   train_loss = 1.278\n",
      "Epoch 2051 Batch  115/175   train_loss = 1.325\n",
      "Epoch 2051 Batch  147/175   train_loss = 1.197\n",
      "Epoch 2052 Batch    4/175   train_loss = 1.249\n",
      "Epoch 2052 Batch   36/175   train_loss = 1.175\n",
      "Epoch 2052 Batch   68/175   train_loss = 1.198\n",
      "Epoch 2052 Batch  100/175   train_loss = 1.243\n",
      "Epoch 2052 Batch  132/175   train_loss = 1.228\n",
      "Epoch 2052 Batch  164/175   train_loss = 1.201\n",
      "Epoch 2053 Batch   21/175   train_loss = 1.208\n",
      "Epoch 2053 Batch   53/175   train_loss = 1.204\n",
      "Epoch 2053 Batch   85/175   train_loss = 1.237\n",
      "Epoch 2053 Batch  117/175   train_loss = 1.248\n",
      "Epoch 2053 Batch  149/175   train_loss = 1.260\n",
      "Epoch 2054 Batch    6/175   train_loss = 1.260\n",
      "Epoch 2054 Batch   38/175   train_loss = 1.174\n",
      "Epoch 2054 Batch   70/175   train_loss = 1.250\n",
      "Epoch 2054 Batch  102/175   train_loss = 1.222\n",
      "Epoch 2054 Batch  134/175   train_loss = 1.185\n",
      "Epoch 2054 Batch  166/175   train_loss = 1.228\n",
      "Epoch 2055 Batch   23/175   train_loss = 1.180\n",
      "Epoch 2055 Batch   55/175   train_loss = 1.277\n",
      "Epoch 2055 Batch   87/175   train_loss = 1.308\n",
      "Epoch 2055 Batch  119/175   train_loss = 1.230\n",
      "Epoch 2055 Batch  151/175   train_loss = 1.279\n",
      "Epoch 2056 Batch    8/175   train_loss = 1.261\n",
      "Epoch 2056 Batch   40/175   train_loss = 1.229\n",
      "Epoch 2056 Batch   72/175   train_loss = 1.262\n",
      "Epoch 2056 Batch  104/175   train_loss = 1.252\n",
      "Epoch 2056 Batch  136/175   train_loss = 1.239\n",
      "Epoch 2056 Batch  168/175   train_loss = 1.255\n",
      "Epoch 2057 Batch   25/175   train_loss = 1.220\n",
      "Epoch 2057 Batch   57/175   train_loss = 1.254\n",
      "Epoch 2057 Batch   89/175   train_loss = 1.251\n",
      "Epoch 2057 Batch  121/175   train_loss = 1.225\n",
      "Epoch 2057 Batch  153/175   train_loss = 1.222\n",
      "Epoch 2058 Batch   10/175   train_loss = 1.179\n",
      "Epoch 2058 Batch   42/175   train_loss = 1.272\n",
      "Epoch 2058 Batch   74/175   train_loss = 1.259\n",
      "Epoch 2058 Batch  106/175   train_loss = 1.258\n",
      "Epoch 2058 Batch  138/175   train_loss = 1.219\n",
      "Epoch 2058 Batch  170/175   train_loss = 1.264\n",
      "Epoch 2059 Batch   27/175   train_loss = 1.213\n",
      "Epoch 2059 Batch   59/175   train_loss = 1.208\n",
      "Epoch 2059 Batch   91/175   train_loss = 1.221\n",
      "Epoch 2059 Batch  123/175   train_loss = 1.199\n",
      "Epoch 2059 Batch  155/175   train_loss = 1.194\n",
      "Epoch 2060 Batch   12/175   train_loss = 1.224\n",
      "Epoch 2060 Batch   44/175   train_loss = 1.188\n",
      "Epoch 2060 Batch   76/175   train_loss = 1.209\n",
      "Epoch 2060 Batch  108/175   train_loss = 1.231\n",
      "Epoch 2060 Batch  140/175   train_loss = 1.218\n",
      "Epoch 2060 Batch  172/175   train_loss = 1.237\n",
      "Epoch 2061 Batch   29/175   train_loss = 1.208\n",
      "Epoch 2061 Batch   61/175   train_loss = 1.262\n",
      "Epoch 2061 Batch   93/175   train_loss = 1.221\n",
      "Epoch 2061 Batch  125/175   train_loss = 1.238\n",
      "Epoch 2061 Batch  157/175   train_loss = 1.226\n",
      "Epoch 2062 Batch   14/175   train_loss = 1.233\n",
      "Epoch 2062 Batch   46/175   train_loss = 1.214\n",
      "Epoch 2062 Batch   78/175   train_loss = 1.186\n",
      "Epoch 2062 Batch  110/175   train_loss = 1.317\n",
      "Epoch 2062 Batch  142/175   train_loss = 1.260\n",
      "Epoch 2062 Batch  174/175   train_loss = 1.234\n",
      "Epoch 2063 Batch   31/175   train_loss = 1.246\n",
      "Epoch 2063 Batch   63/175   train_loss = 1.240\n",
      "Epoch 2063 Batch   95/175   train_loss = 1.209\n",
      "Epoch 2063 Batch  127/175   train_loss = 1.191\n",
      "Epoch 2063 Batch  159/175   train_loss = 1.204\n",
      "Epoch 2064 Batch   16/175   train_loss = 1.208\n",
      "Epoch 2064 Batch   48/175   train_loss = 1.246\n",
      "Epoch 2064 Batch   80/175   train_loss = 1.244\n",
      "Epoch 2064 Batch  112/175   train_loss = 1.222\n",
      "Epoch 2064 Batch  144/175   train_loss = 1.171\n",
      "Epoch 2065 Batch    1/175   train_loss = 1.257\n",
      "Epoch 2065 Batch   33/175   train_loss = 1.266\n",
      "Epoch 2065 Batch   65/175   train_loss = 1.247\n",
      "Epoch 2065 Batch   97/175   train_loss = 1.226\n",
      "Epoch 2065 Batch  129/175   train_loss = 1.214\n",
      "Epoch 2065 Batch  161/175   train_loss = 1.201\n",
      "Epoch 2066 Batch   18/175   train_loss = 1.186\n",
      "Epoch 2066 Batch   50/175   train_loss = 1.221\n",
      "Epoch 2066 Batch   82/175   train_loss = 1.267\n",
      "Epoch 2066 Batch  114/175   train_loss = 1.255\n",
      "Epoch 2066 Batch  146/175   train_loss = 1.198\n",
      "Epoch 2067 Batch    3/175   train_loss = 1.273\n",
      "Epoch 2067 Batch   35/175   train_loss = 1.210\n",
      "Epoch 2067 Batch   67/175   train_loss = 1.189\n",
      "Epoch 2067 Batch   99/175   train_loss = 1.288\n",
      "Epoch 2067 Batch  131/175   train_loss = 1.218\n",
      "Epoch 2067 Batch  163/175   train_loss = 1.225\n",
      "Epoch 2068 Batch   20/175   train_loss = 1.206\n",
      "Epoch 2068 Batch   52/175   train_loss = 1.159\n",
      "Epoch 2068 Batch   84/175   train_loss = 1.243\n",
      "Epoch 2068 Batch  116/175   train_loss = 1.277\n",
      "Epoch 2068 Batch  148/175   train_loss = 1.201\n",
      "Epoch 2069 Batch    5/175   train_loss = 1.220\n",
      "Epoch 2069 Batch   37/175   train_loss = 1.208\n",
      "Epoch 2069 Batch   69/175   train_loss = 1.246\n",
      "Epoch 2069 Batch  101/175   train_loss = 1.266\n",
      "Epoch 2069 Batch  133/175   train_loss = 1.111\n",
      "Epoch 2069 Batch  165/175   train_loss = 1.220\n",
      "Epoch 2070 Batch   22/175   train_loss = 1.145\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2070 Batch   54/175   train_loss = 1.252\n",
      "Epoch 2070 Batch   86/175   train_loss = 1.265\n",
      "Epoch 2070 Batch  118/175   train_loss = 1.276\n",
      "Epoch 2070 Batch  150/175   train_loss = 1.240\n",
      "Epoch 2071 Batch    7/175   train_loss = 1.255\n",
      "Epoch 2071 Batch   39/175   train_loss = 1.183\n",
      "Epoch 2071 Batch   71/175   train_loss = 1.236\n",
      "Epoch 2071 Batch  103/175   train_loss = 1.241\n",
      "Epoch 2071 Batch  135/175   train_loss = 1.180\n",
      "Epoch 2071 Batch  167/175   train_loss = 1.269\n",
      "Epoch 2072 Batch   24/175   train_loss = 1.172\n",
      "Epoch 2072 Batch   56/175   train_loss = 1.223\n",
      "Epoch 2072 Batch   88/175   train_loss = 1.262\n",
      "Epoch 2072 Batch  120/175   train_loss = 1.209\n",
      "Epoch 2072 Batch  152/175   train_loss = 1.193\n",
      "Epoch 2073 Batch    9/175   train_loss = 1.251\n",
      "Epoch 2073 Batch   41/175   train_loss = 1.247\n",
      "Epoch 2073 Batch   73/175   train_loss = 1.218\n",
      "Epoch 2073 Batch  105/175   train_loss = 1.240\n",
      "Epoch 2073 Batch  137/175   train_loss = 1.176\n",
      "Epoch 2073 Batch  169/175   train_loss = 1.233\n",
      "Epoch 2074 Batch   26/175   train_loss = 1.233\n",
      "Epoch 2074 Batch   58/175   train_loss = 1.261\n",
      "Epoch 2074 Batch   90/175   train_loss = 1.231\n",
      "Epoch 2074 Batch  122/175   train_loss = 1.196\n",
      "Epoch 2074 Batch  154/175   train_loss = 1.190\n",
      "Epoch 2075 Batch   11/175   train_loss = 1.206\n",
      "Epoch 2075 Batch   43/175   train_loss = 1.202\n",
      "Epoch 2075 Batch   75/175   train_loss = 1.186\n",
      "Epoch 2075 Batch  107/175   train_loss = 1.268\n",
      "Epoch 2075 Batch  139/175   train_loss = 1.174\n",
      "Epoch 2075 Batch  171/175   train_loss = 1.246\n",
      "Epoch 2076 Batch   28/175   train_loss = 1.212\n",
      "Epoch 2076 Batch   60/175   train_loss = 1.224\n",
      "Epoch 2076 Batch   92/175   train_loss = 1.205\n",
      "Epoch 2076 Batch  124/175   train_loss = 1.215\n",
      "Epoch 2076 Batch  156/175   train_loss = 1.253\n",
      "Epoch 2077 Batch   13/175   train_loss = 1.208\n",
      "Epoch 2077 Batch   45/175   train_loss = 1.192\n",
      "Epoch 2077 Batch   77/175   train_loss = 1.198\n",
      "Epoch 2077 Batch  109/175   train_loss = 1.277\n",
      "Epoch 2077 Batch  141/175   train_loss = 1.175\n",
      "Epoch 2077 Batch  173/175   train_loss = 1.184\n",
      "Epoch 2078 Batch   30/175   train_loss = 1.265\n",
      "Epoch 2078 Batch   62/175   train_loss = 1.252\n",
      "Epoch 2078 Batch   94/175   train_loss = 1.210\n",
      "Epoch 2078 Batch  126/175   train_loss = 1.231\n",
      "Epoch 2078 Batch  158/175   train_loss = 1.213\n",
      "Epoch 2079 Batch   15/175   train_loss = 1.273\n",
      "Epoch 2079 Batch   47/175   train_loss = 1.241\n",
      "Epoch 2079 Batch   79/175   train_loss = 1.286\n",
      "Epoch 2079 Batch  111/175   train_loss = 1.286\n",
      "Epoch 2079 Batch  143/175   train_loss = 1.189\n",
      "Epoch 2080 Batch    0/175   train_loss = 1.219\n",
      "Epoch 2080 Batch   32/175   train_loss = 1.244\n",
      "Epoch 2080 Batch   64/175   train_loss = 1.259\n",
      "Epoch 2080 Batch   96/175   train_loss = 1.268\n",
      "Epoch 2080 Batch  128/175   train_loss = 1.183\n",
      "Epoch 2080 Batch  160/175   train_loss = 1.235\n",
      "Epoch 2081 Batch   17/175   train_loss = 1.207\n",
      "Epoch 2081 Batch   49/175   train_loss = 1.252\n",
      "Epoch 2081 Batch   81/175   train_loss = 1.217\n",
      "Epoch 2081 Batch  113/175   train_loss = 1.259\n",
      "Epoch 2081 Batch  145/175   train_loss = 1.184\n",
      "Epoch 2082 Batch    2/175   train_loss = 1.221\n",
      "Epoch 2082 Batch   34/175   train_loss = 1.257\n",
      "Epoch 2082 Batch   66/175   train_loss = 1.265\n",
      "Epoch 2082 Batch   98/175   train_loss = 1.244\n",
      "Epoch 2082 Batch  130/175   train_loss = 1.232\n",
      "Epoch 2082 Batch  162/175   train_loss = 1.222\n",
      "Epoch 2083 Batch   19/175   train_loss = 1.242\n",
      "Epoch 2083 Batch   51/175   train_loss = 1.158\n",
      "Epoch 2083 Batch   83/175   train_loss = 1.280\n",
      "Epoch 2083 Batch  115/175   train_loss = 1.334\n",
      "Epoch 2083 Batch  147/175   train_loss = 1.197\n",
      "Epoch 2084 Batch    4/175   train_loss = 1.242\n",
      "Epoch 2084 Batch   36/175   train_loss = 1.182\n",
      "Epoch 2084 Batch   68/175   train_loss = 1.226\n",
      "Epoch 2084 Batch  100/175   train_loss = 1.242\n",
      "Epoch 2084 Batch  132/175   train_loss = 1.201\n",
      "Epoch 2084 Batch  164/175   train_loss = 1.182\n",
      "Epoch 2085 Batch   21/175   train_loss = 1.188\n",
      "Epoch 2085 Batch   53/175   train_loss = 1.195\n",
      "Epoch 2085 Batch   85/175   train_loss = 1.239\n",
      "Epoch 2085 Batch  117/175   train_loss = 1.233\n",
      "Epoch 2085 Batch  149/175   train_loss = 1.249\n",
      "Epoch 2086 Batch    6/175   train_loss = 1.251\n",
      "Epoch 2086 Batch   38/175   train_loss = 1.174\n",
      "Epoch 2086 Batch   70/175   train_loss = 1.230\n",
      "Epoch 2086 Batch  102/175   train_loss = 1.257\n",
      "Epoch 2086 Batch  134/175   train_loss = 1.176\n",
      "Epoch 2086 Batch  166/175   train_loss = 1.221\n",
      "Epoch 2087 Batch   23/175   train_loss = 1.164\n",
      "Epoch 2087 Batch   55/175   train_loss = 1.267\n",
      "Epoch 2087 Batch   87/175   train_loss = 1.296\n",
      "Epoch 2087 Batch  119/175   train_loss = 1.209\n",
      "Epoch 2087 Batch  151/175   train_loss = 1.238\n",
      "Epoch 2088 Batch    8/175   train_loss = 1.248\n",
      "Epoch 2088 Batch   40/175   train_loss = 1.226\n",
      "Epoch 2088 Batch   72/175   train_loss = 1.235\n",
      "Epoch 2088 Batch  104/175   train_loss = 1.246\n",
      "Epoch 2088 Batch  136/175   train_loss = 1.205\n",
      "Epoch 2088 Batch  168/175   train_loss = 1.259\n",
      "Epoch 2089 Batch   25/175   train_loss = 1.234\n",
      "Epoch 2089 Batch   57/175   train_loss = 1.259\n",
      "Epoch 2089 Batch   89/175   train_loss = 1.235\n",
      "Epoch 2089 Batch  121/175   train_loss = 1.214\n",
      "Epoch 2089 Batch  153/175   train_loss = 1.215\n",
      "Epoch 2090 Batch   10/175   train_loss = 1.181\n",
      "Epoch 2090 Batch   42/175   train_loss = 1.252\n",
      "Epoch 2090 Batch   74/175   train_loss = 1.264\n",
      "Epoch 2090 Batch  106/175   train_loss = 1.265\n",
      "Epoch 2090 Batch  138/175   train_loss = 1.219\n",
      "Epoch 2090 Batch  170/175   train_loss = 1.284\n",
      "Epoch 2091 Batch   27/175   train_loss = 1.234\n",
      "Epoch 2091 Batch   59/175   train_loss = 1.219\n",
      "Epoch 2091 Batch   91/175   train_loss = 1.208\n",
      "Epoch 2091 Batch  123/175   train_loss = 1.209\n",
      "Epoch 2091 Batch  155/175   train_loss = 1.205\n",
      "Epoch 2092 Batch   12/175   train_loss = 1.266\n",
      "Epoch 2092 Batch   44/175   train_loss = 1.205\n",
      "Epoch 2092 Batch   76/175   train_loss = 1.257\n",
      "Epoch 2092 Batch  108/175   train_loss = 1.240\n",
      "Epoch 2092 Batch  140/175   train_loss = 1.197\n",
      "Epoch 2092 Batch  172/175   train_loss = 1.254\n",
      "Epoch 2093 Batch   29/175   train_loss = 1.220\n",
      "Epoch 2093 Batch   61/175   train_loss = 1.246\n",
      "Epoch 2093 Batch   93/175   train_loss = 1.215\n",
      "Epoch 2093 Batch  125/175   train_loss = 1.212\n",
      "Epoch 2093 Batch  157/175   train_loss = 1.226\n",
      "Epoch 2094 Batch   14/175   train_loss = 1.257\n",
      "Epoch 2094 Batch   46/175   train_loss = 1.211\n",
      "Epoch 2094 Batch   78/175   train_loss = 1.199\n",
      "Epoch 2094 Batch  110/175   train_loss = 1.289\n",
      "Epoch 2094 Batch  142/175   train_loss = 1.245\n",
      "Epoch 2094 Batch  174/175   train_loss = 1.215\n",
      "Epoch 2095 Batch   31/175   train_loss = 1.240\n",
      "Epoch 2095 Batch   63/175   train_loss = 1.238\n",
      "Epoch 2095 Batch   95/175   train_loss = 1.208\n",
      "Epoch 2095 Batch  127/175   train_loss = 1.205\n",
      "Epoch 2095 Batch  159/175   train_loss = 1.191\n",
      "Epoch 2096 Batch   16/175   train_loss = 1.216\n",
      "Epoch 2096 Batch   48/175   train_loss = 1.223\n",
      "Epoch 2096 Batch   80/175   train_loss = 1.224\n",
      "Epoch 2096 Batch  112/175   train_loss = 1.254\n",
      "Epoch 2096 Batch  144/175   train_loss = 1.173\n",
      "Epoch 2097 Batch    1/175   train_loss = 1.278\n",
      "Epoch 2097 Batch   33/175   train_loss = 1.263\n",
      "Epoch 2097 Batch   65/175   train_loss = 1.243\n",
      "Epoch 2097 Batch   97/175   train_loss = 1.202\n",
      "Epoch 2097 Batch  129/175   train_loss = 1.210\n",
      "Epoch 2097 Batch  161/175   train_loss = 1.206\n",
      "Epoch 2098 Batch   18/175   train_loss = 1.185\n",
      "Epoch 2098 Batch   50/175   train_loss = 1.218\n",
      "Epoch 2098 Batch   82/175   train_loss = 1.264\n",
      "Epoch 2098 Batch  114/175   train_loss = 1.269\n",
      "Epoch 2098 Batch  146/175   train_loss = 1.201\n",
      "Epoch 2099 Batch    3/175   train_loss = 1.238\n",
      "Epoch 2099 Batch   35/175   train_loss = 1.218\n",
      "Epoch 2099 Batch   67/175   train_loss = 1.199\n",
      "Epoch 2099 Batch   99/175   train_loss = 1.311\n",
      "Epoch 2099 Batch  131/175   train_loss = 1.228\n",
      "Epoch 2099 Batch  163/175   train_loss = 1.228\n",
      "Epoch 2100 Batch   20/175   train_loss = 1.219\n",
      "Epoch 2100 Batch   52/175   train_loss = 1.175\n",
      "Epoch 2100 Batch   84/175   train_loss = 1.227\n",
      "Epoch 2100 Batch  116/175   train_loss = 1.297\n",
      "Epoch 2100 Batch  148/175   train_loss = 1.217\n",
      "Epoch 2101 Batch    5/175   train_loss = 1.213\n",
      "Epoch 2101 Batch   37/175   train_loss = 1.218\n",
      "Epoch 2101 Batch   69/175   train_loss = 1.232\n",
      "Epoch 2101 Batch  101/175   train_loss = 1.281\n",
      "Epoch 2101 Batch  133/175   train_loss = 1.120\n",
      "Epoch 2101 Batch  165/175   train_loss = 1.235\n",
      "Epoch 2102 Batch   22/175   train_loss = 1.152\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2102 Batch   54/175   train_loss = 1.238\n",
      "Epoch 2102 Batch   86/175   train_loss = 1.293\n",
      "Epoch 2102 Batch  118/175   train_loss = 1.292\n",
      "Epoch 2102 Batch  150/175   train_loss = 1.239\n",
      "Epoch 2103 Batch    7/175   train_loss = 1.269\n",
      "Epoch 2103 Batch   39/175   train_loss = 1.196\n",
      "Epoch 2103 Batch   71/175   train_loss = 1.268\n",
      "Epoch 2103 Batch  103/175   train_loss = 1.248\n",
      "Epoch 2103 Batch  135/175   train_loss = 1.189\n",
      "Epoch 2103 Batch  167/175   train_loss = 1.267\n",
      "Epoch 2104 Batch   24/175   train_loss = 1.198\n",
      "Epoch 2104 Batch   56/175   train_loss = 1.245\n",
      "Epoch 2104 Batch   88/175   train_loss = 1.255\n",
      "Epoch 2104 Batch  120/175   train_loss = 1.218\n",
      "Epoch 2104 Batch  152/175   train_loss = 1.202\n",
      "Epoch 2105 Batch    9/175   train_loss = 1.264\n",
      "Epoch 2105 Batch   41/175   train_loss = 1.251\n",
      "Epoch 2105 Batch   73/175   train_loss = 1.239\n",
      "Epoch 2105 Batch  105/175   train_loss = 1.288\n",
      "Epoch 2105 Batch  137/175   train_loss = 1.201\n",
      "Epoch 2105 Batch  169/175   train_loss = 1.255\n",
      "Epoch 2106 Batch   26/175   train_loss = 1.258\n",
      "Epoch 2106 Batch   58/175   train_loss = 1.266\n",
      "Epoch 2106 Batch   90/175   train_loss = 1.243\n",
      "Epoch 2106 Batch  122/175   train_loss = 1.202\n",
      "Epoch 2106 Batch  154/175   train_loss = 1.205\n",
      "Epoch 2107 Batch   11/175   train_loss = 1.201\n",
      "Epoch 2107 Batch   43/175   train_loss = 1.226\n",
      "Epoch 2107 Batch   75/175   train_loss = 1.210\n",
      "Epoch 2107 Batch  107/175   train_loss = 1.283\n",
      "Epoch 2107 Batch  139/175   train_loss = 1.198\n",
      "Epoch 2107 Batch  171/175   train_loss = 1.280\n",
      "Epoch 2108 Batch   28/175   train_loss = 1.233\n",
      "Epoch 2108 Batch   60/175   train_loss = 1.267\n",
      "Epoch 2108 Batch   92/175   train_loss = 1.221\n",
      "Epoch 2108 Batch  124/175   train_loss = 1.236\n",
      "Epoch 2108 Batch  156/175   train_loss = 1.282\n",
      "Epoch 2109 Batch   13/175   train_loss = 1.243\n",
      "Epoch 2109 Batch   45/175   train_loss = 1.245\n",
      "Epoch 2109 Batch   77/175   train_loss = 1.239\n",
      "Epoch 2109 Batch  109/175   train_loss = 1.275\n",
      "Epoch 2109 Batch  141/175   train_loss = 1.206\n",
      "Epoch 2109 Batch  173/175   train_loss = 1.192\n",
      "Epoch 2110 Batch   30/175   train_loss = 1.309\n",
      "Epoch 2110 Batch   62/175   train_loss = 1.280\n",
      "Epoch 2110 Batch   94/175   train_loss = 1.230\n",
      "Epoch 2110 Batch  126/175   train_loss = 1.252\n",
      "Epoch 2110 Batch  158/175   train_loss = 1.234\n",
      "Epoch 2111 Batch   15/175   train_loss = 1.287\n",
      "Epoch 2111 Batch   47/175   train_loss = 1.270\n",
      "Epoch 2111 Batch   79/175   train_loss = 1.300\n",
      "Epoch 2111 Batch  111/175   train_loss = 1.314\n",
      "Epoch 2111 Batch  143/175   train_loss = 1.230\n",
      "Epoch 2112 Batch    0/175   train_loss = 1.242\n",
      "Epoch 2112 Batch   32/175   train_loss = 1.257\n",
      "Epoch 2112 Batch   64/175   train_loss = 1.285\n",
      "Epoch 2112 Batch   96/175   train_loss = 1.279\n",
      "Epoch 2112 Batch  128/175   train_loss = 1.222\n",
      "Epoch 2112 Batch  160/175   train_loss = 1.234\n",
      "Epoch 2113 Batch   17/175   train_loss = 1.212\n",
      "Epoch 2113 Batch   49/175   train_loss = 1.271\n",
      "Epoch 2113 Batch   81/175   train_loss = 1.216\n",
      "Epoch 2113 Batch  113/175   train_loss = 1.253\n",
      "Epoch 2113 Batch  145/175   train_loss = 1.175\n",
      "Epoch 2114 Batch    2/175   train_loss = 1.211\n",
      "Epoch 2114 Batch   34/175   train_loss = 1.257\n",
      "Epoch 2114 Batch   66/175   train_loss = 1.279\n",
      "Epoch 2114 Batch   98/175   train_loss = 1.265\n",
      "Epoch 2114 Batch  130/175   train_loss = 1.273\n",
      "Epoch 2114 Batch  162/175   train_loss = 1.242\n",
      "Epoch 2115 Batch   19/175   train_loss = 1.220\n",
      "Epoch 2115 Batch   51/175   train_loss = 1.193\n",
      "Epoch 2115 Batch   83/175   train_loss = 1.270\n",
      "Epoch 2115 Batch  115/175   train_loss = 1.335\n",
      "Epoch 2115 Batch  147/175   train_loss = 1.189\n",
      "Epoch 2116 Batch    4/175   train_loss = 1.224\n",
      "Epoch 2116 Batch   36/175   train_loss = 1.184\n",
      "Epoch 2116 Batch   68/175   train_loss = 1.215\n",
      "Epoch 2116 Batch  100/175   train_loss = 1.250\n",
      "Epoch 2116 Batch  132/175   train_loss = 1.183\n",
      "Epoch 2116 Batch  164/175   train_loss = 1.175\n",
      "Epoch 2117 Batch   21/175   train_loss = 1.167\n",
      "Epoch 2117 Batch   53/175   train_loss = 1.207\n",
      "Epoch 2117 Batch   85/175   train_loss = 1.232\n",
      "Epoch 2117 Batch  117/175   train_loss = 1.255\n",
      "Epoch 2117 Batch  149/175   train_loss = 1.242\n",
      "Epoch 2118 Batch    6/175   train_loss = 1.235\n",
      "Epoch 2118 Batch   38/175   train_loss = 1.182\n",
      "Epoch 2118 Batch   70/175   train_loss = 1.230\n",
      "Epoch 2118 Batch  102/175   train_loss = 1.218\n",
      "Epoch 2118 Batch  134/175   train_loss = 1.189\n",
      "Epoch 2118 Batch  166/175   train_loss = 1.226\n",
      "Epoch 2119 Batch   23/175   train_loss = 1.173\n",
      "Epoch 2119 Batch   55/175   train_loss = 1.272\n",
      "Epoch 2119 Batch   87/175   train_loss = 1.294\n",
      "Epoch 2119 Batch  119/175   train_loss = 1.198\n",
      "Epoch 2119 Batch  151/175   train_loss = 1.219\n",
      "Epoch 2120 Batch    8/175   train_loss = 1.232\n",
      "Epoch 2120 Batch   40/175   train_loss = 1.202\n",
      "Epoch 2120 Batch   72/175   train_loss = 1.233\n",
      "Epoch 2120 Batch  104/175   train_loss = 1.252\n",
      "Epoch 2120 Batch  136/175   train_loss = 1.193\n",
      "Epoch 2120 Batch  168/175   train_loss = 1.229\n",
      "Epoch 2121 Batch   25/175   train_loss = 1.215\n",
      "Epoch 2121 Batch   57/175   train_loss = 1.247\n",
      "Epoch 2121 Batch   89/175   train_loss = 1.227\n",
      "Epoch 2121 Batch  121/175   train_loss = 1.181\n",
      "Epoch 2121 Batch  153/175   train_loss = 1.207\n",
      "Epoch 2122 Batch   10/175   train_loss = 1.189\n",
      "Epoch 2122 Batch   42/175   train_loss = 1.250\n",
      "Epoch 2122 Batch   74/175   train_loss = 1.261\n",
      "Epoch 2122 Batch  106/175   train_loss = 1.252\n",
      "Epoch 2122 Batch  138/175   train_loss = 1.215\n",
      "Epoch 2122 Batch  170/175   train_loss = 1.249\n",
      "Epoch 2123 Batch   27/175   train_loss = 1.222\n",
      "Epoch 2123 Batch   59/175   train_loss = 1.224\n",
      "Epoch 2123 Batch   91/175   train_loss = 1.217\n",
      "Epoch 2123 Batch  123/175   train_loss = 1.242\n",
      "Epoch 2123 Batch  155/175   train_loss = 1.203\n",
      "Epoch 2124 Batch   12/175   train_loss = 1.233\n",
      "Epoch 2124 Batch   44/175   train_loss = 1.218\n",
      "Epoch 2124 Batch   76/175   train_loss = 1.215\n",
      "Epoch 2124 Batch  108/175   train_loss = 1.227\n",
      "Epoch 2124 Batch  140/175   train_loss = 1.215\n",
      "Epoch 2124 Batch  172/175   train_loss = 1.258\n",
      "Epoch 2125 Batch   29/175   train_loss = 1.244\n",
      "Epoch 2125 Batch   61/175   train_loss = 1.273\n",
      "Epoch 2125 Batch   93/175   train_loss = 1.223\n",
      "Epoch 2125 Batch  125/175   train_loss = 1.245\n",
      "Epoch 2125 Batch  157/175   train_loss = 1.229\n",
      "Epoch 2126 Batch   14/175   train_loss = 1.252\n",
      "Epoch 2126 Batch   46/175   train_loss = 1.241\n",
      "Epoch 2126 Batch   78/175   train_loss = 1.194\n",
      "Epoch 2126 Batch  110/175   train_loss = 1.312\n",
      "Epoch 2126 Batch  142/175   train_loss = 1.259\n",
      "Epoch 2126 Batch  174/175   train_loss = 1.215\n",
      "Epoch 2127 Batch   31/175   train_loss = 1.257\n",
      "Epoch 2127 Batch   63/175   train_loss = 1.236\n",
      "Epoch 2127 Batch   95/175   train_loss = 1.213\n",
      "Epoch 2127 Batch  127/175   train_loss = 1.181\n",
      "Epoch 2127 Batch  159/175   train_loss = 1.191\n",
      "Epoch 2128 Batch   16/175   train_loss = 1.233\n",
      "Epoch 2128 Batch   48/175   train_loss = 1.245\n",
      "Epoch 2128 Batch   80/175   train_loss = 1.249\n",
      "Epoch 2128 Batch  112/175   train_loss = 1.240\n",
      "Epoch 2128 Batch  144/175   train_loss = 1.169\n",
      "Epoch 2129 Batch    1/175   train_loss = 1.331\n",
      "Epoch 2129 Batch   33/175   train_loss = 1.301\n",
      "Epoch 2129 Batch   65/175   train_loss = 1.271\n",
      "Epoch 2129 Batch   97/175   train_loss = 1.243\n",
      "Epoch 2129 Batch  129/175   train_loss = 1.247\n",
      "Epoch 2129 Batch  161/175   train_loss = 1.232\n",
      "Epoch 2130 Batch   18/175   train_loss = 1.213\n",
      "Epoch 2130 Batch   50/175   train_loss = 1.229\n",
      "Epoch 2130 Batch   82/175   train_loss = 1.278\n",
      "Epoch 2130 Batch  114/175   train_loss = 1.300\n",
      "Epoch 2130 Batch  146/175   train_loss = 1.216\n",
      "Epoch 2131 Batch    3/175   train_loss = 1.260\n",
      "Epoch 2131 Batch   35/175   train_loss = 1.241\n",
      "Epoch 2131 Batch   67/175   train_loss = 1.204\n",
      "Epoch 2131 Batch   99/175   train_loss = 1.284\n",
      "Epoch 2131 Batch  131/175   train_loss = 1.254\n",
      "Epoch 2131 Batch  163/175   train_loss = 1.224\n",
      "Epoch 2132 Batch   20/175   train_loss = 1.209\n",
      "Epoch 2132 Batch   52/175   train_loss = 1.178\n",
      "Epoch 2132 Batch   84/175   train_loss = 1.251\n",
      "Epoch 2132 Batch  116/175   train_loss = 1.287\n",
      "Epoch 2132 Batch  148/175   train_loss = 1.211\n",
      "Epoch 2133 Batch    5/175   train_loss = 1.242\n",
      "Epoch 2133 Batch   37/175   train_loss = 1.226\n",
      "Epoch 2133 Batch   69/175   train_loss = 1.258\n",
      "Epoch 2133 Batch  101/175   train_loss = 1.284\n",
      "Epoch 2133 Batch  133/175   train_loss = 1.136\n",
      "Epoch 2133 Batch  165/175   train_loss = 1.239\n",
      "Epoch 2134 Batch   22/175   train_loss = 1.175\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2134 Batch   54/175   train_loss = 1.251\n",
      "Epoch 2134 Batch   86/175   train_loss = 1.290\n",
      "Epoch 2134 Batch  118/175   train_loss = 1.298\n",
      "Epoch 2134 Batch  150/175   train_loss = 1.226\n",
      "Epoch 2135 Batch    7/175   train_loss = 1.258\n",
      "Epoch 2135 Batch   39/175   train_loss = 1.185\n",
      "Epoch 2135 Batch   71/175   train_loss = 1.276\n",
      "Epoch 2135 Batch  103/175   train_loss = 1.274\n",
      "Epoch 2135 Batch  135/175   train_loss = 1.228\n",
      "Epoch 2135 Batch  167/175   train_loss = 1.295\n",
      "Epoch 2136 Batch   24/175   train_loss = 1.194\n",
      "Epoch 2136 Batch   56/175   train_loss = 1.268\n",
      "Epoch 2136 Batch   88/175   train_loss = 1.267\n",
      "Epoch 2136 Batch  120/175   train_loss = 1.223\n",
      "Epoch 2136 Batch  152/175   train_loss = 1.219\n",
      "Epoch 2137 Batch    9/175   train_loss = 1.288\n",
      "Epoch 2137 Batch   41/175   train_loss = 1.281\n",
      "Epoch 2137 Batch   73/175   train_loss = 1.370\n",
      "Epoch 2137 Batch  105/175   train_loss = 1.325\n",
      "Epoch 2137 Batch  137/175   train_loss = 1.223\n",
      "Epoch 2137 Batch  169/175   train_loss = 1.276\n",
      "Epoch 2138 Batch   26/175   train_loss = 1.256\n",
      "Epoch 2138 Batch   58/175   train_loss = 1.286\n",
      "Epoch 2138 Batch   90/175   train_loss = 1.266\n",
      "Epoch 2138 Batch  122/175   train_loss = 1.219\n",
      "Epoch 2138 Batch  154/175   train_loss = 1.198\n",
      "Epoch 2139 Batch   11/175   train_loss = 1.198\n",
      "Epoch 2139 Batch   43/175   train_loss = 1.255\n",
      "Epoch 2139 Batch   75/175   train_loss = 1.214\n",
      "Epoch 2139 Batch  107/175   train_loss = 1.300\n",
      "Epoch 2139 Batch  139/175   train_loss = 1.206\n",
      "Epoch 2139 Batch  171/175   train_loss = 1.310\n",
      "Epoch 2140 Batch   28/175   train_loss = 1.257\n",
      "Epoch 2140 Batch   60/175   train_loss = 1.258\n",
      "Epoch 2140 Batch   92/175   train_loss = 1.245\n",
      "Epoch 2140 Batch  124/175   train_loss = 1.249\n",
      "Epoch 2140 Batch  156/175   train_loss = 1.287\n",
      "Epoch 2141 Batch   13/175   train_loss = 1.242\n",
      "Epoch 2141 Batch   45/175   train_loss = 1.218\n",
      "Epoch 2141 Batch   77/175   train_loss = 1.221\n",
      "Epoch 2141 Batch  109/175   train_loss = 1.263\n",
      "Epoch 2141 Batch  141/175   train_loss = 1.220\n",
      "Epoch 2141 Batch  173/175   train_loss = 1.192\n",
      "Epoch 2142 Batch   30/175   train_loss = 1.276\n",
      "Epoch 2142 Batch   62/175   train_loss = 1.265\n",
      "Epoch 2142 Batch   94/175   train_loss = 1.204\n",
      "Epoch 2142 Batch  126/175   train_loss = 1.228\n",
      "Epoch 2142 Batch  158/175   train_loss = 1.210\n",
      "Epoch 2143 Batch   15/175   train_loss = 1.279\n",
      "Epoch 2143 Batch   47/175   train_loss = 1.219\n",
      "Epoch 2143 Batch   79/175   train_loss = 1.258\n",
      "Epoch 2143 Batch  111/175   train_loss = 1.285\n",
      "Epoch 2143 Batch  143/175   train_loss = 1.199\n",
      "Epoch 2144 Batch    0/175   train_loss = 1.235\n",
      "Epoch 2144 Batch   32/175   train_loss = 1.253\n",
      "Epoch 2144 Batch   64/175   train_loss = 1.268\n",
      "Epoch 2144 Batch   96/175   train_loss = 1.274\n",
      "Epoch 2144 Batch  128/175   train_loss = 1.187\n",
      "Epoch 2144 Batch  160/175   train_loss = 1.224\n",
      "Epoch 2145 Batch   17/175   train_loss = 1.184\n",
      "Epoch 2145 Batch   49/175   train_loss = 1.251\n",
      "Epoch 2145 Batch   81/175   train_loss = 1.216\n",
      "Epoch 2145 Batch  113/175   train_loss = 1.235\n",
      "Epoch 2145 Batch  145/175   train_loss = 1.183\n",
      "Epoch 2146 Batch    2/175   train_loss = 1.226\n",
      "Epoch 2146 Batch   34/175   train_loss = 1.243\n",
      "Epoch 2146 Batch   66/175   train_loss = 1.292\n",
      "Epoch 2146 Batch   98/175   train_loss = 1.255\n",
      "Epoch 2146 Batch  130/175   train_loss = 1.253\n",
      "Epoch 2146 Batch  162/175   train_loss = 1.316\n",
      "Epoch 2147 Batch   19/175   train_loss = 1.253\n",
      "Epoch 2147 Batch   51/175   train_loss = 1.249\n",
      "Epoch 2147 Batch   83/175   train_loss = 1.313\n",
      "Epoch 2147 Batch  115/175   train_loss = 1.381\n",
      "Epoch 2147 Batch  147/175   train_loss = 1.205\n",
      "Epoch 2148 Batch    4/175   train_loss = 1.258\n",
      "Epoch 2148 Batch   36/175   train_loss = 1.198\n",
      "Epoch 2148 Batch   68/175   train_loss = 1.210\n",
      "Epoch 2148 Batch  100/175   train_loss = 1.256\n",
      "Epoch 2148 Batch  132/175   train_loss = 1.284\n",
      "Epoch 2148 Batch  164/175   train_loss = 1.223\n",
      "Epoch 2149 Batch   21/175   train_loss = 1.205\n",
      "Epoch 2149 Batch   53/175   train_loss = 1.208\n",
      "Epoch 2149 Batch   85/175   train_loss = 1.235\n",
      "Epoch 2149 Batch  117/175   train_loss = 1.260\n",
      "Epoch 2149 Batch  149/175   train_loss = 1.255\n",
      "Epoch 2150 Batch    6/175   train_loss = 1.265\n",
      "Epoch 2150 Batch   38/175   train_loss = 1.209\n",
      "Epoch 2150 Batch   70/175   train_loss = 1.230\n",
      "Epoch 2150 Batch  102/175   train_loss = 1.236\n",
      "Epoch 2150 Batch  134/175   train_loss = 1.230\n",
      "Epoch 2150 Batch  166/175   train_loss = 1.243\n",
      "Epoch 2151 Batch   23/175   train_loss = 1.201\n",
      "Epoch 2151 Batch   55/175   train_loss = 1.298\n",
      "Epoch 2151 Batch   87/175   train_loss = 1.298\n",
      "Epoch 2151 Batch  119/175   train_loss = 1.231\n",
      "Epoch 2151 Batch  151/175   train_loss = 1.251\n",
      "Epoch 2152 Batch    8/175   train_loss = 1.233\n",
      "Epoch 2152 Batch   40/175   train_loss = 1.231\n",
      "Epoch 2152 Batch   72/175   train_loss = 1.233\n",
      "Epoch 2152 Batch  104/175   train_loss = 1.238\n",
      "Epoch 2152 Batch  136/175   train_loss = 1.207\n",
      "Epoch 2152 Batch  168/175   train_loss = 1.256\n",
      "Epoch 2153 Batch   25/175   train_loss = 1.244\n",
      "Epoch 2153 Batch   57/175   train_loss = 1.253\n",
      "Epoch 2153 Batch   89/175   train_loss = 1.225\n",
      "Epoch 2153 Batch  121/175   train_loss = 1.190\n",
      "Epoch 2153 Batch  153/175   train_loss = 1.210\n",
      "Epoch 2154 Batch   10/175   train_loss = 1.176\n",
      "Epoch 2154 Batch   42/175   train_loss = 1.268\n",
      "Epoch 2154 Batch   74/175   train_loss = 1.267\n",
      "Epoch 2154 Batch  106/175   train_loss = 1.263\n",
      "Epoch 2154 Batch  138/175   train_loss = 1.210\n",
      "Epoch 2154 Batch  170/175   train_loss = 1.245\n",
      "Epoch 2155 Batch   27/175   train_loss = 1.204\n",
      "Epoch 2155 Batch   59/175   train_loss = 1.206\n",
      "Epoch 2155 Batch   91/175   train_loss = 1.204\n",
      "Epoch 2155 Batch  123/175   train_loss = 1.192\n",
      "Epoch 2155 Batch  155/175   train_loss = 1.183\n",
      "Epoch 2156 Batch   12/175   train_loss = 1.210\n",
      "Epoch 2156 Batch   44/175   train_loss = 1.215\n",
      "Epoch 2156 Batch   76/175   train_loss = 1.214\n",
      "Epoch 2156 Batch  108/175   train_loss = 1.213\n",
      "Epoch 2156 Batch  140/175   train_loss = 1.224\n",
      "Epoch 2156 Batch  172/175   train_loss = 1.239\n",
      "Epoch 2157 Batch   29/175   train_loss = 1.238\n",
      "Epoch 2157 Batch   61/175   train_loss = 1.264\n",
      "Epoch 2157 Batch   93/175   train_loss = 1.227\n",
      "Epoch 2157 Batch  125/175   train_loss = 1.237\n",
      "Epoch 2157 Batch  157/175   train_loss = 1.232\n",
      "Epoch 2158 Batch   14/175   train_loss = 1.253\n",
      "Epoch 2158 Batch   46/175   train_loss = 1.214\n",
      "Epoch 2158 Batch   78/175   train_loss = 1.208\n",
      "Epoch 2158 Batch  110/175   train_loss = 1.308\n",
      "Epoch 2158 Batch  142/175   train_loss = 1.235\n",
      "Epoch 2158 Batch  174/175   train_loss = 1.212\n",
      "Epoch 2159 Batch   31/175   train_loss = 1.242\n",
      "Epoch 2159 Batch   63/175   train_loss = 1.251\n",
      "Epoch 2159 Batch   95/175   train_loss = 1.207\n",
      "Epoch 2159 Batch  127/175   train_loss = 1.180\n",
      "Epoch 2159 Batch  159/175   train_loss = 1.185\n",
      "Epoch 2160 Batch   16/175   train_loss = 1.256\n",
      "Epoch 2160 Batch   48/175   train_loss = 1.257\n",
      "Epoch 2160 Batch   80/175   train_loss = 1.256\n",
      "Epoch 2160 Batch  112/175   train_loss = 1.251\n",
      "Epoch 2160 Batch  144/175   train_loss = 1.192\n",
      "Epoch 2161 Batch    1/175   train_loss = 1.277\n",
      "Epoch 2161 Batch   33/175   train_loss = 1.253\n",
      "Epoch 2161 Batch   65/175   train_loss = 1.219\n",
      "Epoch 2161 Batch   97/175   train_loss = 1.209\n",
      "Epoch 2161 Batch  129/175   train_loss = 1.242\n",
      "Epoch 2161 Batch  161/175   train_loss = 1.213\n",
      "Epoch 2162 Batch   18/175   train_loss = 1.188\n",
      "Epoch 2162 Batch   50/175   train_loss = 1.202\n",
      "Epoch 2162 Batch   82/175   train_loss = 1.264\n",
      "Epoch 2162 Batch  114/175   train_loss = 1.266\n",
      "Epoch 2162 Batch  146/175   train_loss = 1.219\n",
      "Epoch 2163 Batch    3/175   train_loss = 1.258\n",
      "Epoch 2163 Batch   35/175   train_loss = 1.232\n",
      "Epoch 2163 Batch   67/175   train_loss = 1.192\n",
      "Epoch 2163 Batch   99/175   train_loss = 1.280\n",
      "Epoch 2163 Batch  131/175   train_loss = 1.215\n",
      "Epoch 2163 Batch  163/175   train_loss = 1.222\n",
      "Epoch 2164 Batch   20/175   train_loss = 1.188\n",
      "Epoch 2164 Batch   52/175   train_loss = 1.147\n",
      "Epoch 2164 Batch   84/175   train_loss = 1.233\n",
      "Epoch 2164 Batch  116/175   train_loss = 1.264\n",
      "Epoch 2164 Batch  148/175   train_loss = 1.221\n",
      "Epoch 2165 Batch    5/175   train_loss = 1.217\n",
      "Epoch 2165 Batch   37/175   train_loss = 1.207\n",
      "Epoch 2165 Batch   69/175   train_loss = 1.235\n",
      "Epoch 2165 Batch  101/175   train_loss = 1.269\n",
      "Epoch 2165 Batch  133/175   train_loss = 1.113\n",
      "Epoch 2165 Batch  165/175   train_loss = 1.218\n",
      "Epoch 2166 Batch   22/175   train_loss = 1.188\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2166 Batch   54/175   train_loss = 1.232\n",
      "Epoch 2166 Batch   86/175   train_loss = 1.306\n",
      "Epoch 2166 Batch  118/175   train_loss = 1.309\n",
      "Epoch 2166 Batch  150/175   train_loss = 1.229\n",
      "Epoch 2167 Batch    7/175   train_loss = 1.259\n",
      "Epoch 2167 Batch   39/175   train_loss = 1.174\n",
      "Epoch 2167 Batch   71/175   train_loss = 1.267\n",
      "Epoch 2167 Batch  103/175   train_loss = 1.242\n",
      "Epoch 2167 Batch  135/175   train_loss = 1.215\n",
      "Epoch 2167 Batch  167/175   train_loss = 1.287\n",
      "Epoch 2168 Batch   24/175   train_loss = 1.203\n",
      "Epoch 2168 Batch   56/175   train_loss = 1.263\n",
      "Epoch 2168 Batch   88/175   train_loss = 1.263\n",
      "Epoch 2168 Batch  120/175   train_loss = 1.206\n",
      "Epoch 2168 Batch  152/175   train_loss = 1.219\n",
      "Epoch 2169 Batch    9/175   train_loss = 1.284\n",
      "Epoch 2169 Batch   41/175   train_loss = 1.262\n",
      "Epoch 2169 Batch   73/175   train_loss = 1.249\n",
      "Epoch 2169 Batch  105/175   train_loss = 1.276\n",
      "Epoch 2169 Batch  137/175   train_loss = 1.199\n",
      "Epoch 2169 Batch  169/175   train_loss = 1.262\n",
      "Epoch 2170 Batch   26/175   train_loss = 1.237\n",
      "Epoch 2170 Batch   58/175   train_loss = 1.248\n",
      "Epoch 2170 Batch   90/175   train_loss = 1.235\n",
      "Epoch 2170 Batch  122/175   train_loss = 1.185\n",
      "Epoch 2170 Batch  154/175   train_loss = 1.183\n",
      "Epoch 2171 Batch   11/175   train_loss = 1.202\n",
      "Epoch 2171 Batch   43/175   train_loss = 1.247\n",
      "Epoch 2171 Batch   75/175   train_loss = 1.200\n",
      "Epoch 2171 Batch  107/175   train_loss = 1.274\n",
      "Epoch 2171 Batch  139/175   train_loss = 1.173\n",
      "Epoch 2171 Batch  171/175   train_loss = 1.284\n",
      "Epoch 2172 Batch   28/175   train_loss = 1.226\n",
      "Epoch 2172 Batch   60/175   train_loss = 1.245\n",
      "Epoch 2172 Batch   92/175   train_loss = 1.222\n",
      "Epoch 2172 Batch  124/175   train_loss = 1.230\n",
      "Epoch 2172 Batch  156/175   train_loss = 1.273\n",
      "Epoch 2173 Batch   13/175   train_loss = 1.231\n",
      "Epoch 2173 Batch   45/175   train_loss = 1.223\n",
      "Epoch 2173 Batch   77/175   train_loss = 1.221\n",
      "Epoch 2173 Batch  109/175   train_loss = 1.276\n",
      "Epoch 2173 Batch  141/175   train_loss = 1.198\n",
      "Epoch 2173 Batch  173/175   train_loss = 1.196\n",
      "Epoch 2174 Batch   30/175   train_loss = 1.292\n",
      "Epoch 2174 Batch   62/175   train_loss = 1.259\n",
      "Epoch 2174 Batch   94/175   train_loss = 1.224\n",
      "Epoch 2174 Batch  126/175   train_loss = 1.235\n",
      "Epoch 2174 Batch  158/175   train_loss = 1.221\n",
      "Epoch 2175 Batch   15/175   train_loss = 1.287\n",
      "Epoch 2175 Batch   47/175   train_loss = 1.245\n",
      "Epoch 2175 Batch   79/175   train_loss = 1.270\n",
      "Epoch 2175 Batch  111/175   train_loss = 1.293\n",
      "Epoch 2175 Batch  143/175   train_loss = 1.221\n",
      "Epoch 2176 Batch    0/175   train_loss = 1.227\n",
      "Epoch 2176 Batch   32/175   train_loss = 1.303\n",
      "Epoch 2176 Batch   64/175   train_loss = 1.285\n",
      "Epoch 2176 Batch   96/175   train_loss = 1.290\n",
      "Epoch 2176 Batch  128/175   train_loss = 1.191\n",
      "Epoch 2176 Batch  160/175   train_loss = 1.227\n",
      "Epoch 2177 Batch   17/175   train_loss = 1.202\n",
      "Epoch 2177 Batch   49/175   train_loss = 1.249\n",
      "Epoch 2177 Batch   81/175   train_loss = 1.217\n",
      "Epoch 2177 Batch  113/175   train_loss = 1.255\n",
      "Epoch 2177 Batch  145/175   train_loss = 1.184\n",
      "Epoch 2178 Batch    2/175   train_loss = 1.216\n",
      "Epoch 2178 Batch   34/175   train_loss = 1.256\n",
      "Epoch 2178 Batch   66/175   train_loss = 1.266\n",
      "Epoch 2178 Batch   98/175   train_loss = 1.240\n",
      "Epoch 2178 Batch  130/175   train_loss = 1.249\n",
      "Epoch 2178 Batch  162/175   train_loss = 1.228\n",
      "Epoch 2179 Batch   19/175   train_loss = 1.217\n",
      "Epoch 2179 Batch   51/175   train_loss = 1.164\n",
      "Epoch 2179 Batch   83/175   train_loss = 1.283\n",
      "Epoch 2179 Batch  115/175   train_loss = 1.324\n",
      "Epoch 2179 Batch  147/175   train_loss = 1.204\n",
      "Epoch 2180 Batch    4/175   train_loss = 1.240\n",
      "Epoch 2180 Batch   36/175   train_loss = 1.186\n",
      "Epoch 2180 Batch   68/175   train_loss = 1.192\n",
      "Epoch 2180 Batch  100/175   train_loss = 1.237\n",
      "Epoch 2180 Batch  132/175   train_loss = 1.199\n",
      "Epoch 2180 Batch  164/175   train_loss = 1.184\n",
      "Epoch 2181 Batch   21/175   train_loss = 1.188\n",
      "Epoch 2181 Batch   53/175   train_loss = 1.198\n",
      "Epoch 2181 Batch   85/175   train_loss = 1.228\n",
      "Epoch 2181 Batch  117/175   train_loss = 1.240\n",
      "Epoch 2181 Batch  149/175   train_loss = 1.256\n",
      "Epoch 2182 Batch    6/175   train_loss = 1.241\n",
      "Epoch 2182 Batch   38/175   train_loss = 1.178\n",
      "Epoch 2182 Batch   70/175   train_loss = 1.192\n",
      "Epoch 2182 Batch  102/175   train_loss = 1.246\n",
      "Epoch 2182 Batch  134/175   train_loss = 1.181\n",
      "Epoch 2182 Batch  166/175   train_loss = 1.230\n",
      "Epoch 2183 Batch   23/175   train_loss = 1.172\n",
      "Epoch 2183 Batch   55/175   train_loss = 1.313\n",
      "Epoch 2183 Batch   87/175   train_loss = 1.299\n",
      "Epoch 2183 Batch  119/175   train_loss = 1.231\n",
      "Epoch 2183 Batch  151/175   train_loss = 1.253\n",
      "Epoch 2184 Batch    8/175   train_loss = 1.253\n",
      "Epoch 2184 Batch   40/175   train_loss = 1.216\n",
      "Epoch 2184 Batch   72/175   train_loss = 1.257\n",
      "Epoch 2184 Batch  104/175   train_loss = 1.276\n",
      "Epoch 2184 Batch  136/175   train_loss = 1.226\n",
      "Epoch 2184 Batch  168/175   train_loss = 1.263\n",
      "Epoch 2185 Batch   25/175   train_loss = 1.239\n",
      "Epoch 2185 Batch   57/175   train_loss = 1.284\n",
      "Epoch 2185 Batch   89/175   train_loss = 1.240\n",
      "Epoch 2185 Batch  121/175   train_loss = 1.196\n",
      "Epoch 2185 Batch  153/175   train_loss = 1.233\n",
      "Epoch 2186 Batch   10/175   train_loss = 1.200\n",
      "Epoch 2186 Batch   42/175   train_loss = 1.288\n",
      "Epoch 2186 Batch   74/175   train_loss = 1.280\n",
      "Epoch 2186 Batch  106/175   train_loss = 1.258\n",
      "Epoch 2186 Batch  138/175   train_loss = 1.212\n",
      "Epoch 2186 Batch  170/175   train_loss = 1.268\n",
      "Epoch 2187 Batch   27/175   train_loss = 1.204\n",
      "Epoch 2187 Batch   59/175   train_loss = 1.193\n",
      "Epoch 2187 Batch   91/175   train_loss = 1.208\n",
      "Epoch 2187 Batch  123/175   train_loss = 1.214\n",
      "Epoch 2187 Batch  155/175   train_loss = 1.205\n",
      "Epoch 2188 Batch   12/175   train_loss = 1.218\n",
      "Epoch 2188 Batch   44/175   train_loss = 1.195\n",
      "Epoch 2188 Batch   76/175   train_loss = 1.231\n",
      "Epoch 2188 Batch  108/175   train_loss = 1.241\n",
      "Epoch 2188 Batch  140/175   train_loss = 1.209\n",
      "Epoch 2188 Batch  172/175   train_loss = 1.245\n",
      "Epoch 2189 Batch   29/175   train_loss = 1.225\n",
      "Epoch 2189 Batch   61/175   train_loss = 1.274\n",
      "Epoch 2189 Batch   93/175   train_loss = 1.198\n",
      "Epoch 2189 Batch  125/175   train_loss = 1.237\n",
      "Epoch 2189 Batch  157/175   train_loss = 1.227\n",
      "Epoch 2190 Batch   14/175   train_loss = 1.237\n",
      "Epoch 2190 Batch   46/175   train_loss = 1.239\n",
      "Epoch 2190 Batch   78/175   train_loss = 1.199\n",
      "Epoch 2190 Batch  110/175   train_loss = 1.296\n",
      "Epoch 2190 Batch  142/175   train_loss = 1.243\n",
      "Epoch 2190 Batch  174/175   train_loss = 1.202\n",
      "Epoch 2191 Batch   31/175   train_loss = 1.234\n",
      "Epoch 2191 Batch   63/175   train_loss = 1.241\n",
      "Epoch 2191 Batch   95/175   train_loss = 1.206\n",
      "Epoch 2191 Batch  127/175   train_loss = 1.186\n",
      "Epoch 2191 Batch  159/175   train_loss = 1.170\n",
      "Epoch 2192 Batch   16/175   train_loss = 1.215\n",
      "Epoch 2192 Batch   48/175   train_loss = 1.226\n",
      "Epoch 2192 Batch   80/175   train_loss = 1.296\n",
      "Epoch 2192 Batch  112/175   train_loss = 1.243\n",
      "Epoch 2192 Batch  144/175   train_loss = 1.167\n",
      "Epoch 2193 Batch    1/175   train_loss = 1.251\n",
      "Epoch 2193 Batch   33/175   train_loss = 1.267\n",
      "Epoch 2193 Batch   65/175   train_loss = 1.241\n",
      "Epoch 2193 Batch   97/175   train_loss = 1.207\n",
      "Epoch 2193 Batch  129/175   train_loss = 1.251\n",
      "Epoch 2193 Batch  161/175   train_loss = 1.216\n",
      "Epoch 2194 Batch   18/175   train_loss = 1.196\n",
      "Epoch 2194 Batch   50/175   train_loss = 1.210\n",
      "Epoch 2194 Batch   82/175   train_loss = 1.259\n",
      "Epoch 2194 Batch  114/175   train_loss = 1.265\n",
      "Epoch 2194 Batch  146/175   train_loss = 1.218\n",
      "Epoch 2195 Batch    3/175   train_loss = 1.240\n",
      "Epoch 2195 Batch   35/175   train_loss = 1.216\n",
      "Epoch 2195 Batch   67/175   train_loss = 1.183\n",
      "Epoch 2195 Batch   99/175   train_loss = 1.287\n",
      "Epoch 2195 Batch  131/175   train_loss = 1.216\n",
      "Epoch 2195 Batch  163/175   train_loss = 1.209\n",
      "Epoch 2196 Batch   20/175   train_loss = 1.193\n",
      "Epoch 2196 Batch   52/175   train_loss = 1.157\n",
      "Epoch 2196 Batch   84/175   train_loss = 1.218\n",
      "Epoch 2196 Batch  116/175   train_loss = 1.280\n",
      "Epoch 2196 Batch  148/175   train_loss = 1.249\n",
      "Epoch 2197 Batch    5/175   train_loss = 1.241\n",
      "Epoch 2197 Batch   37/175   train_loss = 1.220\n",
      "Epoch 2197 Batch   69/175   train_loss = 1.242\n",
      "Epoch 2197 Batch  101/175   train_loss = 1.305\n",
      "Epoch 2197 Batch  133/175   train_loss = 1.131\n",
      "Epoch 2197 Batch  165/175   train_loss = 1.221\n",
      "Epoch 2198 Batch   22/175   train_loss = 1.172\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2198 Batch   54/175   train_loss = 1.249\n",
      "Epoch 2198 Batch   86/175   train_loss = 1.271\n",
      "Epoch 2198 Batch  118/175   train_loss = 1.303\n",
      "Epoch 2198 Batch  150/175   train_loss = 1.221\n",
      "Epoch 2199 Batch    7/175   train_loss = 1.265\n",
      "Epoch 2199 Batch   39/175   train_loss = 1.183\n",
      "Epoch 2199 Batch   71/175   train_loss = 1.293\n",
      "Epoch 2199 Batch  103/175   train_loss = 1.252\n",
      "Epoch 2199 Batch  135/175   train_loss = 1.225\n",
      "Epoch 2199 Batch  167/175   train_loss = 1.286\n",
      "Epoch 2200 Batch   24/175   train_loss = 1.162\n",
      "Epoch 2200 Batch   56/175   train_loss = 1.252\n",
      "Epoch 2200 Batch   88/175   train_loss = 1.290\n",
      "Epoch 2200 Batch  120/175   train_loss = 1.244\n",
      "Epoch 2200 Batch  152/175   train_loss = 1.212\n",
      "Epoch 2201 Batch    9/175   train_loss = 1.278\n",
      "Epoch 2201 Batch   41/175   train_loss = 1.289\n",
      "Epoch 2201 Batch   73/175   train_loss = 1.258\n",
      "Epoch 2201 Batch  105/175   train_loss = 1.278\n",
      "Epoch 2201 Batch  137/175   train_loss = 1.186\n",
      "Epoch 2201 Batch  169/175   train_loss = 1.257\n",
      "Epoch 2202 Batch   26/175   train_loss = 1.249\n",
      "Epoch 2202 Batch   58/175   train_loss = 1.252\n",
      "Epoch 2202 Batch   90/175   train_loss = 1.242\n",
      "Epoch 2202 Batch  122/175   train_loss = 1.194\n",
      "Epoch 2202 Batch  154/175   train_loss = 1.202\n",
      "Epoch 2203 Batch   11/175   train_loss = 1.206\n",
      "Epoch 2203 Batch   43/175   train_loss = 1.238\n",
      "Epoch 2203 Batch   75/175   train_loss = 1.208\n",
      "Epoch 2203 Batch  107/175   train_loss = 1.290\n",
      "Epoch 2203 Batch  139/175   train_loss = 1.181\n",
      "Epoch 2203 Batch  171/175   train_loss = 1.278\n",
      "Epoch 2204 Batch   28/175   train_loss = 1.227\n",
      "Epoch 2204 Batch   60/175   train_loss = 1.242\n",
      "Epoch 2204 Batch   92/175   train_loss = 1.209\n",
      "Epoch 2204 Batch  124/175   train_loss = 1.236\n",
      "Epoch 2204 Batch  156/175   train_loss = 1.242\n",
      "Epoch 2205 Batch   13/175   train_loss = 1.205\n",
      "Epoch 2205 Batch   45/175   train_loss = 1.231\n",
      "Epoch 2205 Batch   77/175   train_loss = 1.201\n",
      "Epoch 2205 Batch  109/175   train_loss = 1.251\n",
      "Epoch 2205 Batch  141/175   train_loss = 1.169\n",
      "Epoch 2205 Batch  173/175   train_loss = 1.184\n",
      "Epoch 2206 Batch   30/175   train_loss = 1.289\n",
      "Epoch 2206 Batch   62/175   train_loss = 1.249\n",
      "Epoch 2206 Batch   94/175   train_loss = 1.219\n",
      "Epoch 2206 Batch  126/175   train_loss = 1.245\n",
      "Epoch 2206 Batch  158/175   train_loss = 1.191\n",
      "Epoch 2207 Batch   15/175   train_loss = 1.253\n",
      "Epoch 2207 Batch   47/175   train_loss = 1.227\n",
      "Epoch 2207 Batch   79/175   train_loss = 1.279\n",
      "Epoch 2207 Batch  111/175   train_loss = 1.268\n",
      "Epoch 2207 Batch  143/175   train_loss = 1.217\n",
      "Epoch 2208 Batch    0/175   train_loss = 1.227\n",
      "Epoch 2208 Batch   32/175   train_loss = 1.242\n",
      "Epoch 2208 Batch   64/175   train_loss = 1.256\n",
      "Epoch 2208 Batch   96/175   train_loss = 1.247\n",
      "Epoch 2208 Batch  128/175   train_loss = 1.194\n",
      "Epoch 2208 Batch  160/175   train_loss = 1.231\n",
      "Epoch 2209 Batch   17/175   train_loss = 1.173\n",
      "Epoch 2209 Batch   49/175   train_loss = 1.234\n",
      "Epoch 2209 Batch   81/175   train_loss = 1.201\n",
      "Epoch 2209 Batch  113/175   train_loss = 1.228\n",
      "Epoch 2209 Batch  145/175   train_loss = 1.165\n",
      "Epoch 2210 Batch    2/175   train_loss = 1.200\n",
      "Epoch 2210 Batch   34/175   train_loss = 1.226\n",
      "Epoch 2210 Batch   66/175   train_loss = 1.249\n",
      "Epoch 2210 Batch   98/175   train_loss = 1.266\n",
      "Epoch 2210 Batch  130/175   train_loss = 1.248\n",
      "Epoch 2210 Batch  162/175   train_loss = 1.217\n",
      "Epoch 2211 Batch   19/175   train_loss = 1.240\n",
      "Epoch 2211 Batch   51/175   train_loss = 1.151\n",
      "Epoch 2211 Batch   83/175   train_loss = 1.271\n",
      "Epoch 2211 Batch  115/175   train_loss = 1.323\n",
      "Epoch 2211 Batch  147/175   train_loss = 1.172\n",
      "Epoch 2212 Batch    4/175   train_loss = 1.230\n",
      "Epoch 2212 Batch   36/175   train_loss = 1.179\n",
      "Epoch 2212 Batch   68/175   train_loss = 1.183\n",
      "Epoch 2212 Batch  100/175   train_loss = 1.212\n",
      "Epoch 2212 Batch  132/175   train_loss = 1.199\n",
      "Epoch 2212 Batch  164/175   train_loss = 1.194\n",
      "Epoch 2213 Batch   21/175   train_loss = 1.170\n",
      "Epoch 2213 Batch   53/175   train_loss = 1.210\n",
      "Epoch 2213 Batch   85/175   train_loss = 1.236\n",
      "Epoch 2213 Batch  117/175   train_loss = 1.250\n",
      "Epoch 2213 Batch  149/175   train_loss = 1.257\n",
      "Epoch 2214 Batch    6/175   train_loss = 1.262\n",
      "Epoch 2214 Batch   38/175   train_loss = 1.192\n",
      "Epoch 2214 Batch   70/175   train_loss = 1.230\n",
      "Epoch 2214 Batch  102/175   train_loss = 1.269\n",
      "Epoch 2214 Batch  134/175   train_loss = 1.212\n",
      "Epoch 2214 Batch  166/175   train_loss = 1.240\n",
      "Epoch 2215 Batch   23/175   train_loss = 1.169\n",
      "Epoch 2215 Batch   55/175   train_loss = 1.287\n",
      "Epoch 2215 Batch   87/175   train_loss = 1.289\n",
      "Epoch 2215 Batch  119/175   train_loss = 1.232\n",
      "Epoch 2215 Batch  151/175   train_loss = 1.246\n",
      "Epoch 2216 Batch    8/175   train_loss = 1.253\n",
      "Epoch 2216 Batch   40/175   train_loss = 1.223\n",
      "Epoch 2216 Batch   72/175   train_loss = 1.248\n",
      "Epoch 2216 Batch  104/175   train_loss = 1.249\n",
      "Epoch 2216 Batch  136/175   train_loss = 1.221\n",
      "Epoch 2216 Batch  168/175   train_loss = 1.241\n",
      "Epoch 2217 Batch   25/175   train_loss = 1.243\n",
      "Epoch 2217 Batch   57/175   train_loss = 1.259\n",
      "Epoch 2217 Batch   89/175   train_loss = 1.225\n",
      "Epoch 2217 Batch  121/175   train_loss = 1.206\n",
      "Epoch 2217 Batch  153/175   train_loss = 1.219\n",
      "Epoch 2218 Batch   10/175   train_loss = 1.297\n",
      "Epoch 2218 Batch   42/175   train_loss = 1.293\n",
      "Epoch 2218 Batch   74/175   train_loss = 1.282\n",
      "Epoch 2218 Batch  106/175   train_loss = 1.256\n",
      "Epoch 2218 Batch  138/175   train_loss = 1.209\n",
      "Epoch 2218 Batch  170/175   train_loss = 1.254\n",
      "Epoch 2219 Batch   27/175   train_loss = 1.206\n",
      "Epoch 2219 Batch   59/175   train_loss = 1.200\n",
      "Epoch 2219 Batch   91/175   train_loss = 1.206\n",
      "Epoch 2219 Batch  123/175   train_loss = 1.201\n",
      "Epoch 2219 Batch  155/175   train_loss = 1.217\n",
      "Epoch 2220 Batch   12/175   train_loss = 1.242\n",
      "Epoch 2220 Batch   44/175   train_loss = 1.201\n",
      "Epoch 2220 Batch   76/175   train_loss = 1.208\n",
      "Epoch 2220 Batch  108/175   train_loss = 1.230\n",
      "Epoch 2220 Batch  140/175   train_loss = 1.209\n",
      "Epoch 2220 Batch  172/175   train_loss = 1.240\n",
      "Epoch 2221 Batch   29/175   train_loss = 1.223\n",
      "Epoch 2221 Batch   61/175   train_loss = 1.239\n",
      "Epoch 2221 Batch   93/175   train_loss = 1.229\n",
      "Epoch 2221 Batch  125/175   train_loss = 1.233\n",
      "Epoch 2221 Batch  157/175   train_loss = 1.215\n",
      "Epoch 2222 Batch   14/175   train_loss = 1.237\n",
      "Epoch 2222 Batch   46/175   train_loss = 1.213\n",
      "Epoch 2222 Batch   78/175   train_loss = 1.222\n",
      "Epoch 2222 Batch  110/175   train_loss = 1.302\n",
      "Epoch 2222 Batch  142/175   train_loss = 1.244\n",
      "Epoch 2222 Batch  174/175   train_loss = 1.211\n",
      "Epoch 2223 Batch   31/175   train_loss = 1.238\n",
      "Epoch 2223 Batch   63/175   train_loss = 1.249\n",
      "Epoch 2223 Batch   95/175   train_loss = 1.227\n",
      "Epoch 2223 Batch  127/175   train_loss = 1.193\n",
      "Epoch 2223 Batch  159/175   train_loss = 1.190\n",
      "Epoch 2224 Batch   16/175   train_loss = 1.200\n",
      "Epoch 2224 Batch   48/175   train_loss = 1.217\n",
      "Epoch 2224 Batch   80/175   train_loss = 1.230\n",
      "Epoch 2224 Batch  112/175   train_loss = 1.214\n",
      "Epoch 2224 Batch  144/175   train_loss = 1.151\n",
      "Epoch 2225 Batch    1/175   train_loss = 1.257\n",
      "Epoch 2225 Batch   33/175   train_loss = 1.267\n",
      "Epoch 2225 Batch   65/175   train_loss = 1.214\n",
      "Epoch 2225 Batch   97/175   train_loss = 1.207\n",
      "Epoch 2225 Batch  129/175   train_loss = 1.224\n",
      "Epoch 2225 Batch  161/175   train_loss = 1.194\n",
      "Epoch 2226 Batch   18/175   train_loss = 1.192\n",
      "Epoch 2226 Batch   50/175   train_loss = 1.207\n",
      "Epoch 2226 Batch   82/175   train_loss = 1.240\n",
      "Epoch 2226 Batch  114/175   train_loss = 1.284\n",
      "Epoch 2226 Batch  146/175   train_loss = 1.216\n",
      "Epoch 2227 Batch    3/175   train_loss = 1.257\n",
      "Epoch 2227 Batch   35/175   train_loss = 1.224\n",
      "Epoch 2227 Batch   67/175   train_loss = 1.190\n",
      "Epoch 2227 Batch   99/175   train_loss = 1.298\n",
      "Epoch 2227 Batch  131/175   train_loss = 1.226\n",
      "Epoch 2227 Batch  163/175   train_loss = 1.230\n",
      "Epoch 2228 Batch   20/175   train_loss = 1.205\n",
      "Epoch 2228 Batch   52/175   train_loss = 1.151\n",
      "Epoch 2228 Batch   84/175   train_loss = 1.235\n",
      "Epoch 2228 Batch  116/175   train_loss = 1.285\n",
      "Epoch 2228 Batch  148/175   train_loss = 1.214\n",
      "Epoch 2229 Batch    5/175   train_loss = 1.219\n",
      "Epoch 2229 Batch   37/175   train_loss = 1.189\n",
      "Epoch 2229 Batch   69/175   train_loss = 1.228\n",
      "Epoch 2229 Batch  101/175   train_loss = 1.293\n",
      "Epoch 2229 Batch  133/175   train_loss = 1.141\n",
      "Epoch 2229 Batch  165/175   train_loss = 1.226\n",
      "Epoch 2230 Batch   22/175   train_loss = 1.149\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2230 Batch   54/175   train_loss = 1.255\n",
      "Epoch 2230 Batch   86/175   train_loss = 1.294\n",
      "Epoch 2230 Batch  118/175   train_loss = 1.287\n",
      "Epoch 2230 Batch  150/175   train_loss = 1.237\n",
      "Epoch 2231 Batch    7/175   train_loss = 1.244\n",
      "Epoch 2231 Batch   39/175   train_loss = 1.178\n",
      "Epoch 2231 Batch   71/175   train_loss = 1.265\n",
      "Epoch 2231 Batch  103/175   train_loss = 1.252\n",
      "Epoch 2231 Batch  135/175   train_loss = 1.200\n",
      "Epoch 2231 Batch  167/175   train_loss = 1.275\n",
      "Epoch 2232 Batch   24/175   train_loss = 1.180\n",
      "Epoch 2232 Batch   56/175   train_loss = 1.245\n",
      "Epoch 2232 Batch   88/175   train_loss = 1.282\n",
      "Epoch 2232 Batch  120/175   train_loss = 1.208\n",
      "Epoch 2232 Batch  152/175   train_loss = 1.202\n",
      "Epoch 2233 Batch    9/175   train_loss = 1.258\n",
      "Epoch 2233 Batch   41/175   train_loss = 1.265\n",
      "Epoch 2233 Batch   73/175   train_loss = 1.228\n",
      "Epoch 2233 Batch  105/175   train_loss = 1.278\n",
      "Epoch 2233 Batch  137/175   train_loss = 1.190\n",
      "Epoch 2233 Batch  169/175   train_loss = 1.255\n",
      "Epoch 2234 Batch   26/175   train_loss = 1.232\n",
      "Epoch 2234 Batch   58/175   train_loss = 1.242\n",
      "Epoch 2234 Batch   90/175   train_loss = 1.245\n",
      "Epoch 2234 Batch  122/175   train_loss = 1.201\n",
      "Epoch 2234 Batch  154/175   train_loss = 1.200\n",
      "Epoch 2235 Batch   11/175   train_loss = 1.193\n",
      "Epoch 2235 Batch   43/175   train_loss = 1.219\n",
      "Epoch 2235 Batch   75/175   train_loss = 1.196\n",
      "Epoch 2235 Batch  107/175   train_loss = 1.267\n",
      "Epoch 2235 Batch  139/175   train_loss = 1.182\n",
      "Epoch 2235 Batch  171/175   train_loss = 1.294\n",
      "Epoch 2236 Batch   28/175   train_loss = 1.233\n",
      "Epoch 2236 Batch   60/175   train_loss = 1.227\n",
      "Epoch 2236 Batch   92/175   train_loss = 1.202\n",
      "Epoch 2236 Batch  124/175   train_loss = 1.229\n",
      "Epoch 2236 Batch  156/175   train_loss = 1.261\n",
      "Epoch 2237 Batch   13/175   train_loss = 1.220\n",
      "Epoch 2237 Batch   45/175   train_loss = 1.230\n",
      "Epoch 2237 Batch   77/175   train_loss = 1.213\n",
      "Epoch 2237 Batch  109/175   train_loss = 1.271\n",
      "Epoch 2237 Batch  141/175   train_loss = 1.171\n",
      "Epoch 2237 Batch  173/175   train_loss = 1.202\n",
      "Epoch 2238 Batch   30/175   train_loss = 1.269\n",
      "Epoch 2238 Batch   62/175   train_loss = 1.257\n",
      "Epoch 2238 Batch   94/175   train_loss = 1.218\n",
      "Epoch 2238 Batch  126/175   train_loss = 1.230\n",
      "Epoch 2238 Batch  158/175   train_loss = 1.213\n",
      "Epoch 2239 Batch   15/175   train_loss = 1.275\n",
      "Epoch 2239 Batch   47/175   train_loss = 1.219\n",
      "Epoch 2239 Batch   79/175   train_loss = 1.268\n",
      "Epoch 2239 Batch  111/175   train_loss = 1.263\n",
      "Epoch 2239 Batch  143/175   train_loss = 1.182\n",
      "Epoch 2240 Batch    0/175   train_loss = 1.221\n",
      "Epoch 2240 Batch   32/175   train_loss = 1.260\n",
      "Epoch 2240 Batch   64/175   train_loss = 1.260\n",
      "Epoch 2240 Batch   96/175   train_loss = 1.247\n",
      "Epoch 2240 Batch  128/175   train_loss = 1.168\n",
      "Epoch 2240 Batch  160/175   train_loss = 1.211\n",
      "Epoch 2241 Batch   17/175   train_loss = 1.185\n",
      "Epoch 2241 Batch   49/175   train_loss = 1.256\n",
      "Epoch 2241 Batch   81/175   train_loss = 1.214\n",
      "Epoch 2241 Batch  113/175   train_loss = 1.234\n",
      "Epoch 2241 Batch  145/175   train_loss = 1.189\n",
      "Epoch 2242 Batch    2/175   train_loss = 1.212\n",
      "Epoch 2242 Batch   34/175   train_loss = 1.250\n",
      "Epoch 2242 Batch   66/175   train_loss = 1.265\n",
      "Epoch 2242 Batch   98/175   train_loss = 1.239\n",
      "Epoch 2242 Batch  130/175   train_loss = 1.257\n",
      "Epoch 2242 Batch  162/175   train_loss = 1.228\n",
      "Epoch 2243 Batch   19/175   train_loss = 1.215\n",
      "Epoch 2243 Batch   51/175   train_loss = 1.156\n",
      "Epoch 2243 Batch   83/175   train_loss = 1.275\n",
      "Epoch 2243 Batch  115/175   train_loss = 1.334\n",
      "Epoch 2243 Batch  147/175   train_loss = 1.185\n",
      "Epoch 2244 Batch    4/175   train_loss = 1.259\n",
      "Epoch 2244 Batch   36/175   train_loss = 1.169\n",
      "Epoch 2244 Batch   68/175   train_loss = 1.202\n",
      "Epoch 2244 Batch  100/175   train_loss = 1.242\n",
      "Epoch 2244 Batch  132/175   train_loss = 1.201\n",
      "Epoch 2244 Batch  164/175   train_loss = 1.182\n",
      "Epoch 2245 Batch   21/175   train_loss = 1.184\n",
      "Epoch 2245 Batch   53/175   train_loss = 1.194\n",
      "Epoch 2245 Batch   85/175   train_loss = 1.221\n",
      "Epoch 2245 Batch  117/175   train_loss = 1.234\n",
      "Epoch 2245 Batch  149/175   train_loss = 1.229\n",
      "Epoch 2246 Batch    6/175   train_loss = 1.246\n",
      "Epoch 2246 Batch   38/175   train_loss = 1.172\n",
      "Epoch 2246 Batch   70/175   train_loss = 1.200\n",
      "Epoch 2246 Batch  102/175   train_loss = 1.222\n",
      "Epoch 2246 Batch  134/175   train_loss = 1.182\n",
      "Epoch 2246 Batch  166/175   train_loss = 1.221\n",
      "Epoch 2247 Batch   23/175   train_loss = 1.157\n",
      "Epoch 2247 Batch   55/175   train_loss = 1.281\n",
      "Epoch 2247 Batch   87/175   train_loss = 1.288\n",
      "Epoch 2247 Batch  119/175   train_loss = 1.226\n",
      "Epoch 2247 Batch  151/175   train_loss = 1.368\n",
      "Epoch 2248 Batch    8/175   train_loss = 1.284\n",
      "Epoch 2248 Batch   40/175   train_loss = 1.216\n",
      "Epoch 2248 Batch   72/175   train_loss = 1.335\n",
      "Epoch 2248 Batch  104/175   train_loss = 1.264\n",
      "Epoch 2248 Batch  136/175   train_loss = 1.205\n",
      "Epoch 2248 Batch  168/175   train_loss = 1.248\n",
      "Epoch 2249 Batch   25/175   train_loss = 1.240\n",
      "Epoch 2249 Batch   57/175   train_loss = 1.277\n",
      "Epoch 2249 Batch   89/175   train_loss = 1.279\n",
      "Epoch 2249 Batch  121/175   train_loss = 1.215\n",
      "Epoch 2249 Batch  153/175   train_loss = 1.228\n",
      "Epoch 2250 Batch   10/175   train_loss = 1.204\n",
      "Epoch 2250 Batch   42/175   train_loss = 1.263\n",
      "Epoch 2250 Batch   74/175   train_loss = 1.289\n",
      "Epoch 2250 Batch  106/175   train_loss = 1.278\n",
      "Epoch 2250 Batch  138/175   train_loss = 1.235\n",
      "Epoch 2250 Batch  170/175   train_loss = 1.259\n",
      "Epoch 2251 Batch   27/175   train_loss = 1.234\n",
      "Epoch 2251 Batch   59/175   train_loss = 1.212\n",
      "Epoch 2251 Batch   91/175   train_loss = 1.237\n",
      "Epoch 2251 Batch  123/175   train_loss = 1.237\n",
      "Epoch 2251 Batch  155/175   train_loss = 1.215\n",
      "Epoch 2252 Batch   12/175   train_loss = 1.251\n",
      "Epoch 2252 Batch   44/175   train_loss = 1.201\n",
      "Epoch 2252 Batch   76/175   train_loss = 1.249\n",
      "Epoch 2252 Batch  108/175   train_loss = 1.253\n",
      "Epoch 2252 Batch  140/175   train_loss = 1.200\n",
      "Epoch 2252 Batch  172/175   train_loss = 1.243\n",
      "Epoch 2253 Batch   29/175   train_loss = 1.226\n",
      "Epoch 2253 Batch   61/175   train_loss = 1.250\n",
      "Epoch 2253 Batch   93/175   train_loss = 1.223\n",
      "Epoch 2253 Batch  125/175   train_loss = 1.252\n",
      "Epoch 2253 Batch  157/175   train_loss = 1.224\n",
      "Epoch 2254 Batch   14/175   train_loss = 1.268\n",
      "Epoch 2254 Batch   46/175   train_loss = 1.228\n",
      "Epoch 2254 Batch   78/175   train_loss = 1.236\n",
      "Epoch 2254 Batch  110/175   train_loss = 1.307\n",
      "Epoch 2254 Batch  142/175   train_loss = 1.284\n",
      "Epoch 2254 Batch  174/175   train_loss = 1.237\n",
      "Epoch 2255 Batch   31/175   train_loss = 1.238\n",
      "Epoch 2255 Batch   63/175   train_loss = 1.229\n",
      "Epoch 2255 Batch   95/175   train_loss = 1.206\n",
      "Epoch 2255 Batch  127/175   train_loss = 1.183\n",
      "Epoch 2255 Batch  159/175   train_loss = 1.174\n",
      "Epoch 2256 Batch   16/175   train_loss = 1.204\n",
      "Epoch 2256 Batch   48/175   train_loss = 1.219\n",
      "Epoch 2256 Batch   80/175   train_loss = 1.247\n",
      "Epoch 2256 Batch  112/175   train_loss = 1.216\n",
      "Epoch 2256 Batch  144/175   train_loss = 1.146\n",
      "Epoch 2257 Batch    1/175   train_loss = 1.249\n",
      "Epoch 2257 Batch   33/175   train_loss = 1.241\n",
      "Epoch 2257 Batch   65/175   train_loss = 1.213\n",
      "Epoch 2257 Batch   97/175   train_loss = 1.212\n",
      "Epoch 2257 Batch  129/175   train_loss = 1.219\n",
      "Epoch 2257 Batch  161/175   train_loss = 1.184\n",
      "Epoch 2258 Batch   18/175   train_loss = 1.182\n",
      "Epoch 2258 Batch   50/175   train_loss = 1.219\n",
      "Epoch 2258 Batch   82/175   train_loss = 1.259\n",
      "Epoch 2258 Batch  114/175   train_loss = 1.283\n",
      "Epoch 2258 Batch  146/175   train_loss = 1.224\n",
      "Epoch 2259 Batch    3/175   train_loss = 1.277\n",
      "Epoch 2259 Batch   35/175   train_loss = 1.228\n",
      "Epoch 2259 Batch   67/175   train_loss = 1.194\n",
      "Epoch 2259 Batch   99/175   train_loss = 1.301\n",
      "Epoch 2259 Batch  131/175   train_loss = 1.227\n",
      "Epoch 2259 Batch  163/175   train_loss = 1.223\n",
      "Epoch 2260 Batch   20/175   train_loss = 1.198\n",
      "Epoch 2260 Batch   52/175   train_loss = 1.175\n",
      "Epoch 2260 Batch   84/175   train_loss = 1.220\n",
      "Epoch 2260 Batch  116/175   train_loss = 1.283\n",
      "Epoch 2260 Batch  148/175   train_loss = 1.210\n",
      "Epoch 2261 Batch    5/175   train_loss = 1.223\n",
      "Epoch 2261 Batch   37/175   train_loss = 1.211\n",
      "Epoch 2261 Batch   69/175   train_loss = 1.222\n",
      "Epoch 2261 Batch  101/175   train_loss = 1.293\n",
      "Epoch 2261 Batch  133/175   train_loss = 1.138\n",
      "Epoch 2261 Batch  165/175   train_loss = 1.204\n",
      "Epoch 2262 Batch   22/175   train_loss = 1.166\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2262 Batch   54/175   train_loss = 1.257\n",
      "Epoch 2262 Batch   86/175   train_loss = 1.272\n",
      "Epoch 2262 Batch  118/175   train_loss = 1.272\n",
      "Epoch 2262 Batch  150/175   train_loss = 1.234\n",
      "Epoch 2263 Batch    7/175   train_loss = 1.236\n",
      "Epoch 2263 Batch   39/175   train_loss = 1.173\n",
      "Epoch 2263 Batch   71/175   train_loss = 1.267\n",
      "Epoch 2263 Batch  103/175   train_loss = 1.226\n",
      "Epoch 2263 Batch  135/175   train_loss = 1.205\n",
      "Epoch 2263 Batch  167/175   train_loss = 1.267\n",
      "Epoch 2264 Batch   24/175   train_loss = 1.174\n",
      "Epoch 2264 Batch   56/175   train_loss = 1.268\n",
      "Epoch 2264 Batch   88/175   train_loss = 1.272\n",
      "Epoch 2264 Batch  120/175   train_loss = 1.208\n",
      "Epoch 2264 Batch  152/175   train_loss = 1.206\n",
      "Epoch 2265 Batch    9/175   train_loss = 1.296\n",
      "Epoch 2265 Batch   41/175   train_loss = 1.265\n",
      "Epoch 2265 Batch   73/175   train_loss = 1.252\n",
      "Epoch 2265 Batch  105/175   train_loss = 1.290\n",
      "Epoch 2265 Batch  137/175   train_loss = 1.204\n",
      "Epoch 2265 Batch  169/175   train_loss = 1.269\n",
      "Epoch 2266 Batch   26/175   train_loss = 1.233\n",
      "Epoch 2266 Batch   58/175   train_loss = 1.262\n",
      "Epoch 2266 Batch   90/175   train_loss = 1.236\n",
      "Epoch 2266 Batch  122/175   train_loss = 1.189\n",
      "Epoch 2266 Batch  154/175   train_loss = 1.199\n",
      "Epoch 2267 Batch   11/175   train_loss = 1.207\n",
      "Epoch 2267 Batch   43/175   train_loss = 1.238\n",
      "Epoch 2267 Batch   75/175   train_loss = 1.205\n",
      "Epoch 2267 Batch  107/175   train_loss = 1.287\n",
      "Epoch 2267 Batch  139/175   train_loss = 1.168\n",
      "Epoch 2267 Batch  171/175   train_loss = 1.258\n",
      "Epoch 2268 Batch   28/175   train_loss = 1.217\n",
      "Epoch 2268 Batch   60/175   train_loss = 1.230\n",
      "Epoch 2268 Batch   92/175   train_loss = 1.205\n",
      "Epoch 2268 Batch  124/175   train_loss = 1.281\n",
      "Epoch 2268 Batch  156/175   train_loss = 1.282\n",
      "Epoch 2269 Batch   13/175   train_loss = 1.209\n",
      "Epoch 2269 Batch   45/175   train_loss = 1.217\n",
      "Epoch 2269 Batch   77/175   train_loss = 1.198\n",
      "Epoch 2269 Batch  109/175   train_loss = 1.247\n",
      "Epoch 2269 Batch  141/175   train_loss = 1.174\n",
      "Epoch 2269 Batch  173/175   train_loss = 1.188\n",
      "Epoch 2270 Batch   30/175   train_loss = 1.277\n",
      "Epoch 2270 Batch   62/175   train_loss = 1.233\n",
      "Epoch 2270 Batch   94/175   train_loss = 1.201\n",
      "Epoch 2270 Batch  126/175   train_loss = 1.220\n",
      "Epoch 2270 Batch  158/175   train_loss = 1.217\n",
      "Epoch 2271 Batch   15/175   train_loss = 1.247\n",
      "Epoch 2271 Batch   47/175   train_loss = 1.229\n",
      "Epoch 2271 Batch   79/175   train_loss = 1.239\n",
      "Epoch 2271 Batch  111/175   train_loss = 1.270\n",
      "Epoch 2271 Batch  143/175   train_loss = 1.198\n",
      "Epoch 2272 Batch    0/175   train_loss = 1.200\n",
      "Epoch 2272 Batch   32/175   train_loss = 1.241\n",
      "Epoch 2272 Batch   64/175   train_loss = 1.241\n",
      "Epoch 2272 Batch   96/175   train_loss = 1.231\n",
      "Epoch 2272 Batch  128/175   train_loss = 1.160\n",
      "Epoch 2272 Batch  160/175   train_loss = 1.228\n",
      "Epoch 2273 Batch   17/175   train_loss = 1.167\n",
      "Epoch 2273 Batch   49/175   train_loss = 1.231\n",
      "Epoch 2273 Batch   81/175   train_loss = 1.205\n",
      "Epoch 2273 Batch  113/175   train_loss = 1.221\n",
      "Epoch 2273 Batch  145/175   train_loss = 1.155\n",
      "Epoch 2274 Batch    2/175   train_loss = 1.222\n",
      "Epoch 2274 Batch   34/175   train_loss = 1.246\n",
      "Epoch 2274 Batch   66/175   train_loss = 1.231\n",
      "Epoch 2274 Batch   98/175   train_loss = 1.239\n",
      "Epoch 2274 Batch  130/175   train_loss = 1.230\n",
      "Epoch 2274 Batch  162/175   train_loss = 1.200\n",
      "Epoch 2275 Batch   19/175   train_loss = 1.217\n",
      "Epoch 2275 Batch   51/175   train_loss = 1.158\n",
      "Epoch 2275 Batch   83/175   train_loss = 1.269\n",
      "Epoch 2275 Batch  115/175   train_loss = 1.322\n",
      "Epoch 2275 Batch  147/175   train_loss = 1.180\n",
      "Epoch 2276 Batch    4/175   train_loss = 1.230\n",
      "Epoch 2276 Batch   36/175   train_loss = 1.172\n",
      "Epoch 2276 Batch   68/175   train_loss = 1.186\n",
      "Epoch 2276 Batch  100/175   train_loss = 1.244\n",
      "Epoch 2276 Batch  132/175   train_loss = 1.194\n",
      "Epoch 2276 Batch  164/175   train_loss = 1.192\n",
      "Epoch 2277 Batch   21/175   train_loss = 1.177\n",
      "Epoch 2277 Batch   53/175   train_loss = 1.206\n",
      "Epoch 2277 Batch   85/175   train_loss = 1.231\n",
      "Epoch 2277 Batch  117/175   train_loss = 1.228\n",
      "Epoch 2277 Batch  149/175   train_loss = 1.237\n",
      "Epoch 2278 Batch    6/175   train_loss = 1.246\n",
      "Epoch 2278 Batch   38/175   train_loss = 1.177\n",
      "Epoch 2278 Batch   70/175   train_loss = 1.215\n",
      "Epoch 2278 Batch  102/175   train_loss = 1.227\n",
      "Epoch 2278 Batch  134/175   train_loss = 1.189\n",
      "Epoch 2278 Batch  166/175   train_loss = 1.248\n",
      "Epoch 2279 Batch   23/175   train_loss = 1.167\n",
      "Epoch 2279 Batch   55/175   train_loss = 1.305\n",
      "Epoch 2279 Batch   87/175   train_loss = 1.281\n",
      "Epoch 2279 Batch  119/175   train_loss = 1.222\n",
      "Epoch 2279 Batch  151/175   train_loss = 1.238\n",
      "Epoch 2280 Batch    8/175   train_loss = 1.252\n",
      "Epoch 2280 Batch   40/175   train_loss = 1.216\n",
      "Epoch 2280 Batch   72/175   train_loss = 1.326\n",
      "Epoch 2280 Batch  104/175   train_loss = 1.247\n",
      "Epoch 2280 Batch  136/175   train_loss = 1.219\n",
      "Epoch 2280 Batch  168/175   train_loss = 1.258\n",
      "Epoch 2281 Batch   25/175   train_loss = 1.235\n",
      "Epoch 2281 Batch   57/175   train_loss = 1.239\n",
      "Epoch 2281 Batch   89/175   train_loss = 1.214\n",
      "Epoch 2281 Batch  121/175   train_loss = 1.211\n",
      "Epoch 2281 Batch  153/175   train_loss = 1.221\n",
      "Epoch 2282 Batch   10/175   train_loss = 1.193\n",
      "Epoch 2282 Batch   42/175   train_loss = 1.257\n",
      "Epoch 2282 Batch   74/175   train_loss = 1.276\n",
      "Epoch 2282 Batch  106/175   train_loss = 1.241\n",
      "Epoch 2282 Batch  138/175   train_loss = 1.236\n",
      "Epoch 2282 Batch  170/175   train_loss = 1.269\n",
      "Epoch 2283 Batch   27/175   train_loss = 1.208\n",
      "Epoch 2283 Batch   59/175   train_loss = 1.211\n",
      "Epoch 2283 Batch   91/175   train_loss = 1.212\n",
      "Epoch 2283 Batch  123/175   train_loss = 1.212\n",
      "Epoch 2283 Batch  155/175   train_loss = 1.215\n",
      "Epoch 2284 Batch   12/175   train_loss = 1.241\n",
      "Epoch 2284 Batch   44/175   train_loss = 1.195\n",
      "Epoch 2284 Batch   76/175   train_loss = 1.215\n",
      "Epoch 2284 Batch  108/175   train_loss = 1.249\n",
      "Epoch 2284 Batch  140/175   train_loss = 1.205\n",
      "Epoch 2284 Batch  172/175   train_loss = 1.249\n",
      "Epoch 2285 Batch   29/175   train_loss = 1.208\n",
      "Epoch 2285 Batch   61/175   train_loss = 1.255\n",
      "Epoch 2285 Batch   93/175   train_loss = 1.226\n",
      "Epoch 2285 Batch  125/175   train_loss = 1.233\n",
      "Epoch 2285 Batch  157/175   train_loss = 1.258\n",
      "Epoch 2286 Batch   14/175   train_loss = 1.248\n",
      "Epoch 2286 Batch   46/175   train_loss = 1.228\n",
      "Epoch 2286 Batch   78/175   train_loss = 1.203\n",
      "Epoch 2286 Batch  110/175   train_loss = 1.312\n",
      "Epoch 2286 Batch  142/175   train_loss = 1.268\n",
      "Epoch 2286 Batch  174/175   train_loss = 1.233\n",
      "Epoch 2287 Batch   31/175   train_loss = 1.256\n",
      "Epoch 2287 Batch   63/175   train_loss = 1.266\n",
      "Epoch 2287 Batch   95/175   train_loss = 1.221\n",
      "Epoch 2287 Batch  127/175   train_loss = 1.205\n",
      "Epoch 2287 Batch  159/175   train_loss = 1.201\n",
      "Epoch 2288 Batch   16/175   train_loss = 1.234\n",
      "Epoch 2288 Batch   48/175   train_loss = 1.236\n",
      "Epoch 2288 Batch   80/175   train_loss = 1.260\n",
      "Epoch 2288 Batch  112/175   train_loss = 1.272\n",
      "Epoch 2288 Batch  144/175   train_loss = 1.196\n",
      "Epoch 2289 Batch    1/175   train_loss = 1.271\n",
      "Epoch 2289 Batch   33/175   train_loss = 1.281\n",
      "Epoch 2289 Batch   65/175   train_loss = 1.247\n",
      "Epoch 2289 Batch   97/175   train_loss = 1.259\n",
      "Epoch 2289 Batch  129/175   train_loss = 1.253\n",
      "Epoch 2289 Batch  161/175   train_loss = 1.230\n",
      "Epoch 2290 Batch   18/175   train_loss = 1.189\n",
      "Epoch 2290 Batch   50/175   train_loss = 1.228\n",
      "Epoch 2290 Batch   82/175   train_loss = 1.276\n",
      "Epoch 2290 Batch  114/175   train_loss = 1.282\n",
      "Epoch 2290 Batch  146/175   train_loss = 1.249\n",
      "Epoch 2291 Batch    3/175   train_loss = 1.279\n",
      "Epoch 2291 Batch   35/175   train_loss = 1.225\n",
      "Epoch 2291 Batch   67/175   train_loss = 1.289\n",
      "Epoch 2291 Batch   99/175   train_loss = 1.339\n",
      "Epoch 2291 Batch  131/175   train_loss = 1.289\n",
      "Epoch 2291 Batch  163/175   train_loss = 1.264\n",
      "Epoch 2292 Batch   20/175   train_loss = 1.245\n",
      "Epoch 2292 Batch   52/175   train_loss = 1.202\n",
      "Epoch 2292 Batch   84/175   train_loss = 1.241\n",
      "Epoch 2292 Batch  116/175   train_loss = 1.317\n",
      "Epoch 2292 Batch  148/175   train_loss = 1.233\n",
      "Epoch 2293 Batch    5/175   train_loss = 1.246\n",
      "Epoch 2293 Batch   37/175   train_loss = 1.228\n",
      "Epoch 2293 Batch   69/175   train_loss = 1.283\n",
      "Epoch 2293 Batch  101/175   train_loss = 1.289\n",
      "Epoch 2293 Batch  133/175   train_loss = 1.147\n",
      "Epoch 2293 Batch  165/175   train_loss = 1.241\n",
      "Epoch 2294 Batch   22/175   train_loss = 1.167\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2294 Batch   54/175   train_loss = 1.275\n",
      "Epoch 2294 Batch   86/175   train_loss = 1.302\n",
      "Epoch 2294 Batch  118/175   train_loss = 1.304\n",
      "Epoch 2294 Batch  150/175   train_loss = 1.234\n",
      "Epoch 2295 Batch    7/175   train_loss = 1.271\n",
      "Epoch 2295 Batch   39/175   train_loss = 1.185\n",
      "Epoch 2295 Batch   71/175   train_loss = 1.258\n",
      "Epoch 2295 Batch  103/175   train_loss = 1.227\n",
      "Epoch 2295 Batch  135/175   train_loss = 1.204\n",
      "Epoch 2295 Batch  167/175   train_loss = 1.267\n",
      "Epoch 2296 Batch   24/175   train_loss = 1.180\n",
      "Epoch 2296 Batch   56/175   train_loss = 1.273\n",
      "Epoch 2296 Batch   88/175   train_loss = 1.296\n",
      "Epoch 2296 Batch  120/175   train_loss = 1.197\n",
      "Epoch 2296 Batch  152/175   train_loss = 1.206\n",
      "Epoch 2297 Batch    9/175   train_loss = 1.256\n",
      "Epoch 2297 Batch   41/175   train_loss = 1.268\n",
      "Epoch 2297 Batch   73/175   train_loss = 1.238\n",
      "Epoch 2297 Batch  105/175   train_loss = 1.261\n",
      "Epoch 2297 Batch  137/175   train_loss = 1.207\n",
      "Epoch 2297 Batch  169/175   train_loss = 1.263\n",
      "Epoch 2298 Batch   26/175   train_loss = 1.244\n",
      "Epoch 2298 Batch   58/175   train_loss = 1.269\n",
      "Epoch 2298 Batch   90/175   train_loss = 1.270\n",
      "Epoch 2298 Batch  122/175   train_loss = 1.204\n",
      "Epoch 2298 Batch  154/175   train_loss = 1.207\n",
      "Epoch 2299 Batch   11/175   train_loss = 1.212\n",
      "Epoch 2299 Batch   43/175   train_loss = 1.226\n",
      "Epoch 2299 Batch   75/175   train_loss = 1.207\n",
      "Epoch 2299 Batch  107/175   train_loss = 1.281\n",
      "Epoch 2299 Batch  139/175   train_loss = 1.195\n",
      "Epoch 2299 Batch  171/175   train_loss = 1.275\n",
      "Epoch 2300 Batch   28/175   train_loss = 1.202\n",
      "Epoch 2300 Batch   60/175   train_loss = 1.222\n",
      "Epoch 2300 Batch   92/175   train_loss = 1.198\n",
      "Epoch 2300 Batch  124/175   train_loss = 1.218\n",
      "Epoch 2300 Batch  156/175   train_loss = 1.261\n",
      "Epoch 2301 Batch   13/175   train_loss = 1.202\n",
      "Epoch 2301 Batch   45/175   train_loss = 1.223\n",
      "Epoch 2301 Batch   77/175   train_loss = 1.213\n",
      "Epoch 2301 Batch  109/175   train_loss = 1.293\n",
      "Epoch 2301 Batch  141/175   train_loss = 1.195\n",
      "Epoch 2301 Batch  173/175   train_loss = 1.192\n",
      "Epoch 2302 Batch   30/175   train_loss = 1.277\n",
      "Epoch 2302 Batch   62/175   train_loss = 1.246\n",
      "Epoch 2302 Batch   94/175   train_loss = 1.244\n",
      "Epoch 2302 Batch  126/175   train_loss = 1.235\n",
      "Epoch 2302 Batch  158/175   train_loss = 1.225\n",
      "Epoch 2303 Batch   15/175   train_loss = 1.263\n",
      "Epoch 2303 Batch   47/175   train_loss = 1.260\n",
      "Epoch 2303 Batch   79/175   train_loss = 1.267\n",
      "Epoch 2303 Batch  111/175   train_loss = 1.351\n",
      "Epoch 2303 Batch  143/175   train_loss = 1.246\n",
      "Epoch 2304 Batch    0/175   train_loss = 1.248\n",
      "Epoch 2304 Batch   32/175   train_loss = 1.360\n",
      "Epoch 2304 Batch   64/175   train_loss = 1.253\n",
      "Epoch 2304 Batch   96/175   train_loss = 1.263\n",
      "Epoch 2304 Batch  128/175   train_loss = 1.177\n",
      "Epoch 2304 Batch  160/175   train_loss = 1.234\n",
      "Epoch 2305 Batch   17/175   train_loss = 1.220\n",
      "Epoch 2305 Batch   49/175   train_loss = 1.258\n",
      "Epoch 2305 Batch   81/175   train_loss = 1.218\n",
      "Epoch 2305 Batch  113/175   train_loss = 1.254\n",
      "Epoch 2305 Batch  145/175   train_loss = 1.186\n",
      "Epoch 2306 Batch    2/175   train_loss = 1.248\n",
      "Epoch 2306 Batch   34/175   train_loss = 1.255\n",
      "Epoch 2306 Batch   66/175   train_loss = 1.250\n",
      "Epoch 2306 Batch   98/175   train_loss = 1.277\n",
      "Epoch 2306 Batch  130/175   train_loss = 1.263\n",
      "Epoch 2306 Batch  162/175   train_loss = 1.237\n",
      "Epoch 2307 Batch   19/175   train_loss = 1.218\n",
      "Epoch 2307 Batch   51/175   train_loss = 1.153\n",
      "Epoch 2307 Batch   83/175   train_loss = 1.298\n",
      "Epoch 2307 Batch  115/175   train_loss = 1.346\n",
      "Epoch 2307 Batch  147/175   train_loss = 1.176\n",
      "Epoch 2308 Batch    4/175   train_loss = 1.236\n",
      "Epoch 2308 Batch   36/175   train_loss = 1.182\n",
      "Epoch 2308 Batch   68/175   train_loss = 1.199\n",
      "Epoch 2308 Batch  100/175   train_loss = 1.239\n",
      "Epoch 2308 Batch  132/175   train_loss = 1.253\n",
      "Epoch 2308 Batch  164/175   train_loss = 1.178\n",
      "Epoch 2309 Batch   21/175   train_loss = 1.176\n",
      "Epoch 2309 Batch   53/175   train_loss = 1.206\n",
      "Epoch 2309 Batch   85/175   train_loss = 1.258\n",
      "Epoch 2309 Batch  117/175   train_loss = 1.258\n",
      "Epoch 2309 Batch  149/175   train_loss = 1.242\n",
      "Epoch 2310 Batch    6/175   train_loss = 1.263\n",
      "Epoch 2310 Batch   38/175   train_loss = 1.196\n",
      "Epoch 2310 Batch   70/175   train_loss = 1.220\n",
      "Epoch 2310 Batch  102/175   train_loss = 1.243\n",
      "Epoch 2310 Batch  134/175   train_loss = 1.201\n",
      "Epoch 2310 Batch  166/175   train_loss = 1.236\n",
      "Epoch 2311 Batch   23/175   train_loss = 1.162\n",
      "Epoch 2311 Batch   55/175   train_loss = 1.295\n",
      "Epoch 2311 Batch   87/175   train_loss = 1.302\n",
      "Epoch 2311 Batch  119/175   train_loss = 1.239\n",
      "Epoch 2311 Batch  151/175   train_loss = 1.249\n",
      "Epoch 2312 Batch    8/175   train_loss = 1.250\n",
      "Epoch 2312 Batch   40/175   train_loss = 1.205\n",
      "Epoch 2312 Batch   72/175   train_loss = 1.326\n",
      "Epoch 2312 Batch  104/175   train_loss = 1.240\n",
      "Epoch 2312 Batch  136/175   train_loss = 1.229\n",
      "Epoch 2312 Batch  168/175   train_loss = 1.251\n",
      "Epoch 2313 Batch   25/175   train_loss = 1.239\n",
      "Epoch 2313 Batch   57/175   train_loss = 1.256\n",
      "Epoch 2313 Batch   89/175   train_loss = 1.212\n",
      "Epoch 2313 Batch  121/175   train_loss = 1.207\n",
      "Epoch 2313 Batch  153/175   train_loss = 1.224\n",
      "Epoch 2314 Batch   10/175   train_loss = 1.190\n",
      "Epoch 2314 Batch   42/175   train_loss = 1.297\n",
      "Epoch 2314 Batch   74/175   train_loss = 1.288\n",
      "Epoch 2314 Batch  106/175   train_loss = 1.277\n",
      "Epoch 2314 Batch  138/175   train_loss = 1.220\n",
      "Epoch 2314 Batch  170/175   train_loss = 1.256\n",
      "Epoch 2315 Batch   27/175   train_loss = 1.225\n",
      "Epoch 2315 Batch   59/175   train_loss = 1.197\n",
      "Epoch 2315 Batch   91/175   train_loss = 1.210\n",
      "Epoch 2315 Batch  123/175   train_loss = 1.228\n",
      "Epoch 2315 Batch  155/175   train_loss = 1.199\n",
      "Epoch 2316 Batch   12/175   train_loss = 1.235\n",
      "Epoch 2316 Batch   44/175   train_loss = 1.191\n",
      "Epoch 2316 Batch   76/175   train_loss = 1.199\n",
      "Epoch 2316 Batch  108/175   train_loss = 1.228\n",
      "Epoch 2316 Batch  140/175   train_loss = 1.216\n",
      "Epoch 2316 Batch  172/175   train_loss = 1.244\n",
      "Epoch 2317 Batch   29/175   train_loss = 1.229\n",
      "Epoch 2317 Batch   61/175   train_loss = 1.242\n",
      "Epoch 2317 Batch   93/175   train_loss = 1.207\n",
      "Epoch 2317 Batch  125/175   train_loss = 1.235\n",
      "Epoch 2317 Batch  157/175   train_loss = 1.249\n",
      "Epoch 2318 Batch   14/175   train_loss = 1.233\n",
      "Epoch 2318 Batch   46/175   train_loss = 1.228\n",
      "Epoch 2318 Batch   78/175   train_loss = 1.204\n",
      "Epoch 2318 Batch  110/175   train_loss = 1.302\n",
      "Epoch 2318 Batch  142/175   train_loss = 1.262\n",
      "Epoch 2318 Batch  174/175   train_loss = 1.234\n",
      "Epoch 2319 Batch   31/175   train_loss = 1.252\n",
      "Epoch 2319 Batch   63/175   train_loss = 1.235\n",
      "Epoch 2319 Batch   95/175   train_loss = 1.212\n",
      "Epoch 2319 Batch  127/175   train_loss = 1.206\n",
      "Epoch 2319 Batch  159/175   train_loss = 1.203\n",
      "Epoch 2320 Batch   16/175   train_loss = 1.224\n",
      "Epoch 2320 Batch   48/175   train_loss = 1.233\n",
      "Epoch 2320 Batch   80/175   train_loss = 1.258\n",
      "Epoch 2320 Batch  112/175   train_loss = 1.225\n",
      "Epoch 2320 Batch  144/175   train_loss = 1.172\n",
      "Epoch 2321 Batch    1/175   train_loss = 1.268\n",
      "Epoch 2321 Batch   33/175   train_loss = 1.281\n",
      "Epoch 2321 Batch   65/175   train_loss = 1.223\n",
      "Epoch 2321 Batch   97/175   train_loss = 1.221\n",
      "Epoch 2321 Batch  129/175   train_loss = 1.234\n",
      "Epoch 2321 Batch  161/175   train_loss = 1.188\n",
      "Epoch 2322 Batch   18/175   train_loss = 1.191\n",
      "Epoch 2322 Batch   50/175   train_loss = 1.211\n",
      "Epoch 2322 Batch   82/175   train_loss = 1.254\n",
      "Epoch 2322 Batch  114/175   train_loss = 1.273\n",
      "Epoch 2322 Batch  146/175   train_loss = 1.255\n",
      "Epoch 2323 Batch    3/175   train_loss = 1.248\n",
      "Epoch 2323 Batch   35/175   train_loss = 1.224\n",
      "Epoch 2323 Batch   67/175   train_loss = 1.213\n",
      "Epoch 2323 Batch   99/175   train_loss = 1.302\n",
      "Epoch 2323 Batch  131/175   train_loss = 1.224\n",
      "Epoch 2323 Batch  163/175   train_loss = 1.217\n",
      "Epoch 2324 Batch   20/175   train_loss = 1.225\n",
      "Epoch 2324 Batch   52/175   train_loss = 1.206\n",
      "Epoch 2324 Batch   84/175   train_loss = 1.247\n",
      "Epoch 2324 Batch  116/175   train_loss = 1.276\n",
      "Epoch 2324 Batch  148/175   train_loss = 1.223\n",
      "Epoch 2325 Batch    5/175   train_loss = 1.222\n",
      "Epoch 2325 Batch   37/175   train_loss = 1.209\n",
      "Epoch 2325 Batch   69/175   train_loss = 1.266\n",
      "Epoch 2325 Batch  101/175   train_loss = 1.290\n",
      "Epoch 2325 Batch  133/175   train_loss = 1.146\n",
      "Epoch 2325 Batch  165/175   train_loss = 1.230\n",
      "Epoch 2326 Batch   22/175   train_loss = 1.162\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2326 Batch   54/175   train_loss = 1.256\n",
      "Epoch 2326 Batch   86/175   train_loss = 1.278\n",
      "Epoch 2326 Batch  118/175   train_loss = 1.275\n",
      "Epoch 2326 Batch  150/175   train_loss = 1.224\n",
      "Epoch 2327 Batch    7/175   train_loss = 1.292\n",
      "Epoch 2327 Batch   39/175   train_loss = 1.205\n",
      "Epoch 2327 Batch   71/175   train_loss = 1.270\n",
      "Epoch 2327 Batch  103/175   train_loss = 1.248\n",
      "Epoch 2327 Batch  135/175   train_loss = 1.214\n",
      "Epoch 2327 Batch  167/175   train_loss = 1.277\n",
      "Epoch 2328 Batch   24/175   train_loss = 1.173\n",
      "Epoch 2328 Batch   56/175   train_loss = 1.248\n",
      "Epoch 2328 Batch   88/175   train_loss = 1.273\n",
      "Epoch 2328 Batch  120/175   train_loss = 1.203\n",
      "Epoch 2328 Batch  152/175   train_loss = 1.206\n",
      "Epoch 2329 Batch    9/175   train_loss = 1.262\n",
      "Epoch 2329 Batch   41/175   train_loss = 1.281\n",
      "Epoch 2329 Batch   73/175   train_loss = 1.241\n",
      "Epoch 2329 Batch  105/175   train_loss = 1.269\n",
      "Epoch 2329 Batch  137/175   train_loss = 1.192\n",
      "Epoch 2329 Batch  169/175   train_loss = 1.248\n",
      "Epoch 2330 Batch   26/175   train_loss = 1.255\n",
      "Epoch 2330 Batch   58/175   train_loss = 1.282\n",
      "Epoch 2330 Batch   90/175   train_loss = 1.268\n",
      "Epoch 2330 Batch  122/175   train_loss = 1.205\n",
      "Epoch 2330 Batch  154/175   train_loss = 1.321\n",
      "Epoch 2331 Batch   11/175   train_loss = 1.210\n",
      "Epoch 2331 Batch   43/175   train_loss = 1.244\n",
      "Epoch 2331 Batch   75/175   train_loss = 1.237\n",
      "Epoch 2331 Batch  107/175   train_loss = 1.315\n",
      "Epoch 2331 Batch  139/175   train_loss = 1.181\n",
      "Epoch 2331 Batch  171/175   train_loss = 1.295\n",
      "Epoch 2332 Batch   28/175   train_loss = 1.278\n",
      "Epoch 2332 Batch   60/175   train_loss = 1.237\n",
      "Epoch 2332 Batch   92/175   train_loss = 1.204\n",
      "Epoch 2332 Batch  124/175   train_loss = 1.250\n",
      "Epoch 2332 Batch  156/175   train_loss = 1.273\n",
      "Epoch 2333 Batch   13/175   train_loss = 1.226\n",
      "Epoch 2333 Batch   45/175   train_loss = 1.231\n",
      "Epoch 2333 Batch   77/175   train_loss = 1.220\n",
      "Epoch 2333 Batch  109/175   train_loss = 1.270\n",
      "Epoch 2333 Batch  141/175   train_loss = 1.197\n",
      "Epoch 2333 Batch  173/175   train_loss = 1.225\n",
      "Epoch 2334 Batch   30/175   train_loss = 1.301\n",
      "Epoch 2334 Batch   62/175   train_loss = 1.256\n",
      "Epoch 2334 Batch   94/175   train_loss = 1.235\n",
      "Epoch 2334 Batch  126/175   train_loss = 1.250\n",
      "Epoch 2334 Batch  158/175   train_loss = 1.230\n",
      "Epoch 2335 Batch   15/175   train_loss = 1.254\n",
      "Epoch 2335 Batch   47/175   train_loss = 1.251\n",
      "Epoch 2335 Batch   79/175   train_loss = 1.273\n",
      "Epoch 2335 Batch  111/175   train_loss = 1.263\n",
      "Epoch 2335 Batch  143/175   train_loss = 1.218\n",
      "Epoch 2336 Batch    0/175   train_loss = 1.234\n",
      "Epoch 2336 Batch   32/175   train_loss = 1.276\n",
      "Epoch 2336 Batch   64/175   train_loss = 1.246\n",
      "Epoch 2336 Batch   96/175   train_loss = 1.260\n",
      "Epoch 2336 Batch  128/175   train_loss = 1.191\n",
      "Epoch 2336 Batch  160/175   train_loss = 1.235\n",
      "Epoch 2337 Batch   17/175   train_loss = 1.190\n",
      "Epoch 2337 Batch   49/175   train_loss = 1.248\n",
      "Epoch 2337 Batch   81/175   train_loss = 1.217\n",
      "Epoch 2337 Batch  113/175   train_loss = 1.249\n",
      "Epoch 2337 Batch  145/175   train_loss = 1.176\n",
      "Epoch 2338 Batch    2/175   train_loss = 1.253\n",
      "Epoch 2338 Batch   34/175   train_loss = 1.290\n",
      "Epoch 2338 Batch   66/175   train_loss = 1.279\n",
      "Epoch 2338 Batch   98/175   train_loss = 1.262\n",
      "Epoch 2338 Batch  130/175   train_loss = 1.250\n",
      "Epoch 2338 Batch  162/175   train_loss = 1.238\n",
      "Epoch 2339 Batch   19/175   train_loss = 1.238\n",
      "Epoch 2339 Batch   51/175   train_loss = 1.247\n",
      "Epoch 2339 Batch   83/175   train_loss = 1.299\n",
      "Epoch 2339 Batch  115/175   train_loss = 1.369\n",
      "Epoch 2339 Batch  147/175   train_loss = 1.207\n",
      "Epoch 2340 Batch    4/175   train_loss = 1.260\n",
      "Epoch 2340 Batch   36/175   train_loss = 1.196\n",
      "Epoch 2340 Batch   68/175   train_loss = 1.229\n",
      "Epoch 2340 Batch  100/175   train_loss = 1.243\n",
      "Epoch 2340 Batch  132/175   train_loss = 1.254\n",
      "Epoch 2340 Batch  164/175   train_loss = 1.210\n",
      "Epoch 2341 Batch   21/175   train_loss = 1.193\n",
      "Epoch 2341 Batch   53/175   train_loss = 1.206\n",
      "Epoch 2341 Batch   85/175   train_loss = 1.277\n",
      "Epoch 2341 Batch  117/175   train_loss = 1.255\n",
      "Epoch 2341 Batch  149/175   train_loss = 1.268\n",
      "Epoch 2342 Batch    6/175   train_loss = 1.248\n",
      "Epoch 2342 Batch   38/175   train_loss = 1.187\n",
      "Epoch 2342 Batch   70/175   train_loss = 1.223\n",
      "Epoch 2342 Batch  102/175   train_loss = 1.237\n",
      "Epoch 2342 Batch  134/175   train_loss = 1.205\n",
      "Epoch 2342 Batch  166/175   train_loss = 1.227\n",
      "Epoch 2343 Batch   23/175   train_loss = 1.165\n",
      "Epoch 2343 Batch   55/175   train_loss = 1.304\n",
      "Epoch 2343 Batch   87/175   train_loss = 1.307\n",
      "Epoch 2343 Batch  119/175   train_loss = 1.242\n",
      "Epoch 2343 Batch  151/175   train_loss = 1.283\n",
      "Epoch 2344 Batch    8/175   train_loss = 1.281\n",
      "Epoch 2344 Batch   40/175   train_loss = 1.220\n",
      "Epoch 2344 Batch   72/175   train_loss = 1.286\n",
      "Epoch 2344 Batch  104/175   train_loss = 1.267\n",
      "Epoch 2344 Batch  136/175   train_loss = 1.231\n",
      "Epoch 2344 Batch  168/175   train_loss = 1.261\n",
      "Epoch 2345 Batch   25/175   train_loss = 1.250\n",
      "Epoch 2345 Batch   57/175   train_loss = 1.251\n",
      "Epoch 2345 Batch   89/175   train_loss = 1.270\n",
      "Epoch 2345 Batch  121/175   train_loss = 1.208\n",
      "Epoch 2345 Batch  153/175   train_loss = 1.289\n",
      "Epoch 2346 Batch   10/175   train_loss = 1.223\n",
      "Epoch 2346 Batch   42/175   train_loss = 1.278\n",
      "Epoch 2346 Batch   74/175   train_loss = 1.294\n",
      "Epoch 2346 Batch  106/175   train_loss = 1.294\n",
      "Epoch 2346 Batch  138/175   train_loss = 1.240\n",
      "Epoch 2346 Batch  170/175   train_loss = 1.285\n",
      "Epoch 2347 Batch   27/175   train_loss = 1.246\n",
      "Epoch 2347 Batch   59/175   train_loss = 1.239\n",
      "Epoch 2347 Batch   91/175   train_loss = 1.207\n",
      "Epoch 2347 Batch  123/175   train_loss = 1.215\n",
      "Epoch 2347 Batch  155/175   train_loss = 1.218\n",
      "Epoch 2348 Batch   12/175   train_loss = 1.253\n",
      "Epoch 2348 Batch   44/175   train_loss = 1.193\n",
      "Epoch 2348 Batch   76/175   train_loss = 1.221\n",
      "Epoch 2348 Batch  108/175   train_loss = 1.229\n",
      "Epoch 2348 Batch  140/175   train_loss = 1.246\n",
      "Epoch 2348 Batch  172/175   train_loss = 1.274\n",
      "Epoch 2349 Batch   29/175   train_loss = 1.237\n",
      "Epoch 2349 Batch   61/175   train_loss = 1.252\n",
      "Epoch 2349 Batch   93/175   train_loss = 1.246\n",
      "Epoch 2349 Batch  125/175   train_loss = 1.250\n",
      "Epoch 2349 Batch  157/175   train_loss = 1.264\n",
      "Epoch 2350 Batch   14/175   train_loss = 1.247\n",
      "Epoch 2350 Batch   46/175   train_loss = 1.249\n",
      "Epoch 2350 Batch   78/175   train_loss = 1.217\n",
      "Epoch 2350 Batch  110/175   train_loss = 1.320\n",
      "Epoch 2350 Batch  142/175   train_loss = 1.268\n",
      "Epoch 2350 Batch  174/175   train_loss = 1.241\n",
      "Epoch 2351 Batch   31/175   train_loss = 1.269\n",
      "Epoch 2351 Batch   63/175   train_loss = 1.266\n",
      "Epoch 2351 Batch   95/175   train_loss = 1.237\n",
      "Epoch 2351 Batch  127/175   train_loss = 1.228\n",
      "Epoch 2351 Batch  159/175   train_loss = 1.183\n",
      "Epoch 2352 Batch   16/175   train_loss = 1.239\n",
      "Epoch 2352 Batch   48/175   train_loss = 1.253\n",
      "Epoch 2352 Batch   80/175   train_loss = 1.268\n",
      "Epoch 2352 Batch  112/175   train_loss = 1.238\n",
      "Epoch 2352 Batch  144/175   train_loss = 1.171\n",
      "Epoch 2353 Batch    1/175   train_loss = 1.273\n",
      "Epoch 2353 Batch   33/175   train_loss = 1.273\n",
      "Epoch 2353 Batch   65/175   train_loss = 1.236\n",
      "Epoch 2353 Batch   97/175   train_loss = 1.228\n",
      "Epoch 2353 Batch  129/175   train_loss = 1.235\n",
      "Epoch 2353 Batch  161/175   train_loss = 1.202\n",
      "Epoch 2354 Batch   18/175   train_loss = 1.195\n",
      "Epoch 2354 Batch   50/175   train_loss = 1.258\n",
      "Epoch 2354 Batch   82/175   train_loss = 1.244\n",
      "Epoch 2354 Batch  114/175   train_loss = 1.261\n",
      "Epoch 2354 Batch  146/175   train_loss = 1.231\n",
      "Epoch 2355 Batch    3/175   train_loss = 1.259\n",
      "Epoch 2355 Batch   35/175   train_loss = 1.227\n",
      "Epoch 2355 Batch   67/175   train_loss = 1.196\n",
      "Epoch 2355 Batch   99/175   train_loss = 1.305\n",
      "Epoch 2355 Batch  131/175   train_loss = 1.244\n",
      "Epoch 2355 Batch  163/175   train_loss = 1.219\n",
      "Epoch 2356 Batch   20/175   train_loss = 1.203\n",
      "Epoch 2356 Batch   52/175   train_loss = 1.176\n",
      "Epoch 2356 Batch   84/175   train_loss = 1.223\n",
      "Epoch 2356 Batch  116/175   train_loss = 1.315\n",
      "Epoch 2356 Batch  148/175   train_loss = 1.240\n",
      "Epoch 2357 Batch    5/175   train_loss = 1.243\n",
      "Epoch 2357 Batch   37/175   train_loss = 1.225\n",
      "Epoch 2357 Batch   69/175   train_loss = 1.244\n",
      "Epoch 2357 Batch  101/175   train_loss = 1.288\n",
      "Epoch 2357 Batch  133/175   train_loss = 1.154\n",
      "Epoch 2357 Batch  165/175   train_loss = 1.242\n",
      "Epoch 2358 Batch   22/175   train_loss = 1.170\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2358 Batch   54/175   train_loss = 1.389\n",
      "Epoch 2358 Batch   86/175   train_loss = 1.366\n",
      "Epoch 2358 Batch  118/175   train_loss = 1.339\n",
      "Epoch 2358 Batch  150/175   train_loss = 1.259\n",
      "Epoch 2359 Batch    7/175   train_loss = 1.305\n",
      "Epoch 2359 Batch   39/175   train_loss = 1.219\n",
      "Epoch 2359 Batch   71/175   train_loss = 1.330\n",
      "Epoch 2359 Batch  103/175   train_loss = 1.306\n",
      "Epoch 2359 Batch  135/175   train_loss = 1.250\n",
      "Epoch 2359 Batch  167/175   train_loss = 1.318\n",
      "Epoch 2360 Batch   24/175   train_loss = 1.239\n",
      "Epoch 2360 Batch   56/175   train_loss = 1.294\n",
      "Epoch 2360 Batch   88/175   train_loss = 1.320\n",
      "Epoch 2360 Batch  120/175   train_loss = 1.252\n",
      "Epoch 2360 Batch  152/175   train_loss = 1.266\n",
      "Epoch 2361 Batch    9/175   train_loss = 1.277\n",
      "Epoch 2361 Batch   41/175   train_loss = 1.301\n",
      "Epoch 2361 Batch   73/175   train_loss = 1.279\n",
      "Epoch 2361 Batch  105/175   train_loss = 1.317\n",
      "Epoch 2361 Batch  137/175   train_loss = 1.240\n",
      "Epoch 2361 Batch  169/175   train_loss = 1.284\n",
      "Epoch 2362 Batch   26/175   train_loss = 1.265\n",
      "Epoch 2362 Batch   58/175   train_loss = 1.295\n",
      "Epoch 2362 Batch   90/175   train_loss = 1.286\n",
      "Epoch 2362 Batch  122/175   train_loss = 1.226\n",
      "Epoch 2362 Batch  154/175   train_loss = 1.281\n",
      "Epoch 2363 Batch   11/175   train_loss = 1.223\n",
      "Epoch 2363 Batch   43/175   train_loss = 1.241\n",
      "Epoch 2363 Batch   75/175   train_loss = 1.228\n",
      "Epoch 2363 Batch  107/175   train_loss = 1.302\n",
      "Epoch 2363 Batch  139/175   train_loss = 1.194\n",
      "Epoch 2363 Batch  171/175   train_loss = 1.296\n",
      "Epoch 2364 Batch   28/175   train_loss = 1.239\n",
      "Epoch 2364 Batch   60/175   train_loss = 1.253\n",
      "Epoch 2364 Batch   92/175   train_loss = 1.222\n",
      "Epoch 2364 Batch  124/175   train_loss = 1.238\n",
      "Epoch 2364 Batch  156/175   train_loss = 1.270\n",
      "Epoch 2365 Batch   13/175   train_loss = 1.247\n",
      "Epoch 2365 Batch   45/175   train_loss = 1.246\n",
      "Epoch 2365 Batch   77/175   train_loss = 1.228\n",
      "Epoch 2365 Batch  109/175   train_loss = 1.277\n",
      "Epoch 2365 Batch  141/175   train_loss = 1.208\n",
      "Epoch 2365 Batch  173/175   train_loss = 1.219\n",
      "Epoch 2366 Batch   30/175   train_loss = 1.289\n",
      "Epoch 2366 Batch   62/175   train_loss = 1.246\n",
      "Epoch 2366 Batch   94/175   train_loss = 1.232\n",
      "Epoch 2366 Batch  126/175   train_loss = 1.245\n",
      "Epoch 2366 Batch  158/175   train_loss = 1.242\n",
      "Epoch 2367 Batch   15/175   train_loss = 1.264\n",
      "Epoch 2367 Batch   47/175   train_loss = 1.263\n",
      "Epoch 2367 Batch   79/175   train_loss = 1.272\n",
      "Epoch 2367 Batch  111/175   train_loss = 1.321\n",
      "Epoch 2367 Batch  143/175   train_loss = 1.239\n",
      "Epoch 2368 Batch    0/175   train_loss = 1.243\n",
      "Epoch 2368 Batch   32/175   train_loss = 1.326\n",
      "Epoch 2368 Batch   64/175   train_loss = 1.268\n",
      "Epoch 2368 Batch   96/175   train_loss = 1.268\n",
      "Epoch 2368 Batch  128/175   train_loss = 1.196\n",
      "Epoch 2368 Batch  160/175   train_loss = 1.250\n",
      "Epoch 2369 Batch   17/175   train_loss = 1.215\n",
      "Epoch 2369 Batch   49/175   train_loss = 1.270\n",
      "Epoch 2369 Batch   81/175   train_loss = 1.246\n",
      "Epoch 2369 Batch  113/175   train_loss = 1.250\n",
      "Epoch 2369 Batch  145/175   train_loss = 1.194\n",
      "Epoch 2370 Batch    2/175   train_loss = 1.233\n",
      "Epoch 2370 Batch   34/175   train_loss = 1.261\n",
      "Epoch 2370 Batch   66/175   train_loss = 1.281\n",
      "Epoch 2370 Batch   98/175   train_loss = 1.262\n",
      "Epoch 2370 Batch  130/175   train_loss = 1.262\n",
      "Epoch 2370 Batch  162/175   train_loss = 1.261\n",
      "Epoch 2371 Batch   19/175   train_loss = 1.229\n",
      "Epoch 2371 Batch   51/175   train_loss = 1.177\n",
      "Epoch 2371 Batch   83/175   train_loss = 1.312\n",
      "Epoch 2371 Batch  115/175   train_loss = 1.373\n",
      "Epoch 2371 Batch  147/175   train_loss = 1.206\n",
      "Epoch 2372 Batch    4/175   train_loss = 1.276\n",
      "Epoch 2372 Batch   36/175   train_loss = 1.197\n",
      "Epoch 2372 Batch   68/175   train_loss = 1.235\n",
      "Epoch 2372 Batch  100/175   train_loss = 1.266\n",
      "Epoch 2372 Batch  132/175   train_loss = 1.214\n",
      "Epoch 2372 Batch  164/175   train_loss = 1.190\n",
      "Epoch 2373 Batch   21/175   train_loss = 1.202\n",
      "Epoch 2373 Batch   53/175   train_loss = 1.218\n",
      "Epoch 2373 Batch   85/175   train_loss = 1.250\n",
      "Epoch 2373 Batch  117/175   train_loss = 1.246\n",
      "Epoch 2373 Batch  149/175   train_loss = 1.256\n",
      "Epoch 2374 Batch    6/175   train_loss = 1.249\n",
      "Epoch 2374 Batch   38/175   train_loss = 1.194\n",
      "Epoch 2374 Batch   70/175   train_loss = 1.241\n",
      "Epoch 2374 Batch  102/175   train_loss = 1.237\n",
      "Epoch 2374 Batch  134/175   train_loss = 1.204\n",
      "Epoch 2374 Batch  166/175   train_loss = 1.253\n",
      "Epoch 2375 Batch   23/175   train_loss = 1.188\n",
      "Epoch 2375 Batch   55/175   train_loss = 1.296\n",
      "Epoch 2375 Batch   87/175   train_loss = 1.314\n",
      "Epoch 2375 Batch  119/175   train_loss = 1.258\n",
      "Epoch 2375 Batch  151/175   train_loss = 1.257\n",
      "Epoch 2376 Batch    8/175   train_loss = 1.270\n",
      "Epoch 2376 Batch   40/175   train_loss = 1.248\n",
      "Epoch 2376 Batch   72/175   train_loss = 1.325\n",
      "Epoch 2376 Batch  104/175   train_loss = 1.283\n",
      "Epoch 2376 Batch  136/175   train_loss = 1.236\n",
      "Epoch 2376 Batch  168/175   train_loss = 1.298\n",
      "Epoch 2377 Batch   25/175   train_loss = 1.260\n",
      "Epoch 2377 Batch   57/175   train_loss = 1.267\n",
      "Epoch 2377 Batch   89/175   train_loss = 1.254\n",
      "Epoch 2377 Batch  121/175   train_loss = 1.201\n",
      "Epoch 2377 Batch  153/175   train_loss = 1.265\n",
      "Epoch 2378 Batch   10/175   train_loss = 1.200\n",
      "Epoch 2378 Batch   42/175   train_loss = 1.282\n",
      "Epoch 2378 Batch   74/175   train_loss = 1.294\n",
      "Epoch 2378 Batch  106/175   train_loss = 1.268\n",
      "Epoch 2378 Batch  138/175   train_loss = 1.227\n",
      "Epoch 2378 Batch  170/175   train_loss = 1.263\n",
      "Epoch 2379 Batch   27/175   train_loss = 1.228\n",
      "Epoch 2379 Batch   59/175   train_loss = 1.217\n",
      "Epoch 2379 Batch   91/175   train_loss = 1.265\n",
      "Epoch 2379 Batch  123/175   train_loss = 1.242\n",
      "Epoch 2379 Batch  155/175   train_loss = 1.228\n",
      "Epoch 2380 Batch   12/175   train_loss = 1.235\n",
      "Epoch 2380 Batch   44/175   train_loss = 1.210\n",
      "Epoch 2380 Batch   76/175   train_loss = 1.215\n",
      "Epoch 2380 Batch  108/175   train_loss = 1.224\n",
      "Epoch 2380 Batch  140/175   train_loss = 1.222\n",
      "Epoch 2380 Batch  172/175   train_loss = 1.277\n",
      "Epoch 2381 Batch   29/175   train_loss = 1.229\n",
      "Epoch 2381 Batch   61/175   train_loss = 1.248\n",
      "Epoch 2381 Batch   93/175   train_loss = 1.243\n",
      "Epoch 2381 Batch  125/175   train_loss = 1.277\n",
      "Epoch 2381 Batch  157/175   train_loss = 1.242\n",
      "Epoch 2382 Batch   14/175   train_loss = 1.248\n",
      "Epoch 2382 Batch   46/175   train_loss = 1.386\n",
      "Epoch 2382 Batch   78/175   train_loss = 1.255\n",
      "Epoch 2382 Batch  110/175   train_loss = 1.359\n",
      "Epoch 2382 Batch  142/175   train_loss = 1.292\n",
      "Epoch 2382 Batch  174/175   train_loss = 1.262\n",
      "Epoch 2383 Batch   31/175   train_loss = 1.289\n",
      "Epoch 2383 Batch   63/175   train_loss = 1.300\n",
      "Epoch 2383 Batch   95/175   train_loss = 1.257\n",
      "Epoch 2383 Batch  127/175   train_loss = 1.231\n",
      "Epoch 2383 Batch  159/175   train_loss = 1.220\n",
      "Epoch 2384 Batch   16/175   train_loss = 1.269\n",
      "Epoch 2384 Batch   48/175   train_loss = 1.261\n",
      "Epoch 2384 Batch   80/175   train_loss = 1.296\n",
      "Epoch 2384 Batch  112/175   train_loss = 1.232\n",
      "Epoch 2384 Batch  144/175   train_loss = 1.186\n",
      "Epoch 2385 Batch    1/175   train_loss = 1.283\n",
      "Epoch 2385 Batch   33/175   train_loss = 1.300\n",
      "Epoch 2385 Batch   65/175   train_loss = 1.247\n",
      "Epoch 2385 Batch   97/175   train_loss = 1.233\n",
      "Epoch 2385 Batch  129/175   train_loss = 1.237\n",
      "Epoch 2385 Batch  161/175   train_loss = 1.243\n",
      "Epoch 2386 Batch   18/175   train_loss = 1.222\n",
      "Epoch 2386 Batch   50/175   train_loss = 1.234\n",
      "Epoch 2386 Batch   82/175   train_loss = 1.300\n",
      "Epoch 2386 Batch  114/175   train_loss = 1.286\n",
      "Epoch 2386 Batch  146/175   train_loss = 1.243\n",
      "Epoch 2387 Batch    3/175   train_loss = 1.281\n",
      "Epoch 2387 Batch   35/175   train_loss = 1.229\n",
      "Epoch 2387 Batch   67/175   train_loss = 1.195\n",
      "Epoch 2387 Batch   99/175   train_loss = 1.295\n",
      "Epoch 2387 Batch  131/175   train_loss = 1.227\n",
      "Epoch 2387 Batch  163/175   train_loss = 1.250\n",
      "Epoch 2388 Batch   20/175   train_loss = 1.228\n",
      "Epoch 2388 Batch   52/175   train_loss = 1.190\n",
      "Epoch 2388 Batch   84/175   train_loss = 1.248\n",
      "Epoch 2388 Batch  116/175   train_loss = 1.296\n",
      "Epoch 2388 Batch  148/175   train_loss = 1.218\n",
      "Epoch 2389 Batch    5/175   train_loss = 1.250\n",
      "Epoch 2389 Batch   37/175   train_loss = 1.207\n",
      "Epoch 2389 Batch   69/175   train_loss = 1.263\n",
      "Epoch 2389 Batch  101/175   train_loss = 1.276\n",
      "Epoch 2389 Batch  133/175   train_loss = 1.137\n",
      "Epoch 2389 Batch  165/175   train_loss = 1.240\n",
      "Epoch 2390 Batch   22/175   train_loss = 1.178\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2390 Batch   54/175   train_loss = 1.264\n",
      "Epoch 2390 Batch   86/175   train_loss = 1.308\n",
      "Epoch 2390 Batch  118/175   train_loss = 1.278\n",
      "Epoch 2390 Batch  150/175   train_loss = 1.242\n",
      "Epoch 2391 Batch    7/175   train_loss = 1.274\n",
      "Epoch 2391 Batch   39/175   train_loss = 1.222\n",
      "Epoch 2391 Batch   71/175   train_loss = 1.302\n",
      "Epoch 2391 Batch  103/175   train_loss = 1.318\n",
      "Epoch 2391 Batch  135/175   train_loss = 1.231\n",
      "Epoch 2391 Batch  167/175   train_loss = 1.293\n",
      "Epoch 2392 Batch   24/175   train_loss = 1.176\n",
      "Epoch 2392 Batch   56/175   train_loss = 1.259\n",
      "Epoch 2392 Batch   88/175   train_loss = 1.327\n",
      "Epoch 2392 Batch  120/175   train_loss = 1.245\n",
      "Epoch 2392 Batch  152/175   train_loss = 1.249\n",
      "Epoch 2393 Batch    9/175   train_loss = 1.268\n",
      "Epoch 2393 Batch   41/175   train_loss = 1.273\n",
      "Epoch 2393 Batch   73/175   train_loss = 1.246\n",
      "Epoch 2393 Batch  105/175   train_loss = 1.279\n",
      "Epoch 2393 Batch  137/175   train_loss = 1.218\n",
      "Epoch 2393 Batch  169/175   train_loss = 1.260\n",
      "Epoch 2394 Batch   26/175   train_loss = 1.232\n",
      "Epoch 2394 Batch   58/175   train_loss = 1.275\n",
      "Epoch 2394 Batch   90/175   train_loss = 1.263\n",
      "Epoch 2394 Batch  122/175   train_loss = 1.217\n",
      "Epoch 2394 Batch  154/175   train_loss = 1.282\n",
      "Epoch 2395 Batch   11/175   train_loss = 1.213\n",
      "Epoch 2395 Batch   43/175   train_loss = 1.222\n",
      "Epoch 2395 Batch   75/175   train_loss = 1.204\n",
      "Epoch 2395 Batch  107/175   train_loss = 1.296\n",
      "Epoch 2395 Batch  139/175   train_loss = 1.190\n",
      "Epoch 2395 Batch  171/175   train_loss = 1.263\n",
      "Epoch 2396 Batch   28/175   train_loss = 1.210\n",
      "Epoch 2396 Batch   60/175   train_loss = 1.248\n",
      "Epoch 2396 Batch   92/175   train_loss = 1.197\n",
      "Epoch 2396 Batch  124/175   train_loss = 1.250\n",
      "Epoch 2396 Batch  156/175   train_loss = 1.279\n",
      "Epoch 2397 Batch   13/175   train_loss = 1.237\n",
      "Epoch 2397 Batch   45/175   train_loss = 1.246\n",
      "Epoch 2397 Batch   77/175   train_loss = 1.236\n",
      "Epoch 2397 Batch  109/175   train_loss = 1.295\n",
      "Epoch 2397 Batch  141/175   train_loss = 1.222\n",
      "Epoch 2397 Batch  173/175   train_loss = 1.213\n",
      "Epoch 2398 Batch   30/175   train_loss = 1.307\n",
      "Epoch 2398 Batch   62/175   train_loss = 1.283\n",
      "Epoch 2398 Batch   94/175   train_loss = 1.254\n",
      "Epoch 2398 Batch  126/175   train_loss = 1.282\n",
      "Epoch 2398 Batch  158/175   train_loss = 1.248\n",
      "Epoch 2399 Batch   15/175   train_loss = 1.290\n",
      "Epoch 2399 Batch   47/175   train_loss = 1.289\n",
      "Epoch 2399 Batch   79/175   train_loss = 1.304\n",
      "Epoch 2399 Batch  111/175   train_loss = 1.315\n",
      "Epoch 2399 Batch  143/175   train_loss = 1.240\n",
      "Epoch 2400 Batch    0/175   train_loss = 1.248\n",
      "Epoch 2400 Batch   32/175   train_loss = 1.316\n",
      "Epoch 2400 Batch   64/175   train_loss = 1.313\n",
      "Epoch 2400 Batch   96/175   train_loss = 1.282\n",
      "Epoch 2400 Batch  128/175   train_loss = 1.209\n",
      "Epoch 2400 Batch  160/175   train_loss = 1.256\n",
      "Epoch 2401 Batch   17/175   train_loss = 1.213\n",
      "Epoch 2401 Batch   49/175   train_loss = 1.281\n",
      "Epoch 2401 Batch   81/175   train_loss = 1.274\n",
      "Epoch 2401 Batch  113/175   train_loss = 1.259\n",
      "Epoch 2401 Batch  145/175   train_loss = 1.194\n",
      "Epoch 2402 Batch    2/175   train_loss = 1.267\n",
      "Epoch 2402 Batch   34/175   train_loss = 1.291\n",
      "Epoch 2402 Batch   66/175   train_loss = 1.310\n",
      "Epoch 2402 Batch   98/175   train_loss = 1.291\n",
      "Epoch 2402 Batch  130/175   train_loss = 1.280\n",
      "Epoch 2402 Batch  162/175   train_loss = 1.273\n",
      "Epoch 2403 Batch   19/175   train_loss = 1.260\n",
      "Epoch 2403 Batch   51/175   train_loss = 1.209\n",
      "Epoch 2403 Batch   83/175   train_loss = 1.312\n",
      "Epoch 2403 Batch  115/175   train_loss = 1.365\n",
      "Epoch 2403 Batch  147/175   train_loss = 1.216\n",
      "Epoch 2404 Batch    4/175   train_loss = 1.273\n",
      "Epoch 2404 Batch   36/175   train_loss = 1.228\n",
      "Epoch 2404 Batch   68/175   train_loss = 1.223\n",
      "Epoch 2404 Batch  100/175   train_loss = 1.277\n",
      "Epoch 2404 Batch  132/175   train_loss = 1.224\n",
      "Epoch 2404 Batch  164/175   train_loss = 1.225\n",
      "Epoch 2405 Batch   21/175   train_loss = 1.225\n",
      "Epoch 2405 Batch   53/175   train_loss = 1.234\n",
      "Epoch 2405 Batch   85/175   train_loss = 1.265\n",
      "Epoch 2405 Batch  117/175   train_loss = 1.267\n",
      "Epoch 2405 Batch  149/175   train_loss = 1.269\n",
      "Epoch 2406 Batch    6/175   train_loss = 1.288\n",
      "Epoch 2406 Batch   38/175   train_loss = 1.197\n",
      "Epoch 2406 Batch   70/175   train_loss = 1.230\n",
      "Epoch 2406 Batch  102/175   train_loss = 1.246\n",
      "Epoch 2406 Batch  134/175   train_loss = 1.222\n",
      "Epoch 2406 Batch  166/175   train_loss = 1.252\n",
      "Epoch 2407 Batch   23/175   train_loss = 1.182\n",
      "Epoch 2407 Batch   55/175   train_loss = 1.314\n",
      "Epoch 2407 Batch   87/175   train_loss = 1.324\n",
      "Epoch 2407 Batch  119/175   train_loss = 1.225\n",
      "Epoch 2407 Batch  151/175   train_loss = 1.249\n",
      "Epoch 2408 Batch    8/175   train_loss = 1.270\n",
      "Epoch 2408 Batch   40/175   train_loss = 1.255\n",
      "Epoch 2408 Batch   72/175   train_loss = 1.419\n",
      "Epoch 2408 Batch  104/175   train_loss = 1.286\n",
      "Epoch 2408 Batch  136/175   train_loss = 1.226\n",
      "Epoch 2408 Batch  168/175   train_loss = 1.292\n",
      "Epoch 2409 Batch   25/175   train_loss = 1.268\n",
      "Epoch 2409 Batch   57/175   train_loss = 1.290\n",
      "Epoch 2409 Batch   89/175   train_loss = 1.244\n",
      "Epoch 2409 Batch  121/175   train_loss = 1.194\n",
      "Epoch 2409 Batch  153/175   train_loss = 1.228\n",
      "Epoch 2410 Batch   10/175   train_loss = 1.197\n",
      "Epoch 2410 Batch   42/175   train_loss = 1.288\n",
      "Epoch 2410 Batch   74/175   train_loss = 1.297\n",
      "Epoch 2410 Batch  106/175   train_loss = 1.272\n",
      "Epoch 2410 Batch  138/175   train_loss = 1.251\n",
      "Epoch 2410 Batch  170/175   train_loss = 1.278\n",
      "Epoch 2411 Batch   27/175   train_loss = 1.233\n",
      "Epoch 2411 Batch   59/175   train_loss = 1.230\n",
      "Epoch 2411 Batch   91/175   train_loss = 1.233\n",
      "Epoch 2411 Batch  123/175   train_loss = 1.238\n",
      "Epoch 2411 Batch  155/175   train_loss = 1.218\n",
      "Epoch 2412 Batch   12/175   train_loss = 1.257\n",
      "Epoch 2412 Batch   44/175   train_loss = 1.213\n",
      "Epoch 2412 Batch   76/175   train_loss = 1.236\n",
      "Epoch 2412 Batch  108/175   train_loss = 1.249\n",
      "Epoch 2412 Batch  140/175   train_loss = 1.223\n",
      "Epoch 2412 Batch  172/175   train_loss = 1.265\n",
      "Epoch 2413 Batch   29/175   train_loss = 1.243\n",
      "Epoch 2413 Batch   61/175   train_loss = 1.268\n",
      "Epoch 2413 Batch   93/175   train_loss = 1.252\n",
      "Epoch 2413 Batch  125/175   train_loss = 1.272\n",
      "Epoch 2413 Batch  157/175   train_loss = 1.267\n",
      "Epoch 2414 Batch   14/175   train_loss = 1.281\n",
      "Epoch 2414 Batch   46/175   train_loss = 1.247\n",
      "Epoch 2414 Batch   78/175   train_loss = 1.246\n",
      "Epoch 2414 Batch  110/175   train_loss = 1.334\n",
      "Epoch 2414 Batch  142/175   train_loss = 1.285\n",
      "Epoch 2414 Batch  174/175   train_loss = 1.266\n",
      "Epoch 2415 Batch   31/175   train_loss = 1.294\n",
      "Epoch 2415 Batch   63/175   train_loss = 1.279\n",
      "Epoch 2415 Batch   95/175   train_loss = 1.231\n",
      "Epoch 2415 Batch  127/175   train_loss = 1.213\n",
      "Epoch 2415 Batch  159/175   train_loss = 1.204\n",
      "Epoch 2416 Batch   16/175   train_loss = 1.234\n",
      "Epoch 2416 Batch   48/175   train_loss = 1.263\n",
      "Epoch 2416 Batch   80/175   train_loss = 1.290\n",
      "Epoch 2416 Batch  112/175   train_loss = 1.245\n",
      "Epoch 2416 Batch  144/175   train_loss = 1.189\n",
      "Epoch 2417 Batch    1/175   train_loss = 1.295\n",
      "Epoch 2417 Batch   33/175   train_loss = 1.295\n",
      "Epoch 2417 Batch   65/175   train_loss = 1.254\n",
      "Epoch 2417 Batch   97/175   train_loss = 1.225\n",
      "Epoch 2417 Batch  129/175   train_loss = 1.244\n",
      "Epoch 2417 Batch  161/175   train_loss = 1.226\n",
      "Epoch 2418 Batch   18/175   train_loss = 1.202\n",
      "Epoch 2418 Batch   50/175   train_loss = 1.226\n",
      "Epoch 2418 Batch   82/175   train_loss = 1.282\n",
      "Epoch 2418 Batch  114/175   train_loss = 1.294\n",
      "Epoch 2418 Batch  146/175   train_loss = 1.239\n",
      "Epoch 2419 Batch    3/175   train_loss = 1.278\n",
      "Epoch 2419 Batch   35/175   train_loss = 1.258\n",
      "Epoch 2419 Batch   67/175   train_loss = 1.219\n",
      "Epoch 2419 Batch   99/175   train_loss = 1.342\n",
      "Epoch 2419 Batch  131/175   train_loss = 1.245\n",
      "Epoch 2419 Batch  163/175   train_loss = 1.248\n",
      "Epoch 2420 Batch   20/175   train_loss = 1.231\n",
      "Epoch 2420 Batch   52/175   train_loss = 1.188\n",
      "Epoch 2420 Batch   84/175   train_loss = 1.244\n",
      "Epoch 2420 Batch  116/175   train_loss = 1.344\n",
      "Epoch 2420 Batch  148/175   train_loss = 1.264\n",
      "Epoch 2421 Batch    5/175   train_loss = 1.261\n",
      "Epoch 2421 Batch   37/175   train_loss = 1.223\n",
      "Epoch 2421 Batch   69/175   train_loss = 1.275\n",
      "Epoch 2421 Batch  101/175   train_loss = 1.283\n",
      "Epoch 2421 Batch  133/175   train_loss = 1.150\n",
      "Epoch 2421 Batch  165/175   train_loss = 1.249\n",
      "Epoch 2422 Batch   22/175   train_loss = 1.187\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2422 Batch   54/175   train_loss = 1.307\n",
      "Epoch 2422 Batch   86/175   train_loss = 1.314\n",
      "Epoch 2422 Batch  118/175   train_loss = 1.316\n",
      "Epoch 2422 Batch  150/175   train_loss = 1.258\n",
      "Epoch 2423 Batch    7/175   train_loss = 1.277\n",
      "Epoch 2423 Batch   39/175   train_loss = 1.194\n",
      "Epoch 2423 Batch   71/175   train_loss = 1.289\n",
      "Epoch 2423 Batch  103/175   train_loss = 1.245\n",
      "Epoch 2423 Batch  135/175   train_loss = 1.214\n",
      "Epoch 2423 Batch  167/175   train_loss = 1.279\n",
      "Epoch 2424 Batch   24/175   train_loss = 1.184\n",
      "Epoch 2424 Batch   56/175   train_loss = 1.246\n",
      "Epoch 2424 Batch   88/175   train_loss = 1.297\n",
      "Epoch 2424 Batch  120/175   train_loss = 1.288\n",
      "Epoch 2424 Batch  152/175   train_loss = 1.243\n",
      "Epoch 2425 Batch    9/175   train_loss = 1.281\n",
      "Epoch 2425 Batch   41/175   train_loss = 1.289\n",
      "Epoch 2425 Batch   73/175   train_loss = 1.268\n",
      "Epoch 2425 Batch  105/175   train_loss = 1.282\n",
      "Epoch 2425 Batch  137/175   train_loss = 1.231\n",
      "Epoch 2425 Batch  169/175   train_loss = 1.276\n",
      "Epoch 2426 Batch   26/175   train_loss = 1.274\n",
      "Epoch 2426 Batch   58/175   train_loss = 1.299\n",
      "Epoch 2426 Batch   90/175   train_loss = 1.268\n",
      "Epoch 2426 Batch  122/175   train_loss = 1.248\n",
      "Epoch 2426 Batch  154/175   train_loss = 1.265\n",
      "Epoch 2427 Batch   11/175   train_loss = 1.214\n",
      "Epoch 2427 Batch   43/175   train_loss = 1.236\n",
      "Epoch 2427 Batch   75/175   train_loss = 1.241\n",
      "Epoch 2427 Batch  107/175   train_loss = 1.306\n",
      "Epoch 2427 Batch  139/175   train_loss = 1.194\n",
      "Epoch 2427 Batch  171/175   train_loss = 1.300\n",
      "Epoch 2428 Batch   28/175   train_loss = 1.259\n",
      "Epoch 2428 Batch   60/175   train_loss = 1.256\n",
      "Epoch 2428 Batch   92/175   train_loss = 1.222\n",
      "Epoch 2428 Batch  124/175   train_loss = 1.251\n",
      "Epoch 2428 Batch  156/175   train_loss = 1.282\n",
      "Epoch 2429 Batch   13/175   train_loss = 1.311\n",
      "Epoch 2429 Batch   45/175   train_loss = 1.262\n",
      "Epoch 2429 Batch   77/175   train_loss = 1.238\n",
      "Epoch 2429 Batch  109/175   train_loss = 1.286\n",
      "Epoch 2429 Batch  141/175   train_loss = 1.217\n",
      "Epoch 2429 Batch  173/175   train_loss = 1.222\n",
      "Epoch 2430 Batch   30/175   train_loss = 1.315\n",
      "Epoch 2430 Batch   62/175   train_loss = 1.272\n",
      "Epoch 2430 Batch   94/175   train_loss = 1.216\n",
      "Epoch 2430 Batch  126/175   train_loss = 1.271\n",
      "Epoch 2430 Batch  158/175   train_loss = 1.246\n",
      "Epoch 2431 Batch   15/175   train_loss = 1.279\n",
      "Epoch 2431 Batch   47/175   train_loss = 1.269\n",
      "Epoch 2431 Batch   79/175   train_loss = 1.286\n",
      "Epoch 2431 Batch  111/175   train_loss = 1.292\n",
      "Epoch 2431 Batch  143/175   train_loss = 1.224\n",
      "Epoch 2432 Batch    0/175   train_loss = 1.250\n",
      "Epoch 2432 Batch   32/175   train_loss = 1.285\n",
      "Epoch 2432 Batch   64/175   train_loss = 1.259\n",
      "Epoch 2432 Batch   96/175   train_loss = 1.273\n",
      "Epoch 2432 Batch  128/175   train_loss = 1.190\n",
      "Epoch 2432 Batch  160/175   train_loss = 1.237\n",
      "Epoch 2433 Batch   17/175   train_loss = 1.206\n",
      "Epoch 2433 Batch   49/175   train_loss = 1.258\n",
      "Epoch 2433 Batch   81/175   train_loss = 1.226\n",
      "Epoch 2433 Batch  113/175   train_loss = 1.261\n",
      "Epoch 2433 Batch  145/175   train_loss = 1.176\n",
      "Epoch 2434 Batch    2/175   train_loss = 1.233\n",
      "Epoch 2434 Batch   34/175   train_loss = 1.261\n",
      "Epoch 2434 Batch   66/175   train_loss = 1.287\n",
      "Epoch 2434 Batch   98/175   train_loss = 1.275\n",
      "Epoch 2434 Batch  130/175   train_loss = 1.262\n",
      "Epoch 2434 Batch  162/175   train_loss = 1.249\n",
      "Epoch 2435 Batch   19/175   train_loss = 1.234\n",
      "Epoch 2435 Batch   51/175   train_loss = 1.178\n",
      "Epoch 2435 Batch   83/175   train_loss = 1.286\n",
      "Epoch 2435 Batch  115/175   train_loss = 1.363\n",
      "Epoch 2435 Batch  147/175   train_loss = 1.241\n",
      "Epoch 2436 Batch    4/175   train_loss = 1.282\n",
      "Epoch 2436 Batch   36/175   train_loss = 1.209\n",
      "Epoch 2436 Batch   68/175   train_loss = 1.250\n",
      "Epoch 2436 Batch  100/175   train_loss = 1.267\n",
      "Epoch 2436 Batch  132/175   train_loss = 1.235\n",
      "Epoch 2436 Batch  164/175   train_loss = 1.224\n",
      "Epoch 2437 Batch   21/175   train_loss = 1.211\n",
      "Epoch 2437 Batch   53/175   train_loss = 1.254\n",
      "Epoch 2437 Batch   85/175   train_loss = 1.278\n",
      "Epoch 2437 Batch  117/175   train_loss = 1.267\n",
      "Epoch 2437 Batch  149/175   train_loss = 1.287\n",
      "Epoch 2438 Batch    6/175   train_loss = 1.280\n",
      "Epoch 2438 Batch   38/175   train_loss = 1.205\n",
      "Epoch 2438 Batch   70/175   train_loss = 1.221\n",
      "Epoch 2438 Batch  102/175   train_loss = 1.237\n",
      "Epoch 2438 Batch  134/175   train_loss = 1.222\n",
      "Epoch 2438 Batch  166/175   train_loss = 1.263\n",
      "Epoch 2439 Batch   23/175   train_loss = 1.179\n",
      "Epoch 2439 Batch   55/175   train_loss = 1.307\n",
      "Epoch 2439 Batch   87/175   train_loss = 1.325\n",
      "Epoch 2439 Batch  119/175   train_loss = 1.236\n",
      "Epoch 2439 Batch  151/175   train_loss = 1.263\n",
      "Epoch 2440 Batch    8/175   train_loss = 1.269\n",
      "Epoch 2440 Batch   40/175   train_loss = 1.235\n",
      "Epoch 2440 Batch   72/175   train_loss = 1.296\n",
      "Epoch 2440 Batch  104/175   train_loss = 1.279\n",
      "Epoch 2440 Batch  136/175   train_loss = 1.244\n",
      "Epoch 2440 Batch  168/175   train_loss = 1.289\n",
      "Epoch 2441 Batch   25/175   train_loss = 1.262\n",
      "Epoch 2441 Batch   57/175   train_loss = 1.279\n",
      "Epoch 2441 Batch   89/175   train_loss = 1.257\n",
      "Epoch 2441 Batch  121/175   train_loss = 1.238\n",
      "Epoch 2441 Batch  153/175   train_loss = 1.254\n",
      "Epoch 2442 Batch   10/175   train_loss = 1.192\n",
      "Epoch 2442 Batch   42/175   train_loss = 1.281\n",
      "Epoch 2442 Batch   74/175   train_loss = 1.300\n",
      "Epoch 2442 Batch  106/175   train_loss = 1.287\n",
      "Epoch 2442 Batch  138/175   train_loss = 1.222\n",
      "Epoch 2442 Batch  170/175   train_loss = 1.272\n",
      "Epoch 2443 Batch   27/175   train_loss = 1.225\n",
      "Epoch 2443 Batch   59/175   train_loss = 1.221\n",
      "Epoch 2443 Batch   91/175   train_loss = 1.228\n",
      "Epoch 2443 Batch  123/175   train_loss = 1.231\n",
      "Epoch 2443 Batch  155/175   train_loss = 1.207\n",
      "Epoch 2444 Batch   12/175   train_loss = 1.273\n",
      "Epoch 2444 Batch   44/175   train_loss = 1.218\n",
      "Epoch 2444 Batch   76/175   train_loss = 1.225\n",
      "Epoch 2444 Batch  108/175   train_loss = 1.269\n",
      "Epoch 2444 Batch  140/175   train_loss = 1.221\n",
      "Epoch 2444 Batch  172/175   train_loss = 1.281\n",
      "Epoch 2445 Batch   29/175   train_loss = 1.239\n",
      "Epoch 2445 Batch   61/175   train_loss = 1.257\n",
      "Epoch 2445 Batch   93/175   train_loss = 1.251\n",
      "Epoch 2445 Batch  125/175   train_loss = 1.240\n",
      "Epoch 2445 Batch  157/175   train_loss = 1.231\n",
      "Epoch 2446 Batch   14/175   train_loss = 1.245\n",
      "Epoch 2446 Batch   46/175   train_loss = 1.226\n",
      "Epoch 2446 Batch   78/175   train_loss = 1.216\n",
      "Epoch 2446 Batch  110/175   train_loss = 1.320\n",
      "Epoch 2446 Batch  142/175   train_loss = 1.253\n",
      "Epoch 2446 Batch  174/175   train_loss = 1.226\n",
      "Epoch 2447 Batch   31/175   train_loss = 1.262\n",
      "Epoch 2447 Batch   63/175   train_loss = 1.256\n",
      "Epoch 2447 Batch   95/175   train_loss = 1.215\n",
      "Epoch 2447 Batch  127/175   train_loss = 1.197\n",
      "Epoch 2447 Batch  159/175   train_loss = 1.198\n",
      "Epoch 2448 Batch   16/175   train_loss = 1.242\n",
      "Epoch 2448 Batch   48/175   train_loss = 1.242\n",
      "Epoch 2448 Batch   80/175   train_loss = 1.272\n",
      "Epoch 2448 Batch  112/175   train_loss = 1.300\n",
      "Epoch 2448 Batch  144/175   train_loss = 1.196\n",
      "Epoch 2449 Batch    1/175   train_loss = 1.289\n",
      "Epoch 2449 Batch   33/175   train_loss = 1.281\n",
      "Epoch 2449 Batch   65/175   train_loss = 1.240\n",
      "Epoch 2449 Batch   97/175   train_loss = 1.241\n",
      "Epoch 2449 Batch  129/175   train_loss = 1.227\n",
      "Epoch 2449 Batch  161/175   train_loss = 1.207\n",
      "Epoch 2450 Batch   18/175   train_loss = 1.193\n",
      "Epoch 2450 Batch   50/175   train_loss = 1.217\n",
      "Epoch 2450 Batch   82/175   train_loss = 1.278\n",
      "Epoch 2450 Batch  114/175   train_loss = 1.278\n",
      "Epoch 2450 Batch  146/175   train_loss = 1.232\n",
      "Epoch 2451 Batch    3/175   train_loss = 1.287\n",
      "Epoch 2451 Batch   35/175   train_loss = 1.255\n",
      "Epoch 2451 Batch   67/175   train_loss = 1.204\n",
      "Epoch 2451 Batch   99/175   train_loss = 1.312\n",
      "Epoch 2451 Batch  131/175   train_loss = 1.225\n",
      "Epoch 2451 Batch  163/175   train_loss = 1.256\n",
      "Epoch 2452 Batch   20/175   train_loss = 1.218\n",
      "Epoch 2452 Batch   52/175   train_loss = 1.192\n",
      "Epoch 2452 Batch   84/175   train_loss = 1.250\n",
      "Epoch 2452 Batch  116/175   train_loss = 1.282\n",
      "Epoch 2452 Batch  148/175   train_loss = 1.238\n",
      "Epoch 2453 Batch    5/175   train_loss = 1.260\n",
      "Epoch 2453 Batch   37/175   train_loss = 1.211\n",
      "Epoch 2453 Batch   69/175   train_loss = 1.278\n",
      "Epoch 2453 Batch  101/175   train_loss = 1.323\n",
      "Epoch 2453 Batch  133/175   train_loss = 1.166\n",
      "Epoch 2453 Batch  165/175   train_loss = 1.248\n",
      "Epoch 2454 Batch   22/175   train_loss = 1.191\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2454 Batch   54/175   train_loss = 1.271\n",
      "Epoch 2454 Batch   86/175   train_loss = 1.296\n",
      "Epoch 2454 Batch  118/175   train_loss = 1.308\n",
      "Epoch 2454 Batch  150/175   train_loss = 1.250\n",
      "Epoch 2455 Batch    7/175   train_loss = 1.285\n",
      "Epoch 2455 Batch   39/175   train_loss = 1.194\n",
      "Epoch 2455 Batch   71/175   train_loss = 1.283\n",
      "Epoch 2455 Batch  103/175   train_loss = 1.247\n",
      "Epoch 2455 Batch  135/175   train_loss = 1.247\n",
      "Epoch 2455 Batch  167/175   train_loss = 1.298\n",
      "Epoch 2456 Batch   24/175   train_loss = 1.192\n",
      "Epoch 2456 Batch   56/175   train_loss = 1.257\n",
      "Epoch 2456 Batch   88/175   train_loss = 1.308\n",
      "Epoch 2456 Batch  120/175   train_loss = 1.259\n",
      "Epoch 2456 Batch  152/175   train_loss = 1.253\n",
      "Epoch 2457 Batch    9/175   train_loss = 1.289\n",
      "Epoch 2457 Batch   41/175   train_loss = 1.303\n",
      "Epoch 2457 Batch   73/175   train_loss = 1.279\n",
      "Epoch 2457 Batch  105/175   train_loss = 1.311\n",
      "Epoch 2457 Batch  137/175   train_loss = 1.246\n",
      "Epoch 2457 Batch  169/175   train_loss = 1.282\n",
      "Epoch 2458 Batch   26/175   train_loss = 1.284\n",
      "Epoch 2458 Batch   58/175   train_loss = 1.273\n",
      "Epoch 2458 Batch   90/175   train_loss = 1.280\n",
      "Epoch 2458 Batch  122/175   train_loss = 1.241\n",
      "Epoch 2458 Batch  154/175   train_loss = 1.242\n",
      "Epoch 2459 Batch   11/175   train_loss = 1.237\n",
      "Epoch 2459 Batch   43/175   train_loss = 1.249\n",
      "Epoch 2459 Batch   75/175   train_loss = 1.226\n",
      "Epoch 2459 Batch  107/175   train_loss = 1.347\n",
      "Epoch 2459 Batch  139/175   train_loss = 1.211\n",
      "Epoch 2459 Batch  171/175   train_loss = 1.313\n",
      "Epoch 2460 Batch   28/175   train_loss = 1.265\n",
      "Epoch 2460 Batch   60/175   train_loss = 1.278\n",
      "Epoch 2460 Batch   92/175   train_loss = 1.232\n",
      "Epoch 2460 Batch  124/175   train_loss = 1.287\n",
      "Epoch 2460 Batch  156/175   train_loss = 1.319\n",
      "Epoch 2461 Batch   13/175   train_loss = 1.252\n",
      "Epoch 2461 Batch   45/175   train_loss = 1.256\n",
      "Epoch 2461 Batch   77/175   train_loss = 1.245\n",
      "Epoch 2461 Batch  109/175   train_loss = 1.278\n",
      "Epoch 2461 Batch  141/175   train_loss = 1.221\n",
      "Epoch 2461 Batch  173/175   train_loss = 1.245\n",
      "Epoch 2462 Batch   30/175   train_loss = 1.326\n",
      "Epoch 2462 Batch   62/175   train_loss = 1.267\n",
      "Epoch 2462 Batch   94/175   train_loss = 1.255\n",
      "Epoch 2462 Batch  126/175   train_loss = 1.277\n",
      "Epoch 2462 Batch  158/175   train_loss = 1.235\n",
      "Epoch 2463 Batch   15/175   train_loss = 1.288\n",
      "Epoch 2463 Batch   47/175   train_loss = 1.275\n",
      "Epoch 2463 Batch   79/175   train_loss = 1.311\n",
      "Epoch 2463 Batch  111/175   train_loss = 1.311\n",
      "Epoch 2463 Batch  143/175   train_loss = 1.236\n",
      "Epoch 2464 Batch    0/175   train_loss = 1.230\n",
      "Epoch 2464 Batch   32/175   train_loss = 1.273\n",
      "Epoch 2464 Batch   64/175   train_loss = 1.284\n",
      "Epoch 2464 Batch   96/175   train_loss = 1.310\n",
      "Epoch 2464 Batch  128/175   train_loss = 1.208\n",
      "Epoch 2464 Batch  160/175   train_loss = 1.240\n",
      "Epoch 2465 Batch   17/175   train_loss = 1.224\n",
      "Epoch 2465 Batch   49/175   train_loss = 1.275\n",
      "Epoch 2465 Batch   81/175   train_loss = 1.255\n",
      "Epoch 2465 Batch  113/175   train_loss = 1.276\n",
      "Epoch 2465 Batch  145/175   train_loss = 1.199\n",
      "Epoch 2466 Batch    2/175   train_loss = 1.238\n",
      "Epoch 2466 Batch   34/175   train_loss = 1.279\n",
      "Epoch 2466 Batch   66/175   train_loss = 1.307\n",
      "Epoch 2466 Batch   98/175   train_loss = 1.308\n",
      "Epoch 2466 Batch  130/175   train_loss = 1.272\n",
      "Epoch 2466 Batch  162/175   train_loss = 1.275\n",
      "Epoch 2467 Batch   19/175   train_loss = 1.242\n",
      "Epoch 2467 Batch   51/175   train_loss = 1.208\n",
      "Epoch 2467 Batch   83/175   train_loss = 1.372\n",
      "Epoch 2467 Batch  115/175   train_loss = 1.391\n",
      "Epoch 2467 Batch  147/175   train_loss = 1.208\n",
      "Epoch 2468 Batch    4/175   train_loss = 1.271\n",
      "Epoch 2468 Batch   36/175   train_loss = 1.215\n",
      "Epoch 2468 Batch   68/175   train_loss = 1.230\n",
      "Epoch 2468 Batch  100/175   train_loss = 1.271\n",
      "Epoch 2468 Batch  132/175   train_loss = 1.226\n",
      "Epoch 2468 Batch  164/175   train_loss = 1.219\n",
      "Epoch 2469 Batch   21/175   train_loss = 1.211\n",
      "Epoch 2469 Batch   53/175   train_loss = 1.231\n",
      "Epoch 2469 Batch   85/175   train_loss = 1.270\n",
      "Epoch 2469 Batch  117/175   train_loss = 1.278\n",
      "Epoch 2469 Batch  149/175   train_loss = 1.253\n",
      "Epoch 2470 Batch    6/175   train_loss = 1.277\n",
      "Epoch 2470 Batch   38/175   train_loss = 1.188\n",
      "Epoch 2470 Batch   70/175   train_loss = 1.242\n",
      "Epoch 2470 Batch  102/175   train_loss = 1.252\n",
      "Epoch 2470 Batch  134/175   train_loss = 1.209\n",
      "Epoch 2470 Batch  166/175   train_loss = 1.248\n",
      "Epoch 2471 Batch   23/175   train_loss = 1.183\n",
      "Epoch 2471 Batch   55/175   train_loss = 1.300\n",
      "Epoch 2471 Batch   87/175   train_loss = 1.353\n",
      "Epoch 2471 Batch  119/175   train_loss = 1.241\n",
      "Epoch 2471 Batch  151/175   train_loss = 1.273\n",
      "Epoch 2472 Batch    8/175   train_loss = 1.274\n",
      "Epoch 2472 Batch   40/175   train_loss = 1.240\n",
      "Epoch 2472 Batch   72/175   train_loss = 1.293\n",
      "Epoch 2472 Batch  104/175   train_loss = 1.265\n",
      "Epoch 2472 Batch  136/175   train_loss = 1.254\n",
      "Epoch 2472 Batch  168/175   train_loss = 1.279\n",
      "Epoch 2473 Batch   25/175   train_loss = 1.275\n",
      "Epoch 2473 Batch   57/175   train_loss = 1.290\n",
      "Epoch 2473 Batch   89/175   train_loss = 1.237\n",
      "Epoch 2473 Batch  121/175   train_loss = 1.209\n",
      "Epoch 2473 Batch  153/175   train_loss = 1.253\n",
      "Epoch 2474 Batch   10/175   train_loss = 1.213\n",
      "Epoch 2474 Batch   42/175   train_loss = 1.290\n",
      "Epoch 2474 Batch   74/175   train_loss = 1.324\n",
      "Epoch 2474 Batch  106/175   train_loss = 1.305\n",
      "Epoch 2474 Batch  138/175   train_loss = 1.247\n",
      "Epoch 2474 Batch  170/175   train_loss = 1.287\n",
      "Epoch 2475 Batch   27/175   train_loss = 1.249\n",
      "Epoch 2475 Batch   59/175   train_loss = 1.233\n",
      "Epoch 2475 Batch   91/175   train_loss = 1.254\n",
      "Epoch 2475 Batch  123/175   train_loss = 1.243\n",
      "Epoch 2475 Batch  155/175   train_loss = 1.241\n",
      "Epoch 2476 Batch   12/175   train_loss = 1.240\n",
      "Epoch 2476 Batch   44/175   train_loss = 1.204\n",
      "Epoch 2476 Batch   76/175   train_loss = 1.231\n",
      "Epoch 2476 Batch  108/175   train_loss = 1.267\n",
      "Epoch 2476 Batch  140/175   train_loss = 1.230\n",
      "Epoch 2476 Batch  172/175   train_loss = 1.294\n",
      "Epoch 2477 Batch   29/175   train_loss = 1.245\n",
      "Epoch 2477 Batch   61/175   train_loss = 1.265\n",
      "Epoch 2477 Batch   93/175   train_loss = 1.252\n",
      "Epoch 2477 Batch  125/175   train_loss = 1.259\n",
      "Epoch 2477 Batch  157/175   train_loss = 1.229\n",
      "Epoch 2478 Batch   14/175   train_loss = 1.277\n",
      "Epoch 2478 Batch   46/175   train_loss = 1.247\n",
      "Epoch 2478 Batch   78/175   train_loss = 1.236\n",
      "Epoch 2478 Batch  110/175   train_loss = 1.348\n",
      "Epoch 2478 Batch  142/175   train_loss = 1.283\n",
      "Epoch 2478 Batch  174/175   train_loss = 1.242\n",
      "Epoch 2479 Batch   31/175   train_loss = 1.252\n",
      "Epoch 2479 Batch   63/175   train_loss = 1.268\n",
      "Epoch 2479 Batch   95/175   train_loss = 1.228\n",
      "Epoch 2479 Batch  127/175   train_loss = 1.219\n",
      "Epoch 2479 Batch  159/175   train_loss = 1.229\n",
      "Epoch 2480 Batch   16/175   train_loss = 1.247\n",
      "Epoch 2480 Batch   48/175   train_loss = 1.252\n",
      "Epoch 2480 Batch   80/175   train_loss = 1.284\n",
      "Epoch 2480 Batch  112/175   train_loss = 1.251\n",
      "Epoch 2480 Batch  144/175   train_loss = 1.190\n",
      "Epoch 2481 Batch    1/175   train_loss = 1.272\n",
      "Epoch 2481 Batch   33/175   train_loss = 1.297\n",
      "Epoch 2481 Batch   65/175   train_loss = 1.270\n",
      "Epoch 2481 Batch   97/175   train_loss = 1.244\n",
      "Epoch 2481 Batch  129/175   train_loss = 1.225\n",
      "Epoch 2481 Batch  161/175   train_loss = 1.228\n",
      "Epoch 2482 Batch   18/175   train_loss = 1.259\n",
      "Epoch 2482 Batch   50/175   train_loss = 1.248\n",
      "Epoch 2482 Batch   82/175   train_loss = 1.306\n",
      "Epoch 2482 Batch  114/175   train_loss = 1.301\n",
      "Epoch 2482 Batch  146/175   train_loss = 1.245\n",
      "Epoch 2483 Batch    3/175   train_loss = 1.309\n",
      "Epoch 2483 Batch   35/175   train_loss = 1.240\n",
      "Epoch 2483 Batch   67/175   train_loss = 1.219\n",
      "Epoch 2483 Batch   99/175   train_loss = 1.349\n",
      "Epoch 2483 Batch  131/175   train_loss = 1.256\n",
      "Epoch 2483 Batch  163/175   train_loss = 1.268\n",
      "Epoch 2484 Batch   20/175   train_loss = 1.257\n",
      "Epoch 2484 Batch   52/175   train_loss = 1.199\n",
      "Epoch 2484 Batch   84/175   train_loss = 1.254\n",
      "Epoch 2484 Batch  116/175   train_loss = 1.299\n",
      "Epoch 2484 Batch  148/175   train_loss = 1.231\n",
      "Epoch 2485 Batch    5/175   train_loss = 1.261\n",
      "Epoch 2485 Batch   37/175   train_loss = 1.232\n",
      "Epoch 2485 Batch   69/175   train_loss = 1.283\n",
      "Epoch 2485 Batch  101/175   train_loss = 1.309\n",
      "Epoch 2485 Batch  133/175   train_loss = 1.170\n",
      "Epoch 2485 Batch  165/175   train_loss = 1.305\n",
      "Epoch 2486 Batch   22/175   train_loss = 1.221\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2486 Batch   54/175   train_loss = 1.299\n",
      "Epoch 2486 Batch   86/175   train_loss = 1.316\n",
      "Epoch 2486 Batch  118/175   train_loss = 1.326\n",
      "Epoch 2486 Batch  150/175   train_loss = 1.239\n",
      "Epoch 2487 Batch    7/175   train_loss = 1.271\n",
      "Epoch 2487 Batch   39/175   train_loss = 1.203\n",
      "Epoch 2487 Batch   71/175   train_loss = 1.295\n",
      "Epoch 2487 Batch  103/175   train_loss = 1.259\n",
      "Epoch 2487 Batch  135/175   train_loss = 1.227\n",
      "Epoch 2487 Batch  167/175   train_loss = 1.301\n",
      "Epoch 2488 Batch   24/175   train_loss = 1.198\n",
      "Epoch 2488 Batch   56/175   train_loss = 1.259\n",
      "Epoch 2488 Batch   88/175   train_loss = 1.319\n",
      "Epoch 2488 Batch  120/175   train_loss = 1.244\n",
      "Epoch 2488 Batch  152/175   train_loss = 1.257\n",
      "Epoch 2489 Batch    9/175   train_loss = 1.267\n",
      "Epoch 2489 Batch   41/175   train_loss = 1.274\n",
      "Epoch 2489 Batch   73/175   train_loss = 1.243\n",
      "Epoch 2489 Batch  105/175   train_loss = 1.294\n",
      "Epoch 2489 Batch  137/175   train_loss = 1.211\n",
      "Epoch 2489 Batch  169/175   train_loss = 1.256\n",
      "Epoch 2490 Batch   26/175   train_loss = 1.260\n",
      "Epoch 2490 Batch   58/175   train_loss = 1.275\n",
      "Epoch 2490 Batch   90/175   train_loss = 1.304\n",
      "Epoch 2490 Batch  122/175   train_loss = 1.229\n",
      "Epoch 2490 Batch  154/175   train_loss = 1.233\n",
      "Epoch 2491 Batch   11/175   train_loss = 1.242\n",
      "Epoch 2491 Batch   43/175   train_loss = 1.244\n",
      "Epoch 2491 Batch   75/175   train_loss = 1.225\n",
      "Epoch 2491 Batch  107/175   train_loss = 1.296\n",
      "Epoch 2491 Batch  139/175   train_loss = 1.198\n",
      "Epoch 2491 Batch  171/175   train_loss = 1.286\n",
      "Epoch 2492 Batch   28/175   train_loss = 1.247\n",
      "Epoch 2492 Batch   60/175   train_loss = 1.249\n",
      "Epoch 2492 Batch   92/175   train_loss = 1.193\n",
      "Epoch 2492 Batch  124/175   train_loss = 1.257\n",
      "Epoch 2492 Batch  156/175   train_loss = 1.291\n",
      "Epoch 2493 Batch   13/175   train_loss = 1.243\n",
      "Epoch 2493 Batch   45/175   train_loss = 1.255\n",
      "Epoch 2493 Batch   77/175   train_loss = 1.234\n",
      "Epoch 2493 Batch  109/175   train_loss = 1.287\n",
      "Epoch 2493 Batch  141/175   train_loss = 1.206\n",
      "Epoch 2493 Batch  173/175   train_loss = 1.218\n",
      "Epoch 2494 Batch   30/175   train_loss = 1.312\n",
      "Epoch 2494 Batch   62/175   train_loss = 1.284\n",
      "Epoch 2494 Batch   94/175   train_loss = 1.245\n",
      "Epoch 2494 Batch  126/175   train_loss = 1.267\n",
      "Epoch 2494 Batch  158/175   train_loss = 1.247\n",
      "Epoch 2495 Batch   15/175   train_loss = 1.292\n",
      "Epoch 2495 Batch   47/175   train_loss = 1.279\n",
      "Epoch 2495 Batch   79/175   train_loss = 1.299\n",
      "Epoch 2495 Batch  111/175   train_loss = 1.315\n",
      "Epoch 2495 Batch  143/175   train_loss = 1.234\n",
      "Epoch 2496 Batch    0/175   train_loss = 1.243\n",
      "Epoch 2496 Batch   32/175   train_loss = 1.260\n",
      "Epoch 2496 Batch   64/175   train_loss = 1.272\n",
      "Epoch 2496 Batch   96/175   train_loss = 1.259\n",
      "Epoch 2496 Batch  128/175   train_loss = 1.194\n",
      "Epoch 2496 Batch  160/175   train_loss = 1.238\n",
      "Epoch 2497 Batch   17/175   train_loss = 1.210\n",
      "Epoch 2497 Batch   49/175   train_loss = 1.256\n",
      "Epoch 2497 Batch   81/175   train_loss = 1.236\n",
      "Epoch 2497 Batch  113/175   train_loss = 1.259\n",
      "Epoch 2497 Batch  145/175   train_loss = 1.195\n",
      "Epoch 2498 Batch    2/175   train_loss = 1.232\n",
      "Epoch 2498 Batch   34/175   train_loss = 1.272\n",
      "Epoch 2498 Batch   66/175   train_loss = 1.259\n",
      "Epoch 2498 Batch   98/175   train_loss = 1.263\n",
      "Epoch 2498 Batch  130/175   train_loss = 1.252\n",
      "Epoch 2498 Batch  162/175   train_loss = 1.333\n",
      "Epoch 2499 Batch   19/175   train_loss = 1.266\n",
      "Epoch 2499 Batch   51/175   train_loss = 1.195\n",
      "Epoch 2499 Batch   83/175   train_loss = 1.301\n",
      "Epoch 2499 Batch  115/175   train_loss = 1.385\n",
      "Epoch 2499 Batch  147/175   train_loss = 1.222\n",
      "Epoch 2500 Batch    4/175   train_loss = 1.289\n",
      "Epoch 2500 Batch   36/175   train_loss = 1.230\n",
      "Epoch 2500 Batch   68/175   train_loss = 1.226\n",
      "Epoch 2500 Batch  100/175   train_loss = 1.284\n",
      "Epoch 2500 Batch  132/175   train_loss = 1.217\n",
      "Epoch 2500 Batch  164/175   train_loss = 1.224\n",
      "Epoch 2501 Batch   21/175   train_loss = 1.211\n",
      "Epoch 2501 Batch   53/175   train_loss = 1.230\n",
      "Epoch 2501 Batch   85/175   train_loss = 1.263\n",
      "Epoch 2501 Batch  117/175   train_loss = 1.264\n",
      "Epoch 2501 Batch  149/175   train_loss = 1.253\n",
      "Epoch 2502 Batch    6/175   train_loss = 1.272\n",
      "Epoch 2502 Batch   38/175   train_loss = 1.223\n",
      "Epoch 2502 Batch   70/175   train_loss = 1.241\n",
      "Epoch 2502 Batch  102/175   train_loss = 1.262\n",
      "Epoch 2502 Batch  134/175   train_loss = 1.270\n",
      "Epoch 2502 Batch  166/175   train_loss = 1.274\n",
      "Epoch 2503 Batch   23/175   train_loss = 1.207\n",
      "Epoch 2503 Batch   55/175   train_loss = 1.329\n",
      "Epoch 2503 Batch   87/175   train_loss = 1.336\n",
      "Epoch 2503 Batch  119/175   train_loss = 1.239\n",
      "Epoch 2503 Batch  151/175   train_loss = 1.277\n",
      "Epoch 2504 Batch    8/175   train_loss = 1.274\n",
      "Epoch 2504 Batch   40/175   train_loss = 1.243\n",
      "Epoch 2504 Batch   72/175   train_loss = 1.331\n",
      "Epoch 2504 Batch  104/175   train_loss = 1.287\n",
      "Epoch 2504 Batch  136/175   train_loss = 1.238\n",
      "Epoch 2504 Batch  168/175   train_loss = 1.273\n",
      "Epoch 2505 Batch   25/175   train_loss = 1.228\n",
      "Epoch 2505 Batch   57/175   train_loss = 1.282\n",
      "Epoch 2505 Batch   89/175   train_loss = 1.237\n",
      "Epoch 2505 Batch  121/175   train_loss = 1.223\n",
      "Epoch 2505 Batch  153/175   train_loss = 1.235\n",
      "Epoch 2506 Batch   10/175   train_loss = 1.190\n",
      "Epoch 2506 Batch   42/175   train_loss = 1.296\n",
      "Epoch 2506 Batch   74/175   train_loss = 1.288\n",
      "Epoch 2506 Batch  106/175   train_loss = 1.295\n",
      "Epoch 2506 Batch  138/175   train_loss = 1.225\n",
      "Epoch 2506 Batch  170/175   train_loss = 1.275\n",
      "Epoch 2507 Batch   27/175   train_loss = 1.210\n",
      "Epoch 2507 Batch   59/175   train_loss = 1.212\n",
      "Epoch 2507 Batch   91/175   train_loss = 1.249\n",
      "Epoch 2507 Batch  123/175   train_loss = 1.219\n",
      "Epoch 2507 Batch  155/175   train_loss = 1.234\n",
      "Epoch 2508 Batch   12/175   train_loss = 1.237\n",
      "Epoch 2508 Batch   44/175   train_loss = 1.206\n",
      "Epoch 2508 Batch   76/175   train_loss = 1.228\n",
      "Epoch 2508 Batch  108/175   train_loss = 1.257\n",
      "Epoch 2508 Batch  140/175   train_loss = 1.212\n",
      "Epoch 2508 Batch  172/175   train_loss = 1.280\n",
      "Epoch 2509 Batch   29/175   train_loss = 1.232\n",
      "Epoch 2509 Batch   61/175   train_loss = 1.258\n",
      "Epoch 2509 Batch   93/175   train_loss = 1.243\n",
      "Epoch 2509 Batch  125/175   train_loss = 1.240\n",
      "Epoch 2509 Batch  157/175   train_loss = 1.250\n",
      "Epoch 2510 Batch   14/175   train_loss = 1.250\n",
      "Epoch 2510 Batch   46/175   train_loss = 1.232\n",
      "Epoch 2510 Batch   78/175   train_loss = 1.225\n",
      "Epoch 2510 Batch  110/175   train_loss = 1.332\n",
      "Epoch 2510 Batch  142/175   train_loss = 1.279\n",
      "Epoch 2510 Batch  174/175   train_loss = 1.235\n",
      "Epoch 2511 Batch   31/175   train_loss = 1.259\n",
      "Epoch 2511 Batch   63/175   train_loss = 1.242\n",
      "Epoch 2511 Batch   95/175   train_loss = 1.221\n",
      "Epoch 2511 Batch  127/175   train_loss = 1.203\n",
      "Epoch 2511 Batch  159/175   train_loss = 1.222\n",
      "Epoch 2512 Batch   16/175   train_loss = 1.324\n",
      "Epoch 2512 Batch   48/175   train_loss = 1.274\n",
      "Epoch 2512 Batch   80/175   train_loss = 1.314\n",
      "Epoch 2512 Batch  112/175   train_loss = 1.242\n",
      "Epoch 2512 Batch  144/175   train_loss = 1.185\n",
      "Epoch 2513 Batch    1/175   train_loss = 1.275\n",
      "Epoch 2513 Batch   33/175   train_loss = 1.277\n",
      "Epoch 2513 Batch   65/175   train_loss = 1.257\n",
      "Epoch 2513 Batch   97/175   train_loss = 1.264\n",
      "Epoch 2513 Batch  129/175   train_loss = 1.241\n",
      "Epoch 2513 Batch  161/175   train_loss = 1.234\n",
      "Epoch 2514 Batch   18/175   train_loss = 1.228\n",
      "Epoch 2514 Batch   50/175   train_loss = 1.234\n",
      "Epoch 2514 Batch   82/175   train_loss = 1.281\n",
      "Epoch 2514 Batch  114/175   train_loss = 1.289\n",
      "Epoch 2514 Batch  146/175   train_loss = 1.245\n",
      "Epoch 2515 Batch    3/175   train_loss = 1.284\n",
      "Epoch 2515 Batch   35/175   train_loss = 1.240\n",
      "Epoch 2515 Batch   67/175   train_loss = 1.210\n",
      "Epoch 2515 Batch   99/175   train_loss = 1.295\n",
      "Epoch 2515 Batch  131/175   train_loss = 1.231\n",
      "Epoch 2515 Batch  163/175   train_loss = 1.237\n",
      "Epoch 2516 Batch   20/175   train_loss = 1.236\n",
      "Epoch 2516 Batch   52/175   train_loss = 1.212\n",
      "Epoch 2516 Batch   84/175   train_loss = 1.247\n",
      "Epoch 2516 Batch  116/175   train_loss = 1.288\n",
      "Epoch 2516 Batch  148/175   train_loss = 1.239\n",
      "Epoch 2517 Batch    5/175   train_loss = 1.249\n",
      "Epoch 2517 Batch   37/175   train_loss = 1.252\n",
      "Epoch 2517 Batch   69/175   train_loss = 1.318\n",
      "Epoch 2517 Batch  101/175   train_loss = 1.301\n",
      "Epoch 2517 Batch  133/175   train_loss = 1.153\n",
      "Epoch 2517 Batch  165/175   train_loss = 1.256\n",
      "Epoch 2518 Batch   22/175   train_loss = 1.188\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2518 Batch   54/175   train_loss = 1.262\n",
      "Epoch 2518 Batch   86/175   train_loss = 1.317\n",
      "Epoch 2518 Batch  118/175   train_loss = 1.302\n",
      "Epoch 2518 Batch  150/175   train_loss = 1.256\n",
      "Epoch 2519 Batch    7/175   train_loss = 1.270\n",
      "Epoch 2519 Batch   39/175   train_loss = 1.191\n",
      "Epoch 2519 Batch   71/175   train_loss = 1.272\n",
      "Epoch 2519 Batch  103/175   train_loss = 1.265\n",
      "Epoch 2519 Batch  135/175   train_loss = 1.252\n",
      "Epoch 2519 Batch  167/175   train_loss = 1.300\n",
      "Epoch 2520 Batch   24/175   train_loss = 1.214\n",
      "Epoch 2520 Batch   56/175   train_loss = 1.252\n",
      "Epoch 2520 Batch   88/175   train_loss = 1.304\n",
      "Epoch 2520 Batch  120/175   train_loss = 1.259\n",
      "Epoch 2520 Batch  152/175   train_loss = 1.237\n",
      "Epoch 2521 Batch    9/175   train_loss = 1.281\n",
      "Epoch 2521 Batch   41/175   train_loss = 1.291\n",
      "Epoch 2521 Batch   73/175   train_loss = 1.249\n",
      "Epoch 2521 Batch  105/175   train_loss = 1.287\n",
      "Epoch 2521 Batch  137/175   train_loss = 1.207\n",
      "Epoch 2521 Batch  169/175   train_loss = 1.272\n",
      "Epoch 2522 Batch   26/175   train_loss = 1.257\n",
      "Epoch 2522 Batch   58/175   train_loss = 1.294\n",
      "Epoch 2522 Batch   90/175   train_loss = 1.268\n",
      "Epoch 2522 Batch  122/175   train_loss = 1.223\n",
      "Epoch 2522 Batch  154/175   train_loss = 1.268\n",
      "Epoch 2523 Batch   11/175   train_loss = 1.234\n",
      "Epoch 2523 Batch   43/175   train_loss = 1.254\n",
      "Epoch 2523 Batch   75/175   train_loss = 1.232\n",
      "Epoch 2523 Batch  107/175   train_loss = 1.309\n",
      "Epoch 2523 Batch  139/175   train_loss = 1.197\n",
      "Epoch 2523 Batch  171/175   train_loss = 1.307\n",
      "Epoch 2524 Batch   28/175   train_loss = 1.261\n",
      "Epoch 2524 Batch   60/175   train_loss = 1.252\n",
      "Epoch 2524 Batch   92/175   train_loss = 1.212\n",
      "Epoch 2524 Batch  124/175   train_loss = 1.250\n",
      "Epoch 2524 Batch  156/175   train_loss = 1.293\n",
      "Epoch 2525 Batch   13/175   train_loss = 1.250\n",
      "Epoch 2525 Batch   45/175   train_loss = 1.222\n",
      "Epoch 2525 Batch   77/175   train_loss = 1.237\n",
      "Epoch 2525 Batch  109/175   train_loss = 1.287\n",
      "Epoch 2525 Batch  141/175   train_loss = 1.197\n",
      "Epoch 2525 Batch  173/175   train_loss = 1.213\n",
      "Epoch 2526 Batch   30/175   train_loss = 1.307\n",
      "Epoch 2526 Batch   62/175   train_loss = 1.261\n",
      "Epoch 2526 Batch   94/175   train_loss = 1.245\n",
      "Epoch 2526 Batch  126/175   train_loss = 1.262\n",
      "Epoch 2526 Batch  158/175   train_loss = 1.236\n",
      "Epoch 2527 Batch   15/175   train_loss = 1.270\n",
      "Epoch 2527 Batch   47/175   train_loss = 1.252\n",
      "Epoch 2527 Batch   79/175   train_loss = 1.285\n",
      "Epoch 2527 Batch  111/175   train_loss = 1.300\n",
      "Epoch 2527 Batch  143/175   train_loss = 1.238\n",
      "Epoch 2528 Batch    0/175   train_loss = 1.246\n",
      "Epoch 2528 Batch   32/175   train_loss = 1.251\n",
      "Epoch 2528 Batch   64/175   train_loss = 1.296\n",
      "Epoch 2528 Batch   96/175   train_loss = 1.269\n",
      "Epoch 2528 Batch  128/175   train_loss = 1.202\n",
      "Epoch 2528 Batch  160/175   train_loss = 1.254\n",
      "Epoch 2529 Batch   17/175   train_loss = 1.231\n",
      "Epoch 2529 Batch   49/175   train_loss = 1.271\n",
      "Epoch 2529 Batch   81/175   train_loss = 1.242\n",
      "Epoch 2529 Batch  113/175   train_loss = 1.287\n",
      "Epoch 2529 Batch  145/175   train_loss = 1.203\n",
      "Epoch 2530 Batch    2/175   train_loss = 1.262\n",
      "Epoch 2530 Batch   34/175   train_loss = 1.305\n",
      "Epoch 2530 Batch   66/175   train_loss = 1.300\n",
      "Epoch 2530 Batch   98/175   train_loss = 1.294\n",
      "Epoch 2530 Batch  130/175   train_loss = 1.302\n",
      "Epoch 2530 Batch  162/175   train_loss = 1.276\n",
      "Epoch 2531 Batch   19/175   train_loss = 1.235\n",
      "Epoch 2531 Batch   51/175   train_loss = 1.206\n",
      "Epoch 2531 Batch   83/175   train_loss = 1.317\n",
      "Epoch 2531 Batch  115/175   train_loss = 1.376\n",
      "Epoch 2531 Batch  147/175   train_loss = 1.222\n",
      "Epoch 2532 Batch    4/175   train_loss = 1.289\n",
      "Epoch 2532 Batch   36/175   train_loss = 1.215\n",
      "Epoch 2532 Batch   68/175   train_loss = 1.237\n",
      "Epoch 2532 Batch  100/175   train_loss = 1.290\n",
      "Epoch 2532 Batch  132/175   train_loss = 1.274\n",
      "Epoch 2532 Batch  164/175   train_loss = 1.231\n",
      "Epoch 2533 Batch   21/175   train_loss = 1.237\n",
      "Epoch 2533 Batch   53/175   train_loss = 1.226\n",
      "Epoch 2533 Batch   85/175   train_loss = 1.338\n",
      "Epoch 2533 Batch  117/175   train_loss = 1.296\n",
      "Epoch 2533 Batch  149/175   train_loss = 1.289\n",
      "Epoch 2534 Batch    6/175   train_loss = 1.304\n",
      "Epoch 2534 Batch   38/175   train_loss = 1.203\n",
      "Epoch 2534 Batch   70/175   train_loss = 1.279\n",
      "Epoch 2534 Batch  102/175   train_loss = 1.266\n",
      "Epoch 2534 Batch  134/175   train_loss = 1.232\n",
      "Epoch 2534 Batch  166/175   train_loss = 1.259\n",
      "Epoch 2535 Batch   23/175   train_loss = 1.189\n",
      "Epoch 2535 Batch   55/175   train_loss = 1.317\n",
      "Epoch 2535 Batch   87/175   train_loss = 1.356\n",
      "Epoch 2535 Batch  119/175   train_loss = 1.256\n",
      "Epoch 2535 Batch  151/175   train_loss = 1.269\n",
      "Epoch 2536 Batch    8/175   train_loss = 1.267\n",
      "Epoch 2536 Batch   40/175   train_loss = 1.243\n",
      "Epoch 2536 Batch   72/175   train_loss = 1.319\n",
      "Epoch 2536 Batch  104/175   train_loss = 1.299\n",
      "Epoch 2536 Batch  136/175   train_loss = 1.251\n",
      "Epoch 2536 Batch  168/175   train_loss = 1.290\n",
      "Epoch 2537 Batch   25/175   train_loss = 1.271\n",
      "Epoch 2537 Batch   57/175   train_loss = 1.319\n",
      "Epoch 2537 Batch   89/175   train_loss = 1.262\n",
      "Epoch 2537 Batch  121/175   train_loss = 1.231\n",
      "Epoch 2537 Batch  153/175   train_loss = 1.258\n",
      "Epoch 2538 Batch   10/175   train_loss = 1.206\n",
      "Epoch 2538 Batch   42/175   train_loss = 1.316\n",
      "Epoch 2538 Batch   74/175   train_loss = 1.316\n",
      "Epoch 2538 Batch  106/175   train_loss = 1.287\n",
      "Epoch 2538 Batch  138/175   train_loss = 1.266\n",
      "Epoch 2538 Batch  170/175   train_loss = 1.286\n",
      "Epoch 2539 Batch   27/175   train_loss = 1.247\n",
      "Epoch 2539 Batch   59/175   train_loss = 1.231\n",
      "Epoch 2539 Batch   91/175   train_loss = 1.230\n",
      "Epoch 2539 Batch  123/175   train_loss = 1.278\n",
      "Epoch 2539 Batch  155/175   train_loss = 1.253\n",
      "Epoch 2540 Batch   12/175   train_loss = 1.257\n",
      "Epoch 2540 Batch   44/175   train_loss = 1.226\n",
      "Epoch 2540 Batch   76/175   train_loss = 1.236\n",
      "Epoch 2540 Batch  108/175   train_loss = 1.283\n",
      "Epoch 2540 Batch  140/175   train_loss = 1.238\n",
      "Epoch 2540 Batch  172/175   train_loss = 1.277\n",
      "Epoch 2541 Batch   29/175   train_loss = 1.247\n",
      "Epoch 2541 Batch   61/175   train_loss = 1.274\n",
      "Epoch 2541 Batch   93/175   train_loss = 1.261\n",
      "Epoch 2541 Batch  125/175   train_loss = 1.259\n",
      "Epoch 2541 Batch  157/175   train_loss = 1.251\n",
      "Epoch 2542 Batch   14/175   train_loss = 1.254\n",
      "Epoch 2542 Batch   46/175   train_loss = 1.239\n",
      "Epoch 2542 Batch   78/175   train_loss = 1.215\n",
      "Epoch 2542 Batch  110/175   train_loss = 1.318\n",
      "Epoch 2542 Batch  142/175   train_loss = 1.270\n",
      "Epoch 2542 Batch  174/175   train_loss = 1.230\n",
      "Epoch 2543 Batch   31/175   train_loss = 1.251\n",
      "Epoch 2543 Batch   63/175   train_loss = 1.275\n",
      "Epoch 2543 Batch   95/175   train_loss = 1.238\n",
      "Epoch 2543 Batch  127/175   train_loss = 1.207\n",
      "Epoch 2543 Batch  159/175   train_loss = 1.218\n",
      "Epoch 2544 Batch   16/175   train_loss = 1.249\n",
      "Epoch 2544 Batch   48/175   train_loss = 1.234\n",
      "Epoch 2544 Batch   80/175   train_loss = 1.256\n",
      "Epoch 2544 Batch  112/175   train_loss = 1.270\n",
      "Epoch 2544 Batch  144/175   train_loss = 1.180\n",
      "Epoch 2545 Batch    1/175   train_loss = 1.301\n",
      "Epoch 2545 Batch   33/175   train_loss = 1.285\n",
      "Epoch 2545 Batch   65/175   train_loss = 1.247\n",
      "Epoch 2545 Batch   97/175   train_loss = 1.234\n",
      "Epoch 2545 Batch  129/175   train_loss = 1.240\n",
      "Epoch 2545 Batch  161/175   train_loss = 1.223\n",
      "Epoch 2546 Batch   18/175   train_loss = 1.200\n",
      "Epoch 2546 Batch   50/175   train_loss = 1.221\n",
      "Epoch 2546 Batch   82/175   train_loss = 1.276\n",
      "Epoch 2546 Batch  114/175   train_loss = 1.279\n",
      "Epoch 2546 Batch  146/175   train_loss = 1.238\n",
      "Epoch 2547 Batch    3/175   train_loss = 1.289\n",
      "Epoch 2547 Batch   35/175   train_loss = 1.230\n",
      "Epoch 2547 Batch   67/175   train_loss = 1.198\n",
      "Epoch 2547 Batch   99/175   train_loss = 1.295\n",
      "Epoch 2547 Batch  131/175   train_loss = 1.225\n",
      "Epoch 2547 Batch  163/175   train_loss = 1.234\n",
      "Epoch 2548 Batch   20/175   train_loss = 1.242\n",
      "Epoch 2548 Batch   52/175   train_loss = 1.188\n",
      "Epoch 2548 Batch   84/175   train_loss = 1.245\n",
      "Epoch 2548 Batch  116/175   train_loss = 1.298\n",
      "Epoch 2548 Batch  148/175   train_loss = 1.242\n",
      "Epoch 2549 Batch    5/175   train_loss = 1.248\n",
      "Epoch 2549 Batch   37/175   train_loss = 1.227\n",
      "Epoch 2549 Batch   69/175   train_loss = 1.289\n",
      "Epoch 2549 Batch  101/175   train_loss = 1.288\n",
      "Epoch 2549 Batch  133/175   train_loss = 1.159\n",
      "Epoch 2549 Batch  165/175   train_loss = 1.233\n",
      "Epoch 2550 Batch   22/175   train_loss = 1.177\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2550 Batch   54/175   train_loss = 1.269\n",
      "Epoch 2550 Batch   86/175   train_loss = 1.290\n",
      "Epoch 2550 Batch  118/175   train_loss = 1.315\n",
      "Epoch 2550 Batch  150/175   train_loss = 1.257\n",
      "Epoch 2551 Batch    7/175   train_loss = 1.274\n",
      "Epoch 2551 Batch   39/175   train_loss = 1.194\n",
      "Epoch 2551 Batch   71/175   train_loss = 1.266\n",
      "Epoch 2551 Batch  103/175   train_loss = 1.255\n",
      "Epoch 2551 Batch  135/175   train_loss = 1.216\n",
      "Epoch 2551 Batch  167/175   train_loss = 1.288\n",
      "Epoch 2552 Batch   24/175   train_loss = 1.187\n",
      "Epoch 2552 Batch   56/175   train_loss = 1.506\n",
      "Epoch 2552 Batch   88/175   train_loss = 1.361\n",
      "Epoch 2552 Batch  120/175   train_loss = 1.302\n",
      "Epoch 2552 Batch  152/175   train_loss = 1.263\n",
      "Epoch 2553 Batch    9/175   train_loss = 1.285\n",
      "Epoch 2553 Batch   41/175   train_loss = 1.309\n",
      "Epoch 2553 Batch   73/175   train_loss = 1.273\n",
      "Epoch 2553 Batch  105/175   train_loss = 1.315\n",
      "Epoch 2553 Batch  137/175   train_loss = 1.224\n",
      "Epoch 2553 Batch  169/175   train_loss = 1.293\n",
      "Epoch 2554 Batch   26/175   train_loss = 1.287\n",
      "Epoch 2554 Batch   58/175   train_loss = 1.296\n",
      "Epoch 2554 Batch   90/175   train_loss = 1.281\n",
      "Epoch 2554 Batch  122/175   train_loss = 1.249\n",
      "Epoch 2554 Batch  154/175   train_loss = 1.239\n",
      "Epoch 2555 Batch   11/175   train_loss = 1.235\n",
      "Epoch 2555 Batch   43/175   train_loss = 1.287\n",
      "Epoch 2555 Batch   75/175   train_loss = 1.261\n",
      "Epoch 2555 Batch  107/175   train_loss = 1.300\n",
      "Epoch 2555 Batch  139/175   train_loss = 1.211\n",
      "Epoch 2555 Batch  171/175   train_loss = 1.312\n",
      "Epoch 2556 Batch   28/175   train_loss = 1.248\n",
      "Epoch 2556 Batch   60/175   train_loss = 1.255\n",
      "Epoch 2556 Batch   92/175   train_loss = 1.218\n",
      "Epoch 2556 Batch  124/175   train_loss = 1.256\n",
      "Epoch 2556 Batch  156/175   train_loss = 1.291\n",
      "Epoch 2557 Batch   13/175   train_loss = 1.250\n",
      "Epoch 2557 Batch   45/175   train_loss = 1.283\n",
      "Epoch 2557 Batch   77/175   train_loss = 1.240\n",
      "Epoch 2557 Batch  109/175   train_loss = 1.314\n",
      "Epoch 2557 Batch  141/175   train_loss = 1.202\n",
      "Epoch 2557 Batch  173/175   train_loss = 1.230\n",
      "Epoch 2558 Batch   30/175   train_loss = 1.309\n",
      "Epoch 2558 Batch   62/175   train_loss = 1.288\n",
      "Epoch 2558 Batch   94/175   train_loss = 1.254\n",
      "Epoch 2558 Batch  126/175   train_loss = 1.250\n",
      "Epoch 2558 Batch  158/175   train_loss = 1.246\n",
      "Epoch 2559 Batch   15/175   train_loss = 1.294\n",
      "Epoch 2559 Batch   47/175   train_loss = 1.267\n",
      "Epoch 2559 Batch   79/175   train_loss = 1.284\n",
      "Epoch 2559 Batch  111/175   train_loss = 1.314\n",
      "Epoch 2559 Batch  143/175   train_loss = 1.240\n",
      "Epoch 2560 Batch    0/175   train_loss = 1.243\n",
      "Epoch 2560 Batch   32/175   train_loss = 1.269\n",
      "Epoch 2560 Batch   64/175   train_loss = 1.359\n",
      "Epoch 2560 Batch   96/175   train_loss = 1.267\n",
      "Epoch 2560 Batch  128/175   train_loss = 1.186\n",
      "Epoch 2560 Batch  160/175   train_loss = 1.228\n",
      "Epoch 2561 Batch   17/175   train_loss = 1.214\n",
      "Epoch 2561 Batch   49/175   train_loss = 1.269\n",
      "Epoch 2561 Batch   81/175   train_loss = 1.241\n",
      "Epoch 2561 Batch  113/175   train_loss = 1.263\n",
      "Epoch 2561 Batch  145/175   train_loss = 1.175\n",
      "Epoch 2562 Batch    2/175   train_loss = 1.231\n",
      "Epoch 2562 Batch   34/175   train_loss = 1.278\n",
      "Epoch 2562 Batch   66/175   train_loss = 1.290\n",
      "Epoch 2562 Batch   98/175   train_loss = 1.264\n",
      "Epoch 2562 Batch  130/175   train_loss = 1.271\n",
      "Epoch 2562 Batch  162/175   train_loss = 1.237\n",
      "Epoch 2563 Batch   19/175   train_loss = 1.222\n",
      "Epoch 2563 Batch   51/175   train_loss = 1.186\n",
      "Epoch 2563 Batch   83/175   train_loss = 1.292\n",
      "Epoch 2563 Batch  115/175   train_loss = 1.351\n",
      "Epoch 2563 Batch  147/175   train_loss = 1.204\n",
      "Epoch 2564 Batch    4/175   train_loss = 1.259\n",
      "Epoch 2564 Batch   36/175   train_loss = 1.195\n",
      "Epoch 2564 Batch   68/175   train_loss = 1.222\n",
      "Epoch 2564 Batch  100/175   train_loss = 1.264\n",
      "Epoch 2564 Batch  132/175   train_loss = 1.238\n",
      "Epoch 2564 Batch  164/175   train_loss = 1.207\n",
      "Epoch 2565 Batch   21/175   train_loss = 1.205\n",
      "Epoch 2565 Batch   53/175   train_loss = 1.204\n",
      "Epoch 2565 Batch   85/175   train_loss = 1.276\n",
      "Epoch 2565 Batch  117/175   train_loss = 1.272\n",
      "Epoch 2565 Batch  149/175   train_loss = 1.260\n",
      "Epoch 2566 Batch    6/175   train_loss = 1.268\n",
      "Epoch 2566 Batch   38/175   train_loss = 1.188\n",
      "Epoch 2566 Batch   70/175   train_loss = 1.221\n",
      "Epoch 2566 Batch  102/175   train_loss = 1.243\n",
      "Epoch 2566 Batch  134/175   train_loss = 1.206\n",
      "Epoch 2566 Batch  166/175   train_loss = 1.259\n",
      "Epoch 2567 Batch   23/175   train_loss = 1.213\n",
      "Epoch 2567 Batch   55/175   train_loss = 1.309\n",
      "Epoch 2567 Batch   87/175   train_loss = 1.339\n",
      "Epoch 2567 Batch  119/175   train_loss = 1.269\n",
      "Epoch 2567 Batch  151/175   train_loss = 1.251\n",
      "Epoch 2568 Batch    8/175   train_loss = 1.283\n",
      "Epoch 2568 Batch   40/175   train_loss = 1.249\n",
      "Epoch 2568 Batch   72/175   train_loss = 1.312\n",
      "Epoch 2568 Batch  104/175   train_loss = 1.265\n",
      "Epoch 2568 Batch  136/175   train_loss = 1.255\n",
      "Epoch 2568 Batch  168/175   train_loss = 1.285\n",
      "Epoch 2569 Batch   25/175   train_loss = 1.254\n",
      "Epoch 2569 Batch   57/175   train_loss = 1.293\n",
      "Epoch 2569 Batch   89/175   train_loss = 1.251\n",
      "Epoch 2569 Batch  121/175   train_loss = 1.212\n",
      "Epoch 2569 Batch  153/175   train_loss = 1.243\n",
      "Epoch 2570 Batch   10/175   train_loss = 1.185\n",
      "Epoch 2570 Batch   42/175   train_loss = 1.311\n",
      "Epoch 2570 Batch   74/175   train_loss = 1.296\n",
      "Epoch 2570 Batch  106/175   train_loss = 1.300\n",
      "Epoch 2570 Batch  138/175   train_loss = 1.252\n",
      "Epoch 2570 Batch  170/175   train_loss = 1.305\n",
      "Epoch 2571 Batch   27/175   train_loss = 1.238\n",
      "Epoch 2571 Batch   59/175   train_loss = 1.234\n",
      "Epoch 2571 Batch   91/175   train_loss = 1.236\n",
      "Epoch 2571 Batch  123/175   train_loss = 1.255\n",
      "Epoch 2571 Batch  155/175   train_loss = 1.213\n",
      "Epoch 2572 Batch   12/175   train_loss = 1.238\n",
      "Epoch 2572 Batch   44/175   train_loss = 1.207\n",
      "Epoch 2572 Batch   76/175   train_loss = 1.233\n",
      "Epoch 2572 Batch  108/175   train_loss = 1.279\n",
      "Epoch 2572 Batch  140/175   train_loss = 1.240\n",
      "Epoch 2572 Batch  172/175   train_loss = 1.265\n",
      "Epoch 2573 Batch   29/175   train_loss = 1.260\n",
      "Epoch 2573 Batch   61/175   train_loss = 1.266\n",
      "Epoch 2573 Batch   93/175   train_loss = 1.268\n",
      "Epoch 2573 Batch  125/175   train_loss = 1.263\n",
      "Epoch 2573 Batch  157/175   train_loss = 1.260\n",
      "Epoch 2574 Batch   14/175   train_loss = 1.253\n",
      "Epoch 2574 Batch   46/175   train_loss = 1.225\n",
      "Epoch 2574 Batch   78/175   train_loss = 1.280\n",
      "Epoch 2574 Batch  110/175   train_loss = 1.352\n",
      "Epoch 2574 Batch  142/175   train_loss = 1.279\n",
      "Epoch 2574 Batch  174/175   train_loss = 1.243\n",
      "Epoch 2575 Batch   31/175   train_loss = 1.259\n",
      "Epoch 2575 Batch   63/175   train_loss = 1.258\n",
      "Epoch 2575 Batch   95/175   train_loss = 1.230\n",
      "Epoch 2575 Batch  127/175   train_loss = 1.192\n",
      "Epoch 2575 Batch  159/175   train_loss = 1.194\n",
      "Epoch 2576 Batch   16/175   train_loss = 1.238\n",
      "Epoch 2576 Batch   48/175   train_loss = 1.247\n",
      "Epoch 2576 Batch   80/175   train_loss = 1.282\n",
      "Epoch 2576 Batch  112/175   train_loss = 1.229\n",
      "Epoch 2576 Batch  144/175   train_loss = 1.204\n",
      "Epoch 2577 Batch    1/175   train_loss = 1.288\n",
      "Epoch 2577 Batch   33/175   train_loss = 1.285\n",
      "Epoch 2577 Batch   65/175   train_loss = 1.226\n",
      "Epoch 2577 Batch   97/175   train_loss = 1.386\n",
      "Epoch 2577 Batch  129/175   train_loss = 1.279\n",
      "Epoch 2577 Batch  161/175   train_loss = 1.268\n",
      "Epoch 2578 Batch   18/175   train_loss = 1.242\n",
      "Epoch 2578 Batch   50/175   train_loss = 1.258\n",
      "Epoch 2578 Batch   82/175   train_loss = 1.288\n",
      "Epoch 2578 Batch  114/175   train_loss = 1.303\n",
      "Epoch 2578 Batch  146/175   train_loss = 1.259\n",
      "Epoch 2579 Batch    3/175   train_loss = 1.294\n",
      "Epoch 2579 Batch   35/175   train_loss = 1.242\n",
      "Epoch 2579 Batch   67/175   train_loss = 1.216\n",
      "Epoch 2579 Batch   99/175   train_loss = 1.317\n",
      "Epoch 2579 Batch  131/175   train_loss = 1.227\n",
      "Epoch 2579 Batch  163/175   train_loss = 1.248\n",
      "Epoch 2580 Batch   20/175   train_loss = 1.215\n",
      "Epoch 2580 Batch   52/175   train_loss = 1.198\n",
      "Epoch 2580 Batch   84/175   train_loss = 1.269\n",
      "Epoch 2580 Batch  116/175   train_loss = 1.348\n",
      "Epoch 2580 Batch  148/175   train_loss = 1.243\n",
      "Epoch 2581 Batch    5/175   train_loss = 1.247\n",
      "Epoch 2581 Batch   37/175   train_loss = 1.238\n",
      "Epoch 2581 Batch   69/175   train_loss = 1.278\n",
      "Epoch 2581 Batch  101/175   train_loss = 1.305\n",
      "Epoch 2581 Batch  133/175   train_loss = 1.158\n",
      "Epoch 2581 Batch  165/175   train_loss = 1.244\n",
      "Epoch 2582 Batch   22/175   train_loss = 1.182\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2582 Batch   54/175   train_loss = 1.290\n",
      "Epoch 2582 Batch   86/175   train_loss = 1.345\n",
      "Epoch 2582 Batch  118/175   train_loss = 1.326\n",
      "Epoch 2582 Batch  150/175   train_loss = 1.262\n",
      "Epoch 2583 Batch    7/175   train_loss = 1.282\n",
      "Epoch 2583 Batch   39/175   train_loss = 1.204\n",
      "Epoch 2583 Batch   71/175   train_loss = 1.270\n",
      "Epoch 2583 Batch  103/175   train_loss = 1.266\n",
      "Epoch 2583 Batch  135/175   train_loss = 1.223\n",
      "Epoch 2583 Batch  167/175   train_loss = 1.295\n",
      "Epoch 2584 Batch   24/175   train_loss = 1.189\n",
      "Epoch 2584 Batch   56/175   train_loss = 1.251\n",
      "Epoch 2584 Batch   88/175   train_loss = 1.289\n",
      "Epoch 2584 Batch  120/175   train_loss = 1.234\n",
      "Epoch 2584 Batch  152/175   train_loss = 1.236\n",
      "Epoch 2585 Batch    9/175   train_loss = 1.277\n",
      "Epoch 2585 Batch   41/175   train_loss = 1.266\n",
      "Epoch 2585 Batch   73/175   train_loss = 1.238\n",
      "Epoch 2585 Batch  105/175   train_loss = 1.288\n",
      "Epoch 2585 Batch  137/175   train_loss = 1.217\n",
      "Epoch 2585 Batch  169/175   train_loss = 1.265\n",
      "Epoch 2586 Batch   26/175   train_loss = 1.260\n",
      "Epoch 2586 Batch   58/175   train_loss = 1.268\n",
      "Epoch 2586 Batch   90/175   train_loss = 1.252\n",
      "Epoch 2586 Batch  122/175   train_loss = 1.217\n",
      "Epoch 2586 Batch  154/175   train_loss = 1.232\n",
      "Epoch 2587 Batch   11/175   train_loss = 1.213\n",
      "Epoch 2587 Batch   43/175   train_loss = 1.263\n",
      "Epoch 2587 Batch   75/175   train_loss = 1.223\n",
      "Epoch 2587 Batch  107/175   train_loss = 1.297\n",
      "Epoch 2587 Batch  139/175   train_loss = 1.186\n",
      "Epoch 2587 Batch  171/175   train_loss = 1.293\n",
      "Epoch 2588 Batch   28/175   train_loss = 1.237\n",
      "Epoch 2588 Batch   60/175   train_loss = 1.263\n",
      "Epoch 2588 Batch   92/175   train_loss = 1.226\n",
      "Epoch 2588 Batch  124/175   train_loss = 1.258\n",
      "Epoch 2588 Batch  156/175   train_loss = 1.274\n",
      "Epoch 2589 Batch   13/175   train_loss = 1.249\n",
      "Epoch 2589 Batch   45/175   train_loss = 1.223\n",
      "Epoch 2589 Batch   77/175   train_loss = 1.232\n",
      "Epoch 2589 Batch  109/175   train_loss = 1.268\n",
      "Epoch 2589 Batch  141/175   train_loss = 1.213\n",
      "Epoch 2589 Batch  173/175   train_loss = 1.220\n",
      "Epoch 2590 Batch   30/175   train_loss = 1.315\n",
      "Epoch 2590 Batch   62/175   train_loss = 1.263\n",
      "Epoch 2590 Batch   94/175   train_loss = 1.246\n",
      "Epoch 2590 Batch  126/175   train_loss = 1.264\n",
      "Epoch 2590 Batch  158/175   train_loss = 1.281\n",
      "Epoch 2591 Batch   15/175   train_loss = 1.305\n",
      "Epoch 2591 Batch   47/175   train_loss = 1.279\n",
      "Epoch 2591 Batch   79/175   train_loss = 1.306\n",
      "Epoch 2591 Batch  111/175   train_loss = 1.302\n",
      "Epoch 2591 Batch  143/175   train_loss = 1.245\n",
      "Epoch 2592 Batch    0/175   train_loss = 1.291\n",
      "Epoch 2592 Batch   32/175   train_loss = 1.281\n",
      "Epoch 2592 Batch   64/175   train_loss = 1.294\n",
      "Epoch 2592 Batch   96/175   train_loss = 1.271\n",
      "Epoch 2592 Batch  128/175   train_loss = 1.196\n",
      "Epoch 2592 Batch  160/175   train_loss = 1.274\n",
      "Epoch 2593 Batch   17/175   train_loss = 1.208\n",
      "Epoch 2593 Batch   49/175   train_loss = 1.260\n",
      "Epoch 2593 Batch   81/175   train_loss = 1.255\n",
      "Epoch 2593 Batch  113/175   train_loss = 1.259\n",
      "Epoch 2593 Batch  145/175   train_loss = 1.198\n",
      "Epoch 2594 Batch    2/175   train_loss = 1.247\n",
      "Epoch 2594 Batch   34/175   train_loss = 1.277\n",
      "Epoch 2594 Batch   66/175   train_loss = 1.299\n",
      "Epoch 2594 Batch   98/175   train_loss = 1.290\n",
      "Epoch 2594 Batch  130/175   train_loss = 1.297\n",
      "Epoch 2594 Batch  162/175   train_loss = 1.273\n",
      "Epoch 2595 Batch   19/175   train_loss = 1.237\n",
      "Epoch 2595 Batch   51/175   train_loss = 1.195\n",
      "Epoch 2595 Batch   83/175   train_loss = 1.307\n",
      "Epoch 2595 Batch  115/175   train_loss = 1.368\n",
      "Epoch 2595 Batch  147/175   train_loss = 1.218\n",
      "Epoch 2596 Batch    4/175   train_loss = 1.264\n",
      "Epoch 2596 Batch   36/175   train_loss = 1.201\n",
      "Epoch 2596 Batch   68/175   train_loss = 1.234\n",
      "Epoch 2596 Batch  100/175   train_loss = 1.290\n",
      "Epoch 2596 Batch  132/175   train_loss = 1.219\n",
      "Epoch 2596 Batch  164/175   train_loss = 1.210\n",
      "Epoch 2597 Batch   21/175   train_loss = 1.219\n",
      "Epoch 2597 Batch   53/175   train_loss = 1.217\n",
      "Epoch 2597 Batch   85/175   train_loss = 1.260\n",
      "Epoch 2597 Batch  117/175   train_loss = 1.273\n",
      "Epoch 2597 Batch  149/175   train_loss = 1.274\n",
      "Epoch 2598 Batch    6/175   train_loss = 1.281\n",
      "Epoch 2598 Batch   38/175   train_loss = 1.186\n",
      "Epoch 2598 Batch   70/175   train_loss = 1.233\n",
      "Epoch 2598 Batch  102/175   train_loss = 1.245\n",
      "Epoch 2598 Batch  134/175   train_loss = 1.193\n",
      "Epoch 2598 Batch  166/175   train_loss = 1.284\n",
      "Epoch 2599 Batch   23/175   train_loss = 1.195\n",
      "Epoch 2599 Batch   55/175   train_loss = 1.303\n",
      "Epoch 2599 Batch   87/175   train_loss = 1.344\n",
      "Epoch 2599 Batch  119/175   train_loss = 1.236\n",
      "Epoch 2599 Batch  151/175   train_loss = 1.269\n",
      "Epoch 2600 Batch    8/175   train_loss = 1.261\n",
      "Epoch 2600 Batch   40/175   train_loss = 1.213\n",
      "Epoch 2600 Batch   72/175   train_loss = 1.269\n",
      "Epoch 2600 Batch  104/175   train_loss = 1.251\n",
      "Epoch 2600 Batch  136/175   train_loss = 1.208\n",
      "Epoch 2600 Batch  168/175   train_loss = 1.252\n",
      "Epoch 2601 Batch   25/175   train_loss = 1.244\n",
      "Epoch 2601 Batch   57/175   train_loss = 1.289\n",
      "Epoch 2601 Batch   89/175   train_loss = 1.241\n",
      "Epoch 2601 Batch  121/175   train_loss = 1.205\n",
      "Epoch 2601 Batch  153/175   train_loss = 1.253\n",
      "Epoch 2602 Batch   10/175   train_loss = 1.208\n",
      "Epoch 2602 Batch   42/175   train_loss = 1.264\n",
      "Epoch 2602 Batch   74/175   train_loss = 1.280\n",
      "Epoch 2602 Batch  106/175   train_loss = 1.289\n",
      "Epoch 2602 Batch  138/175   train_loss = 1.240\n",
      "Epoch 2602 Batch  170/175   train_loss = 1.279\n",
      "Epoch 2603 Batch   27/175   train_loss = 1.229\n",
      "Epoch 2603 Batch   59/175   train_loss = 1.223\n",
      "Epoch 2603 Batch   91/175   train_loss = 1.218\n",
      "Epoch 2603 Batch  123/175   train_loss = 1.221\n",
      "Epoch 2603 Batch  155/175   train_loss = 1.209\n",
      "Epoch 2604 Batch   12/175   train_loss = 1.214\n",
      "Epoch 2604 Batch   44/175   train_loss = 1.211\n",
      "Epoch 2604 Batch   76/175   train_loss = 1.227\n",
      "Epoch 2604 Batch  108/175   train_loss = 1.247\n",
      "Epoch 2604 Batch  140/175   train_loss = 1.226\n",
      "Epoch 2604 Batch  172/175   train_loss = 1.285\n",
      "Epoch 2605 Batch   29/175   train_loss = 1.244\n",
      "Epoch 2605 Batch   61/175   train_loss = 1.279\n",
      "Epoch 2605 Batch   93/175   train_loss = 1.254\n",
      "Epoch 2605 Batch  125/175   train_loss = 1.250\n",
      "Epoch 2605 Batch  157/175   train_loss = 1.234\n",
      "Epoch 2606 Batch   14/175   train_loss = 1.259\n",
      "Epoch 2606 Batch   46/175   train_loss = 1.248\n",
      "Epoch 2606 Batch   78/175   train_loss = 1.212\n",
      "Epoch 2606 Batch  110/175   train_loss = 1.315\n",
      "Epoch 2606 Batch  142/175   train_loss = 1.243\n",
      "Epoch 2606 Batch  174/175   train_loss = 1.245\n",
      "Epoch 2607 Batch   31/175   train_loss = 1.264\n",
      "Epoch 2607 Batch   63/175   train_loss = 1.266\n",
      "Epoch 2607 Batch   95/175   train_loss = 1.236\n",
      "Epoch 2607 Batch  127/175   train_loss = 1.217\n",
      "Epoch 2607 Batch  159/175   train_loss = 1.215\n",
      "Epoch 2608 Batch   16/175   train_loss = 1.280\n",
      "Epoch 2608 Batch   48/175   train_loss = 1.257\n",
      "Epoch 2608 Batch   80/175   train_loss = 1.268\n",
      "Epoch 2608 Batch  112/175   train_loss = 1.255\n",
      "Epoch 2608 Batch  144/175   train_loss = 1.176\n",
      "Epoch 2609 Batch    1/175   train_loss = 1.281\n",
      "Epoch 2609 Batch   33/175   train_loss = 1.313\n",
      "Epoch 2609 Batch   65/175   train_loss = 1.254\n",
      "Epoch 2609 Batch   97/175   train_loss = 1.244\n",
      "Epoch 2609 Batch  129/175   train_loss = 1.234\n",
      "Epoch 2609 Batch  161/175   train_loss = 1.209\n",
      "Epoch 2610 Batch   18/175   train_loss = 1.208\n",
      "Epoch 2610 Batch   50/175   train_loss = 1.222\n",
      "Epoch 2610 Batch   82/175   train_loss = 1.255\n",
      "Epoch 2610 Batch  114/175   train_loss = 1.294\n",
      "Epoch 2610 Batch  146/175   train_loss = 1.229\n",
      "Epoch 2611 Batch    3/175   train_loss = 1.287\n",
      "Epoch 2611 Batch   35/175   train_loss = 1.227\n",
      "Epoch 2611 Batch   67/175   train_loss = 1.214\n",
      "Epoch 2611 Batch   99/175   train_loss = 1.327\n",
      "Epoch 2611 Batch  131/175   train_loss = 1.243\n",
      "Epoch 2611 Batch  163/175   train_loss = 1.246\n",
      "Epoch 2612 Batch   20/175   train_loss = 1.221\n",
      "Epoch 2612 Batch   52/175   train_loss = 1.207\n",
      "Epoch 2612 Batch   84/175   train_loss = 1.230\n",
      "Epoch 2612 Batch  116/175   train_loss = 1.280\n",
      "Epoch 2612 Batch  148/175   train_loss = 1.222\n",
      "Epoch 2613 Batch    5/175   train_loss = 1.230\n",
      "Epoch 2613 Batch   37/175   train_loss = 1.242\n",
      "Epoch 2613 Batch   69/175   train_loss = 1.277\n",
      "Epoch 2613 Batch  101/175   train_loss = 1.296\n",
      "Epoch 2613 Batch  133/175   train_loss = 1.166\n",
      "Epoch 2613 Batch  165/175   train_loss = 1.240\n",
      "Epoch 2614 Batch   22/175   train_loss = 1.164\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2614 Batch   54/175   train_loss = 1.255\n",
      "Epoch 2614 Batch   86/175   train_loss = 1.292\n",
      "Epoch 2614 Batch  118/175   train_loss = 1.285\n",
      "Epoch 2614 Batch  150/175   train_loss = 1.230\n",
      "Epoch 2615 Batch    7/175   train_loss = 1.272\n",
      "Epoch 2615 Batch   39/175   train_loss = 1.198\n",
      "Epoch 2615 Batch   71/175   train_loss = 1.280\n",
      "Epoch 2615 Batch  103/175   train_loss = 1.245\n",
      "Epoch 2615 Batch  135/175   train_loss = 1.212\n",
      "Epoch 2615 Batch  167/175   train_loss = 1.284\n",
      "Epoch 2616 Batch   24/175   train_loss = 1.177\n",
      "Epoch 2616 Batch   56/175   train_loss = 1.253\n",
      "Epoch 2616 Batch   88/175   train_loss = 1.311\n",
      "Epoch 2616 Batch  120/175   train_loss = 1.253\n",
      "Epoch 2616 Batch  152/175   train_loss = 1.221\n",
      "Epoch 2617 Batch    9/175   train_loss = 1.264\n",
      "Epoch 2617 Batch   41/175   train_loss = 1.284\n",
      "Epoch 2617 Batch   73/175   train_loss = 1.255\n",
      "Epoch 2617 Batch  105/175   train_loss = 1.278\n",
      "Epoch 2617 Batch  137/175   train_loss = 1.217\n",
      "Epoch 2617 Batch  169/175   train_loss = 1.261\n",
      "Epoch 2618 Batch   26/175   train_loss = 1.249\n",
      "Epoch 2618 Batch   58/175   train_loss = 1.288\n",
      "Epoch 2618 Batch   90/175   train_loss = 1.254\n",
      "Epoch 2618 Batch  122/175   train_loss = 1.220\n",
      "Epoch 2618 Batch  154/175   train_loss = 1.226\n",
      "Epoch 2619 Batch   11/175   train_loss = 1.205\n",
      "Epoch 2619 Batch   43/175   train_loss = 1.258\n",
      "Epoch 2619 Batch   75/175   train_loss = 1.226\n",
      "Epoch 2619 Batch  107/175   train_loss = 1.288\n",
      "Epoch 2619 Batch  139/175   train_loss = 1.172\n",
      "Epoch 2619 Batch  171/175   train_loss = 1.288\n",
      "Epoch 2620 Batch   28/175   train_loss = 1.242\n",
      "Epoch 2620 Batch   60/175   train_loss = 1.247\n",
      "Epoch 2620 Batch   92/175   train_loss = 1.229\n",
      "Epoch 2620 Batch  124/175   train_loss = 1.254\n",
      "Epoch 2620 Batch  156/175   train_loss = 1.281\n",
      "Epoch 2621 Batch   13/175   train_loss = 1.244\n",
      "Epoch 2621 Batch   45/175   train_loss = 1.224\n",
      "Epoch 2621 Batch   77/175   train_loss = 1.284\n",
      "Epoch 2621 Batch  109/175   train_loss = 1.278\n",
      "Epoch 2621 Batch  141/175   train_loss = 1.194\n",
      "Epoch 2621 Batch  173/175   train_loss = 1.210\n",
      "Epoch 2622 Batch   30/175   train_loss = 1.308\n",
      "Epoch 2622 Batch   62/175   train_loss = 1.253\n",
      "Epoch 2622 Batch   94/175   train_loss = 1.245\n",
      "Epoch 2622 Batch  126/175   train_loss = 1.244\n",
      "Epoch 2622 Batch  158/175   train_loss = 1.264\n",
      "Epoch 2623 Batch   15/175   train_loss = 1.271\n",
      "Epoch 2623 Batch   47/175   train_loss = 1.273\n",
      "Epoch 2623 Batch   79/175   train_loss = 1.300\n",
      "Epoch 2623 Batch  111/175   train_loss = 1.311\n",
      "Epoch 2623 Batch  143/175   train_loss = 1.224\n",
      "Epoch 2624 Batch    0/175   train_loss = 1.246\n",
      "Epoch 2624 Batch   32/175   train_loss = 1.247\n",
      "Epoch 2624 Batch   64/175   train_loss = 1.279\n",
      "Epoch 2624 Batch   96/175   train_loss = 1.285\n",
      "Epoch 2624 Batch  128/175   train_loss = 1.209\n",
      "Epoch 2624 Batch  160/175   train_loss = 1.264\n",
      "Epoch 2625 Batch   17/175   train_loss = 1.237\n",
      "Epoch 2625 Batch   49/175   train_loss = 1.283\n",
      "Epoch 2625 Batch   81/175   train_loss = 1.263\n",
      "Epoch 2625 Batch  113/175   train_loss = 1.297\n",
      "Epoch 2625 Batch  145/175   train_loss = 1.191\n",
      "Epoch 2626 Batch    2/175   train_loss = 1.235\n",
      "Epoch 2626 Batch   34/175   train_loss = 1.271\n",
      "Epoch 2626 Batch   66/175   train_loss = 1.282\n",
      "Epoch 2626 Batch   98/175   train_loss = 1.265\n",
      "Epoch 2626 Batch  130/175   train_loss = 1.266\n",
      "Epoch 2626 Batch  162/175   train_loss = 1.248\n",
      "Epoch 2627 Batch   19/175   train_loss = 1.234\n",
      "Epoch 2627 Batch   51/175   train_loss = 1.188\n",
      "Epoch 2627 Batch   83/175   train_loss = 1.312\n",
      "Epoch 2627 Batch  115/175   train_loss = 1.354\n",
      "Epoch 2627 Batch  147/175   train_loss = 1.206\n",
      "Epoch 2628 Batch    4/175   train_loss = 1.263\n",
      "Epoch 2628 Batch   36/175   train_loss = 1.200\n",
      "Epoch 2628 Batch   68/175   train_loss = 1.238\n",
      "Epoch 2628 Batch  100/175   train_loss = 1.283\n",
      "Epoch 2628 Batch  132/175   train_loss = 1.302\n",
      "Epoch 2628 Batch  164/175   train_loss = 1.233\n",
      "Epoch 2629 Batch   21/175   train_loss = 1.219\n",
      "Epoch 2629 Batch   53/175   train_loss = 1.231\n",
      "Epoch 2629 Batch   85/175   train_loss = 1.272\n",
      "Epoch 2629 Batch  117/175   train_loss = 1.272\n",
      "Epoch 2629 Batch  149/175   train_loss = 1.314\n",
      "Epoch 2630 Batch    6/175   train_loss = 1.318\n",
      "Epoch 2630 Batch   38/175   train_loss = 1.195\n",
      "Epoch 2630 Batch   70/175   train_loss = 1.244\n",
      "Epoch 2630 Batch  102/175   train_loss = 1.263\n",
      "Epoch 2630 Batch  134/175   train_loss = 1.230\n",
      "Epoch 2630 Batch  166/175   train_loss = 1.281\n",
      "Epoch 2631 Batch   23/175   train_loss = 1.185\n",
      "Epoch 2631 Batch   55/175   train_loss = 1.303\n",
      "Epoch 2631 Batch   87/175   train_loss = 1.344\n",
      "Epoch 2631 Batch  119/175   train_loss = 1.239\n",
      "Epoch 2631 Batch  151/175   train_loss = 1.273\n",
      "Epoch 2632 Batch    8/175   train_loss = 1.274\n",
      "Epoch 2632 Batch   40/175   train_loss = 1.221\n",
      "Epoch 2632 Batch   72/175   train_loss = 1.321\n",
      "Epoch 2632 Batch  104/175   train_loss = 1.257\n",
      "Epoch 2632 Batch  136/175   train_loss = 1.221\n",
      "Epoch 2632 Batch  168/175   train_loss = 1.261\n",
      "Epoch 2633 Batch   25/175   train_loss = 1.250\n",
      "Epoch 2633 Batch   57/175   train_loss = 1.281\n",
      "Epoch 2633 Batch   89/175   train_loss = 1.243\n",
      "Epoch 2633 Batch  121/175   train_loss = 1.200\n",
      "Epoch 2633 Batch  153/175   train_loss = 1.234\n",
      "Epoch 2634 Batch   10/175   train_loss = 1.203\n",
      "Epoch 2634 Batch   42/175   train_loss = 1.285\n",
      "Epoch 2634 Batch   74/175   train_loss = 1.326\n",
      "Epoch 2634 Batch  106/175   train_loss = 1.345\n",
      "Epoch 2634 Batch  138/175   train_loss = 1.271\n",
      "Epoch 2634 Batch  170/175   train_loss = 1.291\n",
      "Epoch 2635 Batch   27/175   train_loss = 1.245\n",
      "Epoch 2635 Batch   59/175   train_loss = 1.247\n",
      "Epoch 2635 Batch   91/175   train_loss = 1.226\n",
      "Epoch 2635 Batch  123/175   train_loss = 1.243\n",
      "Epoch 2635 Batch  155/175   train_loss = 1.207\n",
      "Epoch 2636 Batch   12/175   train_loss = 1.241\n",
      "Epoch 2636 Batch   44/175   train_loss = 1.218\n",
      "Epoch 2636 Batch   76/175   train_loss = 1.221\n",
      "Epoch 2636 Batch  108/175   train_loss = 1.260\n",
      "Epoch 2636 Batch  140/175   train_loss = 1.218\n",
      "Epoch 2636 Batch  172/175   train_loss = 1.276\n",
      "Epoch 2637 Batch   29/175   train_loss = 1.237\n",
      "Epoch 2637 Batch   61/175   train_loss = 1.272\n",
      "Epoch 2637 Batch   93/175   train_loss = 1.266\n",
      "Epoch 2637 Batch  125/175   train_loss = 1.249\n",
      "Epoch 2637 Batch  157/175   train_loss = 1.256\n",
      "Epoch 2638 Batch   14/175   train_loss = 1.274\n",
      "Epoch 2638 Batch   46/175   train_loss = 1.256\n",
      "Epoch 2638 Batch   78/175   train_loss = 1.226\n",
      "Epoch 2638 Batch  110/175   train_loss = 1.349\n",
      "Epoch 2638 Batch  142/175   train_loss = 1.283\n",
      "Epoch 2638 Batch  174/175   train_loss = 1.238\n",
      "Epoch 2639 Batch   31/175   train_loss = 1.263\n",
      "Epoch 2639 Batch   63/175   train_loss = 1.245\n",
      "Epoch 2639 Batch   95/175   train_loss = 1.229\n",
      "Epoch 2639 Batch  127/175   train_loss = 1.225\n",
      "Epoch 2639 Batch  159/175   train_loss = 1.201\n",
      "Epoch 2640 Batch   16/175   train_loss = 1.204\n",
      "Epoch 2640 Batch   48/175   train_loss = 1.242\n",
      "Epoch 2640 Batch   80/175   train_loss = 1.300\n",
      "Epoch 2640 Batch  112/175   train_loss = 1.248\n",
      "Epoch 2640 Batch  144/175   train_loss = 1.171\n",
      "Epoch 2641 Batch    1/175   train_loss = 1.279\n",
      "Epoch 2641 Batch   33/175   train_loss = 1.308\n",
      "Epoch 2641 Batch   65/175   train_loss = 1.248\n",
      "Epoch 2641 Batch   97/175   train_loss = 1.214\n",
      "Epoch 2641 Batch  129/175   train_loss = 1.240\n",
      "Epoch 2641 Batch  161/175   train_loss = 1.247\n",
      "Epoch 2642 Batch   18/175   train_loss = 1.224\n",
      "Epoch 2642 Batch   50/175   train_loss = 1.248\n",
      "Epoch 2642 Batch   82/175   train_loss = 1.290\n",
      "Epoch 2642 Batch  114/175   train_loss = 1.320\n",
      "Epoch 2642 Batch  146/175   train_loss = 1.244\n",
      "Epoch 2643 Batch    3/175   train_loss = 1.282\n",
      "Epoch 2643 Batch   35/175   train_loss = 1.259\n",
      "Epoch 2643 Batch   67/175   train_loss = 1.238\n",
      "Epoch 2643 Batch   99/175   train_loss = 1.330\n",
      "Epoch 2643 Batch  131/175   train_loss = 1.253\n",
      "Epoch 2643 Batch  163/175   train_loss = 1.272\n",
      "Epoch 2644 Batch   20/175   train_loss = 1.233\n",
      "Epoch 2644 Batch   52/175   train_loss = 1.183\n",
      "Epoch 2644 Batch   84/175   train_loss = 1.251\n",
      "Epoch 2644 Batch  116/175   train_loss = 1.297\n",
      "Epoch 2644 Batch  148/175   train_loss = 1.245\n",
      "Epoch 2645 Batch    5/175   train_loss = 1.221\n",
      "Epoch 2645 Batch   37/175   train_loss = 1.205\n",
      "Epoch 2645 Batch   69/175   train_loss = 1.294\n",
      "Epoch 2645 Batch  101/175   train_loss = 1.285\n",
      "Epoch 2645 Batch  133/175   train_loss = 1.148\n",
      "Epoch 2645 Batch  165/175   train_loss = 1.222\n",
      "Epoch 2646 Batch   22/175   train_loss = 1.178\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2646 Batch   54/175   train_loss = 1.265\n",
      "Epoch 2646 Batch   86/175   train_loss = 1.304\n",
      "Epoch 2646 Batch  118/175   train_loss = 1.300\n",
      "Epoch 2646 Batch  150/175   train_loss = 1.241\n",
      "Epoch 2647 Batch    7/175   train_loss = 1.377\n",
      "Epoch 2647 Batch   39/175   train_loss = 1.222\n",
      "Epoch 2647 Batch   71/175   train_loss = 1.284\n",
      "Epoch 2647 Batch  103/175   train_loss = 1.254\n",
      "Epoch 2647 Batch  135/175   train_loss = 1.243\n",
      "Epoch 2647 Batch  167/175   train_loss = 1.317\n",
      "Epoch 2648 Batch   24/175   train_loss = 1.207\n",
      "Epoch 2648 Batch   56/175   train_loss = 1.287\n",
      "Epoch 2648 Batch   88/175   train_loss = 1.302\n",
      "Epoch 2648 Batch  120/175   train_loss = 1.258\n",
      "Epoch 2648 Batch  152/175   train_loss = 1.257\n",
      "Epoch 2649 Batch    9/175   train_loss = 1.279\n",
      "Epoch 2649 Batch   41/175   train_loss = 1.324\n",
      "Epoch 2649 Batch   73/175   train_loss = 1.262\n",
      "Epoch 2649 Batch  105/175   train_loss = 1.284\n",
      "Epoch 2649 Batch  137/175   train_loss = 1.219\n",
      "Epoch 2649 Batch  169/175   train_loss = 1.270\n",
      "Epoch 2650 Batch   26/175   train_loss = 1.261\n",
      "Epoch 2650 Batch   58/175   train_loss = 1.281\n",
      "Epoch 2650 Batch   90/175   train_loss = 1.290\n",
      "Epoch 2650 Batch  122/175   train_loss = 1.251\n",
      "Epoch 2650 Batch  154/175   train_loss = 1.282\n",
      "Epoch 2651 Batch   11/175   train_loss = 1.221\n",
      "Epoch 2651 Batch   43/175   train_loss = 1.243\n",
      "Epoch 2651 Batch   75/175   train_loss = 1.254\n",
      "Epoch 2651 Batch  107/175   train_loss = 1.282\n",
      "Epoch 2651 Batch  139/175   train_loss = 1.189\n",
      "Epoch 2651 Batch  171/175   train_loss = 1.299\n",
      "Epoch 2652 Batch   28/175   train_loss = 1.244\n",
      "Epoch 2652 Batch   60/175   train_loss = 1.242\n",
      "Epoch 2652 Batch   92/175   train_loss = 1.204\n",
      "Epoch 2652 Batch  124/175   train_loss = 1.259\n",
      "Epoch 2652 Batch  156/175   train_loss = 1.296\n",
      "Epoch 2653 Batch   13/175   train_loss = 1.257\n",
      "Epoch 2653 Batch   45/175   train_loss = 1.244\n",
      "Epoch 2653 Batch   77/175   train_loss = 1.262\n",
      "Epoch 2653 Batch  109/175   train_loss = 1.290\n",
      "Epoch 2653 Batch  141/175   train_loss = 1.215\n",
      "Epoch 2653 Batch  173/175   train_loss = 1.222\n",
      "Epoch 2654 Batch   30/175   train_loss = 1.319\n",
      "Epoch 2654 Batch   62/175   train_loss = 1.274\n",
      "Epoch 2654 Batch   94/175   train_loss = 1.241\n",
      "Epoch 2654 Batch  126/175   train_loss = 1.290\n",
      "Epoch 2654 Batch  158/175   train_loss = 1.245\n",
      "Epoch 2655 Batch   15/175   train_loss = 1.280\n",
      "Epoch 2655 Batch   47/175   train_loss = 1.260\n",
      "Epoch 2655 Batch   79/175   train_loss = 1.296\n",
      "Epoch 2655 Batch  111/175   train_loss = 1.284\n",
      "Epoch 2655 Batch  143/175   train_loss = 1.238\n",
      "Epoch 2656 Batch    0/175   train_loss = 1.255\n",
      "Epoch 2656 Batch   32/175   train_loss = 1.271\n",
      "Epoch 2656 Batch   64/175   train_loss = 1.276\n",
      "Epoch 2656 Batch   96/175   train_loss = 1.294\n",
      "Epoch 2656 Batch  128/175   train_loss = 1.187\n",
      "Epoch 2656 Batch  160/175   train_loss = 1.251\n",
      "Epoch 2657 Batch   17/175   train_loss = 1.202\n",
      "Epoch 2657 Batch   49/175   train_loss = 1.272\n",
      "Epoch 2657 Batch   81/175   train_loss = 1.222\n",
      "Epoch 2657 Batch  113/175   train_loss = 1.263\n",
      "Epoch 2657 Batch  145/175   train_loss = 1.198\n",
      "Epoch 2658 Batch    2/175   train_loss = 1.257\n",
      "Epoch 2658 Batch   34/175   train_loss = 1.312\n",
      "Epoch 2658 Batch   66/175   train_loss = 1.295\n",
      "Epoch 2658 Batch   98/175   train_loss = 1.291\n",
      "Epoch 2658 Batch  130/175   train_loss = 1.268\n",
      "Epoch 2658 Batch  162/175   train_loss = 1.259\n",
      "Epoch 2659 Batch   19/175   train_loss = 1.241\n",
      "Epoch 2659 Batch   51/175   train_loss = 1.202\n",
      "Epoch 2659 Batch   83/175   train_loss = 1.287\n",
      "Epoch 2659 Batch  115/175   train_loss = 1.360\n",
      "Epoch 2659 Batch  147/175   train_loss = 1.187\n",
      "Epoch 2660 Batch    4/175   train_loss = 1.271\n",
      "Epoch 2660 Batch   36/175   train_loss = 1.193\n",
      "Epoch 2660 Batch   68/175   train_loss = 1.227\n",
      "Epoch 2660 Batch  100/175   train_loss = 1.234\n",
      "Epoch 2660 Batch  132/175   train_loss = 1.215\n",
      "Epoch 2660 Batch  164/175   train_loss = 1.223\n",
      "Epoch 2661 Batch   21/175   train_loss = 1.213\n",
      "Epoch 2661 Batch   53/175   train_loss = 1.220\n",
      "Epoch 2661 Batch   85/175   train_loss = 1.275\n",
      "Epoch 2661 Batch  117/175   train_loss = 1.278\n",
      "Epoch 2661 Batch  149/175   train_loss = 1.271\n",
      "Epoch 2662 Batch    6/175   train_loss = 1.272\n",
      "Epoch 2662 Batch   38/175   train_loss = 1.193\n",
      "Epoch 2662 Batch   70/175   train_loss = 1.248\n",
      "Epoch 2662 Batch  102/175   train_loss = 1.252\n",
      "Epoch 2662 Batch  134/175   train_loss = 1.219\n",
      "Epoch 2662 Batch  166/175   train_loss = 1.240\n",
      "Epoch 2663 Batch   23/175   train_loss = 1.198\n",
      "Epoch 2663 Batch   55/175   train_loss = 1.298\n",
      "Epoch 2663 Batch   87/175   train_loss = 1.326\n",
      "Epoch 2663 Batch  119/175   train_loss = 1.220\n",
      "Epoch 2663 Batch  151/175   train_loss = 1.232\n",
      "Epoch 2664 Batch    8/175   train_loss = 1.263\n",
      "Epoch 2664 Batch   40/175   train_loss = 1.233\n",
      "Epoch 2664 Batch   72/175   train_loss = 1.309\n",
      "Epoch 2664 Batch  104/175   train_loss = 1.278\n",
      "Epoch 2664 Batch  136/175   train_loss = 1.239\n",
      "Epoch 2664 Batch  168/175   train_loss = 1.267\n",
      "Epoch 2665 Batch   25/175   train_loss = 1.239\n",
      "Epoch 2665 Batch   57/175   train_loss = 1.263\n",
      "Epoch 2665 Batch   89/175   train_loss = 1.257\n",
      "Epoch 2665 Batch  121/175   train_loss = 1.212\n",
      "Epoch 2665 Batch  153/175   train_loss = 1.239\n",
      "Epoch 2666 Batch   10/175   train_loss = 1.211\n",
      "Epoch 2666 Batch   42/175   train_loss = 1.274\n",
      "Epoch 2666 Batch   74/175   train_loss = 1.284\n",
      "Epoch 2666 Batch  106/175   train_loss = 1.274\n",
      "Epoch 2666 Batch  138/175   train_loss = 1.255\n",
      "Epoch 2666 Batch  170/175   train_loss = 1.291\n",
      "Epoch 2667 Batch   27/175   train_loss = 1.247\n",
      "Epoch 2667 Batch   59/175   train_loss = 1.227\n",
      "Epoch 2667 Batch   91/175   train_loss = 1.233\n",
      "Epoch 2667 Batch  123/175   train_loss = 1.243\n",
      "Epoch 2667 Batch  155/175   train_loss = 1.228\n",
      "Epoch 2668 Batch   12/175   train_loss = 1.265\n",
      "Epoch 2668 Batch   44/175   train_loss = 1.237\n",
      "Epoch 2668 Batch   76/175   train_loss = 1.240\n",
      "Epoch 2668 Batch  108/175   train_loss = 1.276\n",
      "Epoch 2668 Batch  140/175   train_loss = 1.240\n",
      "Epoch 2668 Batch  172/175   train_loss = 1.280\n",
      "Epoch 2669 Batch   29/175   train_loss = 1.290\n",
      "Epoch 2669 Batch   61/175   train_loss = 1.314\n",
      "Epoch 2669 Batch   93/175   train_loss = 1.274\n",
      "Epoch 2669 Batch  125/175   train_loss = 1.280\n",
      "Epoch 2669 Batch  157/175   train_loss = 1.262\n",
      "Epoch 2670 Batch   14/175   train_loss = 1.253\n",
      "Epoch 2670 Batch   46/175   train_loss = 1.241\n",
      "Epoch 2670 Batch   78/175   train_loss = 1.208\n",
      "Epoch 2670 Batch  110/175   train_loss = 1.323\n",
      "Epoch 2670 Batch  142/175   train_loss = 1.262\n",
      "Epoch 2670 Batch  174/175   train_loss = 1.235\n",
      "Epoch 2671 Batch   31/175   train_loss = 1.286\n",
      "Epoch 2671 Batch   63/175   train_loss = 1.329\n",
      "Epoch 2671 Batch   95/175   train_loss = 1.287\n",
      "Epoch 2671 Batch  127/175   train_loss = 1.238\n",
      "Epoch 2671 Batch  159/175   train_loss = 1.226\n",
      "Epoch 2672 Batch   16/175   train_loss = 1.274\n",
      "Epoch 2672 Batch   48/175   train_loss = 1.256\n",
      "Epoch 2672 Batch   80/175   train_loss = 1.284\n",
      "Epoch 2672 Batch  112/175   train_loss = 1.248\n",
      "Epoch 2672 Batch  144/175   train_loss = 1.194\n",
      "Epoch 2673 Batch    1/175   train_loss = 1.291\n",
      "Epoch 2673 Batch   33/175   train_loss = 1.313\n",
      "Epoch 2673 Batch   65/175   train_loss = 1.242\n",
      "Epoch 2673 Batch   97/175   train_loss = 1.236\n",
      "Epoch 2673 Batch  129/175   train_loss = 1.242\n",
      "Epoch 2673 Batch  161/175   train_loss = 1.234\n",
      "Epoch 2674 Batch   18/175   train_loss = 1.206\n",
      "Epoch 2674 Batch   50/175   train_loss = 1.248\n",
      "Epoch 2674 Batch   82/175   train_loss = 1.271\n",
      "Epoch 2674 Batch  114/175   train_loss = 1.306\n",
      "Epoch 2674 Batch  146/175   train_loss = 1.239\n",
      "Epoch 2675 Batch    3/175   train_loss = 1.294\n",
      "Epoch 2675 Batch   35/175   train_loss = 1.226\n",
      "Epoch 2675 Batch   67/175   train_loss = 1.230\n",
      "Epoch 2675 Batch   99/175   train_loss = 1.326\n",
      "Epoch 2675 Batch  131/175   train_loss = 1.237\n",
      "Epoch 2675 Batch  163/175   train_loss = 1.254\n",
      "Epoch 2676 Batch   20/175   train_loss = 1.230\n",
      "Epoch 2676 Batch   52/175   train_loss = 1.210\n",
      "Epoch 2676 Batch   84/175   train_loss = 1.271\n",
      "Epoch 2676 Batch  116/175   train_loss = 1.294\n",
      "Epoch 2676 Batch  148/175   train_loss = 1.247\n",
      "Epoch 2677 Batch    5/175   train_loss = 1.238\n",
      "Epoch 2677 Batch   37/175   train_loss = 1.233\n",
      "Epoch 2677 Batch   69/175   train_loss = 1.269\n",
      "Epoch 2677 Batch  101/175   train_loss = 1.323\n",
      "Epoch 2677 Batch  133/175   train_loss = 1.139\n",
      "Epoch 2677 Batch  165/175   train_loss = 1.240\n",
      "Epoch 2678 Batch   22/175   train_loss = 1.199\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2678 Batch   54/175   train_loss = 1.303\n",
      "Epoch 2678 Batch   86/175   train_loss = 1.317\n",
      "Epoch 2678 Batch  118/175   train_loss = 1.296\n",
      "Epoch 2678 Batch  150/175   train_loss = 1.260\n",
      "Epoch 2679 Batch    7/175   train_loss = 1.270\n",
      "Epoch 2679 Batch   39/175   train_loss = 1.215\n",
      "Epoch 2679 Batch   71/175   train_loss = 1.307\n",
      "Epoch 2679 Batch  103/175   train_loss = 1.256\n",
      "Epoch 2679 Batch  135/175   train_loss = 1.223\n",
      "Epoch 2679 Batch  167/175   train_loss = 1.296\n",
      "Epoch 2680 Batch   24/175   train_loss = 1.205\n",
      "Epoch 2680 Batch   56/175   train_loss = 1.265\n",
      "Epoch 2680 Batch   88/175   train_loss = 1.278\n",
      "Epoch 2680 Batch  120/175   train_loss = 1.232\n",
      "Epoch 2680 Batch  152/175   train_loss = 1.223\n",
      "Epoch 2681 Batch    9/175   train_loss = 1.263\n",
      "Epoch 2681 Batch   41/175   train_loss = 1.294\n",
      "Epoch 2681 Batch   73/175   train_loss = 1.251\n",
      "Epoch 2681 Batch  105/175   train_loss = 1.304\n",
      "Epoch 2681 Batch  137/175   train_loss = 1.238\n",
      "Epoch 2681 Batch  169/175   train_loss = 1.267\n",
      "Epoch 2682 Batch   26/175   train_loss = 1.240\n",
      "Epoch 2682 Batch   58/175   train_loss = 1.301\n",
      "Epoch 2682 Batch   90/175   train_loss = 1.286\n",
      "Epoch 2682 Batch  122/175   train_loss = 1.211\n",
      "Epoch 2682 Batch  154/175   train_loss = 1.317\n",
      "Epoch 2683 Batch   11/175   train_loss = 1.214\n",
      "Epoch 2683 Batch   43/175   train_loss = 1.237\n",
      "Epoch 2683 Batch   75/175   train_loss = 1.225\n",
      "Epoch 2683 Batch  107/175   train_loss = 1.287\n",
      "Epoch 2683 Batch  139/175   train_loss = 1.197\n",
      "Epoch 2683 Batch  171/175   train_loss = 1.307\n",
      "Epoch 2684 Batch   28/175   train_loss = 1.260\n",
      "Epoch 2684 Batch   60/175   train_loss = 1.233\n",
      "Epoch 2684 Batch   92/175   train_loss = 1.207\n",
      "Epoch 2684 Batch  124/175   train_loss = 1.269\n",
      "Epoch 2684 Batch  156/175   train_loss = 1.301\n",
      "Epoch 2685 Batch   13/175   train_loss = 1.273\n",
      "Epoch 2685 Batch   45/175   train_loss = 1.248\n",
      "Epoch 2685 Batch   77/175   train_loss = 1.226\n",
      "Epoch 2685 Batch  109/175   train_loss = 1.292\n",
      "Epoch 2685 Batch  141/175   train_loss = 1.192\n",
      "Epoch 2685 Batch  173/175   train_loss = 1.214\n",
      "Epoch 2686 Batch   30/175   train_loss = 1.300\n",
      "Epoch 2686 Batch   62/175   train_loss = 1.263\n",
      "Epoch 2686 Batch   94/175   train_loss = 1.235\n",
      "Epoch 2686 Batch  126/175   train_loss = 1.264\n",
      "Epoch 2686 Batch  158/175   train_loss = 1.239\n",
      "Epoch 2687 Batch   15/175   train_loss = 1.277\n",
      "Epoch 2687 Batch   47/175   train_loss = 1.268\n",
      "Epoch 2687 Batch   79/175   train_loss = 1.297\n",
      "Epoch 2687 Batch  111/175   train_loss = 1.310\n",
      "Epoch 2687 Batch  143/175   train_loss = 1.218\n",
      "Epoch 2688 Batch    0/175   train_loss = 1.241\n",
      "Epoch 2688 Batch   32/175   train_loss = 1.280\n",
      "Epoch 2688 Batch   64/175   train_loss = 1.298\n",
      "Epoch 2688 Batch   96/175   train_loss = 1.269\n",
      "Epoch 2688 Batch  128/175   train_loss = 1.203\n",
      "Epoch 2688 Batch  160/175   train_loss = 1.272\n",
      "Epoch 2689 Batch   17/175   train_loss = 1.212\n",
      "Epoch 2689 Batch   49/175   train_loss = 1.250\n",
      "Epoch 2689 Batch   81/175   train_loss = 1.221\n",
      "Epoch 2689 Batch  113/175   train_loss = 1.270\n",
      "Epoch 2689 Batch  145/175   train_loss = 1.193\n",
      "Epoch 2690 Batch    2/175   train_loss = 1.221\n",
      "Epoch 2690 Batch   34/175   train_loss = 1.290\n",
      "Epoch 2690 Batch   66/175   train_loss = 1.307\n",
      "Epoch 2690 Batch   98/175   train_loss = 1.280\n",
      "Epoch 2690 Batch  130/175   train_loss = 1.293\n",
      "Epoch 2690 Batch  162/175   train_loss = 1.258\n",
      "Epoch 2691 Batch   19/175   train_loss = 1.251\n",
      "Epoch 2691 Batch   51/175   train_loss = 1.194\n",
      "Epoch 2691 Batch   83/175   train_loss = 1.305\n",
      "Epoch 2691 Batch  115/175   train_loss = 1.385\n",
      "Epoch 2691 Batch  147/175   train_loss = 1.205\n",
      "Epoch 2692 Batch    4/175   train_loss = 1.264\n",
      "Epoch 2692 Batch   36/175   train_loss = 1.197\n",
      "Epoch 2692 Batch   68/175   train_loss = 1.216\n",
      "Epoch 2692 Batch  100/175   train_loss = 1.267\n",
      "Epoch 2692 Batch  132/175   train_loss = 1.233\n",
      "Epoch 2692 Batch  164/175   train_loss = 1.227\n",
      "Epoch 2693 Batch   21/175   train_loss = 1.252\n",
      "Epoch 2693 Batch   53/175   train_loss = 1.242\n",
      "Epoch 2693 Batch   85/175   train_loss = 1.263\n",
      "Epoch 2693 Batch  117/175   train_loss = 1.267\n",
      "Epoch 2693 Batch  149/175   train_loss = 1.265\n",
      "Epoch 2694 Batch    6/175   train_loss = 1.287\n",
      "Epoch 2694 Batch   38/175   train_loss = 1.213\n",
      "Epoch 2694 Batch   70/175   train_loss = 1.248\n",
      "Epoch 2694 Batch  102/175   train_loss = 1.262\n",
      "Epoch 2694 Batch  134/175   train_loss = 1.214\n",
      "Epoch 2694 Batch  166/175   train_loss = 1.240\n",
      "Epoch 2695 Batch   23/175   train_loss = 1.195\n",
      "Epoch 2695 Batch   55/175   train_loss = 1.291\n",
      "Epoch 2695 Batch   87/175   train_loss = 1.339\n",
      "Epoch 2695 Batch  119/175   train_loss = 1.242\n",
      "Epoch 2695 Batch  151/175   train_loss = 1.266\n",
      "Epoch 2696 Batch    8/175   train_loss = 1.276\n",
      "Epoch 2696 Batch   40/175   train_loss = 1.253\n",
      "Epoch 2696 Batch   72/175   train_loss = 1.311\n",
      "Epoch 2696 Batch  104/175   train_loss = 1.271\n",
      "Epoch 2696 Batch  136/175   train_loss = 1.236\n",
      "Epoch 2696 Batch  168/175   train_loss = 1.308\n",
      "Epoch 2697 Batch   25/175   train_loss = 1.300\n",
      "Epoch 2697 Batch   57/175   train_loss = 1.311\n",
      "Epoch 2697 Batch   89/175   train_loss = 1.276\n",
      "Epoch 2697 Batch  121/175   train_loss = 1.215\n",
      "Epoch 2697 Batch  153/175   train_loss = 1.256\n",
      "Epoch 2698 Batch   10/175   train_loss = 1.216\n",
      "Epoch 2698 Batch   42/175   train_loss = 1.308\n",
      "Epoch 2698 Batch   74/175   train_loss = 1.295\n",
      "Epoch 2698 Batch  106/175   train_loss = 1.306\n",
      "Epoch 2698 Batch  138/175   train_loss = 1.251\n",
      "Epoch 2698 Batch  170/175   train_loss = 1.281\n",
      "Epoch 2699 Batch   27/175   train_loss = 1.229\n",
      "Epoch 2699 Batch   59/175   train_loss = 1.235\n",
      "Epoch 2699 Batch   91/175   train_loss = 1.209\n",
      "Epoch 2699 Batch  123/175   train_loss = 1.249\n",
      "Epoch 2699 Batch  155/175   train_loss = 1.222\n",
      "Epoch 2700 Batch   12/175   train_loss = 1.233\n",
      "Epoch 2700 Batch   44/175   train_loss = 1.222\n",
      "Epoch 2700 Batch   76/175   train_loss = 1.219\n",
      "Epoch 2700 Batch  108/175   train_loss = 1.266\n",
      "Epoch 2700 Batch  140/175   train_loss = 1.218\n",
      "Epoch 2700 Batch  172/175   train_loss = 1.300\n",
      "Epoch 2701 Batch   29/175   train_loss = 1.255\n",
      "Epoch 2701 Batch   61/175   train_loss = 1.279\n",
      "Epoch 2701 Batch   93/175   train_loss = 1.252\n",
      "Epoch 2701 Batch  125/175   train_loss = 1.247\n",
      "Epoch 2701 Batch  157/175   train_loss = 1.261\n",
      "Epoch 2702 Batch   14/175   train_loss = 1.251\n",
      "Epoch 2702 Batch   46/175   train_loss = 1.245\n",
      "Epoch 2702 Batch   78/175   train_loss = 1.232\n",
      "Epoch 2702 Batch  110/175   train_loss = 1.329\n",
      "Epoch 2702 Batch  142/175   train_loss = 1.281\n",
      "Epoch 2702 Batch  174/175   train_loss = 1.248\n",
      "Epoch 2703 Batch   31/175   train_loss = 1.306\n",
      "Epoch 2703 Batch   63/175   train_loss = 1.263\n",
      "Epoch 2703 Batch   95/175   train_loss = 1.239\n",
      "Epoch 2703 Batch  127/175   train_loss = 1.213\n",
      "Epoch 2703 Batch  159/175   train_loss = 1.212\n",
      "Epoch 2704 Batch   16/175   train_loss = 1.256\n",
      "Epoch 2704 Batch   48/175   train_loss = 1.257\n",
      "Epoch 2704 Batch   80/175   train_loss = 1.268\n",
      "Epoch 2704 Batch  112/175   train_loss = 1.264\n",
      "Epoch 2704 Batch  144/175   train_loss = 1.181\n",
      "Epoch 2705 Batch    1/175   train_loss = 1.274\n",
      "Epoch 2705 Batch   33/175   train_loss = 1.313\n",
      "Epoch 2705 Batch   65/175   train_loss = 1.226\n",
      "Epoch 2705 Batch   97/175   train_loss = 1.235\n",
      "Epoch 2705 Batch  129/175   train_loss = 1.238\n",
      "Epoch 2705 Batch  161/175   train_loss = 1.237\n",
      "Epoch 2706 Batch   18/175   train_loss = 1.215\n",
      "Epoch 2706 Batch   50/175   train_loss = 1.227\n",
      "Epoch 2706 Batch   82/175   train_loss = 1.290\n",
      "Epoch 2706 Batch  114/175   train_loss = 1.334\n",
      "Epoch 2706 Batch  146/175   train_loss = 1.253\n",
      "Epoch 2707 Batch    3/175   train_loss = 1.298\n",
      "Epoch 2707 Batch   35/175   train_loss = 1.257\n",
      "Epoch 2707 Batch   67/175   train_loss = 1.213\n",
      "Epoch 2707 Batch   99/175   train_loss = 1.316\n",
      "Epoch 2707 Batch  131/175   train_loss = 1.256\n",
      "Epoch 2707 Batch  163/175   train_loss = 1.250\n",
      "Epoch 2708 Batch   20/175   train_loss = 1.256\n",
      "Epoch 2708 Batch   52/175   train_loss = 1.203\n",
      "Epoch 2708 Batch   84/175   train_loss = 1.264\n",
      "Epoch 2708 Batch  116/175   train_loss = 1.336\n",
      "Epoch 2708 Batch  148/175   train_loss = 1.242\n",
      "Epoch 2709 Batch    5/175   train_loss = 1.248\n",
      "Epoch 2709 Batch   37/175   train_loss = 1.211\n",
      "Epoch 2709 Batch   69/175   train_loss = 1.286\n",
      "Epoch 2709 Batch  101/175   train_loss = 1.309\n",
      "Epoch 2709 Batch  133/175   train_loss = 1.152\n",
      "Epoch 2709 Batch  165/175   train_loss = 1.255\n",
      "Epoch 2710 Batch   22/175   train_loss = 1.203\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2710 Batch   54/175   train_loss = 1.280\n",
      "Epoch 2710 Batch   86/175   train_loss = 1.334\n",
      "Epoch 2710 Batch  118/175   train_loss = 1.307\n",
      "Epoch 2710 Batch  150/175   train_loss = 1.249\n",
      "Epoch 2711 Batch    7/175   train_loss = 1.287\n",
      "Epoch 2711 Batch   39/175   train_loss = 1.198\n",
      "Epoch 2711 Batch   71/175   train_loss = 1.290\n",
      "Epoch 2711 Batch  103/175   train_loss = 1.259\n",
      "Epoch 2711 Batch  135/175   train_loss = 1.258\n",
      "Epoch 2711 Batch  167/175   train_loss = 1.291\n",
      "Epoch 2712 Batch   24/175   train_loss = 1.204\n",
      "Epoch 2712 Batch   56/175   train_loss = 1.267\n",
      "Epoch 2712 Batch   88/175   train_loss = 1.310\n",
      "Epoch 2712 Batch  120/175   train_loss = 1.242\n",
      "Epoch 2712 Batch  152/175   train_loss = 1.242\n",
      "Epoch 2713 Batch    9/175   train_loss = 1.284\n",
      "Epoch 2713 Batch   41/175   train_loss = 1.296\n",
      "Epoch 2713 Batch   73/175   train_loss = 1.256\n",
      "Epoch 2713 Batch  105/175   train_loss = 1.294\n",
      "Epoch 2713 Batch  137/175   train_loss = 1.221\n",
      "Epoch 2713 Batch  169/175   train_loss = 1.271\n",
      "Epoch 2714 Batch   26/175   train_loss = 1.250\n",
      "Epoch 2714 Batch   58/175   train_loss = 1.289\n",
      "Epoch 2714 Batch   90/175   train_loss = 1.263\n",
      "Epoch 2714 Batch  122/175   train_loss = 1.239\n",
      "Epoch 2714 Batch  154/175   train_loss = 1.408\n",
      "Epoch 2715 Batch   11/175   train_loss = 1.257\n",
      "Epoch 2715 Batch   43/175   train_loss = 1.249\n",
      "Epoch 2715 Batch   75/175   train_loss = 1.227\n",
      "Epoch 2715 Batch  107/175   train_loss = 1.304\n",
      "Epoch 2715 Batch  139/175   train_loss = 1.197\n",
      "Epoch 2715 Batch  171/175   train_loss = 1.317\n",
      "Epoch 2716 Batch   28/175   train_loss = 1.245\n",
      "Epoch 2716 Batch   60/175   train_loss = 1.255\n",
      "Epoch 2716 Batch   92/175   train_loss = 1.213\n",
      "Epoch 2716 Batch  124/175   train_loss = 1.272\n",
      "Epoch 2716 Batch  156/175   train_loss = 1.309\n",
      "Epoch 2717 Batch   13/175   train_loss = 1.250\n",
      "Epoch 2717 Batch   45/175   train_loss = 1.258\n",
      "Epoch 2717 Batch   77/175   train_loss = 1.252\n",
      "Epoch 2717 Batch  109/175   train_loss = 1.300\n",
      "Epoch 2717 Batch  141/175   train_loss = 1.210\n",
      "Epoch 2717 Batch  173/175   train_loss = 1.237\n",
      "Epoch 2718 Batch   30/175   train_loss = 1.307\n",
      "Epoch 2718 Batch   62/175   train_loss = 1.278\n",
      "Epoch 2718 Batch   94/175   train_loss = 1.232\n",
      "Epoch 2718 Batch  126/175   train_loss = 1.257\n",
      "Epoch 2718 Batch  158/175   train_loss = 1.231\n",
      "Epoch 2719 Batch   15/175   train_loss = 1.285\n",
      "Epoch 2719 Batch   47/175   train_loss = 1.263\n",
      "Epoch 2719 Batch   79/175   train_loss = 1.296\n",
      "Epoch 2719 Batch  111/175   train_loss = 1.306\n",
      "Epoch 2719 Batch  143/175   train_loss = 1.250\n",
      "Epoch 2720 Batch    0/175   train_loss = 1.299\n",
      "Epoch 2720 Batch   32/175   train_loss = 1.293\n",
      "Epoch 2720 Batch   64/175   train_loss = 1.324\n",
      "Epoch 2720 Batch   96/175   train_loss = 1.298\n",
      "Epoch 2720 Batch  128/175   train_loss = 1.222\n",
      "Epoch 2720 Batch  160/175   train_loss = 1.266\n",
      "Epoch 2721 Batch   17/175   train_loss = 1.217\n",
      "Epoch 2721 Batch   49/175   train_loss = 1.267\n",
      "Epoch 2721 Batch   81/175   train_loss = 1.266\n",
      "Epoch 2721 Batch  113/175   train_loss = 1.291\n",
      "Epoch 2721 Batch  145/175   train_loss = 1.209\n",
      "Epoch 2722 Batch    2/175   train_loss = 1.274\n",
      "Epoch 2722 Batch   34/175   train_loss = 1.330\n",
      "Epoch 2722 Batch   66/175   train_loss = 1.313\n",
      "Epoch 2722 Batch   98/175   train_loss = 1.302\n",
      "Epoch 2722 Batch  130/175   train_loss = 1.293\n",
      "Epoch 2722 Batch  162/175   train_loss = 1.263\n",
      "Epoch 2723 Batch   19/175   train_loss = 1.275\n",
      "Epoch 2723 Batch   51/175   train_loss = 1.217\n",
      "Epoch 2723 Batch   83/175   train_loss = 1.312\n",
      "Epoch 2723 Batch  115/175   train_loss = 1.361\n",
      "Epoch 2723 Batch  147/175   train_loss = 1.214\n",
      "Epoch 2724 Batch    4/175   train_loss = 1.300\n",
      "Epoch 2724 Batch   36/175   train_loss = 1.233\n",
      "Epoch 2724 Batch   68/175   train_loss = 1.260\n",
      "Epoch 2724 Batch  100/175   train_loss = 1.289\n",
      "Epoch 2724 Batch  132/175   train_loss = 1.241\n",
      "Epoch 2724 Batch  164/175   train_loss = 1.231\n",
      "Epoch 2725 Batch   21/175   train_loss = 1.228\n",
      "Epoch 2725 Batch   53/175   train_loss = 1.243\n",
      "Epoch 2725 Batch   85/175   train_loss = 1.295\n",
      "Epoch 2725 Batch  117/175   train_loss = 1.273\n",
      "Epoch 2725 Batch  149/175   train_loss = 1.284\n",
      "Epoch 2726 Batch    6/175   train_loss = 1.311\n",
      "Epoch 2726 Batch   38/175   train_loss = 1.199\n",
      "Epoch 2726 Batch   70/175   train_loss = 1.269\n",
      "Epoch 2726 Batch  102/175   train_loss = 1.285\n",
      "Epoch 2726 Batch  134/175   train_loss = 1.227\n",
      "Epoch 2726 Batch  166/175   train_loss = 1.263\n",
      "Epoch 2727 Batch   23/175   train_loss = 1.188\n",
      "Epoch 2727 Batch   55/175   train_loss = 1.310\n",
      "Epoch 2727 Batch   87/175   train_loss = 1.338\n",
      "Epoch 2727 Batch  119/175   train_loss = 1.253\n",
      "Epoch 2727 Batch  151/175   train_loss = 1.271\n",
      "Epoch 2728 Batch    8/175   train_loss = 1.311\n",
      "Epoch 2728 Batch   40/175   train_loss = 1.237\n",
      "Epoch 2728 Batch   72/175   train_loss = 1.279\n",
      "Epoch 2728 Batch  104/175   train_loss = 1.260\n",
      "Epoch 2728 Batch  136/175   train_loss = 1.237\n",
      "Epoch 2728 Batch  168/175   train_loss = 1.275\n",
      "Epoch 2729 Batch   25/175   train_loss = 1.252\n",
      "Epoch 2729 Batch   57/175   train_loss = 1.318\n",
      "Epoch 2729 Batch   89/175   train_loss = 1.255\n",
      "Epoch 2729 Batch  121/175   train_loss = 1.232\n",
      "Epoch 2729 Batch  153/175   train_loss = 1.264\n",
      "Epoch 2730 Batch   10/175   train_loss = 1.201\n",
      "Epoch 2730 Batch   42/175   train_loss = 1.315\n",
      "Epoch 2730 Batch   74/175   train_loss = 1.302\n",
      "Epoch 2730 Batch  106/175   train_loss = 1.328\n",
      "Epoch 2730 Batch  138/175   train_loss = 1.271\n",
      "Epoch 2730 Batch  170/175   train_loss = 1.303\n",
      "Epoch 2731 Batch   27/175   train_loss = 1.240\n",
      "Epoch 2731 Batch   59/175   train_loss = 1.251\n",
      "Epoch 2731 Batch   91/175   train_loss = 1.236\n",
      "Epoch 2731 Batch  123/175   train_loss = 1.250\n",
      "Epoch 2731 Batch  155/175   train_loss = 1.223\n",
      "Epoch 2732 Batch   12/175   train_loss = 1.258\n",
      "Epoch 2732 Batch   44/175   train_loss = 1.221\n",
      "Epoch 2732 Batch   76/175   train_loss = 1.237\n",
      "Epoch 2732 Batch  108/175   train_loss = 1.257\n",
      "Epoch 2732 Batch  140/175   train_loss = 1.230\n",
      "Epoch 2732 Batch  172/175   train_loss = 1.286\n",
      "Epoch 2733 Batch   29/175   train_loss = 1.252\n",
      "Epoch 2733 Batch   61/175   train_loss = 1.286\n",
      "Epoch 2733 Batch   93/175   train_loss = 1.283\n",
      "Epoch 2733 Batch  125/175   train_loss = 1.289\n",
      "Epoch 2733 Batch  157/175   train_loss = 1.266\n",
      "Epoch 2734 Batch   14/175   train_loss = 1.283\n",
      "Epoch 2734 Batch   46/175   train_loss = 1.250\n",
      "Epoch 2734 Batch   78/175   train_loss = 1.237\n",
      "Epoch 2734 Batch  110/175   train_loss = 1.345\n",
      "Epoch 2734 Batch  142/175   train_loss = 1.281\n",
      "Epoch 2734 Batch  174/175   train_loss = 1.254\n",
      "Epoch 2735 Batch   31/175   train_loss = 1.310\n",
      "Epoch 2735 Batch   63/175   train_loss = 1.275\n",
      "Epoch 2735 Batch   95/175   train_loss = 1.255\n",
      "Epoch 2735 Batch  127/175   train_loss = 1.217\n",
      "Epoch 2735 Batch  159/175   train_loss = 1.222\n",
      "Epoch 2736 Batch   16/175   train_loss = 1.270\n",
      "Epoch 2736 Batch   48/175   train_loss = 1.287\n",
      "Epoch 2736 Batch   80/175   train_loss = 1.281\n",
      "Epoch 2736 Batch  112/175   train_loss = 1.255\n",
      "Epoch 2736 Batch  144/175   train_loss = 1.202\n",
      "Epoch 2737 Batch    1/175   train_loss = 1.302\n",
      "Epoch 2737 Batch   33/175   train_loss = 1.322\n",
      "Epoch 2737 Batch   65/175   train_loss = 1.274\n",
      "Epoch 2737 Batch   97/175   train_loss = 1.239\n",
      "Epoch 2737 Batch  129/175   train_loss = 1.262\n",
      "Epoch 2737 Batch  161/175   train_loss = 1.258\n",
      "Epoch 2738 Batch   18/175   train_loss = 1.236\n",
      "Epoch 2738 Batch   50/175   train_loss = 1.282\n",
      "Epoch 2738 Batch   82/175   train_loss = 1.316\n",
      "Epoch 2738 Batch  114/175   train_loss = 1.305\n",
      "Epoch 2738 Batch  146/175   train_loss = 1.239\n",
      "Epoch 2739 Batch    3/175   train_loss = 1.301\n",
      "Epoch 2739 Batch   35/175   train_loss = 1.254\n",
      "Epoch 2739 Batch   67/175   train_loss = 1.226\n",
      "Epoch 2739 Batch   99/175   train_loss = 1.315\n",
      "Epoch 2739 Batch  131/175   train_loss = 1.247\n",
      "Epoch 2739 Batch  163/175   train_loss = 1.277\n",
      "Epoch 2740 Batch   20/175   train_loss = 1.239\n",
      "Epoch 2740 Batch   52/175   train_loss = 1.207\n",
      "Epoch 2740 Batch   84/175   train_loss = 1.259\n",
      "Epoch 2740 Batch  116/175   train_loss = 1.290\n",
      "Epoch 2740 Batch  148/175   train_loss = 1.231\n",
      "Epoch 2741 Batch    5/175   train_loss = 1.240\n",
      "Epoch 2741 Batch   37/175   train_loss = 1.241\n",
      "Epoch 2741 Batch   69/175   train_loss = 1.265\n",
      "Epoch 2741 Batch  101/175   train_loss = 1.297\n",
      "Epoch 2741 Batch  133/175   train_loss = 1.151\n",
      "Epoch 2741 Batch  165/175   train_loss = 1.252\n",
      "Epoch 2742 Batch   22/175   train_loss = 1.217\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2742 Batch   54/175   train_loss = 1.266\n",
      "Epoch 2742 Batch   86/175   train_loss = 1.320\n",
      "Epoch 2742 Batch  118/175   train_loss = 1.314\n",
      "Epoch 2742 Batch  150/175   train_loss = 1.245\n",
      "Epoch 2743 Batch    7/175   train_loss = 1.259\n",
      "Epoch 2743 Batch   39/175   train_loss = 1.195\n",
      "Epoch 2743 Batch   71/175   train_loss = 1.307\n",
      "Epoch 2743 Batch  103/175   train_loss = 1.271\n",
      "Epoch 2743 Batch  135/175   train_loss = 1.241\n",
      "Epoch 2743 Batch  167/175   train_loss = 1.301\n",
      "Epoch 2744 Batch   24/175   train_loss = 1.213\n",
      "Epoch 2744 Batch   56/175   train_loss = 1.249\n",
      "Epoch 2744 Batch   88/175   train_loss = 1.302\n",
      "Epoch 2744 Batch  120/175   train_loss = 1.288\n",
      "Epoch 2744 Batch  152/175   train_loss = 1.242\n",
      "Epoch 2745 Batch    9/175   train_loss = 1.270\n",
      "Epoch 2745 Batch   41/175   train_loss = 1.282\n",
      "Epoch 2745 Batch   73/175   train_loss = 1.263\n",
      "Epoch 2745 Batch  105/175   train_loss = 1.311\n",
      "Epoch 2745 Batch  137/175   train_loss = 1.229\n",
      "Epoch 2745 Batch  169/175   train_loss = 1.270\n",
      "Epoch 2746 Batch   26/175   train_loss = 1.282\n",
      "Epoch 2746 Batch   58/175   train_loss = 1.286\n",
      "Epoch 2746 Batch   90/175   train_loss = 1.279\n",
      "Epoch 2746 Batch  122/175   train_loss = 1.263\n",
      "Epoch 2746 Batch  154/175   train_loss = 1.328\n",
      "Epoch 2747 Batch   11/175   train_loss = 1.236\n",
      "Epoch 2747 Batch   43/175   train_loss = 1.258\n",
      "Epoch 2747 Batch   75/175   train_loss = 1.211\n",
      "Epoch 2747 Batch  107/175   train_loss = 1.301\n",
      "Epoch 2747 Batch  139/175   train_loss = 1.198\n",
      "Epoch 2747 Batch  171/175   train_loss = 1.321\n",
      "Epoch 2748 Batch   28/175   train_loss = 1.257\n",
      "Epoch 2748 Batch   60/175   train_loss = 1.237\n",
      "Epoch 2748 Batch   92/175   train_loss = 1.220\n",
      "Epoch 2748 Batch  124/175   train_loss = 1.264\n",
      "Epoch 2748 Batch  156/175   train_loss = 1.301\n",
      "Epoch 2749 Batch   13/175   train_loss = 1.272\n",
      "Epoch 2749 Batch   45/175   train_loss = 1.264\n",
      "Epoch 2749 Batch   77/175   train_loss = 1.249\n",
      "Epoch 2749 Batch  109/175   train_loss = 1.284\n",
      "Epoch 2749 Batch  141/175   train_loss = 1.219\n",
      "Epoch 2749 Batch  173/175   train_loss = 1.240\n",
      "Epoch 2750 Batch   30/175   train_loss = 1.310\n",
      "Epoch 2750 Batch   62/175   train_loss = 1.291\n",
      "Epoch 2750 Batch   94/175   train_loss = 1.248\n",
      "Epoch 2750 Batch  126/175   train_loss = 1.267\n",
      "Epoch 2750 Batch  158/175   train_loss = 1.254\n",
      "Epoch 2751 Batch   15/175   train_loss = 1.277\n",
      "Epoch 2751 Batch   47/175   train_loss = 1.267\n",
      "Epoch 2751 Batch   79/175   train_loss = 1.268\n",
      "Epoch 2751 Batch  111/175   train_loss = 1.286\n",
      "Epoch 2751 Batch  143/175   train_loss = 1.243\n",
      "Epoch 2752 Batch    0/175   train_loss = 1.248\n",
      "Epoch 2752 Batch   32/175   train_loss = 1.277\n",
      "Epoch 2752 Batch   64/175   train_loss = 1.277\n",
      "Epoch 2752 Batch   96/175   train_loss = 1.285\n",
      "Epoch 2752 Batch  128/175   train_loss = 1.190\n",
      "Epoch 2752 Batch  160/175   train_loss = 1.261\n",
      "Epoch 2753 Batch   17/175   train_loss = 1.225\n",
      "Epoch 2753 Batch   49/175   train_loss = 1.283\n",
      "Epoch 2753 Batch   81/175   train_loss = 1.242\n",
      "Epoch 2753 Batch  113/175   train_loss = 1.273\n",
      "Epoch 2753 Batch  145/175   train_loss = 1.208\n",
      "Epoch 2754 Batch    2/175   train_loss = 1.231\n",
      "Epoch 2754 Batch   34/175   train_loss = 1.301\n",
      "Epoch 2754 Batch   66/175   train_loss = 1.302\n",
      "Epoch 2754 Batch   98/175   train_loss = 1.298\n",
      "Epoch 2754 Batch  130/175   train_loss = 1.286\n",
      "Epoch 2754 Batch  162/175   train_loss = 1.261\n",
      "Epoch 2755 Batch   19/175   train_loss = 1.235\n",
      "Epoch 2755 Batch   51/175   train_loss = 1.211\n",
      "Epoch 2755 Batch   83/175   train_loss = 1.297\n",
      "Epoch 2755 Batch  115/175   train_loss = 1.366\n",
      "Epoch 2755 Batch  147/175   train_loss = 1.207\n",
      "Epoch 2756 Batch    4/175   train_loss = 1.288\n",
      "Epoch 2756 Batch   36/175   train_loss = 1.204\n",
      "Epoch 2756 Batch   68/175   train_loss = 1.242\n",
      "Epoch 2756 Batch  100/175   train_loss = 1.262\n",
      "Epoch 2756 Batch  132/175   train_loss = 1.217\n",
      "Epoch 2756 Batch  164/175   train_loss = 1.291\n",
      "Epoch 2757 Batch   21/175   train_loss = 1.215\n",
      "Epoch 2757 Batch   53/175   train_loss = 1.237\n",
      "Epoch 2757 Batch   85/175   train_loss = 1.275\n",
      "Epoch 2757 Batch  117/175   train_loss = 1.290\n",
      "Epoch 2757 Batch  149/175   train_loss = 1.301\n",
      "Epoch 2758 Batch    6/175   train_loss = 1.303\n",
      "Epoch 2758 Batch   38/175   train_loss = 1.194\n",
      "Epoch 2758 Batch   70/175   train_loss = 1.244\n",
      "Epoch 2758 Batch  102/175   train_loss = 1.269\n",
      "Epoch 2758 Batch  134/175   train_loss = 1.251\n",
      "Epoch 2758 Batch  166/175   train_loss = 1.288\n",
      "Epoch 2759 Batch   23/175   train_loss = 1.197\n",
      "Epoch 2759 Batch   55/175   train_loss = 1.389\n",
      "Epoch 2759 Batch   87/175   train_loss = 1.366\n",
      "Epoch 2759 Batch  119/175   train_loss = 1.294\n",
      "Epoch 2759 Batch  151/175   train_loss = 1.278\n",
      "Epoch 2760 Batch    8/175   train_loss = 1.291\n",
      "Epoch 2760 Batch   40/175   train_loss = 1.248\n",
      "Epoch 2760 Batch   72/175   train_loss = 1.320\n",
      "Epoch 2760 Batch  104/175   train_loss = 1.301\n",
      "Epoch 2760 Batch  136/175   train_loss = 1.241\n",
      "Epoch 2760 Batch  168/175   train_loss = 1.292\n",
      "Epoch 2761 Batch   25/175   train_loss = 1.271\n",
      "Epoch 2761 Batch   57/175   train_loss = 1.293\n",
      "Epoch 2761 Batch   89/175   train_loss = 1.253\n",
      "Epoch 2761 Batch  121/175   train_loss = 1.244\n",
      "Epoch 2761 Batch  153/175   train_loss = 1.258\n",
      "Epoch 2762 Batch   10/175   train_loss = 1.215\n",
      "Epoch 2762 Batch   42/175   train_loss = 1.314\n",
      "Epoch 2762 Batch   74/175   train_loss = 1.296\n",
      "Epoch 2762 Batch  106/175   train_loss = 1.315\n",
      "Epoch 2762 Batch  138/175   train_loss = 1.258\n",
      "Epoch 2762 Batch  170/175   train_loss = 1.304\n",
      "Epoch 2763 Batch   27/175   train_loss = 1.271\n",
      "Epoch 2763 Batch   59/175   train_loss = 1.262\n",
      "Epoch 2763 Batch   91/175   train_loss = 1.251\n",
      "Epoch 2763 Batch  123/175   train_loss = 1.265\n",
      "Epoch 2763 Batch  155/175   train_loss = 1.228\n",
      "Epoch 2764 Batch   12/175   train_loss = 1.254\n",
      "Epoch 2764 Batch   44/175   train_loss = 1.223\n",
      "Epoch 2764 Batch   76/175   train_loss = 1.260\n",
      "Epoch 2764 Batch  108/175   train_loss = 1.283\n",
      "Epoch 2764 Batch  140/175   train_loss = 1.243\n",
      "Epoch 2764 Batch  172/175   train_loss = 1.298\n",
      "Epoch 2765 Batch   29/175   train_loss = 1.263\n",
      "Epoch 2765 Batch   61/175   train_loss = 1.295\n",
      "Epoch 2765 Batch   93/175   train_loss = 1.282\n",
      "Epoch 2765 Batch  125/175   train_loss = 1.295\n",
      "Epoch 2765 Batch  157/175   train_loss = 1.287\n",
      "Epoch 2766 Batch   14/175   train_loss = 1.268\n",
      "Epoch 2766 Batch   46/175   train_loss = 1.256\n",
      "Epoch 2766 Batch   78/175   train_loss = 1.212\n",
      "Epoch 2766 Batch  110/175   train_loss = 1.348\n",
      "Epoch 2766 Batch  142/175   train_loss = 1.284\n",
      "Epoch 2766 Batch  174/175   train_loss = 1.244\n",
      "Epoch 2767 Batch   31/175   train_loss = 1.279\n",
      "Epoch 2767 Batch   63/175   train_loss = 1.274\n",
      "Epoch 2767 Batch   95/175   train_loss = 1.264\n",
      "Epoch 2767 Batch  127/175   train_loss = 1.206\n",
      "Epoch 2767 Batch  159/175   train_loss = 1.229\n",
      "Epoch 2768 Batch   16/175   train_loss = 1.261\n",
      "Epoch 2768 Batch   48/175   train_loss = 1.284\n",
      "Epoch 2768 Batch   80/175   train_loss = 1.279\n",
      "Epoch 2768 Batch  112/175   train_loss = 1.252\n",
      "Epoch 2768 Batch  144/175   train_loss = 1.206\n",
      "Epoch 2769 Batch    1/175   train_loss = 1.278\n",
      "Epoch 2769 Batch   33/175   train_loss = 1.342\n",
      "Epoch 2769 Batch   65/175   train_loss = 1.290\n",
      "Epoch 2769 Batch   97/175   train_loss = 1.241\n",
      "Epoch 2769 Batch  129/175   train_loss = 1.271\n",
      "Epoch 2769 Batch  161/175   train_loss = 1.239\n",
      "Epoch 2770 Batch   18/175   train_loss = 1.232\n",
      "Epoch 2770 Batch   50/175   train_loss = 1.258\n",
      "Epoch 2770 Batch   82/175   train_loss = 1.296\n",
      "Epoch 2770 Batch  114/175   train_loss = 1.301\n",
      "Epoch 2770 Batch  146/175   train_loss = 1.249\n",
      "Epoch 2771 Batch    3/175   train_loss = 1.290\n",
      "Epoch 2771 Batch   35/175   train_loss = 1.258\n",
      "Epoch 2771 Batch   67/175   train_loss = 1.216\n",
      "Epoch 2771 Batch   99/175   train_loss = 1.326\n",
      "Epoch 2771 Batch  131/175   train_loss = 1.250\n",
      "Epoch 2771 Batch  163/175   train_loss = 1.253\n",
      "Epoch 2772 Batch   20/175   train_loss = 1.245\n",
      "Epoch 2772 Batch   52/175   train_loss = 1.217\n",
      "Epoch 2772 Batch   84/175   train_loss = 1.267\n",
      "Epoch 2772 Batch  116/175   train_loss = 1.298\n",
      "Epoch 2772 Batch  148/175   train_loss = 1.253\n",
      "Epoch 2773 Batch    5/175   train_loss = 1.271\n",
      "Epoch 2773 Batch   37/175   train_loss = 1.221\n",
      "Epoch 2773 Batch   69/175   train_loss = 1.283\n",
      "Epoch 2773 Batch  101/175   train_loss = 1.310\n",
      "Epoch 2773 Batch  133/175   train_loss = 1.159\n",
      "Epoch 2773 Batch  165/175   train_loss = 1.253\n",
      "Epoch 2774 Batch   22/175   train_loss = 1.213\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2774 Batch   54/175   train_loss = 1.281\n",
      "Epoch 2774 Batch   86/175   train_loss = 1.332\n",
      "Epoch 2774 Batch  118/175   train_loss = 1.307\n",
      "Epoch 2774 Batch  150/175   train_loss = 1.248\n",
      "Epoch 2775 Batch    7/175   train_loss = 1.275\n",
      "Epoch 2775 Batch   39/175   train_loss = 1.201\n",
      "Epoch 2775 Batch   71/175   train_loss = 1.302\n",
      "Epoch 2775 Batch  103/175   train_loss = 1.271\n",
      "Epoch 2775 Batch  135/175   train_loss = 1.227\n",
      "Epoch 2775 Batch  167/175   train_loss = 1.316\n",
      "Epoch 2776 Batch   24/175   train_loss = 1.205\n",
      "Epoch 2776 Batch   56/175   train_loss = 1.269\n",
      "Epoch 2776 Batch   88/175   train_loss = 1.309\n",
      "Epoch 2776 Batch  120/175   train_loss = 1.274\n",
      "Epoch 2776 Batch  152/175   train_loss = 1.248\n",
      "Epoch 2777 Batch    9/175   train_loss = 1.300\n",
      "Epoch 2777 Batch   41/175   train_loss = 1.301\n",
      "Epoch 2777 Batch   73/175   train_loss = 1.264\n",
      "Epoch 2777 Batch  105/175   train_loss = 1.313\n",
      "Epoch 2777 Batch  137/175   train_loss = 1.220\n",
      "Epoch 2777 Batch  169/175   train_loss = 1.289\n",
      "Epoch 2778 Batch   26/175   train_loss = 1.317\n",
      "Epoch 2778 Batch   58/175   train_loss = 1.313\n",
      "Epoch 2778 Batch   90/175   train_loss = 1.290\n",
      "Epoch 2778 Batch  122/175   train_loss = 1.242\n",
      "Epoch 2778 Batch  154/175   train_loss = 1.393\n",
      "Epoch 2779 Batch   11/175   train_loss = 1.250\n",
      "Epoch 2779 Batch   43/175   train_loss = 1.269\n",
      "Epoch 2779 Batch   75/175   train_loss = 1.233\n",
      "Epoch 2779 Batch  107/175   train_loss = 1.319\n",
      "Epoch 2779 Batch  139/175   train_loss = 1.203\n",
      "Epoch 2779 Batch  171/175   train_loss = 1.306\n",
      "Epoch 2780 Batch   28/175   train_loss = 1.294\n",
      "Epoch 2780 Batch   60/175   train_loss = 1.287\n",
      "Epoch 2780 Batch   92/175   train_loss = 1.246\n",
      "Epoch 2780 Batch  124/175   train_loss = 1.285\n",
      "Epoch 2780 Batch  156/175   train_loss = 1.337\n",
      "Epoch 2781 Batch   13/175   train_loss = 1.264\n",
      "Epoch 2781 Batch   45/175   train_loss = 1.253\n",
      "Epoch 2781 Batch   77/175   train_loss = 1.259\n",
      "Epoch 2781 Batch  109/175   train_loss = 1.306\n",
      "Epoch 2781 Batch  141/175   train_loss = 1.229\n",
      "Epoch 2781 Batch  173/175   train_loss = 1.248\n",
      "Epoch 2782 Batch   30/175   train_loss = 1.312\n",
      "Epoch 2782 Batch   62/175   train_loss = 1.267\n",
      "Epoch 2782 Batch   94/175   train_loss = 1.249\n",
      "Epoch 2782 Batch  126/175   train_loss = 1.260\n",
      "Epoch 2782 Batch  158/175   train_loss = 1.260\n",
      "Epoch 2783 Batch   15/175   train_loss = 1.307\n",
      "Epoch 2783 Batch   47/175   train_loss = 1.284\n",
      "Epoch 2783 Batch   79/175   train_loss = 1.352\n",
      "Epoch 2783 Batch  111/175   train_loss = 1.336\n",
      "Epoch 2783 Batch  143/175   train_loss = 1.261\n",
      "Epoch 2784 Batch    0/175   train_loss = 1.309\n",
      "Epoch 2784 Batch   32/175   train_loss = 1.340\n",
      "Epoch 2784 Batch   64/175   train_loss = 1.342\n",
      "Epoch 2784 Batch   96/175   train_loss = 1.314\n",
      "Epoch 2784 Batch  128/175   train_loss = 1.240\n",
      "Epoch 2784 Batch  160/175   train_loss = 1.276\n",
      "Epoch 2785 Batch   17/175   train_loss = 1.262\n",
      "Epoch 2785 Batch   49/175   train_loss = 1.304\n",
      "Epoch 2785 Batch   81/175   train_loss = 1.259\n",
      "Epoch 2785 Batch  113/175   train_loss = 1.291\n",
      "Epoch 2785 Batch  145/175   train_loss = 1.209\n",
      "Epoch 2786 Batch    2/175   train_loss = 1.243\n",
      "Epoch 2786 Batch   34/175   train_loss = 1.296\n",
      "Epoch 2786 Batch   66/175   train_loss = 1.293\n",
      "Epoch 2786 Batch   98/175   train_loss = 1.286\n",
      "Epoch 2786 Batch  130/175   train_loss = 1.331\n",
      "Epoch 2786 Batch  162/175   train_loss = 1.250\n",
      "Epoch 2787 Batch   19/175   train_loss = 1.243\n",
      "Epoch 2787 Batch   51/175   train_loss = 1.213\n",
      "Epoch 2787 Batch   83/175   train_loss = 1.314\n",
      "Epoch 2787 Batch  115/175   train_loss = 1.361\n",
      "Epoch 2787 Batch  147/175   train_loss = 1.208\n",
      "Epoch 2788 Batch    4/175   train_loss = 1.338\n",
      "Epoch 2788 Batch   36/175   train_loss = 1.215\n",
      "Epoch 2788 Batch   68/175   train_loss = 1.246\n",
      "Epoch 2788 Batch  100/175   train_loss = 1.277\n",
      "Epoch 2788 Batch  132/175   train_loss = 1.230\n",
      "Epoch 2788 Batch  164/175   train_loss = 1.288\n",
      "Epoch 2789 Batch   21/175   train_loss = 1.220\n",
      "Epoch 2789 Batch   53/175   train_loss = 1.259\n",
      "Epoch 2789 Batch   85/175   train_loss = 1.264\n",
      "Epoch 2789 Batch  117/175   train_loss = 1.295\n",
      "Epoch 2789 Batch  149/175   train_loss = 1.275\n",
      "Epoch 2790 Batch    6/175   train_loss = 1.291\n",
      "Epoch 2790 Batch   38/175   train_loss = 1.198\n",
      "Epoch 2790 Batch   70/175   train_loss = 1.231\n",
      "Epoch 2790 Batch  102/175   train_loss = 1.258\n",
      "Epoch 2790 Batch  134/175   train_loss = 1.210\n",
      "Epoch 2790 Batch  166/175   train_loss = 1.271\n",
      "Epoch 2791 Batch   23/175   train_loss = 1.196\n",
      "Epoch 2791 Batch   55/175   train_loss = 1.307\n",
      "Epoch 2791 Batch   87/175   train_loss = 1.352\n",
      "Epoch 2791 Batch  119/175   train_loss = 1.247\n",
      "Epoch 2791 Batch  151/175   train_loss = 1.263\n",
      "Epoch 2792 Batch    8/175   train_loss = 1.262\n",
      "Epoch 2792 Batch   40/175   train_loss = 1.234\n",
      "Epoch 2792 Batch   72/175   train_loss = 1.302\n",
      "Epoch 2792 Batch  104/175   train_loss = 1.304\n",
      "Epoch 2792 Batch  136/175   train_loss = 1.265\n",
      "Epoch 2792 Batch  168/175   train_loss = 1.308\n",
      "Epoch 2793 Batch   25/175   train_loss = 1.269\n",
      "Epoch 2793 Batch   57/175   train_loss = 1.297\n",
      "Epoch 2793 Batch   89/175   train_loss = 1.287\n",
      "Epoch 2793 Batch  121/175   train_loss = 1.230\n",
      "Epoch 2793 Batch  153/175   train_loss = 1.244\n",
      "Epoch 2794 Batch   10/175   train_loss = 1.207\n",
      "Epoch 2794 Batch   42/175   train_loss = 1.309\n",
      "Epoch 2794 Batch   74/175   train_loss = 1.304\n",
      "Epoch 2794 Batch  106/175   train_loss = 1.301\n",
      "Epoch 2794 Batch  138/175   train_loss = 1.286\n",
      "Epoch 2794 Batch  170/175   train_loss = 1.297\n",
      "Epoch 2795 Batch   27/175   train_loss = 1.240\n",
      "Epoch 2795 Batch   59/175   train_loss = 1.242\n",
      "Epoch 2795 Batch   91/175   train_loss = 1.225\n",
      "Epoch 2795 Batch  123/175   train_loss = 1.262\n",
      "Epoch 2795 Batch  155/175   train_loss = 1.243\n",
      "Epoch 2796 Batch   12/175   train_loss = 1.264\n",
      "Epoch 2796 Batch   44/175   train_loss = 1.229\n",
      "Epoch 2796 Batch   76/175   train_loss = 1.243\n",
      "Epoch 2796 Batch  108/175   train_loss = 1.262\n",
      "Epoch 2796 Batch  140/175   train_loss = 1.256\n",
      "Epoch 2796 Batch  172/175   train_loss = 1.303\n",
      "Epoch 2797 Batch   29/175   train_loss = 1.239\n",
      "Epoch 2797 Batch   61/175   train_loss = 1.291\n",
      "Epoch 2797 Batch   93/175   train_loss = 1.265\n",
      "Epoch 2797 Batch  125/175   train_loss = 1.273\n",
      "Epoch 2797 Batch  157/175   train_loss = 1.244\n",
      "Epoch 2798 Batch   14/175   train_loss = 1.291\n",
      "Epoch 2798 Batch   46/175   train_loss = 1.285\n",
      "Epoch 2798 Batch   78/175   train_loss = 1.238\n",
      "Epoch 2798 Batch  110/175   train_loss = 1.339\n",
      "Epoch 2798 Batch  142/175   train_loss = 1.280\n",
      "Epoch 2798 Batch  174/175   train_loss = 1.264\n",
      "Epoch 2799 Batch   31/175   train_loss = 1.285\n",
      "Epoch 2799 Batch   63/175   train_loss = 1.273\n",
      "Epoch 2799 Batch   95/175   train_loss = 1.248\n",
      "Epoch 2799 Batch  127/175   train_loss = 1.198\n",
      "Epoch 2799 Batch  159/175   train_loss = 1.221\n",
      "Epoch 2800 Batch   16/175   train_loss = 1.243\n",
      "Epoch 2800 Batch   48/175   train_loss = 1.260\n",
      "Epoch 2800 Batch   80/175   train_loss = 1.263\n",
      "Epoch 2800 Batch  112/175   train_loss = 1.314\n",
      "Epoch 2800 Batch  144/175   train_loss = 1.192\n",
      "Epoch 2801 Batch    1/175   train_loss = 1.333\n",
      "Epoch 2801 Batch   33/175   train_loss = 1.334\n",
      "Epoch 2801 Batch   65/175   train_loss = 1.258\n",
      "Epoch 2801 Batch   97/175   train_loss = 1.212\n",
      "Epoch 2801 Batch  129/175   train_loss = 1.244\n",
      "Epoch 2801 Batch  161/175   train_loss = 1.233\n",
      "Epoch 2802 Batch   18/175   train_loss = 1.215\n",
      "Epoch 2802 Batch   50/175   train_loss = 1.235\n",
      "Epoch 2802 Batch   82/175   train_loss = 1.273\n",
      "Epoch 2802 Batch  114/175   train_loss = 1.292\n",
      "Epoch 2802 Batch  146/175   train_loss = 1.230\n",
      "Epoch 2803 Batch    3/175   train_loss = 1.294\n",
      "Epoch 2803 Batch   35/175   train_loss = 1.257\n",
      "Epoch 2803 Batch   67/175   train_loss = 1.207\n",
      "Epoch 2803 Batch   99/175   train_loss = 1.318\n",
      "Epoch 2803 Batch  131/175   train_loss = 1.271\n",
      "Epoch 2803 Batch  163/175   train_loss = 1.283\n",
      "Epoch 2804 Batch   20/175   train_loss = 1.258\n",
      "Epoch 2804 Batch   52/175   train_loss = 1.196\n",
      "Epoch 2804 Batch   84/175   train_loss = 1.489\n",
      "Epoch 2804 Batch  116/175   train_loss = 1.394\n",
      "Epoch 2804 Batch  148/175   train_loss = 1.308\n",
      "Epoch 2805 Batch    5/175   train_loss = 1.267\n",
      "Epoch 2805 Batch   37/175   train_loss = 1.260\n",
      "Epoch 2805 Batch   69/175   train_loss = 1.281\n",
      "Epoch 2805 Batch  101/175   train_loss = 1.309\n",
      "Epoch 2805 Batch  133/175   train_loss = 1.164\n",
      "Epoch 2805 Batch  165/175   train_loss = 1.250\n",
      "Epoch 2806 Batch   22/175   train_loss = 1.207\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2806 Batch   54/175   train_loss = 1.303\n",
      "Epoch 2806 Batch   86/175   train_loss = 1.343\n",
      "Epoch 2806 Batch  118/175   train_loss = 1.314\n",
      "Epoch 2806 Batch  150/175   train_loss = 1.260\n",
      "Epoch 2807 Batch    7/175   train_loss = 1.275\n",
      "Epoch 2807 Batch   39/175   train_loss = 1.213\n",
      "Epoch 2807 Batch   71/175   train_loss = 1.293\n",
      "Epoch 2807 Batch  103/175   train_loss = 1.254\n",
      "Epoch 2807 Batch  135/175   train_loss = 1.245\n",
      "Epoch 2807 Batch  167/175   train_loss = 1.320\n",
      "Epoch 2808 Batch   24/175   train_loss = 1.274\n",
      "Epoch 2808 Batch   56/175   train_loss = 1.357\n",
      "Epoch 2808 Batch   88/175   train_loss = 1.349\n",
      "Epoch 2808 Batch  120/175   train_loss = 1.302\n",
      "Epoch 2808 Batch  152/175   train_loss = 1.274\n",
      "Epoch 2809 Batch    9/175   train_loss = 1.292\n",
      "Epoch 2809 Batch   41/175   train_loss = 1.334\n",
      "Epoch 2809 Batch   73/175   train_loss = 1.293\n",
      "Epoch 2809 Batch  105/175   train_loss = 1.319\n",
      "Epoch 2809 Batch  137/175   train_loss = 1.247\n",
      "Epoch 2809 Batch  169/175   train_loss = 1.282\n",
      "Epoch 2810 Batch   26/175   train_loss = 1.253\n",
      "Epoch 2810 Batch   58/175   train_loss = 1.299\n",
      "Epoch 2810 Batch   90/175   train_loss = 1.294\n",
      "Epoch 2810 Batch  122/175   train_loss = 1.260\n",
      "Epoch 2810 Batch  154/175   train_loss = 1.315\n",
      "Epoch 2811 Batch   11/175   train_loss = 1.247\n",
      "Epoch 2811 Batch   43/175   train_loss = 1.261\n",
      "Epoch 2811 Batch   75/175   train_loss = 1.284\n",
      "Epoch 2811 Batch  107/175   train_loss = 1.317\n",
      "Epoch 2811 Batch  139/175   train_loss = 1.222\n",
      "Epoch 2811 Batch  171/175   train_loss = 1.317\n",
      "Epoch 2812 Batch   28/175   train_loss = 1.268\n",
      "Epoch 2812 Batch   60/175   train_loss = 1.263\n",
      "Epoch 2812 Batch   92/175   train_loss = 1.258\n",
      "Epoch 2812 Batch  124/175   train_loss = 1.296\n",
      "Epoch 2812 Batch  156/175   train_loss = 1.333\n",
      "Epoch 2813 Batch   13/175   train_loss = 1.268\n",
      "Epoch 2813 Batch   45/175   train_loss = 1.279\n",
      "Epoch 2813 Batch   77/175   train_loss = 1.250\n",
      "Epoch 2813 Batch  109/175   train_loss = 1.303\n",
      "Epoch 2813 Batch  141/175   train_loss = 1.218\n",
      "Epoch 2813 Batch  173/175   train_loss = 1.231\n",
      "Epoch 2814 Batch   30/175   train_loss = 1.319\n",
      "Epoch 2814 Batch   62/175   train_loss = 1.291\n",
      "Epoch 2814 Batch   94/175   train_loss = 1.266\n",
      "Epoch 2814 Batch  126/175   train_loss = 1.275\n",
      "Epoch 2814 Batch  158/175   train_loss = 1.265\n",
      "Epoch 2815 Batch   15/175   train_loss = 1.301\n",
      "Epoch 2815 Batch   47/175   train_loss = 1.266\n",
      "Epoch 2815 Batch   79/175   train_loss = 1.292\n",
      "Epoch 2815 Batch  111/175   train_loss = 1.312\n",
      "Epoch 2815 Batch  143/175   train_loss = 1.231\n",
      "Epoch 2816 Batch    0/175   train_loss = 1.268\n",
      "Epoch 2816 Batch   32/175   train_loss = 1.286\n",
      "Epoch 2816 Batch   64/175   train_loss = 1.297\n",
      "Epoch 2816 Batch   96/175   train_loss = 1.277\n",
      "Epoch 2816 Batch  128/175   train_loss = 1.211\n",
      "Epoch 2816 Batch  160/175   train_loss = 1.246\n",
      "Epoch 2817 Batch   17/175   train_loss = 1.220\n",
      "Epoch 2817 Batch   49/175   train_loss = 1.297\n",
      "Epoch 2817 Batch   81/175   train_loss = 1.245\n",
      "Epoch 2817 Batch  113/175   train_loss = 1.287\n",
      "Epoch 2817 Batch  145/175   train_loss = 1.227\n",
      "Epoch 2818 Batch    2/175   train_loss = 1.272\n",
      "Epoch 2818 Batch   34/175   train_loss = 1.317\n",
      "Epoch 2818 Batch   66/175   train_loss = 1.289\n",
      "Epoch 2818 Batch   98/175   train_loss = 1.265\n",
      "Epoch 2818 Batch  130/175   train_loss = 1.267\n",
      "Epoch 2818 Batch  162/175   train_loss = 1.259\n",
      "Epoch 2819 Batch   19/175   train_loss = 1.263\n",
      "Epoch 2819 Batch   51/175   train_loss = 1.234\n",
      "Epoch 2819 Batch   83/175   train_loss = 1.308\n",
      "Epoch 2819 Batch  115/175   train_loss = 1.534\n",
      "Epoch 2819 Batch  147/175   train_loss = 1.265\n",
      "Epoch 2820 Batch    4/175   train_loss = 1.316\n",
      "Epoch 2820 Batch   36/175   train_loss = 1.249\n",
      "Epoch 2820 Batch   68/175   train_loss = 1.250\n",
      "Epoch 2820 Batch  100/175   train_loss = 1.302\n",
      "Epoch 2820 Batch  132/175   train_loss = 1.235\n",
      "Epoch 2820 Batch  164/175   train_loss = 1.226\n",
      "Epoch 2821 Batch   21/175   train_loss = 1.240\n",
      "Epoch 2821 Batch   53/175   train_loss = 1.254\n",
      "Epoch 2821 Batch   85/175   train_loss = 1.286\n",
      "Epoch 2821 Batch  117/175   train_loss = 1.287\n",
      "Epoch 2821 Batch  149/175   train_loss = 1.274\n",
      "Epoch 2822 Batch    6/175   train_loss = 1.305\n",
      "Epoch 2822 Batch   38/175   train_loss = 1.221\n",
      "Epoch 2822 Batch   70/175   train_loss = 1.268\n",
      "Epoch 2822 Batch  102/175   train_loss = 1.324\n",
      "Epoch 2822 Batch  134/175   train_loss = 1.278\n",
      "Epoch 2822 Batch  166/175   train_loss = 1.271\n",
      "Epoch 2823 Batch   23/175   train_loss = 1.181\n",
      "Epoch 2823 Batch   55/175   train_loss = 1.323\n",
      "Epoch 2823 Batch   87/175   train_loss = 1.307\n",
      "Epoch 2823 Batch  119/175   train_loss = 1.243\n",
      "Epoch 2823 Batch  151/175   train_loss = 1.253\n",
      "Epoch 2824 Batch    8/175   train_loss = 1.276\n",
      "Epoch 2824 Batch   40/175   train_loss = 1.224\n",
      "Epoch 2824 Batch   72/175   train_loss = 1.300\n",
      "Epoch 2824 Batch  104/175   train_loss = 1.269\n",
      "Epoch 2824 Batch  136/175   train_loss = 1.231\n",
      "Epoch 2824 Batch  168/175   train_loss = 1.272\n",
      "Epoch 2825 Batch   25/175   train_loss = 1.243\n",
      "Epoch 2825 Batch   57/175   train_loss = 1.280\n",
      "Epoch 2825 Batch   89/175   train_loss = 1.251\n",
      "Epoch 2825 Batch  121/175   train_loss = 1.225\n",
      "Epoch 2825 Batch  153/175   train_loss = 1.278\n",
      "Epoch 2826 Batch   10/175   train_loss = 1.196\n",
      "Epoch 2826 Batch   42/175   train_loss = 1.282\n",
      "Epoch 2826 Batch   74/175   train_loss = 1.296\n",
      "Epoch 2826 Batch  106/175   train_loss = 1.365\n",
      "Epoch 2826 Batch  138/175   train_loss = 1.259\n",
      "Epoch 2826 Batch  170/175   train_loss = 1.299\n",
      "Epoch 2827 Batch   27/175   train_loss = 1.245\n",
      "Epoch 2827 Batch   59/175   train_loss = 1.229\n",
      "Epoch 2827 Batch   91/175   train_loss = 1.231\n",
      "Epoch 2827 Batch  123/175   train_loss = 1.247\n",
      "Epoch 2827 Batch  155/175   train_loss = 1.217\n",
      "Epoch 2828 Batch   12/175   train_loss = 1.261\n",
      "Epoch 2828 Batch   44/175   train_loss = 1.239\n",
      "Epoch 2828 Batch   76/175   train_loss = 1.246\n",
      "Epoch 2828 Batch  108/175   train_loss = 1.253\n",
      "Epoch 2828 Batch  140/175   train_loss = 1.262\n",
      "Epoch 2828 Batch  172/175   train_loss = 1.289\n",
      "Epoch 2829 Batch   29/175   train_loss = 1.262\n",
      "Epoch 2829 Batch   61/175   train_loss = 1.292\n",
      "Epoch 2829 Batch   93/175   train_loss = 1.280\n",
      "Epoch 2829 Batch  125/175   train_loss = 1.281\n",
      "Epoch 2829 Batch  157/175   train_loss = 1.261\n",
      "Epoch 2830 Batch   14/175   train_loss = 1.259\n",
      "Epoch 2830 Batch   46/175   train_loss = 1.267\n",
      "Epoch 2830 Batch   78/175   train_loss = 1.246\n",
      "Epoch 2830 Batch  110/175   train_loss = 1.332\n",
      "Epoch 2830 Batch  142/175   train_loss = 1.289\n",
      "Epoch 2830 Batch  174/175   train_loss = 1.253\n",
      "Epoch 2831 Batch   31/175   train_loss = 1.306\n",
      "Epoch 2831 Batch   63/175   train_loss = 1.271\n",
      "Epoch 2831 Batch   95/175   train_loss = 1.257\n",
      "Epoch 2831 Batch  127/175   train_loss = 1.211\n",
      "Epoch 2831 Batch  159/175   train_loss = 1.213\n",
      "Epoch 2832 Batch   16/175   train_loss = 1.277\n",
      "Epoch 2832 Batch   48/175   train_loss = 1.253\n",
      "Epoch 2832 Batch   80/175   train_loss = 1.282\n",
      "Epoch 2832 Batch  112/175   train_loss = 1.265\n",
      "Epoch 2832 Batch  144/175   train_loss = 1.205\n",
      "Epoch 2833 Batch    1/175   train_loss = 1.295\n",
      "Epoch 2833 Batch   33/175   train_loss = 1.328\n",
      "Epoch 2833 Batch   65/175   train_loss = 1.265\n",
      "Epoch 2833 Batch   97/175   train_loss = 1.228\n",
      "Epoch 2833 Batch  129/175   train_loss = 1.248\n",
      "Epoch 2833 Batch  161/175   train_loss = 1.246\n",
      "Epoch 2834 Batch   18/175   train_loss = 1.230\n",
      "Epoch 2834 Batch   50/175   train_loss = 1.242\n",
      "Epoch 2834 Batch   82/175   train_loss = 1.282\n",
      "Epoch 2834 Batch  114/175   train_loss = 1.295\n",
      "Epoch 2834 Batch  146/175   train_loss = 1.243\n",
      "Epoch 2835 Batch    3/175   train_loss = 1.305\n",
      "Epoch 2835 Batch   35/175   train_loss = 1.257\n",
      "Epoch 2835 Batch   67/175   train_loss = 1.210\n",
      "Epoch 2835 Batch   99/175   train_loss = 1.329\n",
      "Epoch 2835 Batch  131/175   train_loss = 1.250\n",
      "Epoch 2835 Batch  163/175   train_loss = 1.257\n",
      "Epoch 2836 Batch   20/175   train_loss = 1.244\n",
      "Epoch 2836 Batch   52/175   train_loss = 1.191\n",
      "Epoch 2836 Batch   84/175   train_loss = 1.266\n",
      "Epoch 2836 Batch  116/175   train_loss = 1.301\n",
      "Epoch 2836 Batch  148/175   train_loss = 1.248\n",
      "Epoch 2837 Batch    5/175   train_loss = 1.266\n",
      "Epoch 2837 Batch   37/175   train_loss = 1.246\n",
      "Epoch 2837 Batch   69/175   train_loss = 1.282\n",
      "Epoch 2837 Batch  101/175   train_loss = 1.297\n",
      "Epoch 2837 Batch  133/175   train_loss = 1.176\n",
      "Epoch 2837 Batch  165/175   train_loss = 1.267\n",
      "Epoch 2838 Batch   22/175   train_loss = 1.254\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2838 Batch   54/175   train_loss = 1.318\n",
      "Epoch 2838 Batch   86/175   train_loss = 1.341\n",
      "Epoch 2838 Batch  118/175   train_loss = 1.325\n",
      "Epoch 2838 Batch  150/175   train_loss = 1.272\n",
      "Epoch 2839 Batch    7/175   train_loss = 1.290\n",
      "Epoch 2839 Batch   39/175   train_loss = 1.220\n",
      "Epoch 2839 Batch   71/175   train_loss = 1.291\n",
      "Epoch 2839 Batch  103/175   train_loss = 1.261\n",
      "Epoch 2839 Batch  135/175   train_loss = 1.226\n",
      "Epoch 2839 Batch  167/175   train_loss = 1.316\n",
      "Epoch 2840 Batch   24/175   train_loss = 1.216\n",
      "Epoch 2840 Batch   56/175   train_loss = 1.276\n",
      "Epoch 2840 Batch   88/175   train_loss = 1.319\n",
      "Epoch 2840 Batch  120/175   train_loss = 1.241\n",
      "Epoch 2840 Batch  152/175   train_loss = 1.241\n",
      "Epoch 2841 Batch    9/175   train_loss = 1.271\n",
      "Epoch 2841 Batch   41/175   train_loss = 1.299\n",
      "Epoch 2841 Batch   73/175   train_loss = 1.271\n",
      "Epoch 2841 Batch  105/175   train_loss = 1.298\n",
      "Epoch 2841 Batch  137/175   train_loss = 1.226\n",
      "Epoch 2841 Batch  169/175   train_loss = 1.269\n",
      "Epoch 2842 Batch   26/175   train_loss = 1.306\n",
      "Epoch 2842 Batch   58/175   train_loss = 1.303\n",
      "Epoch 2842 Batch   90/175   train_loss = 1.295\n",
      "Epoch 2842 Batch  122/175   train_loss = 1.259\n",
      "Epoch 2842 Batch  154/175   train_loss = 1.259\n",
      "Epoch 2843 Batch   11/175   train_loss = 1.244\n",
      "Epoch 2843 Batch   43/175   train_loss = 1.276\n",
      "Epoch 2843 Batch   75/175   train_loss = 1.252\n",
      "Epoch 2843 Batch  107/175   train_loss = 1.312\n",
      "Epoch 2843 Batch  139/175   train_loss = 1.206\n",
      "Epoch 2843 Batch  171/175   train_loss = 1.289\n",
      "Epoch 2844 Batch   28/175   train_loss = 1.263\n",
      "Epoch 2844 Batch   60/175   train_loss = 1.235\n",
      "Epoch 2844 Batch   92/175   train_loss = 1.225\n",
      "Epoch 2844 Batch  124/175   train_loss = 1.277\n",
      "Epoch 2844 Batch  156/175   train_loss = 1.291\n",
      "Epoch 2845 Batch   13/175   train_loss = 1.235\n",
      "Epoch 2845 Batch   45/175   train_loss = 1.266\n",
      "Epoch 2845 Batch   77/175   train_loss = 1.231\n",
      "Epoch 2845 Batch  109/175   train_loss = 1.297\n",
      "Epoch 2845 Batch  141/175   train_loss = 1.222\n",
      "Epoch 2845 Batch  173/175   train_loss = 1.253\n",
      "Epoch 2846 Batch   30/175   train_loss = 1.300\n",
      "Epoch 2846 Batch   62/175   train_loss = 1.256\n",
      "Epoch 2846 Batch   94/175   train_loss = 1.242\n",
      "Epoch 2846 Batch  126/175   train_loss = 1.277\n",
      "Epoch 2846 Batch  158/175   train_loss = 1.254\n",
      "Epoch 2847 Batch   15/175   train_loss = 1.296\n",
      "Epoch 2847 Batch   47/175   train_loss = 1.289\n",
      "Epoch 2847 Batch   79/175   train_loss = 1.299\n",
      "Epoch 2847 Batch  111/175   train_loss = 1.330\n",
      "Epoch 2847 Batch  143/175   train_loss = 1.250\n",
      "Epoch 2848 Batch    0/175   train_loss = 1.272\n",
      "Epoch 2848 Batch   32/175   train_loss = 1.284\n",
      "Epoch 2848 Batch   64/175   train_loss = 1.322\n",
      "Epoch 2848 Batch   96/175   train_loss = 1.332\n",
      "Epoch 2848 Batch  128/175   train_loss = 1.216\n",
      "Epoch 2848 Batch  160/175   train_loss = 1.266\n",
      "Epoch 2849 Batch   17/175   train_loss = 1.258\n",
      "Epoch 2849 Batch   49/175   train_loss = 1.303\n",
      "Epoch 2849 Batch   81/175   train_loss = 1.259\n",
      "Epoch 2849 Batch  113/175   train_loss = 1.306\n",
      "Epoch 2849 Batch  145/175   train_loss = 1.212\n",
      "Epoch 2850 Batch    2/175   train_loss = 1.244\n",
      "Epoch 2850 Batch   34/175   train_loss = 1.318\n",
      "Epoch 2850 Batch   66/175   train_loss = 1.298\n",
      "Epoch 2850 Batch   98/175   train_loss = 1.287\n",
      "Epoch 2850 Batch  130/175   train_loss = 1.291\n",
      "Epoch 2850 Batch  162/175   train_loss = 1.276\n",
      "Epoch 2851 Batch   19/175   train_loss = 1.283\n",
      "Epoch 2851 Batch   51/175   train_loss = 1.282\n",
      "Epoch 2851 Batch   83/175   train_loss = 1.329\n",
      "Epoch 2851 Batch  115/175   train_loss = 1.399\n",
      "Epoch 2851 Batch  147/175   train_loss = 1.242\n",
      "Epoch 2852 Batch    4/175   train_loss = 1.295\n",
      "Epoch 2852 Batch   36/175   train_loss = 1.224\n",
      "Epoch 2852 Batch   68/175   train_loss = 1.262\n",
      "Epoch 2852 Batch  100/175   train_loss = 1.290\n",
      "Epoch 2852 Batch  132/175   train_loss = 1.221\n",
      "Epoch 2852 Batch  164/175   train_loss = 1.251\n",
      "Epoch 2853 Batch   21/175   train_loss = 1.233\n",
      "Epoch 2853 Batch   53/175   train_loss = 1.246\n",
      "Epoch 2853 Batch   85/175   train_loss = 1.282\n",
      "Epoch 2853 Batch  117/175   train_loss = 1.279\n",
      "Epoch 2853 Batch  149/175   train_loss = 1.287\n",
      "Epoch 2854 Batch    6/175   train_loss = 1.288\n",
      "Epoch 2854 Batch   38/175   train_loss = 1.211\n",
      "Epoch 2854 Batch   70/175   train_loss = 1.244\n",
      "Epoch 2854 Batch  102/175   train_loss = 1.261\n",
      "Epoch 2854 Batch  134/175   train_loss = 1.258\n",
      "Epoch 2854 Batch  166/175   train_loss = 1.273\n",
      "Epoch 2855 Batch   23/175   train_loss = 1.179\n",
      "Epoch 2855 Batch   55/175   train_loss = 1.336\n",
      "Epoch 2855 Batch   87/175   train_loss = 1.350\n",
      "Epoch 2855 Batch  119/175   train_loss = 1.244\n",
      "Epoch 2855 Batch  151/175   train_loss = 1.261\n",
      "Epoch 2856 Batch    8/175   train_loss = 1.287\n",
      "Epoch 2856 Batch   40/175   train_loss = 1.226\n",
      "Epoch 2856 Batch   72/175   train_loss = 1.291\n",
      "Epoch 2856 Batch  104/175   train_loss = 1.282\n",
      "Epoch 2856 Batch  136/175   train_loss = 1.229\n",
      "Epoch 2856 Batch  168/175   train_loss = 1.271\n",
      "Epoch 2857 Batch   25/175   train_loss = 1.278\n",
      "Epoch 2857 Batch   57/175   train_loss = 1.299\n",
      "Epoch 2857 Batch   89/175   train_loss = 1.250\n",
      "Epoch 2857 Batch  121/175   train_loss = 1.214\n",
      "Epoch 2857 Batch  153/175   train_loss = 1.253\n",
      "Epoch 2858 Batch   10/175   train_loss = 1.202\n",
      "Epoch 2858 Batch   42/175   train_loss = 1.306\n",
      "Epoch 2858 Batch   74/175   train_loss = 1.312\n",
      "Epoch 2858 Batch  106/175   train_loss = 1.283\n",
      "Epoch 2858 Batch  138/175   train_loss = 1.232\n",
      "Epoch 2858 Batch  170/175   train_loss = 1.294\n",
      "Epoch 2859 Batch   27/175   train_loss = 1.245\n",
      "Epoch 2859 Batch   59/175   train_loss = 1.248\n",
      "Epoch 2859 Batch   91/175   train_loss = 1.241\n",
      "Epoch 2859 Batch  123/175   train_loss = 1.240\n",
      "Epoch 2859 Batch  155/175   train_loss = 1.237\n",
      "Epoch 2860 Batch   12/175   train_loss = 1.243\n",
      "Epoch 2860 Batch   44/175   train_loss = 1.228\n",
      "Epoch 2860 Batch   76/175   train_loss = 1.239\n",
      "Epoch 2860 Batch  108/175   train_loss = 1.249\n",
      "Epoch 2860 Batch  140/175   train_loss = 1.238\n",
      "Epoch 2860 Batch  172/175   train_loss = 1.286\n",
      "Epoch 2861 Batch   29/175   train_loss = 1.262\n",
      "Epoch 2861 Batch   61/175   train_loss = 1.310\n",
      "Epoch 2861 Batch   93/175   train_loss = 1.260\n",
      "Epoch 2861 Batch  125/175   train_loss = 1.264\n",
      "Epoch 2861 Batch  157/175   train_loss = 1.272\n",
      "Epoch 2862 Batch   14/175   train_loss = 1.274\n",
      "Epoch 2862 Batch   46/175   train_loss = 1.246\n",
      "Epoch 2862 Batch   78/175   train_loss = 1.209\n",
      "Epoch 2862 Batch  110/175   train_loss = 1.324\n",
      "Epoch 2862 Batch  142/175   train_loss = 1.282\n",
      "Epoch 2862 Batch  174/175   train_loss = 1.245\n",
      "Epoch 2863 Batch   31/175   train_loss = 1.276\n",
      "Epoch 2863 Batch   63/175   train_loss = 1.277\n",
      "Epoch 2863 Batch   95/175   train_loss = 1.250\n",
      "Epoch 2863 Batch  127/175   train_loss = 1.193\n",
      "Epoch 2863 Batch  159/175   train_loss = 1.198\n",
      "Epoch 2864 Batch   16/175   train_loss = 1.236\n",
      "Epoch 2864 Batch   48/175   train_loss = 1.256\n",
      "Epoch 2864 Batch   80/175   train_loss = 1.280\n",
      "Epoch 2864 Batch  112/175   train_loss = 1.275\n",
      "Epoch 2864 Batch  144/175   train_loss = 1.186\n",
      "Epoch 2865 Batch    1/175   train_loss = 1.299\n",
      "Epoch 2865 Batch   33/175   train_loss = 1.317\n",
      "Epoch 2865 Batch   65/175   train_loss = 1.285\n",
      "Epoch 2865 Batch   97/175   train_loss = 1.244\n",
      "Epoch 2865 Batch  129/175   train_loss = 1.252\n",
      "Epoch 2865 Batch  161/175   train_loss = 1.232\n",
      "Epoch 2866 Batch   18/175   train_loss = 1.208\n",
      "Epoch 2866 Batch   50/175   train_loss = 1.243\n",
      "Epoch 2866 Batch   82/175   train_loss = 1.297\n",
      "Epoch 2866 Batch  114/175   train_loss = 1.320\n",
      "Epoch 2866 Batch  146/175   train_loss = 1.257\n",
      "Epoch 2867 Batch    3/175   train_loss = 1.301\n",
      "Epoch 2867 Batch   35/175   train_loss = 1.248\n",
      "Epoch 2867 Batch   67/175   train_loss = 1.215\n",
      "Epoch 2867 Batch   99/175   train_loss = 1.297\n",
      "Epoch 2867 Batch  131/175   train_loss = 1.220\n",
      "Epoch 2867 Batch  163/175   train_loss = 1.244\n",
      "Epoch 2868 Batch   20/175   train_loss = 1.280\n",
      "Epoch 2868 Batch   52/175   train_loss = 1.186\n",
      "Epoch 2868 Batch   84/175   train_loss = 1.244\n",
      "Epoch 2868 Batch  116/175   train_loss = 1.319\n",
      "Epoch 2868 Batch  148/175   train_loss = 1.235\n",
      "Epoch 2869 Batch    5/175   train_loss = 1.254\n",
      "Epoch 2869 Batch   37/175   train_loss = 1.242\n",
      "Epoch 2869 Batch   69/175   train_loss = 1.275\n",
      "Epoch 2869 Batch  101/175   train_loss = 1.300\n",
      "Epoch 2869 Batch  133/175   train_loss = 1.180\n",
      "Epoch 2869 Batch  165/175   train_loss = 1.283\n",
      "Epoch 2870 Batch   22/175   train_loss = 1.206\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2870 Batch   54/175   train_loss = 1.336\n",
      "Epoch 2870 Batch   86/175   train_loss = 1.337\n",
      "Epoch 2870 Batch  118/175   train_loss = 1.330\n",
      "Epoch 2870 Batch  150/175   train_loss = 1.249\n",
      "Epoch 2871 Batch    7/175   train_loss = 1.289\n",
      "Epoch 2871 Batch   39/175   train_loss = 1.200\n",
      "Epoch 2871 Batch   71/175   train_loss = 1.305\n",
      "Epoch 2871 Batch  103/175   train_loss = 1.286\n",
      "Epoch 2871 Batch  135/175   train_loss = 1.299\n",
      "Epoch 2871 Batch  167/175   train_loss = 1.355\n",
      "Epoch 2872 Batch   24/175   train_loss = 1.220\n",
      "Epoch 2872 Batch   56/175   train_loss = 1.310\n",
      "Epoch 2872 Batch   88/175   train_loss = 1.332\n",
      "Epoch 2872 Batch  120/175   train_loss = 1.362\n",
      "Epoch 2872 Batch  152/175   train_loss = 1.309\n",
      "Epoch 2873 Batch    9/175   train_loss = 1.322\n",
      "Epoch 2873 Batch   41/175   train_loss = 1.323\n",
      "Epoch 2873 Batch   73/175   train_loss = 1.295\n",
      "Epoch 2873 Batch  105/175   train_loss = 1.333\n",
      "Epoch 2873 Batch  137/175   train_loss = 1.249\n",
      "Epoch 2873 Batch  169/175   train_loss = 1.279\n",
      "Epoch 2874 Batch   26/175   train_loss = 1.275\n",
      "Epoch 2874 Batch   58/175   train_loss = 1.292\n",
      "Epoch 2874 Batch   90/175   train_loss = 1.271\n",
      "Epoch 2874 Batch  122/175   train_loss = 1.244\n",
      "Epoch 2874 Batch  154/175   train_loss = 1.306\n",
      "Epoch 2875 Batch   11/175   train_loss = 1.228\n",
      "Epoch 2875 Batch   43/175   train_loss = 1.262\n",
      "Epoch 2875 Batch   75/175   train_loss = 1.254\n",
      "Epoch 2875 Batch  107/175   train_loss = 1.326\n",
      "Epoch 2875 Batch  139/175   train_loss = 1.214\n",
      "Epoch 2875 Batch  171/175   train_loss = 1.312\n",
      "Epoch 2876 Batch   28/175   train_loss = 1.276\n",
      "Epoch 2876 Batch   60/175   train_loss = 1.257\n",
      "Epoch 2876 Batch   92/175   train_loss = 1.228\n",
      "Epoch 2876 Batch  124/175   train_loss = 1.286\n",
      "Epoch 2876 Batch  156/175   train_loss = 1.316\n",
      "Epoch 2877 Batch   13/175   train_loss = 1.271\n",
      "Epoch 2877 Batch   45/175   train_loss = 1.277\n",
      "Epoch 2877 Batch   77/175   train_loss = 1.253\n",
      "Epoch 2877 Batch  109/175   train_loss = 1.324\n",
      "Epoch 2877 Batch  141/175   train_loss = 1.233\n",
      "Epoch 2877 Batch  173/175   train_loss = 1.254\n",
      "Epoch 2878 Batch   30/175   train_loss = 1.324\n",
      "Epoch 2878 Batch   62/175   train_loss = 1.291\n",
      "Epoch 2878 Batch   94/175   train_loss = 1.283\n",
      "Epoch 2878 Batch  126/175   train_loss = 1.277\n",
      "Epoch 2878 Batch  158/175   train_loss = 1.247\n",
      "Epoch 2879 Batch   15/175   train_loss = 1.310\n",
      "Epoch 2879 Batch   47/175   train_loss = 1.292\n",
      "Epoch 2879 Batch   79/175   train_loss = 1.310\n",
      "Epoch 2879 Batch  111/175   train_loss = 1.309\n",
      "Epoch 2879 Batch  143/175   train_loss = 1.253\n",
      "Epoch 2880 Batch    0/175   train_loss = 1.293\n",
      "Epoch 2880 Batch   32/175   train_loss = 1.297\n",
      "Epoch 2880 Batch   64/175   train_loss = 1.300\n",
      "Epoch 2880 Batch   96/175   train_loss = 1.281\n",
      "Epoch 2880 Batch  128/175   train_loss = 1.195\n",
      "Epoch 2880 Batch  160/175   train_loss = 1.246\n",
      "Epoch 2881 Batch   17/175   train_loss = 1.248\n",
      "Epoch 2881 Batch   49/175   train_loss = 1.271\n",
      "Epoch 2881 Batch   81/175   train_loss = 1.251\n",
      "Epoch 2881 Batch  113/175   train_loss = 1.302\n",
      "Epoch 2881 Batch  145/175   train_loss = 1.208\n",
      "Epoch 2882 Batch    2/175   train_loss = 1.262\n",
      "Epoch 2882 Batch   34/175   train_loss = 1.284\n",
      "Epoch 2882 Batch   66/175   train_loss = 1.281\n",
      "Epoch 2882 Batch   98/175   train_loss = 1.274\n",
      "Epoch 2882 Batch  130/175   train_loss = 1.275\n",
      "Epoch 2882 Batch  162/175   train_loss = 1.249\n",
      "Epoch 2883 Batch   19/175   train_loss = 1.268\n",
      "Epoch 2883 Batch   51/175   train_loss = 1.212\n",
      "Epoch 2883 Batch   83/175   train_loss = 1.317\n",
      "Epoch 2883 Batch  115/175   train_loss = 1.378\n",
      "Epoch 2883 Batch  147/175   train_loss = 1.205\n",
      "Epoch 2884 Batch    4/175   train_loss = 1.274\n",
      "Epoch 2884 Batch   36/175   train_loss = 1.236\n",
      "Epoch 2884 Batch   68/175   train_loss = 1.258\n",
      "Epoch 2884 Batch  100/175   train_loss = 1.281\n",
      "Epoch 2884 Batch  132/175   train_loss = 1.215\n",
      "Epoch 2884 Batch  164/175   train_loss = 1.276\n",
      "Epoch 2885 Batch   21/175   train_loss = 1.226\n",
      "Epoch 2885 Batch   53/175   train_loss = 1.227\n",
      "Epoch 2885 Batch   85/175   train_loss = 1.261\n",
      "Epoch 2885 Batch  117/175   train_loss = 1.273\n",
      "Epoch 2885 Batch  149/175   train_loss = 1.275\n",
      "Epoch 2886 Batch    6/175   train_loss = 1.280\n",
      "Epoch 2886 Batch   38/175   train_loss = 1.201\n",
      "Epoch 2886 Batch   70/175   train_loss = 1.251\n",
      "Epoch 2886 Batch  102/175   train_loss = 1.264\n",
      "Epoch 2886 Batch  134/175   train_loss = 1.266\n",
      "Epoch 2886 Batch  166/175   train_loss = 1.266\n",
      "Epoch 2887 Batch   23/175   train_loss = 1.215\n",
      "Epoch 2887 Batch   55/175   train_loss = 1.330\n",
      "Epoch 2887 Batch   87/175   train_loss = 1.364\n",
      "Epoch 2887 Batch  119/175   train_loss = 1.289\n",
      "Epoch 2887 Batch  151/175   train_loss = 1.293\n",
      "Epoch 2888 Batch    8/175   train_loss = 1.281\n",
      "Epoch 2888 Batch   40/175   train_loss = 1.238\n",
      "Epoch 2888 Batch   72/175   train_loss = 1.296\n",
      "Epoch 2888 Batch  104/175   train_loss = 1.284\n",
      "Epoch 2888 Batch  136/175   train_loss = 1.239\n",
      "Epoch 2888 Batch  168/175   train_loss = 1.278\n",
      "Epoch 2889 Batch   25/175   train_loss = 1.243\n",
      "Epoch 2889 Batch   57/175   train_loss = 1.301\n",
      "Epoch 2889 Batch   89/175   train_loss = 1.250\n",
      "Epoch 2889 Batch  121/175   train_loss = 1.233\n",
      "Epoch 2889 Batch  153/175   train_loss = 1.273\n",
      "Epoch 2890 Batch   10/175   train_loss = 1.221\n",
      "Epoch 2890 Batch   42/175   train_loss = 1.316\n",
      "Epoch 2890 Batch   74/175   train_loss = 1.290\n",
      "Epoch 2890 Batch  106/175   train_loss = 1.322\n",
      "Epoch 2890 Batch  138/175   train_loss = 1.254\n",
      "Epoch 2890 Batch  170/175   train_loss = 1.293\n",
      "Epoch 2891 Batch   27/175   train_loss = 1.252\n",
      "Epoch 2891 Batch   59/175   train_loss = 1.239\n",
      "Epoch 2891 Batch   91/175   train_loss = 1.227\n",
      "Epoch 2891 Batch  123/175   train_loss = 1.229\n",
      "Epoch 2891 Batch  155/175   train_loss = 1.208\n",
      "Epoch 2892 Batch   12/175   train_loss = 1.231\n",
      "Epoch 2892 Batch   44/175   train_loss = 1.231\n",
      "Epoch 2892 Batch   76/175   train_loss = 1.257\n",
      "Epoch 2892 Batch  108/175   train_loss = 1.272\n",
      "Epoch 2892 Batch  140/175   train_loss = 1.243\n",
      "Epoch 2892 Batch  172/175   train_loss = 1.269\n",
      "Epoch 2893 Batch   29/175   train_loss = 1.250\n",
      "Epoch 2893 Batch   61/175   train_loss = 1.291\n",
      "Epoch 2893 Batch   93/175   train_loss = 1.288\n",
      "Epoch 2893 Batch  125/175   train_loss = 1.275\n",
      "Epoch 2893 Batch  157/175   train_loss = 1.256\n",
      "Epoch 2894 Batch   14/175   train_loss = 1.253\n",
      "Epoch 2894 Batch   46/175   train_loss = 1.253\n",
      "Epoch 2894 Batch   78/175   train_loss = 1.248\n",
      "Epoch 2894 Batch  110/175   train_loss = 1.341\n",
      "Epoch 2894 Batch  142/175   train_loss = 1.279\n",
      "Epoch 2894 Batch  174/175   train_loss = 1.250\n",
      "Epoch 2895 Batch   31/175   train_loss = 1.288\n",
      "Epoch 2895 Batch   63/175   train_loss = 1.293\n",
      "Epoch 2895 Batch   95/175   train_loss = 1.243\n",
      "Epoch 2895 Batch  127/175   train_loss = 1.218\n",
      "Epoch 2895 Batch  159/175   train_loss = 1.231\n",
      "Epoch 2896 Batch   16/175   train_loss = 1.245\n",
      "Epoch 2896 Batch   48/175   train_loss = 1.282\n",
      "Epoch 2896 Batch   80/175   train_loss = 1.277\n",
      "Epoch 2896 Batch  112/175   train_loss = 1.256\n",
      "Epoch 2896 Batch  144/175   train_loss = 1.198\n",
      "Epoch 2897 Batch    1/175   train_loss = 1.298\n",
      "Epoch 2897 Batch   33/175   train_loss = 1.306\n",
      "Epoch 2897 Batch   65/175   train_loss = 1.259\n",
      "Epoch 2897 Batch   97/175   train_loss = 1.261\n",
      "Epoch 2897 Batch  129/175   train_loss = 1.248\n",
      "Epoch 2897 Batch  161/175   train_loss = 1.245\n",
      "Epoch 2898 Batch   18/175   train_loss = 1.244\n",
      "Epoch 2898 Batch   50/175   train_loss = 1.242\n",
      "Epoch 2898 Batch   82/175   train_loss = 1.293\n",
      "Epoch 2898 Batch  114/175   train_loss = 1.296\n",
      "Epoch 2898 Batch  146/175   train_loss = 1.239\n",
      "Epoch 2899 Batch    3/175   train_loss = 1.294\n",
      "Epoch 2899 Batch   35/175   train_loss = 1.247\n",
      "Epoch 2899 Batch   67/175   train_loss = 1.217\n",
      "Epoch 2899 Batch   99/175   train_loss = 1.336\n",
      "Epoch 2899 Batch  131/175   train_loss = 1.232\n",
      "Epoch 2899 Batch  163/175   train_loss = 1.244\n",
      "Epoch 2900 Batch   20/175   train_loss = 1.213\n",
      "Epoch 2900 Batch   52/175   train_loss = 1.184\n",
      "Epoch 2900 Batch   84/175   train_loss = 1.254\n",
      "Epoch 2900 Batch  116/175   train_loss = 1.282\n",
      "Epoch 2900 Batch  148/175   train_loss = 1.238\n",
      "Epoch 2901 Batch    5/175   train_loss = 1.229\n",
      "Epoch 2901 Batch   37/175   train_loss = 1.233\n",
      "Epoch 2901 Batch   69/175   train_loss = 1.257\n",
      "Epoch 2901 Batch  101/175   train_loss = 1.330\n",
      "Epoch 2901 Batch  133/175   train_loss = 1.166\n",
      "Epoch 2901 Batch  165/175   train_loss = 1.257\n",
      "Epoch 2902 Batch   22/175   train_loss = 1.197\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2902 Batch   54/175   train_loss = 1.331\n",
      "Epoch 2902 Batch   86/175   train_loss = 1.325\n",
      "Epoch 2902 Batch  118/175   train_loss = 1.299\n",
      "Epoch 2902 Batch  150/175   train_loss = 1.275\n",
      "Epoch 2903 Batch    7/175   train_loss = 1.277\n",
      "Epoch 2903 Batch   39/175   train_loss = 1.196\n",
      "Epoch 2903 Batch   71/175   train_loss = 1.286\n",
      "Epoch 2903 Batch  103/175   train_loss = 1.259\n",
      "Epoch 2903 Batch  135/175   train_loss = 1.230\n",
      "Epoch 2903 Batch  167/175   train_loss = 1.305\n",
      "Epoch 2904 Batch   24/175   train_loss = 1.200\n",
      "Epoch 2904 Batch   56/175   train_loss = 1.255\n",
      "Epoch 2904 Batch   88/175   train_loss = 1.303\n",
      "Epoch 2904 Batch  120/175   train_loss = 1.236\n",
      "Epoch 2904 Batch  152/175   train_loss = 1.222\n",
      "Epoch 2905 Batch    9/175   train_loss = 1.260\n",
      "Epoch 2905 Batch   41/175   train_loss = 1.295\n",
      "Epoch 2905 Batch   73/175   train_loss = 1.259\n",
      "Epoch 2905 Batch  105/175   train_loss = 1.284\n",
      "Epoch 2905 Batch  137/175   train_loss = 1.200\n",
      "Epoch 2905 Batch  169/175   train_loss = 1.264\n",
      "Epoch 2906 Batch   26/175   train_loss = 1.242\n",
      "Epoch 2906 Batch   58/175   train_loss = 1.292\n",
      "Epoch 2906 Batch   90/175   train_loss = 1.260\n",
      "Epoch 2906 Batch  122/175   train_loss = 1.218\n",
      "Epoch 2906 Batch  154/175   train_loss = 1.243\n",
      "Epoch 2907 Batch   11/175   train_loss = 1.208\n",
      "Epoch 2907 Batch   43/175   train_loss = 1.250\n",
      "Epoch 2907 Batch   75/175   train_loss = 1.238\n",
      "Epoch 2907 Batch  107/175   train_loss = 1.307\n",
      "Epoch 2907 Batch  139/175   train_loss = 1.184\n",
      "Epoch 2907 Batch  171/175   train_loss = 1.293\n",
      "Epoch 2908 Batch   28/175   train_loss = 1.267\n",
      "Epoch 2908 Batch   60/175   train_loss = 1.254\n",
      "Epoch 2908 Batch   92/175   train_loss = 1.223\n",
      "Epoch 2908 Batch  124/175   train_loss = 1.262\n",
      "Epoch 2908 Batch  156/175   train_loss = 1.287\n",
      "Epoch 2909 Batch   13/175   train_loss = 1.225\n",
      "Epoch 2909 Batch   45/175   train_loss = 1.233\n",
      "Epoch 2909 Batch   77/175   train_loss = 1.251\n",
      "Epoch 2909 Batch  109/175   train_loss = 1.285\n",
      "Epoch 2909 Batch  141/175   train_loss = 1.201\n",
      "Epoch 2909 Batch  173/175   train_loss = 1.218\n",
      "Epoch 2910 Batch   30/175   train_loss = 1.305\n",
      "Epoch 2910 Batch   62/175   train_loss = 1.265\n",
      "Epoch 2910 Batch   94/175   train_loss = 1.270\n",
      "Epoch 2910 Batch  126/175   train_loss = 1.281\n",
      "Epoch 2910 Batch  158/175   train_loss = 1.250\n",
      "Epoch 2911 Batch   15/175   train_loss = 1.288\n",
      "Epoch 2911 Batch   47/175   train_loss = 1.251\n",
      "Epoch 2911 Batch   79/175   train_loss = 1.295\n",
      "Epoch 2911 Batch  111/175   train_loss = 1.313\n",
      "Epoch 2911 Batch  143/175   train_loss = 1.264\n",
      "Epoch 2912 Batch    0/175   train_loss = 1.246\n",
      "Epoch 2912 Batch   32/175   train_loss = 1.282\n",
      "Epoch 2912 Batch   64/175   train_loss = 1.306\n",
      "Epoch 2912 Batch   96/175   train_loss = 1.281\n",
      "Epoch 2912 Batch  128/175   train_loss = 1.205\n",
      "Epoch 2912 Batch  160/175   train_loss = 1.258\n",
      "Epoch 2913 Batch   17/175   train_loss = 1.257\n",
      "Epoch 2913 Batch   49/175   train_loss = 1.372\n",
      "Epoch 2913 Batch   81/175   train_loss = 1.301\n",
      "Epoch 2913 Batch  113/175   train_loss = 1.312\n",
      "Epoch 2913 Batch  145/175   train_loss = 1.232\n",
      "Epoch 2914 Batch    2/175   train_loss = 1.298\n",
      "Epoch 2914 Batch   34/175   train_loss = 1.339\n",
      "Epoch 2914 Batch   66/175   train_loss = 1.321\n",
      "Epoch 2914 Batch   98/175   train_loss = 1.312\n",
      "Epoch 2914 Batch  130/175   train_loss = 1.308\n",
      "Epoch 2914 Batch  162/175   train_loss = 1.305\n",
      "Epoch 2915 Batch   19/175   train_loss = 1.269\n",
      "Epoch 2915 Batch   51/175   train_loss = 1.227\n",
      "Epoch 2915 Batch   83/175   train_loss = 1.311\n",
      "Epoch 2915 Batch  115/175   train_loss = 1.381\n",
      "Epoch 2915 Batch  147/175   train_loss = 1.236\n",
      "Epoch 2916 Batch    4/175   train_loss = 1.281\n",
      "Epoch 2916 Batch   36/175   train_loss = 1.218\n",
      "Epoch 2916 Batch   68/175   train_loss = 1.255\n",
      "Epoch 2916 Batch  100/175   train_loss = 1.294\n",
      "Epoch 2916 Batch  132/175   train_loss = 1.221\n",
      "Epoch 2916 Batch  164/175   train_loss = 1.246\n",
      "Epoch 2917 Batch   21/175   train_loss = 1.228\n",
      "Epoch 2917 Batch   53/175   train_loss = 1.259\n",
      "Epoch 2917 Batch   85/175   train_loss = 1.288\n",
      "Epoch 2917 Batch  117/175   train_loss = 1.285\n",
      "Epoch 2917 Batch  149/175   train_loss = 1.328\n",
      "Epoch 2918 Batch    6/175   train_loss = 1.400\n",
      "Epoch 2918 Batch   38/175   train_loss = 1.257\n",
      "Epoch 2918 Batch   70/175   train_loss = 1.285\n",
      "Epoch 2918 Batch  102/175   train_loss = 1.305\n",
      "Epoch 2918 Batch  134/175   train_loss = 1.246\n",
      "Epoch 2918 Batch  166/175   train_loss = 1.303\n",
      "Epoch 2919 Batch   23/175   train_loss = 1.216\n",
      "Epoch 2919 Batch   55/175   train_loss = 1.338\n",
      "Epoch 2919 Batch   87/175   train_loss = 1.342\n",
      "Epoch 2919 Batch  119/175   train_loss = 1.253\n",
      "Epoch 2919 Batch  151/175   train_loss = 1.295\n",
      "Epoch 2920 Batch    8/175   train_loss = 1.318\n",
      "Epoch 2920 Batch   40/175   train_loss = 1.249\n",
      "Epoch 2920 Batch   72/175   train_loss = 1.300\n",
      "Epoch 2920 Batch  104/175   train_loss = 1.302\n",
      "Epoch 2920 Batch  136/175   train_loss = 1.239\n",
      "Epoch 2920 Batch  168/175   train_loss = 1.290\n",
      "Epoch 2921 Batch   25/175   train_loss = 1.250\n",
      "Epoch 2921 Batch   57/175   train_loss = 1.304\n",
      "Epoch 2921 Batch   89/175   train_loss = 1.267\n",
      "Epoch 2921 Batch  121/175   train_loss = 1.265\n",
      "Epoch 2921 Batch  153/175   train_loss = 1.304\n",
      "Epoch 2922 Batch   10/175   train_loss = 1.258\n",
      "Epoch 2922 Batch   42/175   train_loss = 1.341\n",
      "Epoch 2922 Batch   74/175   train_loss = 1.312\n",
      "Epoch 2922 Batch  106/175   train_loss = 1.304\n",
      "Epoch 2922 Batch  138/175   train_loss = 1.270\n",
      "Epoch 2922 Batch  170/175   train_loss = 1.331\n",
      "Epoch 2923 Batch   27/175   train_loss = 1.256\n",
      "Epoch 2923 Batch   59/175   train_loss = 1.257\n",
      "Epoch 2923 Batch   91/175   train_loss = 1.237\n",
      "Epoch 2923 Batch  123/175   train_loss = 1.248\n",
      "Epoch 2923 Batch  155/175   train_loss = 1.232\n",
      "Epoch 2924 Batch   12/175   train_loss = 1.266\n",
      "Epoch 2924 Batch   44/175   train_loss = 1.220\n",
      "Epoch 2924 Batch   76/175   train_loss = 1.231\n",
      "Epoch 2924 Batch  108/175   train_loss = 1.270\n",
      "Epoch 2924 Batch  140/175   train_loss = 1.236\n",
      "Epoch 2924 Batch  172/175   train_loss = 1.291\n",
      "Epoch 2925 Batch   29/175   train_loss = 1.275\n",
      "Epoch 2925 Batch   61/175   train_loss = 1.278\n",
      "Epoch 2925 Batch   93/175   train_loss = 1.305\n",
      "Epoch 2925 Batch  125/175   train_loss = 1.275\n",
      "Epoch 2925 Batch  157/175   train_loss = 1.251\n",
      "Epoch 2926 Batch   14/175   train_loss = 1.250\n",
      "Epoch 2926 Batch   46/175   train_loss = 1.246\n",
      "Epoch 2926 Batch   78/175   train_loss = 1.238\n",
      "Epoch 2926 Batch  110/175   train_loss = 1.332\n",
      "Epoch 2926 Batch  142/175   train_loss = 1.267\n",
      "Epoch 2926 Batch  174/175   train_loss = 1.238\n",
      "Epoch 2927 Batch   31/175   train_loss = 1.275\n",
      "Epoch 2927 Batch   63/175   train_loss = 1.256\n",
      "Epoch 2927 Batch   95/175   train_loss = 1.236\n",
      "Epoch 2927 Batch  127/175   train_loss = 1.176\n",
      "Epoch 2927 Batch  159/175   train_loss = 1.199\n",
      "Epoch 2928 Batch   16/175   train_loss = 1.260\n",
      "Epoch 2928 Batch   48/175   train_loss = 1.245\n",
      "Epoch 2928 Batch   80/175   train_loss = 1.265\n",
      "Epoch 2928 Batch  112/175   train_loss = 1.229\n",
      "Epoch 2928 Batch  144/175   train_loss = 1.189\n",
      "Epoch 2929 Batch    1/175   train_loss = 1.274\n",
      "Epoch 2929 Batch   33/175   train_loss = 1.296\n",
      "Epoch 2929 Batch   65/175   train_loss = 1.235\n",
      "Epoch 2929 Batch   97/175   train_loss = 1.228\n",
      "Epoch 2929 Batch  129/175   train_loss = 1.247\n",
      "Epoch 2929 Batch  161/175   train_loss = 1.222\n",
      "Epoch 2930 Batch   18/175   train_loss = 1.223\n",
      "Epoch 2930 Batch   50/175   train_loss = 1.235\n",
      "Epoch 2930 Batch   82/175   train_loss = 1.287\n",
      "Epoch 2930 Batch  114/175   train_loss = 1.300\n",
      "Epoch 2930 Batch  146/175   train_loss = 1.249\n",
      "Epoch 2931 Batch    3/175   train_loss = 1.274\n",
      "Epoch 2931 Batch   35/175   train_loss = 1.237\n",
      "Epoch 2931 Batch   67/175   train_loss = 1.224\n",
      "Epoch 2931 Batch   99/175   train_loss = 1.408\n",
      "Epoch 2931 Batch  131/175   train_loss = 1.255\n",
      "Epoch 2931 Batch  163/175   train_loss = 1.273\n",
      "Epoch 2932 Batch   20/175   train_loss = 1.234\n",
      "Epoch 2932 Batch   52/175   train_loss = 1.199\n",
      "Epoch 2932 Batch   84/175   train_loss = 1.280\n",
      "Epoch 2932 Batch  116/175   train_loss = 1.300\n",
      "Epoch 2932 Batch  148/175   train_loss = 1.246\n",
      "Epoch 2933 Batch    5/175   train_loss = 1.249\n",
      "Epoch 2933 Batch   37/175   train_loss = 1.223\n",
      "Epoch 2933 Batch   69/175   train_loss = 1.283\n",
      "Epoch 2933 Batch  101/175   train_loss = 1.308\n",
      "Epoch 2933 Batch  133/175   train_loss = 1.160\n",
      "Epoch 2933 Batch  165/175   train_loss = 1.249\n",
      "Epoch 2934 Batch   22/175   train_loss = 1.229\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2934 Batch   54/175   train_loss = 1.306\n",
      "Epoch 2934 Batch   86/175   train_loss = 1.316\n",
      "Epoch 2934 Batch  118/175   train_loss = 1.305\n",
      "Epoch 2934 Batch  150/175   train_loss = 1.248\n",
      "Epoch 2935 Batch    7/175   train_loss = 1.261\n",
      "Epoch 2935 Batch   39/175   train_loss = 1.216\n",
      "Epoch 2935 Batch   71/175   train_loss = 1.296\n",
      "Epoch 2935 Batch  103/175   train_loss = 1.254\n",
      "Epoch 2935 Batch  135/175   train_loss = 1.225\n",
      "Epoch 2935 Batch  167/175   train_loss = 1.309\n",
      "Epoch 2936 Batch   24/175   train_loss = 1.191\n",
      "Epoch 2936 Batch   56/175   train_loss = 1.303\n",
      "Epoch 2936 Batch   88/175   train_loss = 1.302\n",
      "Epoch 2936 Batch  120/175   train_loss = 1.240\n",
      "Epoch 2936 Batch  152/175   train_loss = 1.235\n",
      "Epoch 2937 Batch    9/175   train_loss = 1.267\n",
      "Epoch 2937 Batch   41/175   train_loss = 1.299\n",
      "Epoch 2937 Batch   73/175   train_loss = 1.303\n",
      "Epoch 2937 Batch  105/175   train_loss = 1.331\n",
      "Epoch 2937 Batch  137/175   train_loss = 1.239\n",
      "Epoch 2937 Batch  169/175   train_loss = 1.311\n",
      "Epoch 2938 Batch   26/175   train_loss = 1.265\n",
      "Epoch 2938 Batch   58/175   train_loss = 1.289\n",
      "Epoch 2938 Batch   90/175   train_loss = 1.312\n",
      "Epoch 2938 Batch  122/175   train_loss = 1.232\n",
      "Epoch 2938 Batch  154/175   train_loss = 1.241\n",
      "Epoch 2939 Batch   11/175   train_loss = 1.235\n",
      "Epoch 2939 Batch   43/175   train_loss = 1.274\n",
      "Epoch 2939 Batch   75/175   train_loss = 1.266\n",
      "Epoch 2939 Batch  107/175   train_loss = 1.311\n",
      "Epoch 2939 Batch  139/175   train_loss = 1.190\n",
      "Epoch 2939 Batch  171/175   train_loss = 1.287\n",
      "Epoch 2940 Batch   28/175   train_loss = 1.271\n",
      "Epoch 2940 Batch   60/175   train_loss = 1.271\n",
      "Epoch 2940 Batch   92/175   train_loss = 1.237\n",
      "Epoch 2940 Batch  124/175   train_loss = 1.272\n",
      "Epoch 2940 Batch  156/175   train_loss = 1.338\n",
      "Epoch 2941 Batch   13/175   train_loss = 1.256\n",
      "Epoch 2941 Batch   45/175   train_loss = 1.234\n",
      "Epoch 2941 Batch   77/175   train_loss = 1.234\n",
      "Epoch 2941 Batch  109/175   train_loss = 1.319\n",
      "Epoch 2941 Batch  141/175   train_loss = 1.231\n",
      "Epoch 2941 Batch  173/175   train_loss = 1.243\n",
      "Epoch 2942 Batch   30/175   train_loss = 1.337\n",
      "Epoch 2942 Batch   62/175   train_loss = 1.290\n",
      "Epoch 2942 Batch   94/175   train_loss = 1.260\n",
      "Epoch 2942 Batch  126/175   train_loss = 1.282\n",
      "Epoch 2942 Batch  158/175   train_loss = 1.270\n",
      "Epoch 2943 Batch   15/175   train_loss = 1.323\n",
      "Epoch 2943 Batch   47/175   train_loss = 1.289\n",
      "Epoch 2943 Batch   79/175   train_loss = 1.318\n",
      "Epoch 2943 Batch  111/175   train_loss = 1.354\n",
      "Epoch 2943 Batch  143/175   train_loss = 1.263\n",
      "Epoch 2944 Batch    0/175   train_loss = 1.277\n",
      "Epoch 2944 Batch   32/175   train_loss = 1.281\n",
      "Epoch 2944 Batch   64/175   train_loss = 1.300\n",
      "Epoch 2944 Batch   96/175   train_loss = 1.278\n",
      "Epoch 2944 Batch  128/175   train_loss = 1.194\n",
      "Epoch 2944 Batch  160/175   train_loss = 1.282\n",
      "Epoch 2945 Batch   17/175   train_loss = 1.264\n",
      "Epoch 2945 Batch   49/175   train_loss = 1.309\n",
      "Epoch 2945 Batch   81/175   train_loss = 1.268\n",
      "Epoch 2945 Batch  113/175   train_loss = 1.319\n",
      "Epoch 2945 Batch  145/175   train_loss = 1.221\n",
      "Epoch 2946 Batch    2/175   train_loss = 1.254\n",
      "Epoch 2946 Batch   34/175   train_loss = 1.292\n",
      "Epoch 2946 Batch   66/175   train_loss = 1.318\n",
      "Epoch 2946 Batch   98/175   train_loss = 1.327\n",
      "Epoch 2946 Batch  130/175   train_loss = 1.321\n",
      "Epoch 2946 Batch  162/175   train_loss = 1.290\n",
      "Epoch 2947 Batch   19/175   train_loss = 1.292\n",
      "Epoch 2947 Batch   51/175   train_loss = 1.259\n",
      "Epoch 2947 Batch   83/175   train_loss = 1.351\n",
      "Epoch 2947 Batch  115/175   train_loss = 1.374\n",
      "Epoch 2947 Batch  147/175   train_loss = 1.231\n",
      "Epoch 2948 Batch    4/175   train_loss = 1.299\n",
      "Epoch 2948 Batch   36/175   train_loss = 1.224\n",
      "Epoch 2948 Batch   68/175   train_loss = 1.276\n",
      "Epoch 2948 Batch  100/175   train_loss = 1.274\n",
      "Epoch 2948 Batch  132/175   train_loss = 1.236\n",
      "Epoch 2948 Batch  164/175   train_loss = 1.295\n",
      "Epoch 2949 Batch   21/175   train_loss = 1.239\n",
      "Epoch 2949 Batch   53/175   train_loss = 1.245\n",
      "Epoch 2949 Batch   85/175   train_loss = 1.289\n",
      "Epoch 2949 Batch  117/175   train_loss = 1.298\n",
      "Epoch 2949 Batch  149/175   train_loss = 1.287\n",
      "Epoch 2950 Batch    6/175   train_loss = 1.307\n",
      "Epoch 2950 Batch   38/175   train_loss = 1.222\n",
      "Epoch 2950 Batch   70/175   train_loss = 1.270\n",
      "Epoch 2950 Batch  102/175   train_loss = 1.305\n",
      "Epoch 2950 Batch  134/175   train_loss = 1.243\n",
      "Epoch 2950 Batch  166/175   train_loss = 1.265\n",
      "Epoch 2951 Batch   23/175   train_loss = 1.234\n",
      "Epoch 2951 Batch   55/175   train_loss = 1.352\n",
      "Epoch 2951 Batch   87/175   train_loss = 1.350\n",
      "Epoch 2951 Batch  119/175   train_loss = 1.272\n",
      "Epoch 2951 Batch  151/175   train_loss = 1.310\n",
      "Epoch 2952 Batch    8/175   train_loss = 1.306\n",
      "Epoch 2952 Batch   40/175   train_loss = 1.248\n",
      "Epoch 2952 Batch   72/175   train_loss = 1.323\n",
      "Epoch 2952 Batch  104/175   train_loss = 1.300\n",
      "Epoch 2952 Batch  136/175   train_loss = 1.262\n",
      "Epoch 2952 Batch  168/175   train_loss = 1.310\n",
      "Epoch 2953 Batch   25/175   train_loss = 1.254\n",
      "Epoch 2953 Batch   57/175   train_loss = 1.309\n",
      "Epoch 2953 Batch   89/175   train_loss = 1.267\n",
      "Epoch 2953 Batch  121/175   train_loss = 1.225\n",
      "Epoch 2953 Batch  153/175   train_loss = 1.291\n",
      "Epoch 2954 Batch   10/175   train_loss = 1.226\n",
      "Epoch 2954 Batch   42/175   train_loss = 1.328\n",
      "Epoch 2954 Batch   74/175   train_loss = 1.332\n",
      "Epoch 2954 Batch  106/175   train_loss = 1.288\n",
      "Epoch 2954 Batch  138/175   train_loss = 1.248\n",
      "Epoch 2954 Batch  170/175   train_loss = 1.294\n",
      "Epoch 2955 Batch   27/175   train_loss = 1.243\n",
      "Epoch 2955 Batch   59/175   train_loss = 1.255\n",
      "Epoch 2955 Batch   91/175   train_loss = 1.247\n",
      "Epoch 2955 Batch  123/175   train_loss = 1.226\n",
      "Epoch 2955 Batch  155/175   train_loss = 1.244\n",
      "Epoch 2956 Batch   12/175   train_loss = 1.284\n",
      "Epoch 2956 Batch   44/175   train_loss = 1.231\n",
      "Epoch 2956 Batch   76/175   train_loss = 1.245\n",
      "Epoch 2956 Batch  108/175   train_loss = 1.281\n",
      "Epoch 2956 Batch  140/175   train_loss = 1.248\n",
      "Epoch 2956 Batch  172/175   train_loss = 1.281\n",
      "Epoch 2957 Batch   29/175   train_loss = 1.326\n",
      "Epoch 2957 Batch   61/175   train_loss = 1.307\n",
      "Epoch 2957 Batch   93/175   train_loss = 1.293\n",
      "Epoch 2957 Batch  125/175   train_loss = 1.281\n",
      "Epoch 2957 Batch  157/175   train_loss = 1.260\n",
      "Epoch 2958 Batch   14/175   train_loss = 1.264\n",
      "Epoch 2958 Batch   46/175   train_loss = 1.253\n",
      "Epoch 2958 Batch   78/175   train_loss = 1.253\n",
      "Epoch 2958 Batch  110/175   train_loss = 1.362\n",
      "Epoch 2958 Batch  142/175   train_loss = 1.277\n",
      "Epoch 2958 Batch  174/175   train_loss = 1.245\n",
      "Epoch 2959 Batch   31/175   train_loss = 1.302\n",
      "Epoch 2959 Batch   63/175   train_loss = 1.259\n",
      "Epoch 2959 Batch   95/175   train_loss = 1.242\n",
      "Epoch 2959 Batch  127/175   train_loss = 1.220\n",
      "Epoch 2959 Batch  159/175   train_loss = 1.218\n",
      "Epoch 2960 Batch   16/175   train_loss = 1.258\n",
      "Epoch 2960 Batch   48/175   train_loss = 1.248\n",
      "Epoch 2960 Batch   80/175   train_loss = 1.276\n",
      "Epoch 2960 Batch  112/175   train_loss = 1.272\n",
      "Epoch 2960 Batch  144/175   train_loss = 1.193\n",
      "Epoch 2961 Batch    1/175   train_loss = 1.306\n",
      "Epoch 2961 Batch   33/175   train_loss = 1.301\n",
      "Epoch 2961 Batch   65/175   train_loss = 1.246\n",
      "Epoch 2961 Batch   97/175   train_loss = 1.241\n",
      "Epoch 2961 Batch  129/175   train_loss = 1.251\n",
      "Epoch 2961 Batch  161/175   train_loss = 1.275\n",
      "Epoch 2962 Batch   18/175   train_loss = 1.239\n",
      "Epoch 2962 Batch   50/175   train_loss = 1.256\n",
      "Epoch 2962 Batch   82/175   train_loss = 1.304\n",
      "Epoch 2962 Batch  114/175   train_loss = 1.343\n",
      "Epoch 2962 Batch  146/175   train_loss = 1.272\n",
      "Epoch 2963 Batch    3/175   train_loss = 1.274\n",
      "Epoch 2963 Batch   35/175   train_loss = 1.268\n",
      "Epoch 2963 Batch   67/175   train_loss = 1.257\n",
      "Epoch 2963 Batch   99/175   train_loss = 1.329\n",
      "Epoch 2963 Batch  131/175   train_loss = 1.254\n",
      "Epoch 2963 Batch  163/175   train_loss = 1.258\n",
      "Epoch 2964 Batch   20/175   train_loss = 1.261\n",
      "Epoch 2964 Batch   52/175   train_loss = 1.201\n",
      "Epoch 2964 Batch   84/175   train_loss = 1.264\n",
      "Epoch 2964 Batch  116/175   train_loss = 1.315\n",
      "Epoch 2964 Batch  148/175   train_loss = 1.271\n",
      "Epoch 2965 Batch    5/175   train_loss = 1.248\n",
      "Epoch 2965 Batch   37/175   train_loss = 1.229\n",
      "Epoch 2965 Batch   69/175   train_loss = 1.266\n",
      "Epoch 2965 Batch  101/175   train_loss = 1.318\n",
      "Epoch 2965 Batch  133/175   train_loss = 1.163\n",
      "Epoch 2965 Batch  165/175   train_loss = 1.247\n",
      "Epoch 2966 Batch   22/175   train_loss = 1.204\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2966 Batch   54/175   train_loss = 1.309\n",
      "Epoch 2966 Batch   86/175   train_loss = 1.335\n",
      "Epoch 2966 Batch  118/175   train_loss = 1.327\n",
      "Epoch 2966 Batch  150/175   train_loss = 1.269\n",
      "Epoch 2967 Batch    7/175   train_loss = 1.292\n",
      "Epoch 2967 Batch   39/175   train_loss = 1.223\n",
      "Epoch 2967 Batch   71/175   train_loss = 1.286\n",
      "Epoch 2967 Batch  103/175   train_loss = 1.278\n",
      "Epoch 2967 Batch  135/175   train_loss = 1.227\n",
      "Epoch 2967 Batch  167/175   train_loss = 1.315\n",
      "Epoch 2968 Batch   24/175   train_loss = 1.218\n",
      "Epoch 2968 Batch   56/175   train_loss = 1.290\n",
      "Epoch 2968 Batch   88/175   train_loss = 1.337\n",
      "Epoch 2968 Batch  120/175   train_loss = 1.264\n",
      "Epoch 2968 Batch  152/175   train_loss = 1.252\n",
      "Epoch 2969 Batch    9/175   train_loss = 1.287\n",
      "Epoch 2969 Batch   41/175   train_loss = 1.301\n",
      "Epoch 2969 Batch   73/175   train_loss = 1.279\n",
      "Epoch 2969 Batch  105/175   train_loss = 1.350\n",
      "Epoch 2969 Batch  137/175   train_loss = 1.244\n",
      "Epoch 2969 Batch  169/175   train_loss = 1.319\n",
      "Epoch 2970 Batch   26/175   train_loss = 1.266\n",
      "Epoch 2970 Batch   58/175   train_loss = 1.301\n",
      "Epoch 2970 Batch   90/175   train_loss = 1.310\n",
      "Epoch 2970 Batch  122/175   train_loss = 1.247\n",
      "Epoch 2970 Batch  154/175   train_loss = 1.285\n",
      "Epoch 2971 Batch   11/175   train_loss = 1.241\n",
      "Epoch 2971 Batch   43/175   train_loss = 1.275\n",
      "Epoch 2971 Batch   75/175   train_loss = 1.252\n",
      "Epoch 2971 Batch  107/175   train_loss = 1.463\n",
      "Epoch 2971 Batch  139/175   train_loss = 1.260\n",
      "Epoch 2971 Batch  171/175   train_loss = 1.382\n",
      "Epoch 2972 Batch   28/175   train_loss = 1.359\n",
      "Epoch 2972 Batch   60/175   train_loss = 1.319\n",
      "Epoch 2972 Batch   92/175   train_loss = 1.272\n",
      "Epoch 2972 Batch  124/175   train_loss = 1.293\n",
      "Epoch 2972 Batch  156/175   train_loss = 1.344\n",
      "Epoch 2973 Batch   13/175   train_loss = 1.280\n",
      "Epoch 2973 Batch   45/175   train_loss = 1.265\n",
      "Epoch 2973 Batch   77/175   train_loss = 1.260\n",
      "Epoch 2973 Batch  109/175   train_loss = 1.311\n",
      "Epoch 2973 Batch  141/175   train_loss = 1.238\n",
      "Epoch 2973 Batch  173/175   train_loss = 1.240\n",
      "Epoch 2974 Batch   30/175   train_loss = 1.334\n",
      "Epoch 2974 Batch   62/175   train_loss = 1.264\n",
      "Epoch 2974 Batch   94/175   train_loss = 1.267\n",
      "Epoch 2974 Batch  126/175   train_loss = 1.290\n",
      "Epoch 2974 Batch  158/175   train_loss = 1.259\n",
      "Epoch 2975 Batch   15/175   train_loss = 1.313\n",
      "Epoch 2975 Batch   47/175   train_loss = 1.320\n",
      "Epoch 2975 Batch   79/175   train_loss = 1.329\n",
      "Epoch 2975 Batch  111/175   train_loss = 1.342\n",
      "Epoch 2975 Batch  143/175   train_loss = 1.314\n",
      "Epoch 2976 Batch    0/175   train_loss = 1.317\n",
      "Epoch 2976 Batch   32/175   train_loss = 1.318\n",
      "Epoch 2976 Batch   64/175   train_loss = 1.307\n",
      "Epoch 2976 Batch   96/175   train_loss = 1.314\n",
      "Epoch 2976 Batch  128/175   train_loss = 1.241\n",
      "Epoch 2976 Batch  160/175   train_loss = 1.279\n",
      "Epoch 2977 Batch   17/175   train_loss = 1.250\n",
      "Epoch 2977 Batch   49/175   train_loss = 1.319\n",
      "Epoch 2977 Batch   81/175   train_loss = 1.264\n",
      "Epoch 2977 Batch  113/175   train_loss = 1.303\n",
      "Epoch 2977 Batch  145/175   train_loss = 1.215\n",
      "Epoch 2978 Batch    2/175   train_loss = 1.254\n",
      "Epoch 2978 Batch   34/175   train_loss = 1.322\n",
      "Epoch 2978 Batch   66/175   train_loss = 1.293\n",
      "Epoch 2978 Batch   98/175   train_loss = 1.301\n",
      "Epoch 2978 Batch  130/175   train_loss = 1.296\n",
      "Epoch 2978 Batch  162/175   train_loss = 1.272\n",
      "Epoch 2979 Batch   19/175   train_loss = 1.270\n",
      "Epoch 2979 Batch   51/175   train_loss = 1.241\n",
      "Epoch 2979 Batch   83/175   train_loss = 1.329\n",
      "Epoch 2979 Batch  115/175   train_loss = 1.384\n",
      "Epoch 2979 Batch  147/175   train_loss = 1.221\n",
      "Epoch 2980 Batch    4/175   train_loss = 1.289\n",
      "Epoch 2980 Batch   36/175   train_loss = 1.221\n",
      "Epoch 2980 Batch   68/175   train_loss = 1.265\n",
      "Epoch 2980 Batch  100/175   train_loss = 1.298\n",
      "Epoch 2980 Batch  132/175   train_loss = 1.235\n",
      "Epoch 2980 Batch  164/175   train_loss = 1.212\n",
      "Epoch 2981 Batch   21/175   train_loss = 1.224\n",
      "Epoch 2981 Batch   53/175   train_loss = 1.240\n",
      "Epoch 2981 Batch   85/175   train_loss = 1.271\n",
      "Epoch 2981 Batch  117/175   train_loss = 1.284\n",
      "Epoch 2981 Batch  149/175   train_loss = 1.280\n",
      "Epoch 2982 Batch    6/175   train_loss = 1.291\n",
      "Epoch 2982 Batch   38/175   train_loss = 1.201\n",
      "Epoch 2982 Batch   70/175   train_loss = 1.253\n",
      "Epoch 2982 Batch  102/175   train_loss = 1.284\n",
      "Epoch 2982 Batch  134/175   train_loss = 1.233\n",
      "Epoch 2982 Batch  166/175   train_loss = 1.274\n",
      "Epoch 2983 Batch   23/175   train_loss = 1.192\n",
      "Epoch 2983 Batch   55/175   train_loss = 1.330\n",
      "Epoch 2983 Batch   87/175   train_loss = 1.338\n",
      "Epoch 2983 Batch  119/175   train_loss = 1.255\n",
      "Epoch 2983 Batch  151/175   train_loss = 1.296\n",
      "Epoch 2984 Batch    8/175   train_loss = 1.269\n",
      "Epoch 2984 Batch   40/175   train_loss = 1.237\n",
      "Epoch 2984 Batch   72/175   train_loss = 1.398\n",
      "Epoch 2984 Batch  104/175   train_loss = 1.283\n",
      "Epoch 2984 Batch  136/175   train_loss = 1.228\n",
      "Epoch 2984 Batch  168/175   train_loss = 1.298\n",
      "Epoch 2985 Batch   25/175   train_loss = 1.299\n",
      "Epoch 2985 Batch   57/175   train_loss = 1.308\n",
      "Epoch 2985 Batch   89/175   train_loss = 1.270\n",
      "Epoch 2985 Batch  121/175   train_loss = 1.237\n",
      "Epoch 2985 Batch  153/175   train_loss = 1.262\n",
      "Epoch 2986 Batch   10/175   train_loss = 1.265\n",
      "Epoch 2986 Batch   42/175   train_loss = 1.315\n",
      "Epoch 2986 Batch   74/175   train_loss = 1.315\n",
      "Epoch 2986 Batch  106/175   train_loss = 1.291\n",
      "Epoch 2986 Batch  138/175   train_loss = 1.279\n",
      "Epoch 2986 Batch  170/175   train_loss = 1.279\n",
      "Epoch 2987 Batch   27/175   train_loss = 1.249\n",
      "Epoch 2987 Batch   59/175   train_loss = 1.259\n",
      "Epoch 2987 Batch   91/175   train_loss = 1.259\n",
      "Epoch 2987 Batch  123/175   train_loss = 1.238\n",
      "Epoch 2987 Batch  155/175   train_loss = 1.238\n",
      "Epoch 2988 Batch   12/175   train_loss = 1.259\n",
      "Epoch 2988 Batch   44/175   train_loss = 1.231\n",
      "Epoch 2988 Batch   76/175   train_loss = 1.223\n",
      "Epoch 2988 Batch  108/175   train_loss = 1.250\n",
      "Epoch 2988 Batch  140/175   train_loss = 1.236\n",
      "Epoch 2988 Batch  172/175   train_loss = 1.290\n",
      "Epoch 2989 Batch   29/175   train_loss = 1.280\n",
      "Epoch 2989 Batch   61/175   train_loss = 1.292\n",
      "Epoch 2989 Batch   93/175   train_loss = 1.265\n",
      "Epoch 2989 Batch  125/175   train_loss = 1.252\n",
      "Epoch 2989 Batch  157/175   train_loss = 1.247\n",
      "Epoch 2990 Batch   14/175   train_loss = 1.266\n",
      "Epoch 2990 Batch   46/175   train_loss = 1.259\n",
      "Epoch 2990 Batch   78/175   train_loss = 1.243\n",
      "Epoch 2990 Batch  110/175   train_loss = 1.350\n",
      "Epoch 2990 Batch  142/175   train_loss = 1.291\n",
      "Epoch 2990 Batch  174/175   train_loss = 1.242\n",
      "Epoch 2991 Batch   31/175   train_loss = 1.293\n",
      "Epoch 2991 Batch   63/175   train_loss = 1.276\n",
      "Epoch 2991 Batch   95/175   train_loss = 1.269\n",
      "Epoch 2991 Batch  127/175   train_loss = 1.208\n",
      "Epoch 2991 Batch  159/175   train_loss = 1.210\n",
      "Epoch 2992 Batch   16/175   train_loss = 1.242\n",
      "Epoch 2992 Batch   48/175   train_loss = 1.291\n",
      "Epoch 2992 Batch   80/175   train_loss = 1.279\n",
      "Epoch 2992 Batch  112/175   train_loss = 1.275\n",
      "Epoch 2992 Batch  144/175   train_loss = 1.208\n",
      "Epoch 2993 Batch    1/175   train_loss = 1.311\n",
      "Epoch 2993 Batch   33/175   train_loss = 1.317\n",
      "Epoch 2993 Batch   65/175   train_loss = 1.246\n",
      "Epoch 2993 Batch   97/175   train_loss = 1.249\n",
      "Epoch 2993 Batch  129/175   train_loss = 1.251\n",
      "Epoch 2993 Batch  161/175   train_loss = 1.241\n",
      "Epoch 2994 Batch   18/175   train_loss = 1.230\n",
      "Epoch 2994 Batch   50/175   train_loss = 1.236\n",
      "Epoch 2994 Batch   82/175   train_loss = 1.301\n",
      "Epoch 2994 Batch  114/175   train_loss = 1.303\n",
      "Epoch 2994 Batch  146/175   train_loss = 1.239\n",
      "Epoch 2995 Batch    3/175   train_loss = 1.278\n",
      "Epoch 2995 Batch   35/175   train_loss = 1.237\n",
      "Epoch 2995 Batch   67/175   train_loss = 1.192\n",
      "Epoch 2995 Batch   99/175   train_loss = 1.278\n",
      "Epoch 2995 Batch  131/175   train_loss = 1.232\n",
      "Epoch 2995 Batch  163/175   train_loss = 1.244\n",
      "Epoch 2996 Batch   20/175   train_loss = 1.245\n",
      "Epoch 2996 Batch   52/175   train_loss = 1.175\n",
      "Epoch 2996 Batch   84/175   train_loss = 1.260\n",
      "Epoch 2996 Batch  116/175   train_loss = 1.275\n",
      "Epoch 2996 Batch  148/175   train_loss = 1.250\n",
      "Epoch 2997 Batch    5/175   train_loss = 1.259\n",
      "Epoch 2997 Batch   37/175   train_loss = 1.237\n",
      "Epoch 2997 Batch   69/175   train_loss = 1.274\n",
      "Epoch 2997 Batch  101/175   train_loss = 1.341\n",
      "Epoch 2997 Batch  133/175   train_loss = 1.185\n",
      "Epoch 2997 Batch  165/175   train_loss = 1.291\n",
      "Epoch 2998 Batch   22/175   train_loss = 1.202\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2998 Batch   54/175   train_loss = 1.295\n",
      "Epoch 2998 Batch   86/175   train_loss = 1.337\n",
      "Epoch 2998 Batch  118/175   train_loss = 1.345\n",
      "Epoch 2998 Batch  150/175   train_loss = 1.273\n",
      "Epoch 2999 Batch    7/175   train_loss = 1.290\n",
      "Epoch 2999 Batch   39/175   train_loss = 1.219\n",
      "Epoch 2999 Batch   71/175   train_loss = 1.288\n",
      "Epoch 2999 Batch  103/175   train_loss = 1.266\n",
      "Epoch 2999 Batch  135/175   train_loss = 1.234\n",
      "Epoch 2999 Batch  167/175   train_loss = 1.334\n",
      "Model Trained and Saved\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "batches = get_batches(int_text, batch_size, seq_length)\n",
    "\n",
    "with tf.Session(graph=train_graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for epoch_i in range(num_epochs):\n",
    "        state = sess.run(initial_state, {input_text: batches[0][0]})\n",
    "\n",
    "        for batch_i, (x, y) in enumerate(batches):\n",
    "            feed = {\n",
    "                input_text: x,\n",
    "                targets: y,\n",
    "                initial_state: state,\n",
    "                lr: learning_rate}\n",
    "            train_loss, state, _ = sess.run([cost, final_state, train_op], feed)\n",
    "\n",
    "            # Show every <show_every_n_batches> batches\n",
    "            if (epoch_i * len(batches) + batch_i) % show_every_n_batches == 0:\n",
    "                print('Epoch {:>3} Batch {:>4}/{}   train_loss = {:.3f}'.format(\n",
    "                    epoch_i,\n",
    "                    batch_i,\n",
    "                    len(batches),\n",
    "                    train_loss))\n",
    "\n",
    "    # Save Model\n",
    "    saver = tf.train.Saver()\n",
    "    saver.save(sess, save_dir)\n",
    "    print('Model Trained and Saved')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Parameters\n",
    "Save `seq_length` and `save_dir` for generating a new TV script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "# Save parameters for checkpoint\n",
    "helper.save_params((seq_length, save_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import helper\n",
    "import problem_unittests as tests\n",
    "\n",
    "_, vocab_to_int, int_to_vocab, token_dict = helper.load_preprocess()\n",
    "seq_length, load_dir = helper.load_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement Generate Functions\n",
    "### Get Tensors\n",
    "Get tensors from `loaded_graph` using the function [`get_tensor_by_name()`](https://www.tensorflow.org/api_docs/python/tf/Graph#get_tensor_by_name).  Get the tensors using the following names:\n",
    "- \"input:0\"\n",
    "- \"initial_state:0\"\n",
    "- \"final_state:0\"\n",
    "- \"probs:0\"\n",
    "\n",
    "Return the tensors in the following tuple `(InputTensor, InitialStateTensor, FinalStateTensor, ProbsTensor)` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def get_tensors(loaded_graph):\n",
    "    \"\"\"\n",
    "    Get input, initial state, final state, and probabilities tensor from <loaded_graph>\n",
    "    :param loaded_graph: TensorFlow graph loaded from file\n",
    "    :return: Tuple (InputTensor, InitialStateTensor, FinalStateTensor, ProbsTensor)\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    input_tensor = loaded_graph.get_tensor_by_name('input:0')\n",
    "    initial_state_tensor = loaded_graph.get_tensor_by_name('initial_state:0')\n",
    "    final_state_tensor = loaded_graph.get_tensor_by_name('final_state:0')\n",
    "    probs_tensor = loaded_graph.get_tensor_by_name('probs:0')\n",
    "    \n",
    "    return input_tensor, initial_state_tensor, final_state_tensor, probs_tensor\n",
    "\n",
    "\n",
    "\n",
    "tests.test_get_tensors(get_tensors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose Word\n",
    "Implement the `pick_word()` function to select the next word using `probabilities`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def pick_word(probabilities, int_to_vocab):\n",
    "    \"\"\"\n",
    "    Pick the next word in the generated text\n",
    "    :param probabilities: Probabilites of the next word\n",
    "    :param int_to_vocab: Dictionary of word ids as the keys and words as the values\n",
    "    :return: String of the predicted word\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    next_word = np.random.choice(len(int_to_vocab), p=probabilities)\n",
    "    prediction = int_to_vocab[next_word]\n",
    "    return prediction\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "tests.test_pick_word(pick_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Product Description\n",
    "This will generate the Description for your product.  Set `gen_length` to the length of product description you want to generate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_description(feature):\n",
    "\n",
    "    gen_length = 100\n",
    "    # Sentences generation setup\n",
    "    gen_sentences = [feature]\n",
    "    prev_state = sess.run(initial_state, {input_text: np.array([[1]])})\n",
    "    # Generate sentences\n",
    "    for n in range(gen_length):\n",
    "        # Dynamic Input\n",
    "        dyn_input = [[vocab_to_int[word] for word in gen_sentences[-seq_length:]]]\n",
    "        dyn_seq_length = len(dyn_input[0])\n",
    "\n",
    "        # Get Prediction\n",
    "        probabilities, prev_state = sess.run(\n",
    "            [probs, final_state],\n",
    "            {input_text: dyn_input, initial_state: prev_state})\n",
    "\n",
    "        pred_word = pick_word(probabilities[dyn_seq_length-1], int_to_vocab)\n",
    "\n",
    "        gen_sentences.append(pred_word)\n",
    "\n",
    "    # Remove tokens\n",
    "    product_desc = ' '.join(gen_sentences)\n",
    "    for key, token in token_dict.items():\n",
    "        ending = ' ' if key in ['\\n', '(', '\"'] else ''\n",
    "        product_desc = product_desc.replace(' ' + token.lower(), key)\n",
    "    product_desc = product_desc.replace('\\n ', '\\n')\n",
    "    product_desc = product_desc.replace('( ', '(')\n",
    "\n",
    "\n",
    "\n",
    "    product_desc = product_desc.split(\"\\n\\n\")[0]\n",
    "    prod_title = product_desc.split(\":\")[0]\n",
    "    try:\n",
    "        desc = product_desc.split(\":\")[1]\n",
    "    except:\n",
    "        product_desc = product_desc + \": \"\n",
    "        desc = product_desc.split(\":\")[1]\n",
    "    if len(prod_title.split(\" \")) <= 10:\n",
    "        if len(desc.split(\" \")) >= 2:\n",
    "            return product_desc\n",
    "    else:\n",
    "        x = generate_description(feature)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_eligibility(desc,feature,feature_list):\n",
    "    counter = 0\n",
    "    n = 0\n",
    "    list_len = len(feature_list)\n",
    "    for ftr in feature_list:\n",
    "        if ftr == feature:\n",
    "            ftr_index = n\n",
    "            break\n",
    "        else:\n",
    "            n+=1\n",
    "    \n",
    "    every_other_ftr = []\n",
    "    \n",
    "    for i in range(1,(list_len)):\n",
    "        every_other_ftr.append(feature_list[(i+ftr_index)%list_len])\n",
    "    for desn in desc:\n",
    "        flag = 0\n",
    "        for features in every_other_ftr:\n",
    "            for keys in desn.split(\":\")[0].split(\" \"):\n",
    "                if features == keys:\n",
    "                    for keys1 in desn.split(\":\")[1].split(\" \"):\n",
    "                        if features == keys1:\n",
    "                            flag+=1\n",
    "                            break\n",
    "        if flag == (list_len-1):\n",
    "            counter = counter + 1\n",
    "    \n",
    "    elig = counter/10.0\n",
    "    return elig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prime_word(featurestr):\n",
    "    \n",
    "    feature_list = featurestr.lower().split(\" \")\n",
    "    n = 0\n",
    "    l = len(feature_list)\n",
    "    eligibility = []\n",
    "    for each_feature in feature_list:\n",
    "        n = 1\n",
    "        descl = []\n",
    "        for i in range(0,10):\n",
    "            desc = generate_description(each_feature)\n",
    "            descl.insert(i,desc)\n",
    "        eligibility.insert(n,get_eligibility(descl,each_feature,feature_list))\n",
    "        n = n+1\n",
    "    lpp = len(eligibility)\n",
    "    fstelig = eligibility[0]\n",
    "    fl = 1\n",
    "    for nm in range(1,lpp):\n",
    "        if eligibility[nm] == fstelig:\n",
    "            fl+=1\n",
    "    if fl == lpp:\n",
    "        prime_words = feature_list\n",
    "    else:\n",
    "        prime_words = [feature_list for _,feature_list in sorted(zip(eligibility,feature_list),reverse=True)] \n",
    "    \n",
    "    m = eligibility[0]\n",
    "    fl = 1\n",
    "    for nm in range(1,lpp):\n",
    "        if eligibility[nm] == m:\n",
    "            fl+=1\n",
    "    if fl == lpp:\n",
    "        flfl = 0\n",
    "    else:\n",
    "        eligibility.sort(reverse=True)\n",
    "    return prime_words\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./savetshirts\n",
      "time required to generate the description is 463.8688716888428\n",
      "\n",
      "\n",
      "Final description :\n",
      "puma black rbr round neck t-shirt: black in colour, this round-neck t-shirt from puma is a steal and minimal design. not this t-shirt a wardrobe must-have. showcasing a fabulous interplay into your casual style wearing this designer t-shirt for men by puma jeans. this regular-fit t-shirt will ensure a comfortable fit. team this t-shirt with black jeans and black high-ankle. �bullet bike� graphic print on the front pockets area regular fit cotton fabric for improved air circulation. created for an urbane character all over and an eye-catching textured design that make it highly appealing\n"
     ]
    }
   ],
   "source": [
    "#main function    \n",
    "import time\n",
    "import numpy as np\n",
    "loaded_graph = tf.Graph()\n",
    "with tf.Session(graph=loaded_graph) as sess:\n",
    "    # Load saved model\n",
    "    loader = tf.train.import_meta_graph(load_dir + '.meta')\n",
    "    loader.restore(sess, load_dir)\n",
    "\n",
    "    # Get Tensors from loaded model\n",
    "    input_text, initial_state, final_state, probs = get_tensors(loaded_graph)\n",
    "    \n",
    "    # Sentences generation setup\n",
    "    prev_state = sess.run(initial_state, {input_text: np.array([[1]])})\n",
    "    gen_length = 100\n",
    "\n",
    "    features_str = \"puma black t-shirt\"\n",
    "    start_time = time.time()\n",
    "    ftrs = features_str.split(\" \")\n",
    "    l = len(ftrs)\n",
    "    prime_words= get_prime_word(features_str)\n",
    "    prime_word = prime_words[0]\n",
    "    lp = len(prime_words)\n",
    "    \n",
    "         \n",
    "        ### TO Implement############################################################################\n",
    "        \n",
    "        #num_prime_desc = 200  \n",
    "        #num_other_keyword = 100\n",
    "        #temp = [num_prime_desc + num_keyword * num_other_keyword, 2]\n",
    "        \n",
    "        #--------------------\n",
    "        #|descrp|num_keywords|\n",
    "        #---------------------\n",
    "        #|       |           |\n",
    "        #---------------------\n",
    "        \n",
    "        \n",
    "        ##TO Do\n",
    "        # 1. select the description which has highest number of keywords\n",
    "        # 2. In case of multiple descrips have same number of keywords, select the first\n",
    "        # 3. In case none of the description has all the keywords, select the best possible and \n",
    "        #    Augment it with the sentence related to missing keywords from the descriptions directly related \n",
    "        #    to that particular keyword\n",
    "        \n",
    "        \n",
    "        # Actual Code Will Go Here\n",
    "        \n",
    "    \n",
    "    first_desc = generate_description(prime_word)\n",
    "    first_title = first_desc.split(\":\")[0]\n",
    "    first_desc_only = first_desc.split(\":\")[1]\n",
    "    flag = 0\n",
    "    for index in range(0,lp):\n",
    "        \n",
    "        if prime_words[index] in first_title and prime_words[index] in first_desc_only:\n",
    "            flag = flag + 1\n",
    "            \n",
    "            \n",
    "            \n",
    "    desc_array = np.array([first_desc,flag])\n",
    "    for i in range(1,10):\n",
    "\n",
    "        desc = generate_description(prime_words[0])\n",
    "    \n",
    "\n",
    "        title = desc.split(\":\")[0]\n",
    "        desc_only = desc.split(\":\")[1]\n",
    "\n",
    "        flag = 0\n",
    "        for indx in range(0,lp):\n",
    "            \n",
    "            if prime_words[indx] in title and prime_words[indx] in desc_only:\n",
    "                flag+=1\n",
    "\n",
    "\n",
    "        new_array = np.array([desc,flag])\n",
    "        desc_array = np.vstack([desc_array,new_array])    \n",
    "\n",
    "    for ix in range(1,lp):\n",
    "        \n",
    "        for i in range(0,5):\n",
    "\n",
    "            desc = generate_description(prime_words[ix])\n",
    "\n",
    "            title = desc.split(\":\")[0]\n",
    "            desc_only = desc.split(\":\")[1]\n",
    "\n",
    "            flag = 0\n",
    "            for indx in range(0,lp):\n",
    "                \n",
    "                if prime_words[indx] in title and prime_words[indx] in desc_only:\n",
    "                    flag+=1\n",
    "                \n",
    "            new_array = np.array([desc,flag])\n",
    "            desc_array = np.vstack([desc_array,new_array])    \n",
    "    \n",
    "            \n",
    "    max = 0   \n",
    "    desc_op = \"\"\n",
    "    for all in desc_array:\n",
    "        if(int(all[1])>=max):\n",
    "            desc_op = all[0]\n",
    "            max = int(all[1])\n",
    "    import webcolors\n",
    "    import helper\n",
    "\n",
    "    def get_brandname():\n",
    "\n",
    "        data_dir = './data/simpsons/t-shirts.txt'\n",
    "        text = helper.load_data(data_dir)\n",
    "        # Ignore notice, since we don't use it for analysing the data\n",
    "        text = text[:]\n",
    "        brands = []\n",
    "        alldesc = text.split(\"\\n\\n\")\n",
    "        for eachdesc in alldesc:\n",
    "            eachtitle = eachdesc.split(\":\")[0]\n",
    "            brandname = eachtitle.split(\" \")[0]\n",
    "            brandname = brandname.replace(\"\\n\",'')\n",
    "            brandname = brandname.replace(\"\\ufeff\",'')\n",
    "            brandname = brandname.lower()\n",
    "            brands.append(brandname)\n",
    "        brands = set(brands)\n",
    "        brands = list(brands)\n",
    "\n",
    "        brands.remove('the')\n",
    "        brands.remove('')\n",
    "        brands.remove('none')\n",
    "        brands.remove('blend')\n",
    "        brands.remove('design')\n",
    "        brands.remove('true')\n",
    "        brands.remove('all')\n",
    "        clrlst = get_colors()\n",
    "        brands = list(set(brands)-set(clrlst))\n",
    "\n",
    "\n",
    "        return brands\n",
    "\n",
    "    def get_colors():\n",
    "        list_of_colors = []\n",
    "        listcolors = webcolors.CSS3_NAMES_TO_HEX\n",
    "        for all in listcolors:\n",
    "            cl = all.split(\":\")[0]\n",
    "            list_of_colors.append(cl)\n",
    "        list_of_colors.append('mauve')\n",
    "        list_of_colors.append('wine')\n",
    "        return list_of_colors\n",
    "\n",
    "    ###############################################################################################################################################\n",
    "    ############################################################################################################################################3\n",
    "    ##################################################################################################################################\n",
    "\n",
    "\n",
    "    #Getting brand names from text files to identify brand in the features string\n",
    "    brandnames = get_brandname()\n",
    "    brand_instr = ''\n",
    "    for allbrands in brandnames:\n",
    "        for allwords in features_str.split(\" \"):\n",
    "            if allbrands == allwords:\n",
    "                brand_instr = allbrands\n",
    "                break\n",
    "\n",
    "\n",
    "\n",
    "    #getting all colors to identify the color in the features string\n",
    "    colorlist = get_colors()\n",
    "    color_instr = ''\n",
    "    for allcolors in colorlist:\n",
    "        for allwords in features_str.split(\" \"):\n",
    "            if allcolors == allwords:\n",
    "                color_instr = allcolors\n",
    "                break\n",
    "            else:\n",
    "                for allword in desc_op.split(\":\")[0].split(\" \"):\n",
    "                    if allcolors == allword:\n",
    "                        color_instr = allcolors\n",
    "                        break\n",
    "    \n",
    "\n",
    "    # to append the brand name in the title if not present\n",
    "\n",
    "    allwordsintitle = desc_op.split(\":\")[0]\n",
    "\n",
    "    for allwords in allwordsintitle.split(\" \"):\n",
    "        if allwords == color_instr:\n",
    "            flg = 0\n",
    "        else:\n",
    "            allwordsintitle = color_instr + \" \" + allwordsintitle\n",
    "            title = desc_op.split(\":\")[0]\n",
    "            allwordsintitle = title.replace(desc_op.split(\":\")[0],allwordsintitle)\n",
    "            break\n",
    "\n",
    "    for allwords in allwordsintitle.split(\" \"):\n",
    "        if allwords == brand_instr:\n",
    "            flg = 0\n",
    "        else:\n",
    "            allwordsintitle = brand_instr + \" \" + allwordsintitle\n",
    "            title = desc_op.split(\":\")[0]\n",
    "            allwordsintitle = title.replace(desc_op.split(\":\")[0],allwordsintitle)\n",
    "            break\n",
    "\n",
    "\n",
    "    #to replace brand names and colours in the description \n",
    "\n",
    "    allwordsindescription = desc_op.split(\":\")[1]\n",
    "\n",
    "    for brandnm in brandnames:\n",
    "        for allwords in allwordsintitle.split(\" \"):\n",
    "            if brandnm == allwords or brandnm+\".\" == allwords or brandnm+\",\" == allwords or brandnm+\";\" == allwords:\n",
    "                allwordsintitle = allwordsintitle.replace(brandnm,brand_instr)\n",
    "        for allwords in allwordsindescription.split(\" \"):\n",
    "            if brandnm == allwords or brandnm+\".\" == allwords or brandnm+\",\" == allwords or brandnm+\";\" == allwords:\n",
    "                allwordsindescription = allwordsindescription.replace(brandnm,brand_instr)\n",
    "\n",
    "\n",
    "    desc_op = desc_op.replace(desc_op.split(\":\")[0],allwordsintitle)\n",
    "    desc_op = desc_op.replace(desc_op.split(\":\")[1],allwordsindescription)\n",
    "    \n",
    "\n",
    "    for clrs in colorlist:\n",
    "        for allwords in allwordsintitle.split(\" \"):\n",
    "            if clrs == allwords or clrs+\".\" == allwords or clrs+\",\" == allwords or clrs+\";\" == allwords:\n",
    "                allwordsintitle = allwordsintitle.replace(clrs,color_instr)\n",
    "        for allwords in allwordsindescription.split(\" \"):\n",
    "            if clrs == allwords or clrs+\".\" == allwords or clrs+\",\" == allwords or clrs+\";\" == allwords:\n",
    "                allwordsindescription = allwordsindescription.replace(clrs,color_instr)\n",
    "\n",
    "    def remove_duplicates(li):\n",
    "        my_set = set()\n",
    "        res = []\n",
    "        for e in li:\n",
    "            if e not in my_set:\n",
    "                res.append(e)\n",
    "                my_set.add(e)\n",
    "        #\n",
    "        return res\n",
    "\n",
    "    allwordsintitlelist = allwordsintitle.split(\" \")\n",
    "    allwordsintitlelist = remove_duplicates(allwordsintitlelist)\n",
    "    allwordsintitle = ' '.join(allwordsintitlelist)\n",
    "\n",
    "    desc_op = desc_op.replace(desc_op.split(\":\")[0],allwordsintitle)\n",
    "    desc_op = desc_op.replace(desc_op.split(\":\")[1],allwordsindescription)\n",
    "    final_time = time.time()\n",
    "    print(\"time required to generate the description is \"+str(final_time - start_time))\n",
    "    print(\"\\n\\nFinal description :\")\n",
    "    print(desc_op)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
