{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Product Description Generation\n",
    "In this project, you can generate descriptions of various apparel items like shirts, jeans, kurtas using RNNs. The descriptions of 10400 products for each apparel item has been used fro jabong.com to train the neural network. The Neural Network built will generate a product description for a product from the given sets of apparels i.e. shirts, jeans, shoes, watches and kurtas.\n",
    "## Get the Data\n",
    "The data consists of 10400 descriptions of different kurtas obtained from jabong.com with distinguishing features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "import helper\n",
    "\n",
    "data_dir = './data/simpsons/kurtas.txt'\n",
    "text = helper.load_data(data_dir)\n",
    "# Ignore notice, since we don't use it for analysing the data\n",
    "text = text[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Stats\n",
      "Roughly the number of unique words: 6314\n",
      "Number of scenes: 23725\n",
      "Average number of sentences in each scene: 0.0\n",
      "Number of lines: 23725\n",
      "Average number of words in each line: 28.915700737618547\n",
      "\n",
      "The sentences 0 to 10:\n",
      "ï»¿Anouk Blue Suit Set: Celebrate your personal style and roots with this elegant blue kurta set from Anouk. Define your figure in this kurta set with a skinny belt for a modern take on this traditional garment.\n",
      "\n",
      "\n",
      "\n",
      "Varanga Grey Printed Kurta Pant Set: Be the cynosure of all eyes by wearing grey kurta pant set by Varanga, This will keep you comfortable throughout the party. Pair it with high heels and look effortlessly chic and fashionable. This set has a soothing colour and will lend you a fresh look this season.\n",
      "\n",
      "\n",
      "\n",
      "Anouk Multicoloured Printed Kurta: This brilliant kurta from Anouk will help you maintain an elegant look all year long. Dress up for your next family event by pairing this multi-coloured piece with your best Kolhapuris and simple jewellery.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "view_sentence_range = (0, 10)\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "\n",
    "print('Dataset Stats')\n",
    "print('Roughly the number of unique words: {}'.format(len({word: None for word in text.split()})))\n",
    "scenes = text.split('\\n\\n')\n",
    "print('Number of scenes: {}'.format(len(scenes)))\n",
    "sentence_count_scene = [scene.count('\\n') for scene in scenes]\n",
    "print('Average number of sentences in each scene: {}'.format(np.average(sentence_count_scene)))\n",
    "\n",
    "sentences = [sentence for scene in scenes for sentence in scene.split('\\n')]\n",
    "print('Number of lines: {}'.format(len(sentences)))\n",
    "word_count_sentence = [len(sentence.split()) for sentence in sentences]\n",
    "print('Average number of words in each line: {}'.format(np.average(word_count_sentence)))\n",
    "\n",
    "print()\n",
    "print('The sentences {} to {}:'.format(*view_sentence_range))\n",
    "print('\\n'.join(text.split('\\n')[view_sentence_range[0]:view_sentence_range[1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement Preprocessing Functions\n",
    "The first thing to do to any dataset is preprocessing.  Implement the following preprocessing functions below:\n",
    "- Lookup Table\n",
    "- Tokenize Punctuation\n",
    "\n",
    "### Lookup Table\n",
    "To create a word embedding, you first need to transform the words to ids.  In this function, create two dictionaries:\n",
    "- Dictionary to go from the words to an id, we'll call `vocab_to_int`\n",
    "- Dictionary to go from the id to word, we'll call `int_to_vocab`\n",
    "\n",
    "Return these dictionaries in the following tuple `(vocab_to_int, int_to_vocab)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import problem_unittests as tests\n",
    "\n",
    "def create_lookup_tables(text):\n",
    "    \"\"\"\n",
    "    Create lookup tables for vocabulary\n",
    "    :param text: The text of tv scripts split into words\n",
    "    :return: A tuple of dicts (vocab_to_int, int_to_vocab)\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    text = list(set(text))   \n",
    "    index = range(len(text))    \n",
    "    int_to_vocab = dict(zip(index, text))   \n",
    "    vocab_to_int = dict(zip(text, index))       \n",
    "    return vocab_to_int, int_to_vocab\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_create_lookup_tables(create_lookup_tables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize Punctuation\n",
    "We'll be splitting the script into a word array using spaces as delimiters.  However, punctuations like periods and exclamation marks make it hard for the neural network to distinguish between the word \"bye\" and \"bye!\".\n",
    "\n",
    "Implement the function `token_lookup` to return a dict that will be used to tokenize symbols like \"!\" into \"||Exclamation_Mark||\".  Create a dictionary for the following symbols where the symbol is the key and value is the token:\n",
    "- Period ( . )\n",
    "- Comma ( , )\n",
    "- Quotation Mark ( \" )\n",
    "- Semicolon ( ; )\n",
    "- Exclamation mark ( ! )\n",
    "- Question mark ( ? )\n",
    "- Left Parentheses ( ( )\n",
    "- Right Parentheses ( ) )\n",
    "- Dash ( -- )\n",
    "- Return ( \\n )\n",
    "\n",
    "This dictionary will be used to token the symbols and add the delimiter (space) around it.  This separates the symbols as it's own word, making it easier for the neural network to predict on the next word. Make sure you don't use a token that could be confused as a word. Instead of using the token \"dash\", try using something like \"||dash||\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def token_lookup():\n",
    "    \"\"\"\n",
    "    Generate a dict to turn punctuation into a token.\n",
    "    :return: Tokenize dictionary where the key is the punctuation and the value is the token\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    token = {}\n",
    "    token['.']=\"||period||\"\n",
    "    token[',']=\"||comma||\"\n",
    "    token['\"']=\"||quotation_mark||\"\n",
    "    token[';']=\"||semicolon||\"\n",
    "    token['!']=\"||exclamation_mark||\"\n",
    "    token['?']=\"||question_mark||\"\n",
    "    token['(']=\"||left_parantheses||\"\n",
    "    token[')']=\"||right_parantheses||\"\n",
    "    token['--'] = \"||dash||\"\n",
    "    token['\\n'] = \"||return||\"\n",
    "    \n",
    "    return token\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_tokenize(token_lookup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess all the data and save it\n",
    "Running the code cell below will preprocess all the data and save it to file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "# Preprocess Training, Validation, and Testing Data\n",
    "helper.preprocess_and_save_data(data_dir, token_lookup, create_lookup_tables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Point\n",
    "This is your first checkpoint. If you ever decide to come back to this notebook or have to restart the notebook, you can start from here. The preprocessed data has been saved to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "import helper\n",
    "import numpy as np\n",
    "import problem_unittests as tests\n",
    "\n",
    "int_text, vocab_to_int, int_to_vocab, token_dict = helper.load_preprocess()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the Neural Network\n",
    "You'll build the components necessary to build a RNN by implementing the following functions below:\n",
    "- get_inputs\n",
    "- get_init_cell\n",
    "- get_embed\n",
    "- build_rnn\n",
    "- build_nn\n",
    "- get_batches\n",
    "\n",
    "### Check the Version of TensorFlow and Access to GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 1.1.0\n",
      "Default GPU Device: /gpu:0\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "from distutils.version import LooseVersion\n",
    "import warnings\n",
    "import tensorflow as tf\n",
    "\n",
    "# Check TensorFlow Version\n",
    "assert LooseVersion(tf.__version__) >= LooseVersion('1.0'), 'Please use TensorFlow version 1.0 or newer'\n",
    "print('TensorFlow Version: {}'.format(tf.__version__))\n",
    "\n",
    "# Check for a GPU\n",
    "if not tf.test.gpu_device_name():\n",
    "    warnings.warn('No GPU found. Please use a GPU to train your neural network.')\n",
    "else:\n",
    "    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input\n",
    "Implement the `get_inputs()` function to create TF Placeholders for the Neural Network.  It should create the following placeholders:\n",
    "- Input text placeholder named \"input\" using the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder) `name` parameter.\n",
    "- Targets placeholder\n",
    "- Learning Rate placeholder\n",
    "\n",
    "Return the placeholders in the following tuple `(Input, Targets, LearningRate)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def get_inputs():\n",
    "    \"\"\"\n",
    "    Create TF Placeholders for input, targets, and learning rate.\n",
    "    :return: Tuple (input, targets, learning rate)\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    INPUT = tf.placeholder(tf.int32, shape=(None,None), name=\"input\")\n",
    "    TARGETS = tf.placeholder(tf.int32, shape=(None,None), name='targets')\n",
    "    LEARNING_RATE = tf.placeholder(tf.float32, shape=None, name='learning_rate')\n",
    "    return INPUT, TARGETS, LEARNING_RATE\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_get_inputs(get_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build RNN Cell and Initialize\n",
    "Stack one or more [`BasicLSTMCells`](https://www.tensorflow.org/api_docs/python/tf/contrib/rnn/BasicLSTMCell) in a [`MultiRNNCell`](https://www.tensorflow.org/api_docs/python/tf/contrib/rnn/MultiRNNCell).\n",
    "- The Rnn size should be set using `rnn_size`\n",
    "- Initalize Cell State using the MultiRNNCell's [`zero_state()`](https://www.tensorflow.org/api_docs/python/tf/contrib/rnn/MultiRNNCell#zero_state) function\n",
    "    - Apply the name \"initial_state\" to the initial state using [`tf.identity()`](https://www.tensorflow.org/api_docs/python/tf/identity)\n",
    "\n",
    "Return the cell and initial state in the following tuple `(Cell, InitialState)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def get_init_cell(batch_size, rnn_size):\n",
    "    \"\"\"\n",
    "    Create an RNN Cell and initialize it.\n",
    "    :param batch_size: Size of batches\n",
    "    :param rnn_size: Size of RNNs\n",
    "    :return: Tuple (cell, initialize state)\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    lstm1 = tf.contrib.rnn.BasicLSTMCell(rnn_size)\n",
    "    lstm2 = tf.contrib.rnn.BasicLSTMCell(rnn_size)\n",
    "    lstm3 = tf.contrib.rnn.BasicLSTMCell(rnn_size)\n",
    "    lstm4 = tf.contrib.rnn.BasicLSTMCell(rnn_size)\n",
    "    cell = tf.contrib.rnn.MultiRNNCell([lstm1,lstm2,lstm3,lstm4])\n",
    "    initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "    initial_state = tf.identity(initial_state, name='initial_state')\n",
    "   \n",
    "     \n",
    "    return cell, initial_state\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_get_init_cell(get_init_cell)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Embedding\n",
    "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def get_embed(input_data, vocab_size, embed_dim):\n",
    "    \"\"\"\n",
    "    Create embedding for <input_data>.\n",
    "    :param input_data: TF placeholder for text input.\n",
    "    :param vocab_size: Number of words in vocabulary.\n",
    "    :param embed_dim: Number of embedding dimensionstea\n",
    "    :return: Embedded input.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    embedding = tf.Variable(tf.random_uniform((vocab_size, embed_dim), -1, 1))\n",
    "    embed = tf.nn.embedding_lookup(embedding, input_data)\n",
    "    return embed\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_get_embed(get_embed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build RNN\n",
    "You created a RNN Cell in the `get_init_cell()` function.  Time to use the cell to create a RNN.\n",
    "- Build the RNN using the [`tf.nn.dynamic_rnn()`](https://www.tensorflow.org/api_docs/python/tf/nn/dynamic_rnn)\n",
    " - Apply the name \"final_state\" to the final state using [`tf.identity()`](https://www.tensorflow.org/api_docs/python/tf/identity)\n",
    "\n",
    "Return the outputs and final_state state in the following tuple `(Outputs, FinalState)` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def build_rnn(cell, inputs):\n",
    "    \"\"\"\n",
    "    Create a RNN using a RNN Cell\n",
    "    :param cell: RNN Cell\n",
    "    :param inputs: Input text data\n",
    "    :return: Tuple (Outputs, Final State)\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    Outputs, States = tf.nn.dynamic_rnn(cell, inputs,dtype='float32')\n",
    "    FinalState = tf.identity(States,name=\"final_state\")\n",
    "    return Outputs, FinalState\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_build_rnn(build_rnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the Neural Network\n",
    "Apply the functions you implemented above to:\n",
    "- Apply embedding to `input_data` using your `get_embed(input_data, vocab_size, embed_dim)` function.\n",
    "- Build RNN using `cell` and your `build_rnn(cell, inputs)` function.\n",
    "- Apply a fully connected layer with a linear activation and `vocab_size` as the number of outputs.\n",
    "\n",
    "Return the logits and final state in the following tuple (Logits, FinalState) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def build_nn(cell, rnn_size, input_data, vocab_size, embed_dim):\n",
    "    \"\"\"\n",
    "    Build part of the neural network\n",
    "    :param cell: RNN cell\n",
    "    :param rnn_size: Size of rnns\n",
    "    :param input_data: Input data\n",
    "    :param vocab_size: Vocabulary size\n",
    "    :param embed_dim: Number of embedding dimensions\n",
    "    :return: Tuple (Logits, FinalState)\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    embed = get_embed(input_data, vocab_size, embed_dim)\n",
    "    outputs, final_state = build_rnn(cell, embed)\n",
    "    logits = tf.contrib.layers.fully_connected(outputs, vocab_size, activation_fn=None,\n",
    "                                               weights_initializer=tf.truncated_normal_initializer(stddev=0.1),\n",
    "                                               biases_initializer=tf.zeros_initializer())\n",
    "    return logits, final_state\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_build_nn(build_nn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batches\n",
    "Implement `get_batches` to create batches of input and targets using `int_text`.  The batches should be a Numpy array with the shape `(number of batches, 2, batch size, sequence length)`. Each batch contains two elements:\n",
    "- The first element is a single batch of **input** with the shape `[batch size, sequence length]`\n",
    "- The second element is a single batch of **targets** with the shape `[batch size, sequence length]`\n",
    "\n",
    "If you can't fill the last batch with enough data, drop the last batch.\n",
    "\n",
    "For example, `get_batches([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20], 3, 2)` would return a Numpy array of the following:\n",
    "```\n",
    "[\n",
    "  # First Batch\n",
    "  [\n",
    "    # Batch of Input\n",
    "    [[ 1  2], [ 7  8], [13 14]]\n",
    "    # Batch of targets\n",
    "    [[ 2  3], [ 8  9], [14 15]]\n",
    "  ]\n",
    "\n",
    "  # Second Batch\n",
    "  [\n",
    "    # Batch of Input\n",
    "    [[ 3  4], [ 9 10], [15 16]]\n",
    "    # Batch of targets\n",
    "    [[ 4  5], [10 11], [16 17]]\n",
    "  ]\n",
    "\n",
    "  # Third Batch\n",
    "  [\n",
    "    # Batch of Input\n",
    "    [[ 5  6], [11 12], [17 18]]\n",
    "    # Batch of targets\n",
    "    [[ 6  7], [12 13], [18  1]]\n",
    "  ]\n",
    "]\n",
    "```\n",
    "\n",
    "Notice that the last target value in the last batch is the first input value of the first batch. In this case, `1`. This is a common technique used when creating sequence batches, although it is rather unintuitive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def get_batches(int_text, batch_size, seq_length):\n",
    "    \"\"\"\n",
    "    Return batches of input and target\n",
    "    :param int_text: Text with the words replaced by their ids\n",
    "    :param batch_size: The size of batch\n",
    "    :param seq_length: The length of sequence\n",
    "    :return: Batches as a Numpy array\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    num_batches = int(len(int_text) / (batch_size * seq_length))\n",
    "    in_data = np.array(int_text[: num_batches * batch_size * seq_length])\n",
    "    out_data = np.array(int_text[1: num_batches * batch_size * seq_length + 1])\n",
    "    out_data[-1] = in_data[0]\n",
    "\n",
    "    in_batches = np.split(in_data.reshape(batch_size, -1), num_batches, 1)\n",
    "    out_batches = np.split(out_data.reshape(batch_size, -1), num_batches, 1)\n",
    "    return np.array(list(zip(in_batches, out_batches)))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_get_batches(get_batches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Training\n",
    "### Hyperparameters\n",
    "Tune the following parameters:\n",
    "\n",
    "- Set `num_epochs` to the number of epochs.\n",
    "- Set `batch_size` to the batch size.\n",
    "- Set `rnn_size` to the size of the RNNs.\n",
    "- Set `embed_dim` to the size of the embedding.\n",
    "- Set `seq_length` to the length of sequence.\n",
    "- Set `learning_rate` to the learning rate.\n",
    "- Set `show_every_n_batches` to the number of batches the neural network should print progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of Epochs\n",
    "num_epochs = 3000\n",
    "# Batch Size\n",
    "batch_size = 512\n",
    "# RNN Size\n",
    "rnn_size = 256\n",
    "# Embedding Dimension Size\n",
    "embed_dim = 256\n",
    "# Sequence Length\n",
    "seq_length = 16\n",
    "# Learning Rate\n",
    "learning_rate = 0.01\n",
    "# Show stats for every n number of batches\n",
    "show_every_n_batches = 32\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "save_dir = './savekurtas'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the Graph\n",
    "Build the graph using the neural network you implemented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "from tensorflow.contrib import seq2seq\n",
    "\n",
    "train_graph = tf.Graph()\n",
    "with train_graph.as_default():\n",
    "    vocab_size = len(int_to_vocab)\n",
    "    input_text, targets, lr = get_inputs()\n",
    "    input_data_shape = tf.shape(input_text)\n",
    "    cell, initial_state = get_init_cell(input_data_shape[0], rnn_size)\n",
    "    logits, final_state = build_nn(cell, rnn_size, input_text, vocab_size, embed_dim)\n",
    "\n",
    "    # Probabilities for generating words\n",
    "    probs = tf.nn.softmax(logits, name='probs')\n",
    "\n",
    "    # Loss function\n",
    "    cost = seq2seq.sequence_loss(\n",
    "        logits,\n",
    "        targets,\n",
    "        tf.ones([input_data_shape[0], input_data_shape[1]]))\n",
    "\n",
    "    # Optimizer\n",
    "    optimizer = tf.train.AdamOptimizer(lr)\n",
    "\n",
    "    # Gradient Clipping\n",
    "    gradients = optimizer.compute_gradients(cost)\n",
    "    capped_gradients = [(tf.clip_by_value(grad, -1., 1.), var) for grad, var in gradients if grad is not None]\n",
    "    train_op = optimizer.apply_gradients(capped_gradients)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train\n",
    "Train the neural network on the preprocessed data.  If you have a hard time getting a good loss, check the [forums](https://discussions.udacity.com/) to see if anyone is having the same problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   0 Batch    0/96   train_loss = 8.292\n",
      "Epoch   0 Batch   32/96   train_loss = 5.514\n",
      "Epoch   0 Batch   64/96   train_loss = 5.462\n",
      "Epoch   1 Batch    0/96   train_loss = 5.211\n",
      "Epoch   1 Batch   32/96   train_loss = 4.938\n",
      "Epoch   1 Batch   64/96   train_loss = 4.491\n",
      "Epoch   2 Batch    0/96   train_loss = 3.999\n",
      "Epoch   2 Batch   32/96   train_loss = 3.723\n",
      "Epoch   2 Batch   64/96   train_loss = 3.248\n",
      "Epoch   3 Batch    0/96   train_loss = 2.665\n",
      "Epoch   3 Batch   32/96   train_loss = 2.504\n",
      "Epoch   3 Batch   64/96   train_loss = 2.272\n",
      "Epoch   4 Batch    0/96   train_loss = 1.971\n",
      "Epoch   4 Batch   32/96   train_loss = 2.023\n",
      "Epoch   4 Batch   64/96   train_loss = 1.933\n",
      "Epoch   5 Batch    0/96   train_loss = 1.719\n",
      "Epoch   5 Batch   32/96   train_loss = 1.809\n",
      "Epoch   5 Batch   64/96   train_loss = 1.746\n",
      "Epoch   6 Batch    0/96   train_loss = 1.598\n",
      "Epoch   6 Batch   32/96   train_loss = 1.656\n",
      "Epoch   6 Batch   64/96   train_loss = 1.626\n",
      "Epoch   7 Batch    0/96   train_loss = 1.498\n",
      "Epoch   7 Batch   32/96   train_loss = 1.574\n",
      "Epoch   7 Batch   64/96   train_loss = 1.552\n",
      "Epoch   8 Batch    0/96   train_loss = 1.433\n",
      "Epoch   8 Batch   32/96   train_loss = 1.501\n",
      "Epoch   8 Batch   64/96   train_loss = 1.494\n",
      "Epoch   9 Batch    0/96   train_loss = 1.405\n",
      "Epoch   9 Batch   32/96   train_loss = 1.463\n",
      "Epoch   9 Batch   64/96   train_loss = 1.439\n",
      "Epoch  10 Batch    0/96   train_loss = 1.354\n",
      "Epoch  10 Batch   32/96   train_loss = 1.424\n",
      "Epoch  10 Batch   64/96   train_loss = 1.412\n",
      "Epoch  11 Batch    0/96   train_loss = 1.315\n",
      "Epoch  11 Batch   32/96   train_loss = 1.389\n",
      "Epoch  11 Batch   64/96   train_loss = 1.370\n",
      "Epoch  12 Batch    0/96   train_loss = 1.300\n",
      "Epoch  12 Batch   32/96   train_loss = 1.356\n",
      "Epoch  12 Batch   64/96   train_loss = 1.354\n",
      "Epoch  13 Batch    0/96   train_loss = 1.288\n",
      "Epoch  13 Batch   32/96   train_loss = 1.340\n",
      "Epoch  13 Batch   64/96   train_loss = 1.327\n",
      "Epoch  14 Batch    0/96   train_loss = 1.263\n",
      "Epoch  14 Batch   32/96   train_loss = 1.323\n",
      "Epoch  14 Batch   64/96   train_loss = 1.292\n",
      "Epoch  15 Batch    0/96   train_loss = 1.231\n",
      "Epoch  15 Batch   32/96   train_loss = 1.291\n",
      "Epoch  15 Batch   64/96   train_loss = 1.279\n",
      "Epoch  16 Batch    0/96   train_loss = 1.224\n",
      "Epoch  16 Batch   32/96   train_loss = 1.279\n",
      "Epoch  16 Batch   64/96   train_loss = 1.257\n",
      "Epoch  17 Batch    0/96   train_loss = 1.224\n",
      "Epoch  17 Batch   32/96   train_loss = 1.264\n",
      "Epoch  17 Batch   64/96   train_loss = 1.255\n",
      "Epoch  18 Batch    0/96   train_loss = 1.202\n",
      "Epoch  18 Batch   32/96   train_loss = 1.249\n",
      "Epoch  18 Batch   64/96   train_loss = 1.233\n",
      "Epoch  19 Batch    0/96   train_loss = 1.189\n",
      "Epoch  19 Batch   32/96   train_loss = 1.239\n",
      "Epoch  19 Batch   64/96   train_loss = 1.222\n",
      "Epoch  20 Batch    0/96   train_loss = 1.164\n",
      "Epoch  20 Batch   32/96   train_loss = 1.223\n",
      "Epoch  20 Batch   64/96   train_loss = 1.219\n",
      "Epoch  21 Batch    0/96   train_loss = 1.141\n",
      "Epoch  21 Batch   32/96   train_loss = 1.212\n",
      "Epoch  21 Batch   64/96   train_loss = 1.203\n",
      "Epoch  22 Batch    0/96   train_loss = 1.143\n",
      "Epoch  22 Batch   32/96   train_loss = 1.200\n",
      "Epoch  22 Batch   64/96   train_loss = 1.192\n",
      "Epoch  23 Batch    0/96   train_loss = 1.136\n",
      "Epoch  23 Batch   32/96   train_loss = 1.193\n",
      "Epoch  23 Batch   64/96   train_loss = 1.189\n",
      "Epoch  24 Batch    0/96   train_loss = 1.126\n",
      "Epoch  24 Batch   32/96   train_loss = 1.180\n",
      "Epoch  24 Batch   64/96   train_loss = 1.175\n",
      "Epoch  25 Batch    0/96   train_loss = 1.121\n",
      "Epoch  25 Batch   32/96   train_loss = 1.180\n",
      "Epoch  25 Batch   64/96   train_loss = 1.157\n",
      "Epoch  26 Batch    0/96   train_loss = 1.106\n",
      "Epoch  26 Batch   32/96   train_loss = 1.172\n",
      "Epoch  26 Batch   64/96   train_loss = 1.153\n",
      "Epoch  27 Batch    0/96   train_loss = 1.093\n",
      "Epoch  27 Batch   32/96   train_loss = 1.166\n",
      "Epoch  27 Batch   64/96   train_loss = 1.137\n",
      "Epoch  28 Batch    0/96   train_loss = 1.093\n",
      "Epoch  28 Batch   32/96   train_loss = 1.161\n",
      "Epoch  28 Batch   64/96   train_loss = 1.140\n",
      "Epoch  29 Batch    0/96   train_loss = 1.094\n",
      "Epoch  29 Batch   32/96   train_loss = 1.151\n",
      "Epoch  29 Batch   64/96   train_loss = 1.128\n",
      "Epoch  30 Batch    0/96   train_loss = 1.094\n",
      "Epoch  30 Batch   32/96   train_loss = 1.145\n",
      "Epoch  30 Batch   64/96   train_loss = 1.126\n",
      "Epoch  31 Batch    0/96   train_loss = 1.087\n",
      "Epoch  31 Batch   32/96   train_loss = 1.152\n",
      "Epoch  31 Batch   64/96   train_loss = 1.108\n",
      "Epoch  32 Batch    0/96   train_loss = 1.066\n",
      "Epoch  32 Batch   32/96   train_loss = 1.157\n",
      "Epoch  32 Batch   64/96   train_loss = 1.109\n",
      "Epoch  33 Batch    0/96   train_loss = 1.071\n",
      "Epoch  33 Batch   32/96   train_loss = 1.156\n",
      "Epoch  33 Batch   64/96   train_loss = 1.109\n",
      "Epoch  34 Batch    0/96   train_loss = 1.070\n",
      "Epoch  34 Batch   32/96   train_loss = 1.148\n",
      "Epoch  34 Batch   64/96   train_loss = 1.118\n",
      "Epoch  35 Batch    0/96   train_loss = 1.069\n",
      "Epoch  35 Batch   32/96   train_loss = 1.138\n",
      "Epoch  35 Batch   64/96   train_loss = 1.112\n",
      "Epoch  36 Batch    0/96   train_loss = 1.066\n",
      "Epoch  36 Batch   32/96   train_loss = 1.141\n",
      "Epoch  36 Batch   64/96   train_loss = 1.112\n",
      "Epoch  37 Batch    0/96   train_loss = 1.064\n",
      "Epoch  37 Batch   32/96   train_loss = 1.127\n",
      "Epoch  37 Batch   64/96   train_loss = 1.112\n",
      "Epoch  38 Batch    0/96   train_loss = 1.058\n",
      "Epoch  38 Batch   32/96   train_loss = 1.122\n",
      "Epoch  38 Batch   64/96   train_loss = 1.094\n",
      "Epoch  39 Batch    0/96   train_loss = 1.060\n",
      "Epoch  39 Batch   32/96   train_loss = 1.116\n",
      "Epoch  39 Batch   64/96   train_loss = 1.091\n",
      "Epoch  40 Batch    0/96   train_loss = 1.036\n",
      "Epoch  40 Batch   32/96   train_loss = 1.105\n",
      "Epoch  40 Batch   64/96   train_loss = 1.090\n",
      "Epoch  41 Batch    0/96   train_loss = 1.032\n",
      "Epoch  41 Batch   32/96   train_loss = 1.103\n",
      "Epoch  41 Batch   64/96   train_loss = 1.079\n",
      "Epoch  42 Batch    0/96   train_loss = 1.034\n",
      "Epoch  42 Batch   32/96   train_loss = 1.116\n",
      "Epoch  42 Batch   64/96   train_loss = 1.071\n",
      "Epoch  43 Batch    0/96   train_loss = 1.030\n",
      "Epoch  43 Batch   32/96   train_loss = 1.099\n",
      "Epoch  43 Batch   64/96   train_loss = 1.075\n",
      "Epoch  44 Batch    0/96   train_loss = 1.031\n",
      "Epoch  44 Batch   32/96   train_loss = 1.098\n",
      "Epoch  44 Batch   64/96   train_loss = 1.073\n",
      "Epoch  45 Batch    0/96   train_loss = 1.038\n",
      "Epoch  45 Batch   32/96   train_loss = 1.095\n",
      "Epoch  45 Batch   64/96   train_loss = 1.081\n",
      "Epoch  46 Batch    0/96   train_loss = 1.025\n",
      "Epoch  46 Batch   32/96   train_loss = 1.081\n",
      "Epoch  46 Batch   64/96   train_loss = 1.075\n",
      "Epoch  47 Batch    0/96   train_loss = 1.026\n",
      "Epoch  47 Batch   32/96   train_loss = 1.084\n",
      "Epoch  47 Batch   64/96   train_loss = 1.060\n",
      "Epoch  48 Batch    0/96   train_loss = 1.036\n",
      "Epoch  48 Batch   32/96   train_loss = 1.086\n",
      "Epoch  48 Batch   64/96   train_loss = 1.061\n",
      "Epoch  49 Batch    0/96   train_loss = 1.034\n",
      "Epoch  49 Batch   32/96   train_loss = 1.092\n",
      "Epoch  49 Batch   64/96   train_loss = 1.048\n",
      "Epoch  50 Batch    0/96   train_loss = 1.027\n",
      "Epoch  50 Batch   32/96   train_loss = 1.096\n",
      "Epoch  50 Batch   64/96   train_loss = 1.036\n",
      "Epoch  51 Batch    0/96   train_loss = 1.041\n",
      "Epoch  51 Batch   32/96   train_loss = 1.087\n",
      "Epoch  51 Batch   64/96   train_loss = 1.052\n",
      "Epoch  52 Batch    0/96   train_loss = 1.018\n",
      "Epoch  52 Batch   32/96   train_loss = 1.097\n",
      "Epoch  52 Batch   64/96   train_loss = 1.060\n",
      "Epoch  53 Batch    0/96   train_loss = 1.013\n",
      "Epoch  53 Batch   32/96   train_loss = 1.084\n",
      "Epoch  53 Batch   64/96   train_loss = 1.052\n",
      "Epoch  54 Batch    0/96   train_loss = 1.030\n",
      "Epoch  54 Batch   32/96   train_loss = 1.089\n",
      "Epoch  54 Batch   64/96   train_loss = 1.051\n",
      "Epoch  55 Batch    0/96   train_loss = 1.033\n",
      "Epoch  55 Batch   32/96   train_loss = 1.072\n",
      "Epoch  55 Batch   64/96   train_loss = 1.035\n",
      "Epoch  56 Batch    0/96   train_loss = 1.019\n",
      "Epoch  56 Batch   32/96   train_loss = 1.064\n",
      "Epoch  56 Batch   64/96   train_loss = 1.034\n",
      "Epoch  57 Batch    0/96   train_loss = 1.021\n",
      "Epoch  57 Batch   32/96   train_loss = 1.065\n",
      "Epoch  57 Batch   64/96   train_loss = 1.024\n",
      "Epoch  58 Batch    0/96   train_loss = 1.009\n",
      "Epoch  58 Batch   32/96   train_loss = 1.061\n",
      "Epoch  58 Batch   64/96   train_loss = 1.027\n",
      "Epoch  59 Batch    0/96   train_loss = 0.999\n",
      "Epoch  59 Batch   32/96   train_loss = 1.059\n",
      "Epoch  59 Batch   64/96   train_loss = 1.022\n",
      "Epoch  60 Batch    0/96   train_loss = 0.988\n",
      "Epoch  60 Batch   32/96   train_loss = 1.055\n",
      "Epoch  60 Batch   64/96   train_loss = 1.022\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  61 Batch    0/96   train_loss = 0.992\n",
      "Epoch  61 Batch   32/96   train_loss = 1.053\n",
      "Epoch  61 Batch   64/96   train_loss = 1.023\n",
      "Epoch  62 Batch    0/96   train_loss = 0.981\n",
      "Epoch  62 Batch   32/96   train_loss = 1.059\n",
      "Epoch  62 Batch   64/96   train_loss = 1.009\n",
      "Epoch  63 Batch    0/96   train_loss = 0.984\n",
      "Epoch  63 Batch   32/96   train_loss = 1.050\n",
      "Epoch  63 Batch   64/96   train_loss = 1.013\n",
      "Epoch  64 Batch    0/96   train_loss = 0.979\n",
      "Epoch  64 Batch   32/96   train_loss = 1.064\n",
      "Epoch  64 Batch   64/96   train_loss = 1.015\n",
      "Epoch  65 Batch    0/96   train_loss = 0.990\n",
      "Epoch  65 Batch   32/96   train_loss = 1.059\n",
      "Epoch  65 Batch   64/96   train_loss = 1.018\n",
      "Epoch  66 Batch    0/96   train_loss = 0.985\n",
      "Epoch  66 Batch   32/96   train_loss = 1.045\n",
      "Epoch  66 Batch   64/96   train_loss = 1.025\n",
      "Epoch  67 Batch    0/96   train_loss = 0.988\n",
      "Epoch  67 Batch   32/96   train_loss = 1.056\n",
      "Epoch  67 Batch   64/96   train_loss = 1.041\n",
      "Epoch  68 Batch    0/96   train_loss = 0.994\n",
      "Epoch  68 Batch   32/96   train_loss = 1.048\n",
      "Epoch  68 Batch   64/96   train_loss = 1.030\n",
      "Epoch  69 Batch    0/96   train_loss = 0.991\n",
      "Epoch  69 Batch   32/96   train_loss = 1.067\n",
      "Epoch  69 Batch   64/96   train_loss = 1.025\n",
      "Epoch  70 Batch    0/96   train_loss = 0.991\n",
      "Epoch  70 Batch   32/96   train_loss = 1.064\n",
      "Epoch  70 Batch   64/96   train_loss = 1.033\n",
      "Epoch  71 Batch    0/96   train_loss = 1.007\n",
      "Epoch  71 Batch   32/96   train_loss = 1.057\n",
      "Epoch  71 Batch   64/96   train_loss = 1.029\n",
      "Epoch  72 Batch    0/96   train_loss = 0.992\n",
      "Epoch  72 Batch   32/96   train_loss = 1.046\n",
      "Epoch  72 Batch   64/96   train_loss = 1.028\n",
      "Epoch  73 Batch    0/96   train_loss = 0.979\n",
      "Epoch  73 Batch   32/96   train_loss = 1.040\n",
      "Epoch  73 Batch   64/96   train_loss = 1.025\n",
      "Epoch  74 Batch    0/96   train_loss = 0.973\n",
      "Epoch  74 Batch   32/96   train_loss = 1.042\n",
      "Epoch  74 Batch   64/96   train_loss = 1.026\n",
      "Epoch  75 Batch    0/96   train_loss = 0.982\n",
      "Epoch  75 Batch   32/96   train_loss = 1.045\n",
      "Epoch  75 Batch   64/96   train_loss = 1.020\n",
      "Epoch  76 Batch    0/96   train_loss = 0.972\n",
      "Epoch  76 Batch   32/96   train_loss = 1.034\n",
      "Epoch  76 Batch   64/96   train_loss = 1.020\n",
      "Epoch  77 Batch    0/96   train_loss = 0.978\n",
      "Epoch  77 Batch   32/96   train_loss = 1.043\n",
      "Epoch  77 Batch   64/96   train_loss = 1.022\n",
      "Epoch  78 Batch    0/96   train_loss = 0.975\n",
      "Epoch  78 Batch   32/96   train_loss = 1.037\n",
      "Epoch  78 Batch   64/96   train_loss = 1.009\n",
      "Epoch  79 Batch    0/96   train_loss = 0.971\n",
      "Epoch  79 Batch   32/96   train_loss = 1.038\n",
      "Epoch  79 Batch   64/96   train_loss = 1.006\n",
      "Epoch  80 Batch    0/96   train_loss = 0.977\n",
      "Epoch  80 Batch   32/96   train_loss = 1.032\n",
      "Epoch  80 Batch   64/96   train_loss = 1.009\n",
      "Epoch  81 Batch    0/96   train_loss = 0.970\n",
      "Epoch  81 Batch   32/96   train_loss = 1.037\n",
      "Epoch  81 Batch   64/96   train_loss = 1.007\n",
      "Epoch  82 Batch    0/96   train_loss = 0.983\n",
      "Epoch  82 Batch   32/96   train_loss = 1.034\n",
      "Epoch  82 Batch   64/96   train_loss = 1.006\n",
      "Epoch  83 Batch    0/96   train_loss = 0.976\n",
      "Epoch  83 Batch   32/96   train_loss = 1.035\n",
      "Epoch  83 Batch   64/96   train_loss = 1.007\n",
      "Epoch  84 Batch    0/96   train_loss = 0.976\n",
      "Epoch  84 Batch   32/96   train_loss = 1.026\n",
      "Epoch  84 Batch   64/96   train_loss = 1.002\n",
      "Epoch  85 Batch    0/96   train_loss = 0.972\n",
      "Epoch  85 Batch   32/96   train_loss = 1.034\n",
      "Epoch  85 Batch   64/96   train_loss = 1.004\n",
      "Epoch  86 Batch    0/96   train_loss = 0.970\n",
      "Epoch  86 Batch   32/96   train_loss = 1.036\n",
      "Epoch  86 Batch   64/96   train_loss = 1.006\n",
      "Epoch  87 Batch    0/96   train_loss = 0.980\n",
      "Epoch  87 Batch   32/96   train_loss = 1.027\n",
      "Epoch  87 Batch   64/96   train_loss = 1.013\n",
      "Epoch  88 Batch    0/96   train_loss = 0.974\n",
      "Epoch  88 Batch   32/96   train_loss = 1.035\n",
      "Epoch  88 Batch   64/96   train_loss = 1.010\n",
      "Epoch  89 Batch    0/96   train_loss = 0.981\n",
      "Epoch  89 Batch   32/96   train_loss = 1.032\n",
      "Epoch  89 Batch   64/96   train_loss = 1.006\n",
      "Epoch  90 Batch    0/96   train_loss = 0.976\n",
      "Epoch  90 Batch   32/96   train_loss = 1.034\n",
      "Epoch  90 Batch   64/96   train_loss = 1.003\n",
      "Epoch  91 Batch    0/96   train_loss = 0.990\n",
      "Epoch  91 Batch   32/96   train_loss = 1.044\n",
      "Epoch  91 Batch   64/96   train_loss = 1.001\n",
      "Epoch  92 Batch    0/96   train_loss = 0.971\n",
      "Epoch  92 Batch   32/96   train_loss = 1.038\n",
      "Epoch  92 Batch   64/96   train_loss = 1.011\n",
      "Epoch  93 Batch    0/96   train_loss = 0.987\n",
      "Epoch  93 Batch   32/96   train_loss = 1.049\n",
      "Epoch  93 Batch   64/96   train_loss = 1.008\n",
      "Epoch  94 Batch    0/96   train_loss = 0.998\n",
      "Epoch  94 Batch   32/96   train_loss = 1.048\n",
      "Epoch  94 Batch   64/96   train_loss = 1.014\n",
      "Epoch  95 Batch    0/96   train_loss = 0.994\n",
      "Epoch  95 Batch   32/96   train_loss = 1.057\n",
      "Epoch  95 Batch   64/96   train_loss = 1.019\n",
      "Epoch  96 Batch    0/96   train_loss = 1.008\n",
      "Epoch  96 Batch   32/96   train_loss = 1.049\n",
      "Epoch  96 Batch   64/96   train_loss = 1.029\n",
      "Epoch  97 Batch    0/96   train_loss = 0.994\n",
      "Epoch  97 Batch   32/96   train_loss = 1.044\n",
      "Epoch  97 Batch   64/96   train_loss = 1.035\n",
      "Epoch  98 Batch    0/96   train_loss = 0.994\n",
      "Epoch  98 Batch   32/96   train_loss = 1.034\n",
      "Epoch  98 Batch   64/96   train_loss = 1.006\n",
      "Epoch  99 Batch    0/96   train_loss = 0.988\n",
      "Epoch  99 Batch   32/96   train_loss = 1.050\n",
      "Epoch  99 Batch   64/96   train_loss = 1.004\n",
      "Epoch 100 Batch    0/96   train_loss = 0.989\n",
      "Epoch 100 Batch   32/96   train_loss = 1.065\n",
      "Epoch 100 Batch   64/96   train_loss = 1.003\n",
      "Epoch 101 Batch    0/96   train_loss = 0.976\n",
      "Epoch 101 Batch   32/96   train_loss = 1.059\n",
      "Epoch 101 Batch   64/96   train_loss = 1.002\n",
      "Epoch 102 Batch    0/96   train_loss = 0.977\n",
      "Epoch 102 Batch   32/96   train_loss = 1.049\n",
      "Epoch 102 Batch   64/96   train_loss = 0.997\n",
      "Epoch 103 Batch    0/96   train_loss = 0.967\n",
      "Epoch 103 Batch   32/96   train_loss = 1.038\n",
      "Epoch 103 Batch   64/96   train_loss = 0.987\n",
      "Epoch 104 Batch    0/96   train_loss = 0.964\n",
      "Epoch 104 Batch   32/96   train_loss = 1.026\n",
      "Epoch 104 Batch   64/96   train_loss = 1.006\n",
      "Epoch 105 Batch    0/96   train_loss = 0.964\n",
      "Epoch 105 Batch   32/96   train_loss = 1.026\n",
      "Epoch 105 Batch   64/96   train_loss = 0.991\n",
      "Epoch 106 Batch    0/96   train_loss = 0.959\n",
      "Epoch 106 Batch   32/96   train_loss = 1.023\n",
      "Epoch 106 Batch   64/96   train_loss = 0.991\n",
      "Epoch 107 Batch    0/96   train_loss = 0.960\n",
      "Epoch 107 Batch   32/96   train_loss = 1.031\n",
      "Epoch 107 Batch   64/96   train_loss = 0.987\n",
      "Epoch 108 Batch    0/96   train_loss = 0.967\n",
      "Epoch 108 Batch   32/96   train_loss = 1.017\n",
      "Epoch 108 Batch   64/96   train_loss = 0.995\n",
      "Epoch 109 Batch    0/96   train_loss = 0.961\n",
      "Epoch 109 Batch   32/96   train_loss = 1.015\n",
      "Epoch 109 Batch   64/96   train_loss = 0.992\n",
      "Epoch 110 Batch    0/96   train_loss = 0.965\n",
      "Epoch 110 Batch   32/96   train_loss = 1.022\n",
      "Epoch 110 Batch   64/96   train_loss = 0.996\n",
      "Epoch 111 Batch    0/96   train_loss = 0.960\n",
      "Epoch 111 Batch   32/96   train_loss = 1.016\n",
      "Epoch 111 Batch   64/96   train_loss = 0.989\n",
      "Epoch 112 Batch    0/96   train_loss = 0.957\n",
      "Epoch 112 Batch   32/96   train_loss = 1.019\n",
      "Epoch 112 Batch   64/96   train_loss = 0.989\n",
      "Epoch 113 Batch    0/96   train_loss = 0.956\n",
      "Epoch 113 Batch   32/96   train_loss = 1.019\n",
      "Epoch 113 Batch   64/96   train_loss = 0.990\n",
      "Epoch 114 Batch    0/96   train_loss = 0.966\n",
      "Epoch 114 Batch   32/96   train_loss = 1.017\n",
      "Epoch 114 Batch   64/96   train_loss = 0.995\n",
      "Epoch 115 Batch    0/96   train_loss = 0.963\n",
      "Epoch 115 Batch   32/96   train_loss = 1.026\n",
      "Epoch 115 Batch   64/96   train_loss = 0.981\n",
      "Epoch 116 Batch    0/96   train_loss = 0.962\n",
      "Epoch 116 Batch   32/96   train_loss = 1.012\n",
      "Epoch 116 Batch   64/96   train_loss = 0.995\n",
      "Epoch 117 Batch    0/96   train_loss = 0.946\n",
      "Epoch 117 Batch   32/96   train_loss = 1.015\n",
      "Epoch 117 Batch   64/96   train_loss = 0.993\n",
      "Epoch 118 Batch    0/96   train_loss = 0.950\n",
      "Epoch 118 Batch   32/96   train_loss = 1.024\n",
      "Epoch 118 Batch   64/96   train_loss = 0.981\n",
      "Epoch 119 Batch    0/96   train_loss = 0.946\n",
      "Epoch 119 Batch   32/96   train_loss = 1.026\n",
      "Epoch 119 Batch   64/96   train_loss = 0.985\n",
      "Epoch 120 Batch    0/96   train_loss = 0.951\n",
      "Epoch 120 Batch   32/96   train_loss = 1.012\n",
      "Epoch 120 Batch   64/96   train_loss = 0.977\n",
      "Epoch 121 Batch    0/96   train_loss = 0.948\n",
      "Epoch 121 Batch   32/96   train_loss = 1.011\n",
      "Epoch 121 Batch   64/96   train_loss = 0.988\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 122 Batch    0/96   train_loss = 0.958\n",
      "Epoch 122 Batch   32/96   train_loss = 1.029\n",
      "Epoch 122 Batch   64/96   train_loss = 0.985\n",
      "Epoch 123 Batch    0/96   train_loss = 0.957\n",
      "Epoch 123 Batch   32/96   train_loss = 1.022\n",
      "Epoch 123 Batch   64/96   train_loss = 0.998\n",
      "Epoch 124 Batch    0/96   train_loss = 0.953\n",
      "Epoch 124 Batch   32/96   train_loss = 1.004\n",
      "Epoch 124 Batch   64/96   train_loss = 0.991\n",
      "Epoch 125 Batch    0/96   train_loss = 0.949\n",
      "Epoch 125 Batch   32/96   train_loss = 1.014\n",
      "Epoch 125 Batch   64/96   train_loss = 0.982\n",
      "Epoch 126 Batch    0/96   train_loss = 0.942\n",
      "Epoch 126 Batch   32/96   train_loss = 1.011\n",
      "Epoch 126 Batch   64/96   train_loss = 0.989\n",
      "Epoch 127 Batch    0/96   train_loss = 0.956\n",
      "Epoch 127 Batch   32/96   train_loss = 1.009\n",
      "Epoch 127 Batch   64/96   train_loss = 0.979\n",
      "Epoch 128 Batch    0/96   train_loss = 0.945\n",
      "Epoch 128 Batch   32/96   train_loss = 1.013\n",
      "Epoch 128 Batch   64/96   train_loss = 0.976\n",
      "Epoch 129 Batch    0/96   train_loss = 0.950\n",
      "Epoch 129 Batch   32/96   train_loss = 1.014\n",
      "Epoch 129 Batch   64/96   train_loss = 0.974\n",
      "Epoch 130 Batch    0/96   train_loss = 0.952\n",
      "Epoch 130 Batch   32/96   train_loss = 1.022\n",
      "Epoch 130 Batch   64/96   train_loss = 0.979\n",
      "Epoch 131 Batch    0/96   train_loss = 0.953\n",
      "Epoch 131 Batch   32/96   train_loss = 1.025\n",
      "Epoch 131 Batch   64/96   train_loss = 0.979\n",
      "Epoch 132 Batch    0/96   train_loss = 0.949\n",
      "Epoch 132 Batch   32/96   train_loss = 1.018\n",
      "Epoch 132 Batch   64/96   train_loss = 0.988\n",
      "Epoch 133 Batch    0/96   train_loss = 0.960\n",
      "Epoch 133 Batch   32/96   train_loss = 1.028\n",
      "Epoch 133 Batch   64/96   train_loss = 0.981\n",
      "Epoch 134 Batch    0/96   train_loss = 0.969\n",
      "Epoch 134 Batch   32/96   train_loss = 1.020\n",
      "Epoch 134 Batch   64/96   train_loss = 0.978\n",
      "Epoch 135 Batch    0/96   train_loss = 0.963\n",
      "Epoch 135 Batch   32/96   train_loss = 1.028\n",
      "Epoch 135 Batch   64/96   train_loss = 0.969\n",
      "Epoch 136 Batch    0/96   train_loss = 0.956\n",
      "Epoch 136 Batch   32/96   train_loss = 1.012\n",
      "Epoch 136 Batch   64/96   train_loss = 0.984\n",
      "Epoch 137 Batch    0/96   train_loss = 0.947\n",
      "Epoch 137 Batch   32/96   train_loss = 1.019\n",
      "Epoch 137 Batch   64/96   train_loss = 0.962\n",
      "Epoch 138 Batch    0/96   train_loss = 0.949\n",
      "Epoch 138 Batch   32/96   train_loss = 1.016\n",
      "Epoch 138 Batch   64/96   train_loss = 0.969\n",
      "Epoch 139 Batch    0/96   train_loss = 0.951\n",
      "Epoch 139 Batch   32/96   train_loss = 1.007\n",
      "Epoch 139 Batch   64/96   train_loss = 0.972\n",
      "Epoch 140 Batch    0/96   train_loss = 0.946\n",
      "Epoch 140 Batch   32/96   train_loss = 1.011\n",
      "Epoch 140 Batch   64/96   train_loss = 0.972\n",
      "Epoch 141 Batch    0/96   train_loss = 0.950\n",
      "Epoch 141 Batch   32/96   train_loss = 1.023\n",
      "Epoch 141 Batch   64/96   train_loss = 0.971\n",
      "Epoch 142 Batch    0/96   train_loss = 0.953\n",
      "Epoch 142 Batch   32/96   train_loss = 1.021\n",
      "Epoch 142 Batch   64/96   train_loss = 0.959\n",
      "Epoch 143 Batch    0/96   train_loss = 0.936\n",
      "Epoch 143 Batch   32/96   train_loss = 1.007\n",
      "Epoch 143 Batch   64/96   train_loss = 0.968\n",
      "Epoch 144 Batch    0/96   train_loss = 0.950\n",
      "Epoch 144 Batch   32/96   train_loss = 1.011\n",
      "Epoch 144 Batch   64/96   train_loss = 0.968\n",
      "Epoch 145 Batch    0/96   train_loss = 0.950\n",
      "Epoch 145 Batch   32/96   train_loss = 1.011\n",
      "Epoch 145 Batch   64/96   train_loss = 0.979\n",
      "Epoch 146 Batch    0/96   train_loss = 0.952\n",
      "Epoch 146 Batch   32/96   train_loss = 1.012\n",
      "Epoch 146 Batch   64/96   train_loss = 0.988\n",
      "Epoch 147 Batch    0/96   train_loss = 0.946\n",
      "Epoch 147 Batch   32/96   train_loss = 1.022\n",
      "Epoch 147 Batch   64/96   train_loss = 0.977\n",
      "Epoch 148 Batch    0/96   train_loss = 0.952\n",
      "Epoch 148 Batch   32/96   train_loss = 1.026\n",
      "Epoch 148 Batch   64/96   train_loss = 0.977\n",
      "Epoch 149 Batch    0/96   train_loss = 0.964\n",
      "Epoch 149 Batch   32/96   train_loss = 1.017\n",
      "Epoch 149 Batch   64/96   train_loss = 0.973\n",
      "Epoch 150 Batch    0/96   train_loss = 0.944\n",
      "Epoch 150 Batch   32/96   train_loss = 1.015\n",
      "Epoch 150 Batch   64/96   train_loss = 0.980\n",
      "Epoch 151 Batch    0/96   train_loss = 0.945\n",
      "Epoch 151 Batch   32/96   train_loss = 1.019\n",
      "Epoch 151 Batch   64/96   train_loss = 0.993\n",
      "Epoch 152 Batch    0/96   train_loss = 0.942\n",
      "Epoch 152 Batch   32/96   train_loss = 1.015\n",
      "Epoch 152 Batch   64/96   train_loss = 0.977\n",
      "Epoch 153 Batch    0/96   train_loss = 0.953\n",
      "Epoch 153 Batch   32/96   train_loss = 1.023\n",
      "Epoch 153 Batch   64/96   train_loss = 0.977\n",
      "Epoch 154 Batch    0/96   train_loss = 0.963\n",
      "Epoch 154 Batch   32/96   train_loss = 1.017\n",
      "Epoch 154 Batch   64/96   train_loss = 0.980\n",
      "Epoch 155 Batch    0/96   train_loss = 0.949\n",
      "Epoch 155 Batch   32/96   train_loss = 1.011\n",
      "Epoch 155 Batch   64/96   train_loss = 0.969\n",
      "Epoch 156 Batch    0/96   train_loss = 0.960\n",
      "Epoch 156 Batch   32/96   train_loss = 1.018\n",
      "Epoch 156 Batch   64/96   train_loss = 0.977\n",
      "Epoch 157 Batch    0/96   train_loss = 0.960\n",
      "Epoch 157 Batch   32/96   train_loss = 1.030\n",
      "Epoch 157 Batch   64/96   train_loss = 0.985\n",
      "Epoch 158 Batch    0/96   train_loss = 0.957\n",
      "Epoch 158 Batch   32/96   train_loss = 1.014\n",
      "Epoch 158 Batch   64/96   train_loss = 0.985\n",
      "Epoch 159 Batch    0/96   train_loss = 0.952\n",
      "Epoch 159 Batch   32/96   train_loss = 1.017\n",
      "Epoch 159 Batch   64/96   train_loss = 0.974\n",
      "Epoch 160 Batch    0/96   train_loss = 0.945\n",
      "Epoch 160 Batch   32/96   train_loss = 1.019\n",
      "Epoch 160 Batch   64/96   train_loss = 0.978\n",
      "Epoch 161 Batch    0/96   train_loss = 0.952\n",
      "Epoch 161 Batch   32/96   train_loss = 1.020\n",
      "Epoch 161 Batch   64/96   train_loss = 0.973\n",
      "Epoch 162 Batch    0/96   train_loss = 0.943\n",
      "Epoch 162 Batch   32/96   train_loss = 1.013\n",
      "Epoch 162 Batch   64/96   train_loss = 0.965\n",
      "Epoch 163 Batch    0/96   train_loss = 0.957\n",
      "Epoch 163 Batch   32/96   train_loss = 1.014\n",
      "Epoch 163 Batch   64/96   train_loss = 0.965\n",
      "Epoch 164 Batch    0/96   train_loss = 0.961\n",
      "Epoch 164 Batch   32/96   train_loss = 1.013\n",
      "Epoch 164 Batch   64/96   train_loss = 0.977\n",
      "Epoch 165 Batch    0/96   train_loss = 0.956\n",
      "Epoch 165 Batch   32/96   train_loss = 1.021\n",
      "Epoch 165 Batch   64/96   train_loss = 0.978\n",
      "Epoch 166 Batch    0/96   train_loss = 0.960\n",
      "Epoch 166 Batch   32/96   train_loss = 1.015\n",
      "Epoch 166 Batch   64/96   train_loss = 0.978\n",
      "Epoch 167 Batch    0/96   train_loss = 0.959\n",
      "Epoch 167 Batch   32/96   train_loss = 1.011\n",
      "Epoch 167 Batch   64/96   train_loss = 0.975\n",
      "Epoch 168 Batch    0/96   train_loss = 0.958\n",
      "Epoch 168 Batch   32/96   train_loss = 1.030\n",
      "Epoch 168 Batch   64/96   train_loss = 0.977\n",
      "Epoch 169 Batch    0/96   train_loss = 0.958\n",
      "Epoch 169 Batch   32/96   train_loss = 1.021\n",
      "Epoch 169 Batch   64/96   train_loss = 0.986\n",
      "Epoch 170 Batch    0/96   train_loss = 0.946\n",
      "Epoch 170 Batch   32/96   train_loss = 1.019\n",
      "Epoch 170 Batch   64/96   train_loss = 0.988\n",
      "Epoch 171 Batch    0/96   train_loss = 0.952\n",
      "Epoch 171 Batch   32/96   train_loss = 1.012\n",
      "Epoch 171 Batch   64/96   train_loss = 0.981\n",
      "Epoch 172 Batch    0/96   train_loss = 0.946\n",
      "Epoch 172 Batch   32/96   train_loss = 1.015\n",
      "Epoch 172 Batch   64/96   train_loss = 0.982\n",
      "Epoch 173 Batch    0/96   train_loss = 0.952\n",
      "Epoch 173 Batch   32/96   train_loss = 0.995\n",
      "Epoch 173 Batch   64/96   train_loss = 0.975\n",
      "Epoch 174 Batch    0/96   train_loss = 0.946\n",
      "Epoch 174 Batch   32/96   train_loss = 1.010\n",
      "Epoch 174 Batch   64/96   train_loss = 0.975\n",
      "Epoch 175 Batch    0/96   train_loss = 0.945\n",
      "Epoch 175 Batch   32/96   train_loss = 1.005\n",
      "Epoch 175 Batch   64/96   train_loss = 0.972\n",
      "Epoch 176 Batch    0/96   train_loss = 0.943\n",
      "Epoch 176 Batch   32/96   train_loss = 1.013\n",
      "Epoch 176 Batch   64/96   train_loss = 0.977\n",
      "Epoch 177 Batch    0/96   train_loss = 0.936\n",
      "Epoch 177 Batch   32/96   train_loss = 1.025\n",
      "Epoch 177 Batch   64/96   train_loss = 0.962\n",
      "Epoch 178 Batch    0/96   train_loss = 0.947\n",
      "Epoch 178 Batch   32/96   train_loss = 1.020\n",
      "Epoch 178 Batch   64/96   train_loss = 0.963\n",
      "Epoch 179 Batch    0/96   train_loss = 0.941\n",
      "Epoch 179 Batch   32/96   train_loss = 1.019\n",
      "Epoch 179 Batch   64/96   train_loss = 0.969\n",
      "Epoch 180 Batch    0/96   train_loss = 0.932\n",
      "Epoch 180 Batch   32/96   train_loss = 1.013\n",
      "Epoch 180 Batch   64/96   train_loss = 0.977\n",
      "Epoch 181 Batch    0/96   train_loss = 0.929\n",
      "Epoch 181 Batch   32/96   train_loss = 1.008\n",
      "Epoch 181 Batch   64/96   train_loss = 0.972\n",
      "Epoch 182 Batch    0/96   train_loss = 0.939\n",
      "Epoch 182 Batch   32/96   train_loss = 0.995\n",
      "Epoch 182 Batch   64/96   train_loss = 0.970\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 183 Batch    0/96   train_loss = 0.943\n",
      "Epoch 183 Batch   32/96   train_loss = 1.003\n",
      "Epoch 183 Batch   64/96   train_loss = 0.964\n",
      "Epoch 184 Batch    0/96   train_loss = 0.930\n",
      "Epoch 184 Batch   32/96   train_loss = 0.997\n",
      "Epoch 184 Batch   64/96   train_loss = 0.960\n",
      "Epoch 185 Batch    0/96   train_loss = 0.948\n",
      "Epoch 185 Batch   32/96   train_loss = 1.004\n",
      "Epoch 185 Batch   64/96   train_loss = 0.960\n",
      "Epoch 186 Batch    0/96   train_loss = 0.937\n",
      "Epoch 186 Batch   32/96   train_loss = 1.000\n",
      "Epoch 186 Batch   64/96   train_loss = 0.973\n",
      "Epoch 187 Batch    0/96   train_loss = 0.932\n",
      "Epoch 187 Batch   32/96   train_loss = 1.007\n",
      "Epoch 187 Batch   64/96   train_loss = 0.979\n",
      "Epoch 188 Batch    0/96   train_loss = 0.937\n",
      "Epoch 188 Batch   32/96   train_loss = 1.008\n",
      "Epoch 188 Batch   64/96   train_loss = 0.975\n",
      "Epoch 189 Batch    0/96   train_loss = 0.938\n",
      "Epoch 189 Batch   32/96   train_loss = 1.006\n",
      "Epoch 189 Batch   64/96   train_loss = 0.967\n",
      "Epoch 190 Batch    0/96   train_loss = 0.929\n",
      "Epoch 190 Batch   32/96   train_loss = 1.010\n",
      "Epoch 190 Batch   64/96   train_loss = 0.964\n",
      "Epoch 191 Batch    0/96   train_loss = 0.930\n",
      "Epoch 191 Batch   32/96   train_loss = 0.989\n",
      "Epoch 191 Batch   64/96   train_loss = 0.975\n",
      "Epoch 192 Batch    0/96   train_loss = 0.923\n",
      "Epoch 192 Batch   32/96   train_loss = 0.996\n",
      "Epoch 192 Batch   64/96   train_loss = 0.967\n",
      "Epoch 193 Batch    0/96   train_loss = 0.926\n",
      "Epoch 193 Batch   32/96   train_loss = 0.992\n",
      "Epoch 193 Batch   64/96   train_loss = 0.974\n",
      "Epoch 194 Batch    0/96   train_loss = 0.926\n",
      "Epoch 194 Batch   32/96   train_loss = 0.996\n",
      "Epoch 194 Batch   64/96   train_loss = 0.967\n",
      "Epoch 195 Batch    0/96   train_loss = 0.924\n",
      "Epoch 195 Batch   32/96   train_loss = 0.993\n",
      "Epoch 195 Batch   64/96   train_loss = 0.967\n",
      "Epoch 196 Batch    0/96   train_loss = 0.926\n",
      "Epoch 196 Batch   32/96   train_loss = 0.975\n",
      "Epoch 196 Batch   64/96   train_loss = 0.965\n",
      "Epoch 197 Batch    0/96   train_loss = 0.933\n",
      "Epoch 197 Batch   32/96   train_loss = 0.993\n",
      "Epoch 197 Batch   64/96   train_loss = 0.964\n",
      "Epoch 198 Batch    0/96   train_loss = 0.940\n",
      "Epoch 198 Batch   32/96   train_loss = 0.993\n",
      "Epoch 198 Batch   64/96   train_loss = 0.955\n",
      "Epoch 199 Batch    0/96   train_loss = 0.924\n",
      "Epoch 199 Batch   32/96   train_loss = 0.982\n",
      "Epoch 199 Batch   64/96   train_loss = 0.976\n",
      "Epoch 200 Batch    0/96   train_loss = 0.931\n",
      "Epoch 200 Batch   32/96   train_loss = 0.987\n",
      "Epoch 200 Batch   64/96   train_loss = 0.956\n",
      "Epoch 201 Batch    0/96   train_loss = 0.918\n",
      "Epoch 201 Batch   32/96   train_loss = 0.998\n",
      "Epoch 201 Batch   64/96   train_loss = 0.956\n",
      "Epoch 202 Batch    0/96   train_loss = 0.929\n",
      "Epoch 202 Batch   32/96   train_loss = 0.993\n",
      "Epoch 202 Batch   64/96   train_loss = 0.969\n",
      "Epoch 203 Batch    0/96   train_loss = 0.929\n",
      "Epoch 203 Batch   32/96   train_loss = 1.011\n",
      "Epoch 203 Batch   64/96   train_loss = 0.968\n",
      "Epoch 204 Batch    0/96   train_loss = 0.947\n",
      "Epoch 204 Batch   32/96   train_loss = 1.002\n",
      "Epoch 204 Batch   64/96   train_loss = 0.976\n",
      "Epoch 205 Batch    0/96   train_loss = 0.936\n",
      "Epoch 205 Batch   32/96   train_loss = 0.995\n",
      "Epoch 205 Batch   64/96   train_loss = 0.967\n",
      "Epoch 206 Batch    0/96   train_loss = 0.930\n",
      "Epoch 206 Batch   32/96   train_loss = 1.000\n",
      "Epoch 206 Batch   64/96   train_loss = 0.969\n",
      "Epoch 207 Batch    0/96   train_loss = 0.932\n",
      "Epoch 207 Batch   32/96   train_loss = 0.999\n",
      "Epoch 207 Batch   64/96   train_loss = 0.958\n",
      "Epoch 208 Batch    0/96   train_loss = 0.935\n",
      "Epoch 208 Batch   32/96   train_loss = 1.001\n",
      "Epoch 208 Batch   64/96   train_loss = 0.967\n",
      "Epoch 209 Batch    0/96   train_loss = 0.928\n",
      "Epoch 209 Batch   32/96   train_loss = 1.008\n",
      "Epoch 209 Batch   64/96   train_loss = 0.970\n",
      "Epoch 210 Batch    0/96   train_loss = 0.934\n",
      "Epoch 210 Batch   32/96   train_loss = 0.994\n",
      "Epoch 210 Batch   64/96   train_loss = 0.975\n",
      "Epoch 211 Batch    0/96   train_loss = 0.939\n",
      "Epoch 211 Batch   32/96   train_loss = 1.006\n",
      "Epoch 211 Batch   64/96   train_loss = 0.969\n",
      "Epoch 212 Batch    0/96   train_loss = 0.944\n",
      "Epoch 212 Batch   32/96   train_loss = 1.003\n",
      "Epoch 212 Batch   64/96   train_loss = 0.982\n",
      "Epoch 213 Batch    0/96   train_loss = 0.927\n",
      "Epoch 213 Batch   32/96   train_loss = 0.996\n",
      "Epoch 213 Batch   64/96   train_loss = 0.959\n",
      "Epoch 214 Batch    0/96   train_loss = 0.935\n",
      "Epoch 214 Batch   32/96   train_loss = 0.989\n",
      "Epoch 214 Batch   64/96   train_loss = 0.958\n",
      "Epoch 215 Batch    0/96   train_loss = 0.930\n",
      "Epoch 215 Batch   32/96   train_loss = 0.992\n",
      "Epoch 215 Batch   64/96   train_loss = 0.954\n",
      "Epoch 216 Batch    0/96   train_loss = 0.937\n",
      "Epoch 216 Batch   32/96   train_loss = 0.998\n",
      "Epoch 216 Batch   64/96   train_loss = 0.959\n",
      "Epoch 217 Batch    0/96   train_loss = 0.947\n",
      "Epoch 217 Batch   32/96   train_loss = 1.005\n",
      "Epoch 217 Batch   64/96   train_loss = 0.968\n",
      "Epoch 218 Batch    0/96   train_loss = 0.936\n",
      "Epoch 218 Batch   32/96   train_loss = 0.999\n",
      "Epoch 218 Batch   64/96   train_loss = 0.972\n",
      "Epoch 219 Batch    0/96   train_loss = 0.925\n",
      "Epoch 219 Batch   32/96   train_loss = 0.994\n",
      "Epoch 219 Batch   64/96   train_loss = 0.969\n",
      "Epoch 220 Batch    0/96   train_loss = 0.937\n",
      "Epoch 220 Batch   32/96   train_loss = 1.002\n",
      "Epoch 220 Batch   64/96   train_loss = 0.988\n",
      "Epoch 221 Batch    0/96   train_loss = 0.941\n",
      "Epoch 221 Batch   32/96   train_loss = 0.993\n",
      "Epoch 221 Batch   64/96   train_loss = 0.975\n",
      "Epoch 222 Batch    0/96   train_loss = 0.932\n",
      "Epoch 222 Batch   32/96   train_loss = 0.997\n",
      "Epoch 222 Batch   64/96   train_loss = 0.955\n",
      "Epoch 223 Batch    0/96   train_loss = 0.929\n",
      "Epoch 223 Batch   32/96   train_loss = 0.993\n",
      "Epoch 223 Batch   64/96   train_loss = 0.968\n",
      "Epoch 224 Batch    0/96   train_loss = 0.935\n",
      "Epoch 224 Batch   32/96   train_loss = 1.011\n",
      "Epoch 224 Batch   64/96   train_loss = 0.961\n",
      "Epoch 225 Batch    0/96   train_loss = 0.925\n",
      "Epoch 225 Batch   32/96   train_loss = 0.998\n",
      "Epoch 225 Batch   64/96   train_loss = 0.969\n",
      "Epoch 226 Batch    0/96   train_loss = 0.924\n",
      "Epoch 226 Batch   32/96   train_loss = 1.000\n",
      "Epoch 226 Batch   64/96   train_loss = 0.965\n",
      "Epoch 227 Batch    0/96   train_loss = 0.920\n",
      "Epoch 227 Batch   32/96   train_loss = 0.994\n",
      "Epoch 227 Batch   64/96   train_loss = 0.966\n",
      "Epoch 228 Batch    0/96   train_loss = 0.931\n",
      "Epoch 228 Batch   32/96   train_loss = 1.001\n",
      "Epoch 228 Batch   64/96   train_loss = 0.972\n",
      "Epoch 229 Batch    0/96   train_loss = 0.927\n",
      "Epoch 229 Batch   32/96   train_loss = 1.015\n",
      "Epoch 229 Batch   64/96   train_loss = 0.972\n",
      "Epoch 230 Batch    0/96   train_loss = 0.923\n",
      "Epoch 230 Batch   32/96   train_loss = 1.006\n",
      "Epoch 230 Batch   64/96   train_loss = 0.965\n",
      "Epoch 231 Batch    0/96   train_loss = 0.926\n",
      "Epoch 231 Batch   32/96   train_loss = 0.998\n",
      "Epoch 231 Batch   64/96   train_loss = 0.964\n",
      "Epoch 232 Batch    0/96   train_loss = 0.916\n",
      "Epoch 232 Batch   32/96   train_loss = 1.001\n",
      "Epoch 232 Batch   64/96   train_loss = 0.967\n",
      "Epoch 233 Batch    0/96   train_loss = 0.935\n",
      "Epoch 233 Batch   32/96   train_loss = 0.993\n",
      "Epoch 233 Batch   64/96   train_loss = 0.972\n",
      "Epoch 234 Batch    0/96   train_loss = 0.936\n",
      "Epoch 234 Batch   32/96   train_loss = 0.990\n",
      "Epoch 234 Batch   64/96   train_loss = 0.963\n",
      "Epoch 235 Batch    0/96   train_loss = 0.930\n",
      "Epoch 235 Batch   32/96   train_loss = 0.985\n",
      "Epoch 235 Batch   64/96   train_loss = 0.957\n",
      "Epoch 236 Batch    0/96   train_loss = 0.926\n",
      "Epoch 236 Batch   32/96   train_loss = 0.988\n",
      "Epoch 236 Batch   64/96   train_loss = 0.952\n",
      "Epoch 237 Batch    0/96   train_loss = 0.927\n",
      "Epoch 237 Batch   32/96   train_loss = 0.995\n",
      "Epoch 237 Batch   64/96   train_loss = 0.966\n",
      "Epoch 238 Batch    0/96   train_loss = 0.916\n",
      "Epoch 238 Batch   32/96   train_loss = 0.987\n",
      "Epoch 238 Batch   64/96   train_loss = 0.975\n",
      "Epoch 239 Batch    0/96   train_loss = 0.924\n",
      "Epoch 239 Batch   32/96   train_loss = 0.990\n",
      "Epoch 239 Batch   64/96   train_loss = 0.969\n",
      "Epoch 240 Batch    0/96   train_loss = 0.923\n",
      "Epoch 240 Batch   32/96   train_loss = 0.985\n",
      "Epoch 240 Batch   64/96   train_loss = 0.955\n",
      "Epoch 241 Batch    0/96   train_loss = 0.925\n",
      "Epoch 241 Batch   32/96   train_loss = 0.982\n",
      "Epoch 241 Batch   64/96   train_loss = 0.970\n",
      "Epoch 242 Batch    0/96   train_loss = 0.925\n",
      "Epoch 242 Batch   32/96   train_loss = 0.990\n",
      "Epoch 242 Batch   64/96   train_loss = 0.963\n",
      "Epoch 243 Batch    0/96   train_loss = 0.922\n",
      "Epoch 243 Batch   32/96   train_loss = 0.992\n",
      "Epoch 243 Batch   64/96   train_loss = 0.967\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 244 Batch    0/96   train_loss = 0.915\n",
      "Epoch 244 Batch   32/96   train_loss = 0.997\n",
      "Epoch 244 Batch   64/96   train_loss = 0.965\n",
      "Epoch 245 Batch    0/96   train_loss = 0.913\n",
      "Epoch 245 Batch   32/96   train_loss = 0.990\n",
      "Epoch 245 Batch   64/96   train_loss = 0.958\n",
      "Epoch 246 Batch    0/96   train_loss = 0.920\n",
      "Epoch 246 Batch   32/96   train_loss = 0.994\n",
      "Epoch 246 Batch   64/96   train_loss = 0.957\n",
      "Epoch 247 Batch    0/96   train_loss = 0.928\n",
      "Epoch 247 Batch   32/96   train_loss = 0.981\n",
      "Epoch 247 Batch   64/96   train_loss = 0.957\n",
      "Epoch 248 Batch    0/96   train_loss = 0.923\n",
      "Epoch 248 Batch   32/96   train_loss = 0.989\n",
      "Epoch 248 Batch   64/96   train_loss = 0.966\n",
      "Epoch 249 Batch    0/96   train_loss = 0.925\n",
      "Epoch 249 Batch   32/96   train_loss = 0.988\n",
      "Epoch 249 Batch   64/96   train_loss = 0.962\n",
      "Epoch 250 Batch    0/96   train_loss = 0.919\n",
      "Epoch 250 Batch   32/96   train_loss = 0.984\n",
      "Epoch 250 Batch   64/96   train_loss = 0.965\n",
      "Epoch 251 Batch    0/96   train_loss = 0.918\n",
      "Epoch 251 Batch   32/96   train_loss = 0.983\n",
      "Epoch 251 Batch   64/96   train_loss = 0.960\n",
      "Epoch 252 Batch    0/96   train_loss = 0.915\n",
      "Epoch 252 Batch   32/96   train_loss = 0.987\n",
      "Epoch 252 Batch   64/96   train_loss = 0.960\n",
      "Epoch 253 Batch    0/96   train_loss = 0.922\n",
      "Epoch 253 Batch   32/96   train_loss = 1.012\n",
      "Epoch 253 Batch   64/96   train_loss = 0.966\n",
      "Epoch 254 Batch    0/96   train_loss = 0.920\n",
      "Epoch 254 Batch   32/96   train_loss = 1.002\n",
      "Epoch 254 Batch   64/96   train_loss = 0.969\n",
      "Epoch 255 Batch    0/96   train_loss = 0.919\n",
      "Epoch 255 Batch   32/96   train_loss = 0.984\n",
      "Epoch 255 Batch   64/96   train_loss = 0.956\n",
      "Epoch 256 Batch    0/96   train_loss = 0.915\n",
      "Epoch 256 Batch   32/96   train_loss = 0.994\n",
      "Epoch 256 Batch   64/96   train_loss = 0.955\n",
      "Epoch 257 Batch    0/96   train_loss = 0.921\n",
      "Epoch 257 Batch   32/96   train_loss = 0.986\n",
      "Epoch 257 Batch   64/96   train_loss = 0.955\n",
      "Epoch 258 Batch    0/96   train_loss = 0.924\n",
      "Epoch 258 Batch   32/96   train_loss = 0.984\n",
      "Epoch 258 Batch   64/96   train_loss = 0.956\n",
      "Epoch 259 Batch    0/96   train_loss = 0.919\n",
      "Epoch 259 Batch   32/96   train_loss = 0.991\n",
      "Epoch 259 Batch   64/96   train_loss = 0.952\n",
      "Epoch 260 Batch    0/96   train_loss = 0.909\n",
      "Epoch 260 Batch   32/96   train_loss = 0.988\n",
      "Epoch 260 Batch   64/96   train_loss = 0.964\n",
      "Epoch 261 Batch    0/96   train_loss = 0.925\n",
      "Epoch 261 Batch   32/96   train_loss = 0.990\n",
      "Epoch 261 Batch   64/96   train_loss = 0.956\n",
      "Epoch 262 Batch    0/96   train_loss = 0.909\n",
      "Epoch 262 Batch   32/96   train_loss = 1.006\n",
      "Epoch 262 Batch   64/96   train_loss = 0.947\n",
      "Epoch 263 Batch    0/96   train_loss = 0.905\n",
      "Epoch 263 Batch   32/96   train_loss = 0.992\n",
      "Epoch 263 Batch   64/96   train_loss = 0.947\n",
      "Epoch 264 Batch    0/96   train_loss = 0.903\n",
      "Epoch 264 Batch   32/96   train_loss = 0.981\n",
      "Epoch 264 Batch   64/96   train_loss = 0.951\n",
      "Epoch 265 Batch    0/96   train_loss = 0.917\n",
      "Epoch 265 Batch   32/96   train_loss = 0.995\n",
      "Epoch 265 Batch   64/96   train_loss = 0.954\n",
      "Epoch 266 Batch    0/96   train_loss = 0.914\n",
      "Epoch 266 Batch   32/96   train_loss = 0.990\n",
      "Epoch 266 Batch   64/96   train_loss = 0.957\n",
      "Epoch 267 Batch    0/96   train_loss = 0.906\n",
      "Epoch 267 Batch   32/96   train_loss = 0.989\n",
      "Epoch 267 Batch   64/96   train_loss = 0.964\n",
      "Epoch 268 Batch    0/96   train_loss = 0.910\n",
      "Epoch 268 Batch   32/96   train_loss = 0.984\n",
      "Epoch 268 Batch   64/96   train_loss = 0.951\n",
      "Epoch 269 Batch    0/96   train_loss = 0.923\n",
      "Epoch 269 Batch   32/96   train_loss = 0.997\n",
      "Epoch 269 Batch   64/96   train_loss = 0.953\n",
      "Epoch 270 Batch    0/96   train_loss = 0.920\n",
      "Epoch 270 Batch   32/96   train_loss = 0.996\n",
      "Epoch 270 Batch   64/96   train_loss = 0.951\n",
      "Epoch 271 Batch    0/96   train_loss = 0.921\n",
      "Epoch 271 Batch   32/96   train_loss = 0.972\n",
      "Epoch 271 Batch   64/96   train_loss = 0.964\n",
      "Epoch 272 Batch    0/96   train_loss = 0.916\n",
      "Epoch 272 Batch   32/96   train_loss = 0.993\n",
      "Epoch 272 Batch   64/96   train_loss = 0.964\n",
      "Epoch 273 Batch    0/96   train_loss = 0.924\n",
      "Epoch 273 Batch   32/96   train_loss = 0.991\n",
      "Epoch 273 Batch   64/96   train_loss = 0.968\n",
      "Epoch 274 Batch    0/96   train_loss = 0.940\n",
      "Epoch 274 Batch   32/96   train_loss = 0.990\n",
      "Epoch 274 Batch   64/96   train_loss = 0.969\n",
      "Epoch 275 Batch    0/96   train_loss = 0.922\n",
      "Epoch 275 Batch   32/96   train_loss = 0.993\n",
      "Epoch 275 Batch   64/96   train_loss = 0.972\n",
      "Epoch 276 Batch    0/96   train_loss = 0.919\n",
      "Epoch 276 Batch   32/96   train_loss = 0.982\n",
      "Epoch 276 Batch   64/96   train_loss = 0.962\n",
      "Epoch 277 Batch    0/96   train_loss = 0.924\n",
      "Epoch 277 Batch   32/96   train_loss = 0.979\n",
      "Epoch 277 Batch   64/96   train_loss = 0.960\n",
      "Epoch 278 Batch    0/96   train_loss = 0.922\n",
      "Epoch 278 Batch   32/96   train_loss = 0.982\n",
      "Epoch 278 Batch   64/96   train_loss = 0.957\n",
      "Epoch 279 Batch    0/96   train_loss = 0.921\n",
      "Epoch 279 Batch   32/96   train_loss = 0.988\n",
      "Epoch 279 Batch   64/96   train_loss = 0.962\n",
      "Epoch 280 Batch    0/96   train_loss = 0.915\n",
      "Epoch 280 Batch   32/96   train_loss = 0.984\n",
      "Epoch 280 Batch   64/96   train_loss = 0.960\n",
      "Epoch 281 Batch    0/96   train_loss = 0.916\n",
      "Epoch 281 Batch   32/96   train_loss = 0.990\n",
      "Epoch 281 Batch   64/96   train_loss = 0.948\n",
      "Epoch 282 Batch    0/96   train_loss = 0.908\n",
      "Epoch 282 Batch   32/96   train_loss = 0.983\n",
      "Epoch 282 Batch   64/96   train_loss = 0.949\n",
      "Epoch 283 Batch    0/96   train_loss = 0.903\n",
      "Epoch 283 Batch   32/96   train_loss = 0.974\n",
      "Epoch 283 Batch   64/96   train_loss = 0.958\n",
      "Epoch 284 Batch    0/96   train_loss = 0.927\n",
      "Epoch 284 Batch   32/96   train_loss = 0.978\n",
      "Epoch 284 Batch   64/96   train_loss = 0.955\n",
      "Epoch 285 Batch    0/96   train_loss = 0.914\n",
      "Epoch 285 Batch   32/96   train_loss = 0.984\n",
      "Epoch 285 Batch   64/96   train_loss = 0.960\n",
      "Epoch 286 Batch    0/96   train_loss = 0.917\n",
      "Epoch 286 Batch   32/96   train_loss = 0.990\n",
      "Epoch 286 Batch   64/96   train_loss = 0.959\n",
      "Epoch 287 Batch    0/96   train_loss = 0.926\n",
      "Epoch 287 Batch   32/96   train_loss = 0.989\n",
      "Epoch 287 Batch   64/96   train_loss = 0.968\n",
      "Epoch 288 Batch    0/96   train_loss = 0.937\n",
      "Epoch 288 Batch   32/96   train_loss = 1.003\n",
      "Epoch 288 Batch   64/96   train_loss = 0.954\n",
      "Epoch 289 Batch    0/96   train_loss = 0.916\n",
      "Epoch 289 Batch   32/96   train_loss = 1.002\n",
      "Epoch 289 Batch   64/96   train_loss = 0.947\n",
      "Epoch 290 Batch    0/96   train_loss = 0.916\n",
      "Epoch 290 Batch   32/96   train_loss = 0.996\n",
      "Epoch 290 Batch   64/96   train_loss = 0.958\n",
      "Epoch 291 Batch    0/96   train_loss = 0.918\n",
      "Epoch 291 Batch   32/96   train_loss = 0.999\n",
      "Epoch 291 Batch   64/96   train_loss = 0.955\n",
      "Epoch 292 Batch    0/96   train_loss = 0.919\n",
      "Epoch 292 Batch   32/96   train_loss = 1.005\n",
      "Epoch 292 Batch   64/96   train_loss = 0.961\n",
      "Epoch 293 Batch    0/96   train_loss = 0.929\n",
      "Epoch 293 Batch   32/96   train_loss = 0.995\n",
      "Epoch 293 Batch   64/96   train_loss = 0.957\n",
      "Epoch 294 Batch    0/96   train_loss = 0.923\n",
      "Epoch 294 Batch   32/96   train_loss = 1.014\n",
      "Epoch 294 Batch   64/96   train_loss = 0.963\n",
      "Epoch 295 Batch    0/96   train_loss = 0.931\n",
      "Epoch 295 Batch   32/96   train_loss = 1.009\n",
      "Epoch 295 Batch   64/96   train_loss = 0.961\n",
      "Epoch 296 Batch    0/96   train_loss = 0.949\n",
      "Epoch 296 Batch   32/96   train_loss = 1.005\n",
      "Epoch 296 Batch   64/96   train_loss = 0.954\n",
      "Epoch 297 Batch    0/96   train_loss = 0.923\n",
      "Epoch 297 Batch   32/96   train_loss = 0.998\n",
      "Epoch 297 Batch   64/96   train_loss = 0.960\n",
      "Epoch 298 Batch    0/96   train_loss = 0.931\n",
      "Epoch 298 Batch   32/96   train_loss = 0.991\n",
      "Epoch 298 Batch   64/96   train_loss = 0.962\n",
      "Epoch 299 Batch    0/96   train_loss = 0.922\n",
      "Epoch 299 Batch   32/96   train_loss = 0.993\n",
      "Epoch 299 Batch   64/96   train_loss = 0.960\n",
      "Epoch 300 Batch    0/96   train_loss = 0.920\n",
      "Epoch 300 Batch   32/96   train_loss = 1.007\n",
      "Epoch 300 Batch   64/96   train_loss = 0.950\n",
      "Epoch 301 Batch    0/96   train_loss = 0.929\n",
      "Epoch 301 Batch   32/96   train_loss = 0.999\n",
      "Epoch 301 Batch   64/96   train_loss = 0.957\n",
      "Epoch 302 Batch    0/96   train_loss = 0.916\n",
      "Epoch 302 Batch   32/96   train_loss = 0.986\n",
      "Epoch 302 Batch   64/96   train_loss = 0.948\n",
      "Epoch 303 Batch    0/96   train_loss = 0.916\n",
      "Epoch 303 Batch   32/96   train_loss = 0.997\n",
      "Epoch 303 Batch   64/96   train_loss = 0.943\n",
      "Epoch 304 Batch    0/96   train_loss = 0.913\n",
      "Epoch 304 Batch   32/96   train_loss = 1.004\n",
      "Epoch 304 Batch   64/96   train_loss = 0.947\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 305 Batch    0/96   train_loss = 0.915\n",
      "Epoch 305 Batch   32/96   train_loss = 0.994\n",
      "Epoch 305 Batch   64/96   train_loss = 0.952\n",
      "Epoch 306 Batch    0/96   train_loss = 0.926\n",
      "Epoch 306 Batch   32/96   train_loss = 1.001\n",
      "Epoch 306 Batch   64/96   train_loss = 0.952\n",
      "Epoch 307 Batch    0/96   train_loss = 0.918\n",
      "Epoch 307 Batch   32/96   train_loss = 0.993\n",
      "Epoch 307 Batch   64/96   train_loss = 0.951\n",
      "Epoch 308 Batch    0/96   train_loss = 0.929\n",
      "Epoch 308 Batch   32/96   train_loss = 0.996\n",
      "Epoch 308 Batch   64/96   train_loss = 0.954\n",
      "Epoch 309 Batch    0/96   train_loss = 0.925\n",
      "Epoch 309 Batch   32/96   train_loss = 0.996\n",
      "Epoch 309 Batch   64/96   train_loss = 0.959\n",
      "Epoch 310 Batch    0/96   train_loss = 0.930\n",
      "Epoch 310 Batch   32/96   train_loss = 0.997\n",
      "Epoch 310 Batch   64/96   train_loss = 0.968\n",
      "Epoch 311 Batch    0/96   train_loss = 0.911\n",
      "Epoch 311 Batch   32/96   train_loss = 0.995\n",
      "Epoch 311 Batch   64/96   train_loss = 0.964\n",
      "Epoch 312 Batch    0/96   train_loss = 0.914\n",
      "Epoch 312 Batch   32/96   train_loss = 0.998\n",
      "Epoch 312 Batch   64/96   train_loss = 0.954\n",
      "Epoch 313 Batch    0/96   train_loss = 0.925\n",
      "Epoch 313 Batch   32/96   train_loss = 0.996\n",
      "Epoch 313 Batch   64/96   train_loss = 0.955\n",
      "Epoch 314 Batch    0/96   train_loss = 0.919\n",
      "Epoch 314 Batch   32/96   train_loss = 1.013\n",
      "Epoch 314 Batch   64/96   train_loss = 0.953\n",
      "Epoch 315 Batch    0/96   train_loss = 0.921\n",
      "Epoch 315 Batch   32/96   train_loss = 1.002\n",
      "Epoch 315 Batch   64/96   train_loss = 0.955\n",
      "Epoch 316 Batch    0/96   train_loss = 0.915\n",
      "Epoch 316 Batch   32/96   train_loss = 0.987\n",
      "Epoch 316 Batch   64/96   train_loss = 0.966\n",
      "Epoch 317 Batch    0/96   train_loss = 0.925\n",
      "Epoch 317 Batch   32/96   train_loss = 0.985\n",
      "Epoch 317 Batch   64/96   train_loss = 0.972\n",
      "Epoch 318 Batch    0/96   train_loss = 0.925\n",
      "Epoch 318 Batch   32/96   train_loss = 0.998\n",
      "Epoch 318 Batch   64/96   train_loss = 0.934\n",
      "Epoch 319 Batch    0/96   train_loss = 0.914\n",
      "Epoch 319 Batch   32/96   train_loss = 1.001\n",
      "Epoch 319 Batch   64/96   train_loss = 0.960\n",
      "Epoch 320 Batch    0/96   train_loss = 0.910\n",
      "Epoch 320 Batch   32/96   train_loss = 0.977\n",
      "Epoch 320 Batch   64/96   train_loss = 0.962\n",
      "Epoch 321 Batch    0/96   train_loss = 0.919\n",
      "Epoch 321 Batch   32/96   train_loss = 0.982\n",
      "Epoch 321 Batch   64/96   train_loss = 0.961\n",
      "Epoch 322 Batch    0/96   train_loss = 0.910\n",
      "Epoch 322 Batch   32/96   train_loss = 0.980\n",
      "Epoch 322 Batch   64/96   train_loss = 0.963\n",
      "Epoch 323 Batch    0/96   train_loss = 0.925\n",
      "Epoch 323 Batch   32/96   train_loss = 0.971\n",
      "Epoch 323 Batch   64/96   train_loss = 0.958\n",
      "Epoch 324 Batch    0/96   train_loss = 0.917\n",
      "Epoch 324 Batch   32/96   train_loss = 0.973\n",
      "Epoch 324 Batch   64/96   train_loss = 0.948\n",
      "Epoch 325 Batch    0/96   train_loss = 0.916\n",
      "Epoch 325 Batch   32/96   train_loss = 0.979\n",
      "Epoch 325 Batch   64/96   train_loss = 0.941\n",
      "Epoch 326 Batch    0/96   train_loss = 0.906\n",
      "Epoch 326 Batch   32/96   train_loss = 0.982\n",
      "Epoch 326 Batch   64/96   train_loss = 0.943\n",
      "Epoch 327 Batch    0/96   train_loss = 0.911\n",
      "Epoch 327 Batch   32/96   train_loss = 0.982\n",
      "Epoch 327 Batch   64/96   train_loss = 0.950\n",
      "Epoch 328 Batch    0/96   train_loss = 0.903\n",
      "Epoch 328 Batch   32/96   train_loss = 0.993\n",
      "Epoch 328 Batch   64/96   train_loss = 0.950\n",
      "Epoch 329 Batch    0/96   train_loss = 0.911\n",
      "Epoch 329 Batch   32/96   train_loss = 0.994\n",
      "Epoch 329 Batch   64/96   train_loss = 0.966\n",
      "Epoch 330 Batch    0/96   train_loss = 0.910\n",
      "Epoch 330 Batch   32/96   train_loss = 0.996\n",
      "Epoch 330 Batch   64/96   train_loss = 0.951\n",
      "Epoch 331 Batch    0/96   train_loss = 0.911\n",
      "Epoch 331 Batch   32/96   train_loss = 0.991\n",
      "Epoch 331 Batch   64/96   train_loss = 0.953\n",
      "Epoch 332 Batch    0/96   train_loss = 0.925\n",
      "Epoch 332 Batch   32/96   train_loss = 0.980\n",
      "Epoch 332 Batch   64/96   train_loss = 0.959\n",
      "Epoch 333 Batch    0/96   train_loss = 0.923\n",
      "Epoch 333 Batch   32/96   train_loss = 0.986\n",
      "Epoch 333 Batch   64/96   train_loss = 0.957\n",
      "Epoch 334 Batch    0/96   train_loss = 0.917\n",
      "Epoch 334 Batch   32/96   train_loss = 0.989\n",
      "Epoch 334 Batch   64/96   train_loss = 0.948\n",
      "Epoch 335 Batch    0/96   train_loss = 0.906\n",
      "Epoch 335 Batch   32/96   train_loss = 0.986\n",
      "Epoch 335 Batch   64/96   train_loss = 0.955\n",
      "Epoch 336 Batch    0/96   train_loss = 0.911\n",
      "Epoch 336 Batch   32/96   train_loss = 0.980\n",
      "Epoch 336 Batch   64/96   train_loss = 0.946\n",
      "Epoch 337 Batch    0/96   train_loss = 0.918\n",
      "Epoch 337 Batch   32/96   train_loss = 0.987\n",
      "Epoch 337 Batch   64/96   train_loss = 0.964\n",
      "Epoch 338 Batch    0/96   train_loss = 0.917\n",
      "Epoch 338 Batch   32/96   train_loss = 0.986\n",
      "Epoch 338 Batch   64/96   train_loss = 0.948\n",
      "Epoch 339 Batch    0/96   train_loss = 0.927\n",
      "Epoch 339 Batch   32/96   train_loss = 1.000\n",
      "Epoch 339 Batch   64/96   train_loss = 0.963\n",
      "Epoch 340 Batch    0/96   train_loss = 0.921\n",
      "Epoch 340 Batch   32/96   train_loss = 0.984\n",
      "Epoch 340 Batch   64/96   train_loss = 0.950\n",
      "Epoch 341 Batch    0/96   train_loss = 0.930\n",
      "Epoch 341 Batch   32/96   train_loss = 0.971\n",
      "Epoch 341 Batch   64/96   train_loss = 0.948\n",
      "Epoch 342 Batch    0/96   train_loss = 0.916\n",
      "Epoch 342 Batch   32/96   train_loss = 0.991\n",
      "Epoch 342 Batch   64/96   train_loss = 0.947\n",
      "Epoch 343 Batch    0/96   train_loss = 0.912\n",
      "Epoch 343 Batch   32/96   train_loss = 0.986\n",
      "Epoch 343 Batch   64/96   train_loss = 0.957\n",
      "Epoch 344 Batch    0/96   train_loss = 0.916\n",
      "Epoch 344 Batch   32/96   train_loss = 0.973\n",
      "Epoch 344 Batch   64/96   train_loss = 0.946\n",
      "Epoch 345 Batch    0/96   train_loss = 0.913\n",
      "Epoch 345 Batch   32/96   train_loss = 0.978\n",
      "Epoch 345 Batch   64/96   train_loss = 0.936\n",
      "Epoch 346 Batch    0/96   train_loss = 0.916\n",
      "Epoch 346 Batch   32/96   train_loss = 0.988\n",
      "Epoch 346 Batch   64/96   train_loss = 0.941\n",
      "Epoch 347 Batch    0/96   train_loss = 0.925\n",
      "Epoch 347 Batch   32/96   train_loss = 0.993\n",
      "Epoch 347 Batch   64/96   train_loss = 0.945\n",
      "Epoch 348 Batch    0/96   train_loss = 0.913\n",
      "Epoch 348 Batch   32/96   train_loss = 0.986\n",
      "Epoch 348 Batch   64/96   train_loss = 0.951\n",
      "Epoch 349 Batch    0/96   train_loss = 0.916\n",
      "Epoch 349 Batch   32/96   train_loss = 0.983\n",
      "Epoch 349 Batch   64/96   train_loss = 0.937\n",
      "Epoch 350 Batch    0/96   train_loss = 0.918\n",
      "Epoch 350 Batch   32/96   train_loss = 0.990\n",
      "Epoch 350 Batch   64/96   train_loss = 0.953\n",
      "Epoch 351 Batch    0/96   train_loss = 0.919\n",
      "Epoch 351 Batch   32/96   train_loss = 0.982\n",
      "Epoch 351 Batch   64/96   train_loss = 0.942\n",
      "Epoch 352 Batch    0/96   train_loss = 0.921\n",
      "Epoch 352 Batch   32/96   train_loss = 0.984\n",
      "Epoch 352 Batch   64/96   train_loss = 0.945\n",
      "Epoch 353 Batch    0/96   train_loss = 0.924\n",
      "Epoch 353 Batch   32/96   train_loss = 0.970\n",
      "Epoch 353 Batch   64/96   train_loss = 0.952\n",
      "Epoch 354 Batch    0/96   train_loss = 0.915\n",
      "Epoch 354 Batch   32/96   train_loss = 0.978\n",
      "Epoch 354 Batch   64/96   train_loss = 0.953\n",
      "Epoch 355 Batch    0/96   train_loss = 0.924\n",
      "Epoch 355 Batch   32/96   train_loss = 0.977\n",
      "Epoch 355 Batch   64/96   train_loss = 0.957\n",
      "Epoch 356 Batch    0/96   train_loss = 0.911\n",
      "Epoch 356 Batch   32/96   train_loss = 0.986\n",
      "Epoch 356 Batch   64/96   train_loss = 0.946\n",
      "Epoch 357 Batch    0/96   train_loss = 0.911\n",
      "Epoch 357 Batch   32/96   train_loss = 0.994\n",
      "Epoch 357 Batch   64/96   train_loss = 0.951\n",
      "Epoch 358 Batch    0/96   train_loss = 0.908\n",
      "Epoch 358 Batch   32/96   train_loss = 0.980\n",
      "Epoch 358 Batch   64/96   train_loss = 0.956\n",
      "Epoch 359 Batch    0/96   train_loss = 0.918\n",
      "Epoch 359 Batch   32/96   train_loss = 0.978\n",
      "Epoch 359 Batch   64/96   train_loss = 0.932\n",
      "Epoch 360 Batch    0/96   train_loss = 0.913\n",
      "Epoch 360 Batch   32/96   train_loss = 0.983\n",
      "Epoch 360 Batch   64/96   train_loss = 0.953\n",
      "Epoch 361 Batch    0/96   train_loss = 0.902\n",
      "Epoch 361 Batch   32/96   train_loss = 0.984\n",
      "Epoch 361 Batch   64/96   train_loss = 0.942\n",
      "Epoch 362 Batch    0/96   train_loss = 0.903\n",
      "Epoch 362 Batch   32/96   train_loss = 0.985\n",
      "Epoch 362 Batch   64/96   train_loss = 0.940\n",
      "Epoch 363 Batch    0/96   train_loss = 0.891\n",
      "Epoch 363 Batch   32/96   train_loss = 0.992\n",
      "Epoch 363 Batch   64/96   train_loss = 0.942\n",
      "Epoch 364 Batch    0/96   train_loss = 0.921\n",
      "Epoch 364 Batch   32/96   train_loss = 0.987\n",
      "Epoch 364 Batch   64/96   train_loss = 0.948\n",
      "Epoch 365 Batch    0/96   train_loss = 0.909\n",
      "Epoch 365 Batch   32/96   train_loss = 0.995\n",
      "Epoch 365 Batch   64/96   train_loss = 0.958\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 366 Batch    0/96   train_loss = 0.911\n",
      "Epoch 366 Batch   32/96   train_loss = 0.974\n",
      "Epoch 366 Batch   64/96   train_loss = 0.946\n",
      "Epoch 367 Batch    0/96   train_loss = 0.906\n",
      "Epoch 367 Batch   32/96   train_loss = 0.986\n",
      "Epoch 367 Batch   64/96   train_loss = 0.947\n",
      "Epoch 368 Batch    0/96   train_loss = 0.905\n",
      "Epoch 368 Batch   32/96   train_loss = 0.980\n",
      "Epoch 368 Batch   64/96   train_loss = 0.940\n",
      "Epoch 369 Batch    0/96   train_loss = 0.904\n",
      "Epoch 369 Batch   32/96   train_loss = 0.986\n",
      "Epoch 369 Batch   64/96   train_loss = 0.957\n",
      "Epoch 370 Batch    0/96   train_loss = 0.908\n",
      "Epoch 370 Batch   32/96   train_loss = 0.977\n",
      "Epoch 370 Batch   64/96   train_loss = 0.939\n",
      "Epoch 371 Batch    0/96   train_loss = 0.905\n",
      "Epoch 371 Batch   32/96   train_loss = 0.996\n",
      "Epoch 371 Batch   64/96   train_loss = 0.949\n",
      "Epoch 372 Batch    0/96   train_loss = 0.901\n",
      "Epoch 372 Batch   32/96   train_loss = 0.990\n",
      "Epoch 372 Batch   64/96   train_loss = 0.946\n",
      "Epoch 373 Batch    0/96   train_loss = 0.905\n",
      "Epoch 373 Batch   32/96   train_loss = 0.985\n",
      "Epoch 373 Batch   64/96   train_loss = 0.939\n",
      "Epoch 374 Batch    0/96   train_loss = 0.909\n",
      "Epoch 374 Batch   32/96   train_loss = 0.977\n",
      "Epoch 374 Batch   64/96   train_loss = 0.941\n",
      "Epoch 375 Batch    0/96   train_loss = 0.898\n",
      "Epoch 375 Batch   32/96   train_loss = 0.980\n",
      "Epoch 375 Batch   64/96   train_loss = 0.942\n",
      "Epoch 376 Batch    0/96   train_loss = 0.904\n",
      "Epoch 376 Batch   32/96   train_loss = 0.985\n",
      "Epoch 376 Batch   64/96   train_loss = 0.937\n",
      "Epoch 377 Batch    0/96   train_loss = 0.912\n",
      "Epoch 377 Batch   32/96   train_loss = 0.969\n",
      "Epoch 377 Batch   64/96   train_loss = 0.944\n",
      "Epoch 378 Batch    0/96   train_loss = 0.914\n",
      "Epoch 378 Batch   32/96   train_loss = 0.984\n",
      "Epoch 378 Batch   64/96   train_loss = 0.942\n",
      "Epoch 379 Batch    0/96   train_loss = 0.898\n",
      "Epoch 379 Batch   32/96   train_loss = 0.978\n",
      "Epoch 379 Batch   64/96   train_loss = 0.945\n",
      "Epoch 380 Batch    0/96   train_loss = 0.905\n",
      "Epoch 380 Batch   32/96   train_loss = 0.974\n",
      "Epoch 380 Batch   64/96   train_loss = 0.946\n",
      "Epoch 381 Batch    0/96   train_loss = 0.907\n",
      "Epoch 381 Batch   32/96   train_loss = 0.968\n",
      "Epoch 381 Batch   64/96   train_loss = 0.943\n",
      "Epoch 382 Batch    0/96   train_loss = 0.898\n",
      "Epoch 382 Batch   32/96   train_loss = 0.970\n",
      "Epoch 382 Batch   64/96   train_loss = 0.968\n",
      "Epoch 383 Batch    0/96   train_loss = 0.908\n",
      "Epoch 383 Batch   32/96   train_loss = 0.993\n",
      "Epoch 383 Batch   64/96   train_loss = 0.963\n",
      "Epoch 384 Batch    0/96   train_loss = 0.912\n",
      "Epoch 384 Batch   32/96   train_loss = 0.980\n",
      "Epoch 384 Batch   64/96   train_loss = 0.956\n",
      "Epoch 385 Batch    0/96   train_loss = 0.910\n",
      "Epoch 385 Batch   32/96   train_loss = 0.990\n",
      "Epoch 385 Batch   64/96   train_loss = 0.944\n",
      "Epoch 386 Batch    0/96   train_loss = 0.909\n",
      "Epoch 386 Batch   32/96   train_loss = 0.985\n",
      "Epoch 386 Batch   64/96   train_loss = 0.953\n",
      "Epoch 387 Batch    0/96   train_loss = 0.914\n",
      "Epoch 387 Batch   32/96   train_loss = 0.991\n",
      "Epoch 387 Batch   64/96   train_loss = 0.941\n",
      "Epoch 388 Batch    0/96   train_loss = 0.893\n",
      "Epoch 388 Batch   32/96   train_loss = 0.983\n",
      "Epoch 388 Batch   64/96   train_loss = 0.940\n",
      "Epoch 389 Batch    0/96   train_loss = 0.904\n",
      "Epoch 389 Batch   32/96   train_loss = 0.978\n",
      "Epoch 389 Batch   64/96   train_loss = 0.954\n",
      "Epoch 390 Batch    0/96   train_loss = 0.893\n",
      "Epoch 390 Batch   32/96   train_loss = 0.975\n",
      "Epoch 390 Batch   64/96   train_loss = 0.949\n",
      "Epoch 391 Batch    0/96   train_loss = 0.897\n",
      "Epoch 391 Batch   32/96   train_loss = 0.981\n",
      "Epoch 391 Batch   64/96   train_loss = 0.957\n",
      "Epoch 392 Batch    0/96   train_loss = 0.913\n",
      "Epoch 392 Batch   32/96   train_loss = 0.973\n",
      "Epoch 392 Batch   64/96   train_loss = 0.960\n",
      "Epoch 393 Batch    0/96   train_loss = 0.910\n",
      "Epoch 393 Batch   32/96   train_loss = 0.971\n",
      "Epoch 393 Batch   64/96   train_loss = 0.949\n",
      "Epoch 394 Batch    0/96   train_loss = 0.904\n",
      "Epoch 394 Batch   32/96   train_loss = 0.967\n",
      "Epoch 394 Batch   64/96   train_loss = 0.956\n",
      "Epoch 395 Batch    0/96   train_loss = 0.902\n",
      "Epoch 395 Batch   32/96   train_loss = 0.972\n",
      "Epoch 395 Batch   64/96   train_loss = 0.965\n",
      "Epoch 396 Batch    0/96   train_loss = 0.893\n",
      "Epoch 396 Batch   32/96   train_loss = 0.973\n",
      "Epoch 396 Batch   64/96   train_loss = 0.959\n",
      "Epoch 397 Batch    0/96   train_loss = 0.904\n",
      "Epoch 397 Batch   32/96   train_loss = 0.963\n",
      "Epoch 397 Batch   64/96   train_loss = 0.971\n",
      "Epoch 398 Batch    0/96   train_loss = 0.907\n",
      "Epoch 398 Batch   32/96   train_loss = 0.976\n",
      "Epoch 398 Batch   64/96   train_loss = 0.949\n",
      "Epoch 399 Batch    0/96   train_loss = 0.912\n",
      "Epoch 399 Batch   32/96   train_loss = 0.977\n",
      "Epoch 399 Batch   64/96   train_loss = 0.946\n",
      "Epoch 400 Batch    0/96   train_loss = 0.916\n",
      "Epoch 400 Batch   32/96   train_loss = 0.981\n",
      "Epoch 400 Batch   64/96   train_loss = 0.930\n",
      "Epoch 401 Batch    0/96   train_loss = 0.908\n",
      "Epoch 401 Batch   32/96   train_loss = 0.967\n",
      "Epoch 401 Batch   64/96   train_loss = 0.929\n",
      "Epoch 402 Batch    0/96   train_loss = 0.904\n",
      "Epoch 402 Batch   32/96   train_loss = 0.983\n",
      "Epoch 402 Batch   64/96   train_loss = 0.937\n",
      "Epoch 403 Batch    0/96   train_loss = 0.904\n",
      "Epoch 403 Batch   32/96   train_loss = 0.974\n",
      "Epoch 403 Batch   64/96   train_loss = 0.938\n",
      "Epoch 404 Batch    0/96   train_loss = 0.901\n",
      "Epoch 404 Batch   32/96   train_loss = 0.968\n",
      "Epoch 404 Batch   64/96   train_loss = 0.942\n",
      "Epoch 405 Batch    0/96   train_loss = 0.904\n",
      "Epoch 405 Batch   32/96   train_loss = 0.967\n",
      "Epoch 405 Batch   64/96   train_loss = 0.919\n",
      "Epoch 406 Batch    0/96   train_loss = 0.894\n",
      "Epoch 406 Batch   32/96   train_loss = 0.972\n",
      "Epoch 406 Batch   64/96   train_loss = 0.944\n",
      "Epoch 407 Batch    0/96   train_loss = 0.900\n",
      "Epoch 407 Batch   32/96   train_loss = 0.961\n",
      "Epoch 407 Batch   64/96   train_loss = 0.946\n",
      "Epoch 408 Batch    0/96   train_loss = 0.900\n",
      "Epoch 408 Batch   32/96   train_loss = 0.966\n",
      "Epoch 408 Batch   64/96   train_loss = 0.935\n",
      "Epoch 409 Batch    0/96   train_loss = 0.894\n",
      "Epoch 409 Batch   32/96   train_loss = 0.978\n",
      "Epoch 409 Batch   64/96   train_loss = 0.947\n",
      "Epoch 410 Batch    0/96   train_loss = 0.895\n",
      "Epoch 410 Batch   32/96   train_loss = 0.973\n",
      "Epoch 410 Batch   64/96   train_loss = 0.951\n",
      "Epoch 411 Batch    0/96   train_loss = 0.890\n",
      "Epoch 411 Batch   32/96   train_loss = 0.984\n",
      "Epoch 411 Batch   64/96   train_loss = 0.944\n",
      "Epoch 412 Batch    0/96   train_loss = 0.896\n",
      "Epoch 412 Batch   32/96   train_loss = 0.970\n",
      "Epoch 412 Batch   64/96   train_loss = 0.933\n",
      "Epoch 413 Batch    0/96   train_loss = 0.896\n",
      "Epoch 413 Batch   32/96   train_loss = 0.976\n",
      "Epoch 413 Batch   64/96   train_loss = 0.933\n",
      "Epoch 414 Batch    0/96   train_loss = 0.897\n",
      "Epoch 414 Batch   32/96   train_loss = 0.975\n",
      "Epoch 414 Batch   64/96   train_loss = 0.938\n",
      "Epoch 415 Batch    0/96   train_loss = 0.905\n",
      "Epoch 415 Batch   32/96   train_loss = 0.971\n",
      "Epoch 415 Batch   64/96   train_loss = 0.948\n",
      "Epoch 416 Batch    0/96   train_loss = 0.898\n",
      "Epoch 416 Batch   32/96   train_loss = 0.964\n",
      "Epoch 416 Batch   64/96   train_loss = 0.954\n",
      "Epoch 417 Batch    0/96   train_loss = 0.910\n",
      "Epoch 417 Batch   32/96   train_loss = 0.976\n",
      "Epoch 417 Batch   64/96   train_loss = 0.943\n",
      "Epoch 418 Batch    0/96   train_loss = 0.892\n",
      "Epoch 418 Batch   32/96   train_loss = 0.971\n",
      "Epoch 418 Batch   64/96   train_loss = 0.947\n",
      "Epoch 419 Batch    0/96   train_loss = 0.894\n",
      "Epoch 419 Batch   32/96   train_loss = 0.986\n",
      "Epoch 419 Batch   64/96   train_loss = 0.946\n",
      "Epoch 420 Batch    0/96   train_loss = 0.901\n",
      "Epoch 420 Batch   32/96   train_loss = 0.980\n",
      "Epoch 420 Batch   64/96   train_loss = 0.937\n",
      "Epoch 421 Batch    0/96   train_loss = 0.904\n",
      "Epoch 421 Batch   32/96   train_loss = 0.983\n",
      "Epoch 421 Batch   64/96   train_loss = 0.944\n",
      "Epoch 422 Batch    0/96   train_loss = 0.897\n",
      "Epoch 422 Batch   32/96   train_loss = 0.989\n",
      "Epoch 422 Batch   64/96   train_loss = 0.938\n",
      "Epoch 423 Batch    0/96   train_loss = 0.894\n",
      "Epoch 423 Batch   32/96   train_loss = 0.988\n",
      "Epoch 423 Batch   64/96   train_loss = 0.942\n",
      "Epoch 424 Batch    0/96   train_loss = 0.901\n",
      "Epoch 424 Batch   32/96   train_loss = 0.996\n",
      "Epoch 424 Batch   64/96   train_loss = 0.933\n",
      "Epoch 425 Batch    0/96   train_loss = 0.900\n",
      "Epoch 425 Batch   32/96   train_loss = 0.993\n",
      "Epoch 425 Batch   64/96   train_loss = 0.946\n",
      "Epoch 426 Batch    0/96   train_loss = 0.896\n",
      "Epoch 426 Batch   32/96   train_loss = 0.978\n",
      "Epoch 426 Batch   64/96   train_loss = 0.949\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 427 Batch    0/96   train_loss = 0.890\n",
      "Epoch 427 Batch   32/96   train_loss = 0.974\n",
      "Epoch 427 Batch   64/96   train_loss = 0.955\n",
      "Epoch 428 Batch    0/96   train_loss = 0.892\n",
      "Epoch 428 Batch   32/96   train_loss = 0.976\n",
      "Epoch 428 Batch   64/96   train_loss = 0.944\n",
      "Epoch 429 Batch    0/96   train_loss = 0.895\n",
      "Epoch 429 Batch   32/96   train_loss = 0.988\n",
      "Epoch 429 Batch   64/96   train_loss = 0.941\n",
      "Epoch 430 Batch    0/96   train_loss = 0.899\n",
      "Epoch 430 Batch   32/96   train_loss = 0.972\n",
      "Epoch 430 Batch   64/96   train_loss = 0.944\n",
      "Epoch 431 Batch    0/96   train_loss = 0.902\n",
      "Epoch 431 Batch   32/96   train_loss = 0.979\n",
      "Epoch 431 Batch   64/96   train_loss = 0.946\n",
      "Epoch 432 Batch    0/96   train_loss = 0.906\n",
      "Epoch 432 Batch   32/96   train_loss = 0.985\n",
      "Epoch 432 Batch   64/96   train_loss = 0.942\n",
      "Epoch 433 Batch    0/96   train_loss = 0.910\n",
      "Epoch 433 Batch   32/96   train_loss = 0.991\n",
      "Epoch 433 Batch   64/96   train_loss = 0.951\n",
      "Epoch 434 Batch    0/96   train_loss = 0.906\n",
      "Epoch 434 Batch   32/96   train_loss = 0.983\n",
      "Epoch 434 Batch   64/96   train_loss = 0.933\n",
      "Epoch 435 Batch    0/96   train_loss = 0.902\n",
      "Epoch 435 Batch   32/96   train_loss = 0.978\n",
      "Epoch 435 Batch   64/96   train_loss = 0.944\n",
      "Epoch 436 Batch    0/96   train_loss = 0.900\n",
      "Epoch 436 Batch   32/96   train_loss = 0.971\n",
      "Epoch 436 Batch   64/96   train_loss = 0.929\n",
      "Epoch 437 Batch    0/96   train_loss = 0.903\n",
      "Epoch 437 Batch   32/96   train_loss = 0.983\n",
      "Epoch 437 Batch   64/96   train_loss = 0.941\n",
      "Epoch 438 Batch    0/96   train_loss = 0.904\n",
      "Epoch 438 Batch   32/96   train_loss = 0.988\n",
      "Epoch 438 Batch   64/96   train_loss = 0.959\n",
      "Epoch 439 Batch    0/96   train_loss = 0.913\n",
      "Epoch 439 Batch   32/96   train_loss = 0.973\n",
      "Epoch 439 Batch   64/96   train_loss = 0.948\n",
      "Epoch 440 Batch    0/96   train_loss = 0.909\n",
      "Epoch 440 Batch   32/96   train_loss = 0.977\n",
      "Epoch 440 Batch   64/96   train_loss = 0.930\n",
      "Epoch 441 Batch    0/96   train_loss = 0.909\n",
      "Epoch 441 Batch   32/96   train_loss = 0.975\n",
      "Epoch 441 Batch   64/96   train_loss = 0.948\n",
      "Epoch 442 Batch    0/96   train_loss = 0.900\n",
      "Epoch 442 Batch   32/96   train_loss = 0.979\n",
      "Epoch 442 Batch   64/96   train_loss = 0.944\n",
      "Epoch 443 Batch    0/96   train_loss = 0.907\n",
      "Epoch 443 Batch   32/96   train_loss = 0.975\n",
      "Epoch 443 Batch   64/96   train_loss = 0.949\n",
      "Epoch 444 Batch    0/96   train_loss = 0.903\n",
      "Epoch 444 Batch   32/96   train_loss = 0.964\n",
      "Epoch 444 Batch   64/96   train_loss = 0.949\n",
      "Epoch 445 Batch    0/96   train_loss = 0.902\n",
      "Epoch 445 Batch   32/96   train_loss = 0.985\n",
      "Epoch 445 Batch   64/96   train_loss = 0.934\n",
      "Epoch 446 Batch    0/96   train_loss = 0.904\n",
      "Epoch 446 Batch   32/96   train_loss = 0.980\n",
      "Epoch 446 Batch   64/96   train_loss = 0.943\n",
      "Epoch 447 Batch    0/96   train_loss = 0.895\n",
      "Epoch 447 Batch   32/96   train_loss = 0.979\n",
      "Epoch 447 Batch   64/96   train_loss = 0.943\n",
      "Epoch 448 Batch    0/96   train_loss = 0.897\n",
      "Epoch 448 Batch   32/96   train_loss = 0.975\n",
      "Epoch 448 Batch   64/96   train_loss = 0.937\n",
      "Epoch 449 Batch    0/96   train_loss = 0.890\n",
      "Epoch 449 Batch   32/96   train_loss = 0.979\n",
      "Epoch 449 Batch   64/96   train_loss = 0.934\n",
      "Epoch 450 Batch    0/96   train_loss = 0.903\n",
      "Epoch 450 Batch   32/96   train_loss = 0.977\n",
      "Epoch 450 Batch   64/96   train_loss = 0.928\n",
      "Epoch 451 Batch    0/96   train_loss = 0.886\n",
      "Epoch 451 Batch   32/96   train_loss = 0.967\n",
      "Epoch 451 Batch   64/96   train_loss = 0.941\n",
      "Epoch 452 Batch    0/96   train_loss = 0.893\n",
      "Epoch 452 Batch   32/96   train_loss = 0.966\n",
      "Epoch 452 Batch   64/96   train_loss = 0.934\n",
      "Epoch 453 Batch    0/96   train_loss = 0.895\n",
      "Epoch 453 Batch   32/96   train_loss = 0.966\n",
      "Epoch 453 Batch   64/96   train_loss = 0.927\n",
      "Epoch 454 Batch    0/96   train_loss = 0.896\n",
      "Epoch 454 Batch   32/96   train_loss = 0.958\n",
      "Epoch 454 Batch   64/96   train_loss = 0.922\n",
      "Epoch 455 Batch    0/96   train_loss = 0.888\n",
      "Epoch 455 Batch   32/96   train_loss = 0.958\n",
      "Epoch 455 Batch   64/96   train_loss = 0.928\n",
      "Epoch 456 Batch    0/96   train_loss = 0.891\n",
      "Epoch 456 Batch   32/96   train_loss = 0.961\n",
      "Epoch 456 Batch   64/96   train_loss = 0.933\n",
      "Epoch 457 Batch    0/96   train_loss = 0.904\n",
      "Epoch 457 Batch   32/96   train_loss = 0.961\n",
      "Epoch 457 Batch   64/96   train_loss = 0.937\n",
      "Epoch 458 Batch    0/96   train_loss = 0.893\n",
      "Epoch 458 Batch   32/96   train_loss = 0.962\n",
      "Epoch 458 Batch   64/96   train_loss = 0.927\n",
      "Epoch 459 Batch    0/96   train_loss = 0.903\n",
      "Epoch 459 Batch   32/96   train_loss = 0.957\n",
      "Epoch 459 Batch   64/96   train_loss = 0.929\n",
      "Epoch 460 Batch    0/96   train_loss = 0.899\n",
      "Epoch 460 Batch   32/96   train_loss = 0.974\n",
      "Epoch 460 Batch   64/96   train_loss = 0.951\n",
      "Epoch 461 Batch    0/96   train_loss = 0.895\n",
      "Epoch 461 Batch   32/96   train_loss = 0.974\n",
      "Epoch 461 Batch   64/96   train_loss = 0.940\n",
      "Epoch 462 Batch    0/96   train_loss = 0.905\n",
      "Epoch 462 Batch   32/96   train_loss = 0.983\n",
      "Epoch 462 Batch   64/96   train_loss = 0.931\n",
      "Epoch 463 Batch    0/96   train_loss = 0.885\n",
      "Epoch 463 Batch   32/96   train_loss = 0.978\n",
      "Epoch 463 Batch   64/96   train_loss = 0.932\n",
      "Epoch 464 Batch    0/96   train_loss = 0.892\n",
      "Epoch 464 Batch   32/96   train_loss = 0.981\n",
      "Epoch 464 Batch   64/96   train_loss = 0.938\n",
      "Epoch 465 Batch    0/96   train_loss = 0.905\n",
      "Epoch 465 Batch   32/96   train_loss = 0.976\n",
      "Epoch 465 Batch   64/96   train_loss = 0.929\n",
      "Epoch 466 Batch    0/96   train_loss = 0.906\n",
      "Epoch 466 Batch   32/96   train_loss = 0.980\n",
      "Epoch 466 Batch   64/96   train_loss = 0.921\n",
      "Epoch 467 Batch    0/96   train_loss = 0.909\n",
      "Epoch 467 Batch   32/96   train_loss = 0.990\n",
      "Epoch 467 Batch   64/96   train_loss = 0.942\n",
      "Epoch 468 Batch    0/96   train_loss = 0.900\n",
      "Epoch 468 Batch   32/96   train_loss = 0.987\n",
      "Epoch 468 Batch   64/96   train_loss = 0.938\n",
      "Epoch 469 Batch    0/96   train_loss = 0.901\n",
      "Epoch 469 Batch   32/96   train_loss = 0.991\n",
      "Epoch 469 Batch   64/96   train_loss = 0.933\n",
      "Epoch 470 Batch    0/96   train_loss = 0.908\n",
      "Epoch 470 Batch   32/96   train_loss = 0.987\n",
      "Epoch 470 Batch   64/96   train_loss = 0.929\n",
      "Epoch 471 Batch    0/96   train_loss = 0.903\n",
      "Epoch 471 Batch   32/96   train_loss = 0.979\n",
      "Epoch 471 Batch   64/96   train_loss = 0.936\n",
      "Epoch 472 Batch    0/96   train_loss = 0.900\n",
      "Epoch 472 Batch   32/96   train_loss = 0.989\n",
      "Epoch 472 Batch   64/96   train_loss = 0.944\n",
      "Epoch 473 Batch    0/96   train_loss = 0.905\n",
      "Epoch 473 Batch   32/96   train_loss = 0.976\n",
      "Epoch 473 Batch   64/96   train_loss = 0.924\n",
      "Epoch 474 Batch    0/96   train_loss = 0.903\n",
      "Epoch 474 Batch   32/96   train_loss = 0.983\n",
      "Epoch 474 Batch   64/96   train_loss = 0.933\n",
      "Epoch 475 Batch    0/96   train_loss = 0.903\n",
      "Epoch 475 Batch   32/96   train_loss = 0.982\n",
      "Epoch 475 Batch   64/96   train_loss = 0.941\n",
      "Epoch 476 Batch    0/96   train_loss = 0.906\n",
      "Epoch 476 Batch   32/96   train_loss = 0.988\n",
      "Epoch 476 Batch   64/96   train_loss = 0.941\n",
      "Epoch 477 Batch    0/96   train_loss = 0.916\n",
      "Epoch 477 Batch   32/96   train_loss = 0.987\n",
      "Epoch 477 Batch   64/96   train_loss = 0.924\n",
      "Epoch 478 Batch    0/96   train_loss = 0.904\n",
      "Epoch 478 Batch   32/96   train_loss = 0.990\n",
      "Epoch 478 Batch   64/96   train_loss = 0.921\n",
      "Epoch 479 Batch    0/96   train_loss = 0.904\n",
      "Epoch 479 Batch   32/96   train_loss = 0.990\n",
      "Epoch 479 Batch   64/96   train_loss = 0.934\n",
      "Epoch 480 Batch    0/96   train_loss = 0.895\n",
      "Epoch 480 Batch   32/96   train_loss = 0.977\n",
      "Epoch 480 Batch   64/96   train_loss = 0.943\n",
      "Epoch 481 Batch    0/96   train_loss = 0.918\n",
      "Epoch 481 Batch   32/96   train_loss = 0.978\n",
      "Epoch 481 Batch   64/96   train_loss = 0.958\n",
      "Epoch 482 Batch    0/96   train_loss = 0.904\n",
      "Epoch 482 Batch   32/96   train_loss = 0.990\n",
      "Epoch 482 Batch   64/96   train_loss = 0.950\n",
      "Epoch 483 Batch    0/96   train_loss = 0.928\n",
      "Epoch 483 Batch   32/96   train_loss = 0.978\n",
      "Epoch 483 Batch   64/96   train_loss = 0.936\n",
      "Epoch 484 Batch    0/96   train_loss = 0.925\n",
      "Epoch 484 Batch   32/96   train_loss = 0.972\n",
      "Epoch 484 Batch   64/96   train_loss = 0.944\n",
      "Epoch 485 Batch    0/96   train_loss = 0.910\n",
      "Epoch 485 Batch   32/96   train_loss = 0.989\n",
      "Epoch 485 Batch   64/96   train_loss = 0.940\n",
      "Epoch 486 Batch    0/96   train_loss = 0.915\n",
      "Epoch 486 Batch   32/96   train_loss = 0.995\n",
      "Epoch 486 Batch   64/96   train_loss = 0.933\n",
      "Epoch 487 Batch    0/96   train_loss = 0.910\n",
      "Epoch 487 Batch   32/96   train_loss = 0.981\n",
      "Epoch 487 Batch   64/96   train_loss = 0.939\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 488 Batch    0/96   train_loss = 0.906\n",
      "Epoch 488 Batch   32/96   train_loss = 0.986\n",
      "Epoch 488 Batch   64/96   train_loss = 0.936\n",
      "Epoch 489 Batch    0/96   train_loss = 0.893\n",
      "Epoch 489 Batch   32/96   train_loss = 0.975\n",
      "Epoch 489 Batch   64/96   train_loss = 0.924\n",
      "Epoch 490 Batch    0/96   train_loss = 0.902\n",
      "Epoch 490 Batch   32/96   train_loss = 0.970\n",
      "Epoch 490 Batch   64/96   train_loss = 0.928\n",
      "Epoch 491 Batch    0/96   train_loss = 0.904\n",
      "Epoch 491 Batch   32/96   train_loss = 0.980\n",
      "Epoch 491 Batch   64/96   train_loss = 0.929\n",
      "Epoch 492 Batch    0/96   train_loss = 0.920\n",
      "Epoch 492 Batch   32/96   train_loss = 0.985\n",
      "Epoch 492 Batch   64/96   train_loss = 0.936\n",
      "Epoch 493 Batch    0/96   train_loss = 0.900\n",
      "Epoch 493 Batch   32/96   train_loss = 0.974\n",
      "Epoch 493 Batch   64/96   train_loss = 0.927\n",
      "Epoch 494 Batch    0/96   train_loss = 0.898\n",
      "Epoch 494 Batch   32/96   train_loss = 0.975\n",
      "Epoch 494 Batch   64/96   train_loss = 0.928\n",
      "Epoch 495 Batch    0/96   train_loss = 0.903\n",
      "Epoch 495 Batch   32/96   train_loss = 0.971\n",
      "Epoch 495 Batch   64/96   train_loss = 0.938\n",
      "Epoch 496 Batch    0/96   train_loss = 0.889\n",
      "Epoch 496 Batch   32/96   train_loss = 0.978\n",
      "Epoch 496 Batch   64/96   train_loss = 0.925\n",
      "Epoch 497 Batch    0/96   train_loss = 0.889\n",
      "Epoch 497 Batch   32/96   train_loss = 0.974\n",
      "Epoch 497 Batch   64/96   train_loss = 0.921\n",
      "Epoch 498 Batch    0/96   train_loss = 0.892\n",
      "Epoch 498 Batch   32/96   train_loss = 0.974\n",
      "Epoch 498 Batch   64/96   train_loss = 0.913\n",
      "Epoch 499 Batch    0/96   train_loss = 0.897\n",
      "Epoch 499 Batch   32/96   train_loss = 0.963\n",
      "Epoch 499 Batch   64/96   train_loss = 0.923\n",
      "Epoch 500 Batch    0/96   train_loss = 0.891\n",
      "Epoch 500 Batch   32/96   train_loss = 0.970\n",
      "Epoch 500 Batch   64/96   train_loss = 0.925\n",
      "Epoch 501 Batch    0/96   train_loss = 0.888\n",
      "Epoch 501 Batch   32/96   train_loss = 0.982\n",
      "Epoch 501 Batch   64/96   train_loss = 0.929\n",
      "Epoch 502 Batch    0/96   train_loss = 0.888\n",
      "Epoch 502 Batch   32/96   train_loss = 0.978\n",
      "Epoch 502 Batch   64/96   train_loss = 0.934\n",
      "Epoch 503 Batch    0/96   train_loss = 0.899\n",
      "Epoch 503 Batch   32/96   train_loss = 0.968\n",
      "Epoch 503 Batch   64/96   train_loss = 0.930\n",
      "Epoch 504 Batch    0/96   train_loss = 0.887\n",
      "Epoch 504 Batch   32/96   train_loss = 0.961\n",
      "Epoch 504 Batch   64/96   train_loss = 0.923\n",
      "Epoch 505 Batch    0/96   train_loss = 0.879\n",
      "Epoch 505 Batch   32/96   train_loss = 0.951\n",
      "Epoch 505 Batch   64/96   train_loss = 0.916\n",
      "Epoch 506 Batch    0/96   train_loss = 0.885\n",
      "Epoch 506 Batch   32/96   train_loss = 0.962\n",
      "Epoch 506 Batch   64/96   train_loss = 0.923\n",
      "Epoch 507 Batch    0/96   train_loss = 0.887\n",
      "Epoch 507 Batch   32/96   train_loss = 0.965\n",
      "Epoch 507 Batch   64/96   train_loss = 0.919\n",
      "Epoch 508 Batch    0/96   train_loss = 0.893\n",
      "Epoch 508 Batch   32/96   train_loss = 0.970\n",
      "Epoch 508 Batch   64/96   train_loss = 0.920\n",
      "Epoch 509 Batch    0/96   train_loss = 0.898\n",
      "Epoch 509 Batch   32/96   train_loss = 0.969\n",
      "Epoch 509 Batch   64/96   train_loss = 0.925\n",
      "Epoch 510 Batch    0/96   train_loss = 0.897\n",
      "Epoch 510 Batch   32/96   train_loss = 0.967\n",
      "Epoch 510 Batch   64/96   train_loss = 0.917\n",
      "Epoch 511 Batch    0/96   train_loss = 0.890\n",
      "Epoch 511 Batch   32/96   train_loss = 0.950\n",
      "Epoch 511 Batch   64/96   train_loss = 0.930\n",
      "Epoch 512 Batch    0/96   train_loss = 0.900\n",
      "Epoch 512 Batch   32/96   train_loss = 0.959\n",
      "Epoch 512 Batch   64/96   train_loss = 0.922\n",
      "Epoch 513 Batch    0/96   train_loss = 0.893\n",
      "Epoch 513 Batch   32/96   train_loss = 0.967\n",
      "Epoch 513 Batch   64/96   train_loss = 0.923\n",
      "Epoch 514 Batch    0/96   train_loss = 0.891\n",
      "Epoch 514 Batch   32/96   train_loss = 0.958\n",
      "Epoch 514 Batch   64/96   train_loss = 0.924\n",
      "Epoch 515 Batch    0/96   train_loss = 0.899\n",
      "Epoch 515 Batch   32/96   train_loss = 0.971\n",
      "Epoch 515 Batch   64/96   train_loss = 0.919\n",
      "Epoch 516 Batch    0/96   train_loss = 0.893\n",
      "Epoch 516 Batch   32/96   train_loss = 0.960\n",
      "Epoch 516 Batch   64/96   train_loss = 0.906\n",
      "Epoch 517 Batch    0/96   train_loss = 0.892\n",
      "Epoch 517 Batch   32/96   train_loss = 0.952\n",
      "Epoch 517 Batch   64/96   train_loss = 0.915\n",
      "Epoch 518 Batch    0/96   train_loss = 0.886\n",
      "Epoch 518 Batch   32/96   train_loss = 0.965\n",
      "Epoch 518 Batch   64/96   train_loss = 0.931\n",
      "Epoch 519 Batch    0/96   train_loss = 0.896\n",
      "Epoch 519 Batch   32/96   train_loss = 0.966\n",
      "Epoch 519 Batch   64/96   train_loss = 0.918\n",
      "Epoch 520 Batch    0/96   train_loss = 0.897\n",
      "Epoch 520 Batch   32/96   train_loss = 0.954\n",
      "Epoch 520 Batch   64/96   train_loss = 0.912\n",
      "Epoch 521 Batch    0/96   train_loss = 0.890\n",
      "Epoch 521 Batch   32/96   train_loss = 0.959\n",
      "Epoch 521 Batch   64/96   train_loss = 0.929\n",
      "Epoch 522 Batch    0/96   train_loss = 0.892\n",
      "Epoch 522 Batch   32/96   train_loss = 0.946\n",
      "Epoch 522 Batch   64/96   train_loss = 0.915\n",
      "Epoch 523 Batch    0/96   train_loss = 0.885\n",
      "Epoch 523 Batch   32/96   train_loss = 0.967\n",
      "Epoch 523 Batch   64/96   train_loss = 0.910\n",
      "Epoch 524 Batch    0/96   train_loss = 0.889\n",
      "Epoch 524 Batch   32/96   train_loss = 0.967\n",
      "Epoch 524 Batch   64/96   train_loss = 0.907\n",
      "Epoch 525 Batch    0/96   train_loss = 0.904\n",
      "Epoch 525 Batch   32/96   train_loss = 0.970\n",
      "Epoch 525 Batch   64/96   train_loss = 0.909\n",
      "Epoch 526 Batch    0/96   train_loss = 0.897\n",
      "Epoch 526 Batch   32/96   train_loss = 0.970\n",
      "Epoch 526 Batch   64/96   train_loss = 0.921\n",
      "Epoch 527 Batch    0/96   train_loss = 0.886\n",
      "Epoch 527 Batch   32/96   train_loss = 0.956\n",
      "Epoch 527 Batch   64/96   train_loss = 0.921\n",
      "Epoch 528 Batch    0/96   train_loss = 0.896\n",
      "Epoch 528 Batch   32/96   train_loss = 0.964\n",
      "Epoch 528 Batch   64/96   train_loss = 0.929\n",
      "Epoch 529 Batch    0/96   train_loss = 0.892\n",
      "Epoch 529 Batch   32/96   train_loss = 0.960\n",
      "Epoch 529 Batch   64/96   train_loss = 0.920\n",
      "Epoch 530 Batch    0/96   train_loss = 0.894\n",
      "Epoch 530 Batch   32/96   train_loss = 0.958\n",
      "Epoch 530 Batch   64/96   train_loss = 0.925\n",
      "Epoch 531 Batch    0/96   train_loss = 0.896\n",
      "Epoch 531 Batch   32/96   train_loss = 0.969\n",
      "Epoch 531 Batch   64/96   train_loss = 0.928\n",
      "Epoch 532 Batch    0/96   train_loss = 0.895\n",
      "Epoch 532 Batch   32/96   train_loss = 0.955\n",
      "Epoch 532 Batch   64/96   train_loss = 0.924\n",
      "Epoch 533 Batch    0/96   train_loss = 0.899\n",
      "Epoch 533 Batch   32/96   train_loss = 0.954\n",
      "Epoch 533 Batch   64/96   train_loss = 0.920\n",
      "Epoch 534 Batch    0/96   train_loss = 0.902\n",
      "Epoch 534 Batch   32/96   train_loss = 0.953\n",
      "Epoch 534 Batch   64/96   train_loss = 0.905\n",
      "Epoch 535 Batch    0/96   train_loss = 0.884\n",
      "Epoch 535 Batch   32/96   train_loss = 0.952\n",
      "Epoch 535 Batch   64/96   train_loss = 0.897\n",
      "Epoch 536 Batch    0/96   train_loss = 0.893\n",
      "Epoch 536 Batch   32/96   train_loss = 0.958\n",
      "Epoch 536 Batch   64/96   train_loss = 0.905\n",
      "Epoch 537 Batch    0/96   train_loss = 0.900\n",
      "Epoch 537 Batch   32/96   train_loss = 0.968\n",
      "Epoch 537 Batch   64/96   train_loss = 0.926\n",
      "Epoch 538 Batch    0/96   train_loss = 0.904\n",
      "Epoch 538 Batch   32/96   train_loss = 0.956\n",
      "Epoch 538 Batch   64/96   train_loss = 0.912\n",
      "Epoch 539 Batch    0/96   train_loss = 0.902\n",
      "Epoch 539 Batch   32/96   train_loss = 0.948\n",
      "Epoch 539 Batch   64/96   train_loss = 0.911\n",
      "Epoch 540 Batch    0/96   train_loss = 0.885\n",
      "Epoch 540 Batch   32/96   train_loss = 0.961\n",
      "Epoch 540 Batch   64/96   train_loss = 0.904\n",
      "Epoch 541 Batch    0/96   train_loss = 0.910\n",
      "Epoch 541 Batch   32/96   train_loss = 0.960\n",
      "Epoch 541 Batch   64/96   train_loss = 0.911\n",
      "Epoch 542 Batch    0/96   train_loss = 0.895\n",
      "Epoch 542 Batch   32/96   train_loss = 0.955\n",
      "Epoch 542 Batch   64/96   train_loss = 0.930\n",
      "Epoch 543 Batch    0/96   train_loss = 0.894\n",
      "Epoch 543 Batch   32/96   train_loss = 0.983\n",
      "Epoch 543 Batch   64/96   train_loss = 0.927\n",
      "Epoch 544 Batch    0/96   train_loss = 0.892\n",
      "Epoch 544 Batch   32/96   train_loss = 0.979\n",
      "Epoch 544 Batch   64/96   train_loss = 0.917\n",
      "Epoch 545 Batch    0/96   train_loss = 0.909\n",
      "Epoch 545 Batch   32/96   train_loss = 0.966\n",
      "Epoch 545 Batch   64/96   train_loss = 0.913\n",
      "Epoch 546 Batch    0/96   train_loss = 0.902\n",
      "Epoch 546 Batch   32/96   train_loss = 0.973\n",
      "Epoch 546 Batch   64/96   train_loss = 0.929\n",
      "Epoch 547 Batch    0/96   train_loss = 0.909\n",
      "Epoch 547 Batch   32/96   train_loss = 0.961\n",
      "Epoch 547 Batch   64/96   train_loss = 0.924\n",
      "Epoch 548 Batch    0/96   train_loss = 0.896\n",
      "Epoch 548 Batch   32/96   train_loss = 0.965\n",
      "Epoch 548 Batch   64/96   train_loss = 0.923\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 549 Batch    0/96   train_loss = 0.893\n",
      "Epoch 549 Batch   32/96   train_loss = 0.957\n",
      "Epoch 549 Batch   64/96   train_loss = 0.909\n",
      "Epoch 550 Batch    0/96   train_loss = 0.902\n",
      "Epoch 550 Batch   32/96   train_loss = 0.971\n",
      "Epoch 550 Batch   64/96   train_loss = 0.927\n",
      "Epoch 551 Batch    0/96   train_loss = 0.904\n",
      "Epoch 551 Batch   32/96   train_loss = 0.972\n",
      "Epoch 551 Batch   64/96   train_loss = 0.936\n",
      "Epoch 552 Batch    0/96   train_loss = 0.900\n",
      "Epoch 552 Batch   32/96   train_loss = 0.965\n",
      "Epoch 552 Batch   64/96   train_loss = 0.923\n",
      "Epoch 553 Batch    0/96   train_loss = 0.886\n",
      "Epoch 553 Batch   32/96   train_loss = 0.960\n",
      "Epoch 553 Batch   64/96   train_loss = 0.909\n",
      "Epoch 554 Batch    0/96   train_loss = 0.892\n",
      "Epoch 554 Batch   32/96   train_loss = 0.957\n",
      "Epoch 554 Batch   64/96   train_loss = 0.915\n",
      "Epoch 555 Batch    0/96   train_loss = 0.898\n",
      "Epoch 555 Batch   32/96   train_loss = 0.950\n",
      "Epoch 555 Batch   64/96   train_loss = 0.911\n",
      "Epoch 556 Batch    0/96   train_loss = 0.889\n",
      "Epoch 556 Batch   32/96   train_loss = 0.960\n",
      "Epoch 556 Batch   64/96   train_loss = 0.919\n",
      "Epoch 557 Batch    0/96   train_loss = 0.892\n",
      "Epoch 557 Batch   32/96   train_loss = 0.960\n",
      "Epoch 557 Batch   64/96   train_loss = 0.916\n",
      "Epoch 558 Batch    0/96   train_loss = 0.885\n",
      "Epoch 558 Batch   32/96   train_loss = 0.952\n",
      "Epoch 558 Batch   64/96   train_loss = 0.908\n",
      "Epoch 559 Batch    0/96   train_loss = 0.888\n",
      "Epoch 559 Batch   32/96   train_loss = 0.955\n",
      "Epoch 559 Batch   64/96   train_loss = 0.911\n",
      "Epoch 560 Batch    0/96   train_loss = 0.901\n",
      "Epoch 560 Batch   32/96   train_loss = 0.956\n",
      "Epoch 560 Batch   64/96   train_loss = 0.906\n",
      "Epoch 561 Batch    0/96   train_loss = 0.884\n",
      "Epoch 561 Batch   32/96   train_loss = 0.960\n",
      "Epoch 561 Batch   64/96   train_loss = 0.913\n",
      "Epoch 562 Batch    0/96   train_loss = 0.891\n",
      "Epoch 562 Batch   32/96   train_loss = 0.960\n",
      "Epoch 562 Batch   64/96   train_loss = 0.914\n",
      "Epoch 563 Batch    0/96   train_loss = 0.883\n",
      "Epoch 563 Batch   32/96   train_loss = 0.972\n",
      "Epoch 563 Batch   64/96   train_loss = 0.922\n",
      "Epoch 564 Batch    0/96   train_loss = 0.889\n",
      "Epoch 564 Batch   32/96   train_loss = 0.974\n",
      "Epoch 564 Batch   64/96   train_loss = 0.927\n",
      "Epoch 565 Batch    0/96   train_loss = 0.886\n",
      "Epoch 565 Batch   32/96   train_loss = 0.965\n",
      "Epoch 565 Batch   64/96   train_loss = 0.921\n",
      "Epoch 566 Batch    0/96   train_loss = 0.890\n",
      "Epoch 566 Batch   32/96   train_loss = 0.961\n",
      "Epoch 566 Batch   64/96   train_loss = 0.918\n",
      "Epoch 567 Batch    0/96   train_loss = 0.887\n",
      "Epoch 567 Batch   32/96   train_loss = 0.948\n",
      "Epoch 567 Batch   64/96   train_loss = 0.913\n",
      "Epoch 568 Batch    0/96   train_loss = 0.891\n",
      "Epoch 568 Batch   32/96   train_loss = 0.949\n",
      "Epoch 568 Batch   64/96   train_loss = 0.910\n",
      "Epoch 569 Batch    0/96   train_loss = 0.887\n",
      "Epoch 569 Batch   32/96   train_loss = 0.967\n",
      "Epoch 569 Batch   64/96   train_loss = 0.911\n",
      "Epoch 570 Batch    0/96   train_loss = 0.873\n",
      "Epoch 570 Batch   32/96   train_loss = 0.961\n",
      "Epoch 570 Batch   64/96   train_loss = 0.910\n",
      "Epoch 571 Batch    0/96   train_loss = 0.880\n",
      "Epoch 571 Batch   32/96   train_loss = 0.962\n",
      "Epoch 571 Batch   64/96   train_loss = 0.903\n",
      "Epoch 572 Batch    0/96   train_loss = 0.891\n",
      "Epoch 572 Batch   32/96   train_loss = 0.966\n",
      "Epoch 572 Batch   64/96   train_loss = 0.920\n",
      "Epoch 573 Batch    0/96   train_loss = 0.896\n",
      "Epoch 573 Batch   32/96   train_loss = 0.958\n",
      "Epoch 573 Batch   64/96   train_loss = 0.921\n",
      "Epoch 574 Batch    0/96   train_loss = 0.894\n",
      "Epoch 574 Batch   32/96   train_loss = 0.958\n",
      "Epoch 574 Batch   64/96   train_loss = 0.915\n",
      "Epoch 575 Batch    0/96   train_loss = 0.894\n",
      "Epoch 575 Batch   32/96   train_loss = 0.969\n",
      "Epoch 575 Batch   64/96   train_loss = 0.913\n",
      "Epoch 576 Batch    0/96   train_loss = 0.893\n",
      "Epoch 576 Batch   32/96   train_loss = 0.956\n",
      "Epoch 576 Batch   64/96   train_loss = 0.914\n",
      "Epoch 577 Batch    0/96   train_loss = 0.901\n",
      "Epoch 577 Batch   32/96   train_loss = 0.962\n",
      "Epoch 577 Batch   64/96   train_loss = 0.927\n",
      "Epoch 578 Batch    0/96   train_loss = 0.880\n",
      "Epoch 578 Batch   32/96   train_loss = 0.963\n",
      "Epoch 578 Batch   64/96   train_loss = 0.920\n",
      "Epoch 579 Batch    0/96   train_loss = 0.886\n",
      "Epoch 579 Batch   32/96   train_loss = 0.955\n",
      "Epoch 579 Batch   64/96   train_loss = 0.921\n",
      "Epoch 580 Batch    0/96   train_loss = 0.877\n",
      "Epoch 580 Batch   32/96   train_loss = 0.952\n",
      "Epoch 580 Batch   64/96   train_loss = 0.916\n",
      "Epoch 581 Batch    0/96   train_loss = 0.888\n",
      "Epoch 581 Batch   32/96   train_loss = 0.939\n",
      "Epoch 581 Batch   64/96   train_loss = 0.910\n",
      "Epoch 582 Batch    0/96   train_loss = 0.887\n",
      "Epoch 582 Batch   32/96   train_loss = 0.942\n",
      "Epoch 582 Batch   64/96   train_loss = 0.931\n",
      "Epoch 583 Batch    0/96   train_loss = 0.891\n",
      "Epoch 583 Batch   32/96   train_loss = 0.959\n",
      "Epoch 583 Batch   64/96   train_loss = 0.920\n",
      "Epoch 584 Batch    0/96   train_loss = 0.885\n",
      "Epoch 584 Batch   32/96   train_loss = 0.952\n",
      "Epoch 584 Batch   64/96   train_loss = 0.915\n",
      "Epoch 585 Batch    0/96   train_loss = 0.883\n",
      "Epoch 585 Batch   32/96   train_loss = 0.952\n",
      "Epoch 585 Batch   64/96   train_loss = 0.908\n",
      "Epoch 586 Batch    0/96   train_loss = 0.872\n",
      "Epoch 586 Batch   32/96   train_loss = 0.967\n",
      "Epoch 586 Batch   64/96   train_loss = 0.906\n",
      "Epoch 587 Batch    0/96   train_loss = 0.881\n",
      "Epoch 587 Batch   32/96   train_loss = 0.953\n",
      "Epoch 587 Batch   64/96   train_loss = 0.919\n",
      "Epoch 588 Batch    0/96   train_loss = 0.874\n",
      "Epoch 588 Batch   32/96   train_loss = 0.942\n",
      "Epoch 588 Batch   64/96   train_loss = 0.915\n",
      "Epoch 589 Batch    0/96   train_loss = 0.871\n",
      "Epoch 589 Batch   32/96   train_loss = 0.945\n",
      "Epoch 589 Batch   64/96   train_loss = 0.910\n",
      "Epoch 590 Batch    0/96   train_loss = 0.891\n",
      "Epoch 590 Batch   32/96   train_loss = 0.947\n",
      "Epoch 590 Batch   64/96   train_loss = 0.897\n",
      "Epoch 591 Batch    0/96   train_loss = 0.883\n",
      "Epoch 591 Batch   32/96   train_loss = 0.955\n",
      "Epoch 591 Batch   64/96   train_loss = 0.924\n",
      "Epoch 592 Batch    0/96   train_loss = 0.884\n",
      "Epoch 592 Batch   32/96   train_loss = 0.953\n",
      "Epoch 592 Batch   64/96   train_loss = 0.903\n",
      "Epoch 593 Batch    0/96   train_loss = 0.871\n",
      "Epoch 593 Batch   32/96   train_loss = 0.956\n",
      "Epoch 593 Batch   64/96   train_loss = 0.904\n",
      "Epoch 594 Batch    0/96   train_loss = 0.873\n",
      "Epoch 594 Batch   32/96   train_loss = 0.957\n",
      "Epoch 594 Batch   64/96   train_loss = 0.905\n",
      "Epoch 595 Batch    0/96   train_loss = 0.887\n",
      "Epoch 595 Batch   32/96   train_loss = 0.942\n",
      "Epoch 595 Batch   64/96   train_loss = 0.901\n",
      "Epoch 596 Batch    0/96   train_loss = 0.890\n",
      "Epoch 596 Batch   32/96   train_loss = 0.943\n",
      "Epoch 596 Batch   64/96   train_loss = 0.906\n",
      "Epoch 597 Batch    0/96   train_loss = 0.888\n",
      "Epoch 597 Batch   32/96   train_loss = 0.941\n",
      "Epoch 597 Batch   64/96   train_loss = 0.913\n",
      "Epoch 598 Batch    0/96   train_loss = 0.900\n",
      "Epoch 598 Batch   32/96   train_loss = 0.955\n",
      "Epoch 598 Batch   64/96   train_loss = 0.918\n",
      "Epoch 599 Batch    0/96   train_loss = 0.895\n",
      "Epoch 599 Batch   32/96   train_loss = 0.960\n",
      "Epoch 599 Batch   64/96   train_loss = 0.912\n",
      "Epoch 600 Batch    0/96   train_loss = 0.893\n",
      "Epoch 600 Batch   32/96   train_loss = 0.949\n",
      "Epoch 600 Batch   64/96   train_loss = 0.916\n",
      "Epoch 601 Batch    0/96   train_loss = 0.894\n",
      "Epoch 601 Batch   32/96   train_loss = 0.951\n",
      "Epoch 601 Batch   64/96   train_loss = 0.909\n",
      "Epoch 602 Batch    0/96   train_loss = 0.878\n",
      "Epoch 602 Batch   32/96   train_loss = 0.960\n",
      "Epoch 602 Batch   64/96   train_loss = 0.903\n",
      "Epoch 603 Batch    0/96   train_loss = 0.878\n",
      "Epoch 603 Batch   32/96   train_loss = 0.955\n",
      "Epoch 603 Batch   64/96   train_loss = 0.900\n",
      "Epoch 604 Batch    0/96   train_loss = 0.881\n",
      "Epoch 604 Batch   32/96   train_loss = 0.954\n",
      "Epoch 604 Batch   64/96   train_loss = 0.917\n",
      "Epoch 605 Batch    0/96   train_loss = 0.889\n",
      "Epoch 605 Batch   32/96   train_loss = 0.940\n",
      "Epoch 605 Batch   64/96   train_loss = 0.907\n",
      "Epoch 606 Batch    0/96   train_loss = 0.899\n",
      "Epoch 606 Batch   32/96   train_loss = 0.958\n",
      "Epoch 606 Batch   64/96   train_loss = 0.912\n",
      "Epoch 607 Batch    0/96   train_loss = 0.880\n",
      "Epoch 607 Batch   32/96   train_loss = 0.947\n",
      "Epoch 607 Batch   64/96   train_loss = 0.919\n",
      "Epoch 608 Batch    0/96   train_loss = 0.891\n",
      "Epoch 608 Batch   32/96   train_loss = 0.953\n",
      "Epoch 608 Batch   64/96   train_loss = 0.903\n",
      "Epoch 609 Batch    0/96   train_loss = 0.891\n",
      "Epoch 609 Batch   32/96   train_loss = 0.952\n",
      "Epoch 609 Batch   64/96   train_loss = 0.908\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 610 Batch    0/96   train_loss = 0.895\n",
      "Epoch 610 Batch   32/96   train_loss = 0.959\n",
      "Epoch 610 Batch   64/96   train_loss = 0.904\n",
      "Epoch 611 Batch    0/96   train_loss = 0.892\n",
      "Epoch 611 Batch   32/96   train_loss = 0.960\n",
      "Epoch 611 Batch   64/96   train_loss = 0.903\n",
      "Epoch 612 Batch    0/96   train_loss = 0.905\n",
      "Epoch 612 Batch   32/96   train_loss = 0.953\n",
      "Epoch 612 Batch   64/96   train_loss = 0.907\n",
      "Epoch 613 Batch    0/96   train_loss = 0.894\n",
      "Epoch 613 Batch   32/96   train_loss = 0.979\n",
      "Epoch 613 Batch   64/96   train_loss = 0.913\n",
      "Epoch 614 Batch    0/96   train_loss = 0.900\n",
      "Epoch 614 Batch   32/96   train_loss = 0.984\n",
      "Epoch 614 Batch   64/96   train_loss = 0.918\n",
      "Epoch 615 Batch    0/96   train_loss = 0.911\n",
      "Epoch 615 Batch   32/96   train_loss = 0.974\n",
      "Epoch 615 Batch   64/96   train_loss = 0.927\n",
      "Epoch 616 Batch    0/96   train_loss = 0.925\n",
      "Epoch 616 Batch   32/96   train_loss = 0.975\n",
      "Epoch 616 Batch   64/96   train_loss = 0.922\n",
      "Epoch 617 Batch    0/96   train_loss = 0.900\n",
      "Epoch 617 Batch   32/96   train_loss = 0.983\n",
      "Epoch 617 Batch   64/96   train_loss = 0.934\n",
      "Epoch 618 Batch    0/96   train_loss = 0.898\n",
      "Epoch 618 Batch   32/96   train_loss = 0.977\n",
      "Epoch 618 Batch   64/96   train_loss = 0.923\n",
      "Epoch 619 Batch    0/96   train_loss = 0.905\n",
      "Epoch 619 Batch   32/96   train_loss = 0.972\n",
      "Epoch 619 Batch   64/96   train_loss = 0.919\n",
      "Epoch 620 Batch    0/96   train_loss = 0.890\n",
      "Epoch 620 Batch   32/96   train_loss = 0.967\n",
      "Epoch 620 Batch   64/96   train_loss = 0.908\n",
      "Epoch 621 Batch    0/96   train_loss = 0.897\n",
      "Epoch 621 Batch   32/96   train_loss = 0.969\n",
      "Epoch 621 Batch   64/96   train_loss = 0.919\n",
      "Epoch 622 Batch    0/96   train_loss = 0.897\n",
      "Epoch 622 Batch   32/96   train_loss = 0.973\n",
      "Epoch 622 Batch   64/96   train_loss = 0.916\n",
      "Epoch 623 Batch    0/96   train_loss = 0.902\n",
      "Epoch 623 Batch   32/96   train_loss = 0.975\n",
      "Epoch 623 Batch   64/96   train_loss = 0.917\n",
      "Epoch 624 Batch    0/96   train_loss = 0.892\n",
      "Epoch 624 Batch   32/96   train_loss = 0.975\n",
      "Epoch 624 Batch   64/96   train_loss = 0.913\n",
      "Epoch 625 Batch    0/96   train_loss = 0.897\n",
      "Epoch 625 Batch   32/96   train_loss = 0.969\n",
      "Epoch 625 Batch   64/96   train_loss = 0.914\n",
      "Epoch 626 Batch    0/96   train_loss = 0.905\n",
      "Epoch 626 Batch   32/96   train_loss = 0.962\n",
      "Epoch 626 Batch   64/96   train_loss = 0.918\n",
      "Epoch 627 Batch    0/96   train_loss = 0.895\n",
      "Epoch 627 Batch   32/96   train_loss = 0.967\n",
      "Epoch 627 Batch   64/96   train_loss = 0.908\n",
      "Epoch 628 Batch    0/96   train_loss = 0.883\n",
      "Epoch 628 Batch   32/96   train_loss = 0.970\n",
      "Epoch 628 Batch   64/96   train_loss = 0.904\n",
      "Epoch 629 Batch    0/96   train_loss = 0.879\n",
      "Epoch 629 Batch   32/96   train_loss = 0.959\n",
      "Epoch 629 Batch   64/96   train_loss = 0.919\n",
      "Epoch 630 Batch    0/96   train_loss = 0.883\n",
      "Epoch 630 Batch   32/96   train_loss = 0.959\n",
      "Epoch 630 Batch   64/96   train_loss = 0.912\n",
      "Epoch 631 Batch    0/96   train_loss = 0.876\n",
      "Epoch 631 Batch   32/96   train_loss = 0.956\n",
      "Epoch 631 Batch   64/96   train_loss = 0.895\n",
      "Epoch 632 Batch    0/96   train_loss = 0.887\n",
      "Epoch 632 Batch   32/96   train_loss = 0.960\n",
      "Epoch 632 Batch   64/96   train_loss = 0.914\n",
      "Epoch 633 Batch    0/96   train_loss = 0.884\n",
      "Epoch 633 Batch   32/96   train_loss = 0.968\n",
      "Epoch 633 Batch   64/96   train_loss = 0.912\n",
      "Epoch 634 Batch    0/96   train_loss = 0.881\n",
      "Epoch 634 Batch   32/96   train_loss = 0.960\n",
      "Epoch 634 Batch   64/96   train_loss = 0.905\n",
      "Epoch 635 Batch    0/96   train_loss = 0.886\n",
      "Epoch 635 Batch   32/96   train_loss = 0.960\n",
      "Epoch 635 Batch   64/96   train_loss = 0.908\n",
      "Epoch 636 Batch    0/96   train_loss = 0.879\n",
      "Epoch 636 Batch   32/96   train_loss = 0.952\n",
      "Epoch 636 Batch   64/96   train_loss = 0.890\n",
      "Epoch 637 Batch    0/96   train_loss = 0.886\n",
      "Epoch 637 Batch   32/96   train_loss = 0.946\n",
      "Epoch 637 Batch   64/96   train_loss = 0.902\n",
      "Epoch 638 Batch    0/96   train_loss = 0.876\n",
      "Epoch 638 Batch   32/96   train_loss = 0.951\n",
      "Epoch 638 Batch   64/96   train_loss = 0.908\n",
      "Epoch 639 Batch    0/96   train_loss = 0.889\n",
      "Epoch 639 Batch   32/96   train_loss = 0.952\n",
      "Epoch 639 Batch   64/96   train_loss = 0.916\n",
      "Epoch 640 Batch    0/96   train_loss = 0.887\n",
      "Epoch 640 Batch   32/96   train_loss = 0.958\n",
      "Epoch 640 Batch   64/96   train_loss = 0.923\n",
      "Epoch 641 Batch    0/96   train_loss = 0.873\n",
      "Epoch 641 Batch   32/96   train_loss = 0.961\n",
      "Epoch 641 Batch   64/96   train_loss = 0.907\n",
      "Epoch 642 Batch    0/96   train_loss = 0.884\n",
      "Epoch 642 Batch   32/96   train_loss = 0.954\n",
      "Epoch 642 Batch   64/96   train_loss = 0.887\n",
      "Epoch 643 Batch    0/96   train_loss = 0.899\n",
      "Epoch 643 Batch   32/96   train_loss = 0.958\n",
      "Epoch 643 Batch   64/96   train_loss = 0.904\n",
      "Epoch 644 Batch    0/96   train_loss = 0.879\n",
      "Epoch 644 Batch   32/96   train_loss = 0.963\n",
      "Epoch 644 Batch   64/96   train_loss = 0.901\n",
      "Epoch 645 Batch    0/96   train_loss = 0.892\n",
      "Epoch 645 Batch   32/96   train_loss = 0.965\n",
      "Epoch 645 Batch   64/96   train_loss = 0.912\n",
      "Epoch 646 Batch    0/96   train_loss = 0.873\n",
      "Epoch 646 Batch   32/96   train_loss = 0.961\n",
      "Epoch 646 Batch   64/96   train_loss = 0.895\n",
      "Epoch 647 Batch    0/96   train_loss = 0.889\n",
      "Epoch 647 Batch   32/96   train_loss = 0.954\n",
      "Epoch 647 Batch   64/96   train_loss = 0.910\n",
      "Epoch 648 Batch    0/96   train_loss = 0.887\n",
      "Epoch 648 Batch   32/96   train_loss = 0.957\n",
      "Epoch 648 Batch   64/96   train_loss = 0.910\n",
      "Epoch 649 Batch    0/96   train_loss = 0.886\n",
      "Epoch 649 Batch   32/96   train_loss = 0.959\n",
      "Epoch 649 Batch   64/96   train_loss = 0.906\n",
      "Epoch 650 Batch    0/96   train_loss = 0.897\n",
      "Epoch 650 Batch   32/96   train_loss = 0.959\n",
      "Epoch 650 Batch   64/96   train_loss = 0.901\n",
      "Epoch 651 Batch    0/96   train_loss = 0.897\n",
      "Epoch 651 Batch   32/96   train_loss = 0.961\n",
      "Epoch 651 Batch   64/96   train_loss = 0.903\n",
      "Epoch 652 Batch    0/96   train_loss = 0.890\n",
      "Epoch 652 Batch   32/96   train_loss = 0.955\n",
      "Epoch 652 Batch   64/96   train_loss = 0.909\n",
      "Epoch 653 Batch    0/96   train_loss = 0.886\n",
      "Epoch 653 Batch   32/96   train_loss = 0.955\n",
      "Epoch 653 Batch   64/96   train_loss = 0.901\n",
      "Epoch 654 Batch    0/96   train_loss = 0.892\n",
      "Epoch 654 Batch   32/96   train_loss = 0.968\n",
      "Epoch 654 Batch   64/96   train_loss = 0.902\n",
      "Epoch 655 Batch    0/96   train_loss = 0.875\n",
      "Epoch 655 Batch   32/96   train_loss = 0.959\n",
      "Epoch 655 Batch   64/96   train_loss = 0.920\n",
      "Epoch 656 Batch    0/96   train_loss = 0.892\n",
      "Epoch 656 Batch   32/96   train_loss = 0.956\n",
      "Epoch 656 Batch   64/96   train_loss = 0.904\n",
      "Epoch 657 Batch    0/96   train_loss = 0.899\n",
      "Epoch 657 Batch   32/96   train_loss = 0.966\n",
      "Epoch 657 Batch   64/96   train_loss = 0.928\n",
      "Epoch 658 Batch    0/96   train_loss = 0.892\n",
      "Epoch 658 Batch   32/96   train_loss = 0.965\n",
      "Epoch 658 Batch   64/96   train_loss = 0.922\n",
      "Epoch 659 Batch    0/96   train_loss = 0.895\n",
      "Epoch 659 Batch   32/96   train_loss = 0.965\n",
      "Epoch 659 Batch   64/96   train_loss = 0.919\n",
      "Epoch 660 Batch    0/96   train_loss = 0.895\n",
      "Epoch 660 Batch   32/96   train_loss = 0.953\n",
      "Epoch 660 Batch   64/96   train_loss = 0.930\n",
      "Epoch 661 Batch    0/96   train_loss = 0.890\n",
      "Epoch 661 Batch   32/96   train_loss = 0.958\n",
      "Epoch 661 Batch   64/96   train_loss = 0.920\n",
      "Epoch 662 Batch    0/96   train_loss = 0.897\n",
      "Epoch 662 Batch   32/96   train_loss = 0.961\n",
      "Epoch 662 Batch   64/96   train_loss = 0.918\n",
      "Epoch 663 Batch    0/96   train_loss = 0.891\n",
      "Epoch 663 Batch   32/96   train_loss = 0.964\n",
      "Epoch 663 Batch   64/96   train_loss = 0.925\n",
      "Epoch 664 Batch    0/96   train_loss = 0.901\n",
      "Epoch 664 Batch   32/96   train_loss = 0.963\n",
      "Epoch 664 Batch   64/96   train_loss = 0.923\n",
      "Epoch 665 Batch    0/96   train_loss = 0.903\n",
      "Epoch 665 Batch   32/96   train_loss = 0.951\n",
      "Epoch 665 Batch   64/96   train_loss = 0.930\n",
      "Epoch 666 Batch    0/96   train_loss = 0.906\n",
      "Epoch 666 Batch   32/96   train_loss = 0.973\n",
      "Epoch 666 Batch   64/96   train_loss = 0.939\n",
      "Epoch 667 Batch    0/96   train_loss = 0.907\n",
      "Epoch 667 Batch   32/96   train_loss = 0.955\n",
      "Epoch 667 Batch   64/96   train_loss = 0.931\n",
      "Epoch 668 Batch    0/96   train_loss = 0.907\n",
      "Epoch 668 Batch   32/96   train_loss = 0.951\n",
      "Epoch 668 Batch   64/96   train_loss = 0.930\n",
      "Epoch 669 Batch    0/96   train_loss = 0.894\n",
      "Epoch 669 Batch   32/96   train_loss = 0.950\n",
      "Epoch 669 Batch   64/96   train_loss = 0.926\n",
      "Epoch 670 Batch    0/96   train_loss = 0.900\n",
      "Epoch 670 Batch   32/96   train_loss = 0.948\n",
      "Epoch 670 Batch   64/96   train_loss = 0.929\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 671 Batch    0/96   train_loss = 0.895\n",
      "Epoch 671 Batch   32/96   train_loss = 0.962\n",
      "Epoch 671 Batch   64/96   train_loss = 0.921\n",
      "Epoch 672 Batch    0/96   train_loss = 0.889\n",
      "Epoch 672 Batch   32/96   train_loss = 0.973\n",
      "Epoch 672 Batch   64/96   train_loss = 0.927\n",
      "Epoch 673 Batch    0/96   train_loss = 0.901\n",
      "Epoch 673 Batch   32/96   train_loss = 0.966\n",
      "Epoch 673 Batch   64/96   train_loss = 0.909\n",
      "Epoch 674 Batch    0/96   train_loss = 0.892\n",
      "Epoch 674 Batch   32/96   train_loss = 0.958\n",
      "Epoch 674 Batch   64/96   train_loss = 0.917\n",
      "Epoch 675 Batch    0/96   train_loss = 0.893\n",
      "Epoch 675 Batch   32/96   train_loss = 0.956\n",
      "Epoch 675 Batch   64/96   train_loss = 0.914\n",
      "Epoch 676 Batch    0/96   train_loss = 0.899\n",
      "Epoch 676 Batch   32/96   train_loss = 0.943\n",
      "Epoch 676 Batch   64/96   train_loss = 0.910\n",
      "Epoch 677 Batch    0/96   train_loss = 0.885\n",
      "Epoch 677 Batch   32/96   train_loss = 0.947\n",
      "Epoch 677 Batch   64/96   train_loss = 0.921\n",
      "Epoch 678 Batch    0/96   train_loss = 0.892\n",
      "Epoch 678 Batch   32/96   train_loss = 0.948\n",
      "Epoch 678 Batch   64/96   train_loss = 0.912\n",
      "Epoch 679 Batch    0/96   train_loss = 0.884\n",
      "Epoch 679 Batch   32/96   train_loss = 0.946\n",
      "Epoch 679 Batch   64/96   train_loss = 0.903\n",
      "Epoch 680 Batch    0/96   train_loss = 0.899\n",
      "Epoch 680 Batch   32/96   train_loss = 0.953\n",
      "Epoch 680 Batch   64/96   train_loss = 0.912\n",
      "Epoch 681 Batch    0/96   train_loss = 0.895\n",
      "Epoch 681 Batch   32/96   train_loss = 0.951\n",
      "Epoch 681 Batch   64/96   train_loss = 0.907\n",
      "Epoch 682 Batch    0/96   train_loss = 0.882\n",
      "Epoch 682 Batch   32/96   train_loss = 0.960\n",
      "Epoch 682 Batch   64/96   train_loss = 0.909\n",
      "Epoch 683 Batch    0/96   train_loss = 0.881\n",
      "Epoch 683 Batch   32/96   train_loss = 0.942\n",
      "Epoch 683 Batch   64/96   train_loss = 0.909\n",
      "Epoch 684 Batch    0/96   train_loss = 0.884\n",
      "Epoch 684 Batch   32/96   train_loss = 0.945\n",
      "Epoch 684 Batch   64/96   train_loss = 0.922\n",
      "Epoch 685 Batch    0/96   train_loss = 0.894\n",
      "Epoch 685 Batch   32/96   train_loss = 0.954\n",
      "Epoch 685 Batch   64/96   train_loss = 0.919\n",
      "Epoch 686 Batch    0/96   train_loss = 0.886\n",
      "Epoch 686 Batch   32/96   train_loss = 0.948\n",
      "Epoch 686 Batch   64/96   train_loss = 0.917\n",
      "Epoch 687 Batch    0/96   train_loss = 0.892\n",
      "Epoch 687 Batch   32/96   train_loss = 0.960\n",
      "Epoch 687 Batch   64/96   train_loss = 0.907\n",
      "Epoch 688 Batch    0/96   train_loss = 0.892\n",
      "Epoch 688 Batch   32/96   train_loss = 0.952\n",
      "Epoch 688 Batch   64/96   train_loss = 0.919\n",
      "Epoch 689 Batch    0/96   train_loss = 0.898\n",
      "Epoch 689 Batch   32/96   train_loss = 0.959\n",
      "Epoch 689 Batch   64/96   train_loss = 0.924\n",
      "Epoch 690 Batch    0/96   train_loss = 0.900\n",
      "Epoch 690 Batch   32/96   train_loss = 0.964\n",
      "Epoch 690 Batch   64/96   train_loss = 0.925\n",
      "Epoch 691 Batch    0/96   train_loss = 0.890\n",
      "Epoch 691 Batch   32/96   train_loss = 0.954\n",
      "Epoch 691 Batch   64/96   train_loss = 0.917\n",
      "Epoch 692 Batch    0/96   train_loss = 0.882\n",
      "Epoch 692 Batch   32/96   train_loss = 0.962\n",
      "Epoch 692 Batch   64/96   train_loss = 0.908\n",
      "Epoch 693 Batch    0/96   train_loss = 0.888\n",
      "Epoch 693 Batch   32/96   train_loss = 0.955\n",
      "Epoch 693 Batch   64/96   train_loss = 0.918\n",
      "Epoch 694 Batch    0/96   train_loss = 0.890\n",
      "Epoch 694 Batch   32/96   train_loss = 0.956\n",
      "Epoch 694 Batch   64/96   train_loss = 0.916\n",
      "Epoch 695 Batch    0/96   train_loss = 0.882\n",
      "Epoch 695 Batch   32/96   train_loss = 0.955\n",
      "Epoch 695 Batch   64/96   train_loss = 0.909\n",
      "Epoch 696 Batch    0/96   train_loss = 0.880\n",
      "Epoch 696 Batch   32/96   train_loss = 0.951\n",
      "Epoch 696 Batch   64/96   train_loss = 0.935\n",
      "Epoch 697 Batch    0/96   train_loss = 0.887\n",
      "Epoch 697 Batch   32/96   train_loss = 0.945\n",
      "Epoch 697 Batch   64/96   train_loss = 0.918\n",
      "Epoch 698 Batch    0/96   train_loss = 0.887\n",
      "Epoch 698 Batch   32/96   train_loss = 0.958\n",
      "Epoch 698 Batch   64/96   train_loss = 0.919\n",
      "Epoch 699 Batch    0/96   train_loss = 0.884\n",
      "Epoch 699 Batch   32/96   train_loss = 0.952\n",
      "Epoch 699 Batch   64/96   train_loss = 0.914\n",
      "Epoch 700 Batch    0/96   train_loss = 0.877\n",
      "Epoch 700 Batch   32/96   train_loss = 0.953\n",
      "Epoch 700 Batch   64/96   train_loss = 0.921\n",
      "Epoch 701 Batch    0/96   train_loss = 0.892\n",
      "Epoch 701 Batch   32/96   train_loss = 0.951\n",
      "Epoch 701 Batch   64/96   train_loss = 0.910\n",
      "Epoch 702 Batch    0/96   train_loss = 0.886\n",
      "Epoch 702 Batch   32/96   train_loss = 0.954\n",
      "Epoch 702 Batch   64/96   train_loss = 0.924\n",
      "Epoch 703 Batch    0/96   train_loss = 0.888\n",
      "Epoch 703 Batch   32/96   train_loss = 0.953\n",
      "Epoch 703 Batch   64/96   train_loss = 0.914\n",
      "Epoch 704 Batch    0/96   train_loss = 0.885\n",
      "Epoch 704 Batch   32/96   train_loss = 0.956\n",
      "Epoch 704 Batch   64/96   train_loss = 0.929\n",
      "Epoch 705 Batch    0/96   train_loss = 0.894\n",
      "Epoch 705 Batch   32/96   train_loss = 0.949\n",
      "Epoch 705 Batch   64/96   train_loss = 0.917\n",
      "Epoch 706 Batch    0/96   train_loss = 0.880\n",
      "Epoch 706 Batch   32/96   train_loss = 0.944\n",
      "Epoch 706 Batch   64/96   train_loss = 0.915\n",
      "Epoch 707 Batch    0/96   train_loss = 0.875\n",
      "Epoch 707 Batch   32/96   train_loss = 0.947\n",
      "Epoch 707 Batch   64/96   train_loss = 0.921\n",
      "Epoch 708 Batch    0/96   train_loss = 0.899\n",
      "Epoch 708 Batch   32/96   train_loss = 0.945\n",
      "Epoch 708 Batch   64/96   train_loss = 0.912\n",
      "Epoch 709 Batch    0/96   train_loss = 0.877\n",
      "Epoch 709 Batch   32/96   train_loss = 0.956\n",
      "Epoch 709 Batch   64/96   train_loss = 0.908\n",
      "Epoch 710 Batch    0/96   train_loss = 0.896\n",
      "Epoch 710 Batch   32/96   train_loss = 0.955\n",
      "Epoch 710 Batch   64/96   train_loss = 0.921\n",
      "Epoch 711 Batch    0/96   train_loss = 0.893\n",
      "Epoch 711 Batch   32/96   train_loss = 0.957\n",
      "Epoch 711 Batch   64/96   train_loss = 0.907\n",
      "Epoch 712 Batch    0/96   train_loss = 0.905\n",
      "Epoch 712 Batch   32/96   train_loss = 0.944\n",
      "Epoch 712 Batch   64/96   train_loss = 0.912\n",
      "Epoch 713 Batch    0/96   train_loss = 0.896\n",
      "Epoch 713 Batch   32/96   train_loss = 0.952\n",
      "Epoch 713 Batch   64/96   train_loss = 0.912\n",
      "Epoch 714 Batch    0/96   train_loss = 0.892\n",
      "Epoch 714 Batch   32/96   train_loss = 0.936\n",
      "Epoch 714 Batch   64/96   train_loss = 0.900\n",
      "Epoch 715 Batch    0/96   train_loss = 0.899\n",
      "Epoch 715 Batch   32/96   train_loss = 0.942\n",
      "Epoch 715 Batch   64/96   train_loss = 0.913\n",
      "Epoch 716 Batch    0/96   train_loss = 0.887\n",
      "Epoch 716 Batch   32/96   train_loss = 0.935\n",
      "Epoch 716 Batch   64/96   train_loss = 0.905\n",
      "Epoch 717 Batch    0/96   train_loss = 0.890\n",
      "Epoch 717 Batch   32/96   train_loss = 0.954\n",
      "Epoch 717 Batch   64/96   train_loss = 0.915\n",
      "Epoch 718 Batch    0/96   train_loss = 0.891\n",
      "Epoch 718 Batch   32/96   train_loss = 0.944\n",
      "Epoch 718 Batch   64/96   train_loss = 0.909\n",
      "Epoch 719 Batch    0/96   train_loss = 0.884\n",
      "Epoch 719 Batch   32/96   train_loss = 0.943\n",
      "Epoch 719 Batch   64/96   train_loss = 0.902\n",
      "Epoch 720 Batch    0/96   train_loss = 0.881\n",
      "Epoch 720 Batch   32/96   train_loss = 0.949\n",
      "Epoch 720 Batch   64/96   train_loss = 0.906\n",
      "Epoch 721 Batch    0/96   train_loss = 0.890\n",
      "Epoch 721 Batch   32/96   train_loss = 0.943\n",
      "Epoch 721 Batch   64/96   train_loss = 0.898\n",
      "Epoch 722 Batch    0/96   train_loss = 0.883\n",
      "Epoch 722 Batch   32/96   train_loss = 0.947\n",
      "Epoch 722 Batch   64/96   train_loss = 0.904\n",
      "Epoch 723 Batch    0/96   train_loss = 0.889\n",
      "Epoch 723 Batch   32/96   train_loss = 0.937\n",
      "Epoch 723 Batch   64/96   train_loss = 0.911\n",
      "Epoch 724 Batch    0/96   train_loss = 0.878\n",
      "Epoch 724 Batch   32/96   train_loss = 0.948\n",
      "Epoch 724 Batch   64/96   train_loss = 0.913\n",
      "Epoch 725 Batch    0/96   train_loss = 0.877\n",
      "Epoch 725 Batch   32/96   train_loss = 0.951\n",
      "Epoch 725 Batch   64/96   train_loss = 0.915\n",
      "Epoch 726 Batch    0/96   train_loss = 0.886\n",
      "Epoch 726 Batch   32/96   train_loss = 0.940\n",
      "Epoch 726 Batch   64/96   train_loss = 0.910\n",
      "Epoch 727 Batch    0/96   train_loss = 0.881\n",
      "Epoch 727 Batch   32/96   train_loss = 0.932\n",
      "Epoch 727 Batch   64/96   train_loss = 0.919\n",
      "Epoch 728 Batch    0/96   train_loss = 0.877\n",
      "Epoch 728 Batch   32/96   train_loss = 0.949\n",
      "Epoch 728 Batch   64/96   train_loss = 0.912\n",
      "Epoch 729 Batch    0/96   train_loss = 0.890\n",
      "Epoch 729 Batch   32/96   train_loss = 0.945\n",
      "Epoch 729 Batch   64/96   train_loss = 0.916\n",
      "Epoch 730 Batch    0/96   train_loss = 0.891\n",
      "Epoch 730 Batch   32/96   train_loss = 0.944\n",
      "Epoch 730 Batch   64/96   train_loss = 0.916\n",
      "Epoch 731 Batch    0/96   train_loss = 0.900\n",
      "Epoch 731 Batch   32/96   train_loss = 0.948\n",
      "Epoch 731 Batch   64/96   train_loss = 0.922\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 732 Batch    0/96   train_loss = 0.902\n",
      "Epoch 732 Batch   32/96   train_loss = 0.956\n",
      "Epoch 732 Batch   64/96   train_loss = 0.912\n",
      "Epoch 733 Batch    0/96   train_loss = 0.898\n",
      "Epoch 733 Batch   32/96   train_loss = 0.960\n",
      "Epoch 733 Batch   64/96   train_loss = 0.927\n",
      "Epoch 734 Batch    0/96   train_loss = 0.884\n",
      "Epoch 734 Batch   32/96   train_loss = 0.946\n",
      "Epoch 734 Batch   64/96   train_loss = 0.916\n",
      "Epoch 735 Batch    0/96   train_loss = 0.885\n",
      "Epoch 735 Batch   32/96   train_loss = 0.952\n",
      "Epoch 735 Batch   64/96   train_loss = 0.922\n",
      "Epoch 736 Batch    0/96   train_loss = 0.895\n",
      "Epoch 736 Batch   32/96   train_loss = 0.968\n",
      "Epoch 736 Batch   64/96   train_loss = 0.915\n",
      "Epoch 737 Batch    0/96   train_loss = 0.900\n",
      "Epoch 737 Batch   32/96   train_loss = 0.948\n",
      "Epoch 737 Batch   64/96   train_loss = 0.912\n",
      "Epoch 738 Batch    0/96   train_loss = 0.887\n",
      "Epoch 738 Batch   32/96   train_loss = 0.944\n",
      "Epoch 738 Batch   64/96   train_loss = 0.900\n",
      "Epoch 739 Batch    0/96   train_loss = 0.876\n",
      "Epoch 739 Batch   32/96   train_loss = 0.939\n",
      "Epoch 739 Batch   64/96   train_loss = 0.900\n",
      "Epoch 740 Batch    0/96   train_loss = 0.879\n",
      "Epoch 740 Batch   32/96   train_loss = 0.929\n",
      "Epoch 740 Batch   64/96   train_loss = 0.909\n",
      "Epoch 741 Batch    0/96   train_loss = 0.893\n",
      "Epoch 741 Batch   32/96   train_loss = 0.946\n",
      "Epoch 741 Batch   64/96   train_loss = 0.914\n",
      "Epoch 742 Batch    0/96   train_loss = 0.894\n",
      "Epoch 742 Batch   32/96   train_loss = 0.945\n",
      "Epoch 742 Batch   64/96   train_loss = 0.925\n",
      "Epoch 743 Batch    0/96   train_loss = 0.882\n",
      "Epoch 743 Batch   32/96   train_loss = 0.951\n",
      "Epoch 743 Batch   64/96   train_loss = 0.917\n",
      "Epoch 744 Batch    0/96   train_loss = 0.894\n",
      "Epoch 744 Batch   32/96   train_loss = 0.938\n",
      "Epoch 744 Batch   64/96   train_loss = 0.913\n",
      "Epoch 745 Batch    0/96   train_loss = 0.887\n",
      "Epoch 745 Batch   32/96   train_loss = 0.942\n",
      "Epoch 745 Batch   64/96   train_loss = 0.904\n",
      "Epoch 746 Batch    0/96   train_loss = 0.881\n",
      "Epoch 746 Batch   32/96   train_loss = 0.932\n",
      "Epoch 746 Batch   64/96   train_loss = 0.898\n",
      "Epoch 747 Batch    0/96   train_loss = 0.895\n",
      "Epoch 747 Batch   32/96   train_loss = 0.935\n",
      "Epoch 747 Batch   64/96   train_loss = 0.901\n",
      "Epoch 748 Batch    0/96   train_loss = 0.884\n",
      "Epoch 748 Batch   32/96   train_loss = 0.941\n",
      "Epoch 748 Batch   64/96   train_loss = 0.904\n",
      "Epoch 749 Batch    0/96   train_loss = 0.883\n",
      "Epoch 749 Batch   32/96   train_loss = 0.929\n",
      "Epoch 749 Batch   64/96   train_loss = 0.900\n",
      "Epoch 750 Batch    0/96   train_loss = 0.881\n",
      "Epoch 750 Batch   32/96   train_loss = 0.938\n",
      "Epoch 750 Batch   64/96   train_loss = 0.897\n",
      "Epoch 751 Batch    0/96   train_loss = 0.875\n",
      "Epoch 751 Batch   32/96   train_loss = 0.941\n",
      "Epoch 751 Batch   64/96   train_loss = 0.902\n",
      "Epoch 752 Batch    0/96   train_loss = 0.878\n",
      "Epoch 752 Batch   32/96   train_loss = 0.933\n",
      "Epoch 752 Batch   64/96   train_loss = 0.895\n",
      "Epoch 753 Batch    0/96   train_loss = 0.894\n",
      "Epoch 753 Batch   32/96   train_loss = 0.949\n",
      "Epoch 753 Batch   64/96   train_loss = 0.900\n",
      "Epoch 754 Batch    0/96   train_loss = 0.888\n",
      "Epoch 754 Batch   32/96   train_loss = 0.941\n",
      "Epoch 754 Batch   64/96   train_loss = 0.904\n",
      "Epoch 755 Batch    0/96   train_loss = 0.896\n",
      "Epoch 755 Batch   32/96   train_loss = 0.947\n",
      "Epoch 755 Batch   64/96   train_loss = 0.912\n",
      "Epoch 756 Batch    0/96   train_loss = 0.891\n",
      "Epoch 756 Batch   32/96   train_loss = 0.943\n",
      "Epoch 756 Batch   64/96   train_loss = 0.910\n",
      "Epoch 757 Batch    0/96   train_loss = 0.890\n",
      "Epoch 757 Batch   32/96   train_loss = 0.938\n",
      "Epoch 757 Batch   64/96   train_loss = 0.915\n",
      "Epoch 758 Batch    0/96   train_loss = 0.887\n",
      "Epoch 758 Batch   32/96   train_loss = 0.939\n",
      "Epoch 758 Batch   64/96   train_loss = 0.900\n",
      "Epoch 759 Batch    0/96   train_loss = 0.894\n",
      "Epoch 759 Batch   32/96   train_loss = 0.953\n",
      "Epoch 759 Batch   64/96   train_loss = 0.900\n",
      "Epoch 760 Batch    0/96   train_loss = 0.901\n",
      "Epoch 760 Batch   32/96   train_loss = 0.935\n",
      "Epoch 760 Batch   64/96   train_loss = 0.900\n",
      "Epoch 761 Batch    0/96   train_loss = 0.878\n",
      "Epoch 761 Batch   32/96   train_loss = 0.938\n",
      "Epoch 761 Batch   64/96   train_loss = 0.911\n",
      "Epoch 762 Batch    0/96   train_loss = 0.885\n",
      "Epoch 762 Batch   32/96   train_loss = 0.941\n",
      "Epoch 762 Batch   64/96   train_loss = 0.904\n",
      "Epoch 763 Batch    0/96   train_loss = 0.889\n",
      "Epoch 763 Batch   32/96   train_loss = 0.953\n",
      "Epoch 763 Batch   64/96   train_loss = 0.905\n",
      "Epoch 764 Batch    0/96   train_loss = 0.876\n",
      "Epoch 764 Batch   32/96   train_loss = 0.955\n",
      "Epoch 764 Batch   64/96   train_loss = 0.911\n",
      "Epoch 765 Batch    0/96   train_loss = 0.909\n",
      "Epoch 765 Batch   32/96   train_loss = 0.940\n",
      "Epoch 765 Batch   64/96   train_loss = 0.912\n",
      "Epoch 766 Batch    0/96   train_loss = 0.883\n",
      "Epoch 766 Batch   32/96   train_loss = 0.938\n",
      "Epoch 766 Batch   64/96   train_loss = 0.912\n",
      "Epoch 767 Batch    0/96   train_loss = 0.888\n",
      "Epoch 767 Batch   32/96   train_loss = 0.940\n",
      "Epoch 767 Batch   64/96   train_loss = 0.914\n",
      "Epoch 768 Batch    0/96   train_loss = 0.899\n",
      "Epoch 768 Batch   32/96   train_loss = 0.950\n",
      "Epoch 768 Batch   64/96   train_loss = 0.907\n",
      "Epoch 769 Batch    0/96   train_loss = 0.884\n",
      "Epoch 769 Batch   32/96   train_loss = 0.940\n",
      "Epoch 769 Batch   64/96   train_loss = 0.927\n",
      "Epoch 770 Batch    0/96   train_loss = 0.889\n",
      "Epoch 770 Batch   32/96   train_loss = 0.938\n",
      "Epoch 770 Batch   64/96   train_loss = 0.894\n",
      "Epoch 771 Batch    0/96   train_loss = 0.888\n",
      "Epoch 771 Batch   32/96   train_loss = 0.938\n",
      "Epoch 771 Batch   64/96   train_loss = 0.901\n",
      "Epoch 772 Batch    0/96   train_loss = 0.887\n",
      "Epoch 772 Batch   32/96   train_loss = 0.953\n",
      "Epoch 772 Batch   64/96   train_loss = 0.910\n",
      "Epoch 773 Batch    0/96   train_loss = 0.878\n",
      "Epoch 773 Batch   32/96   train_loss = 0.938\n",
      "Epoch 773 Batch   64/96   train_loss = 0.901\n",
      "Epoch 774 Batch    0/96   train_loss = 0.891\n",
      "Epoch 774 Batch   32/96   train_loss = 0.938\n",
      "Epoch 774 Batch   64/96   train_loss = 0.895\n",
      "Epoch 775 Batch    0/96   train_loss = 0.873\n",
      "Epoch 775 Batch   32/96   train_loss = 0.937\n",
      "Epoch 775 Batch   64/96   train_loss = 0.902\n",
      "Epoch 776 Batch    0/96   train_loss = 0.883\n",
      "Epoch 776 Batch   32/96   train_loss = 0.937\n",
      "Epoch 776 Batch   64/96   train_loss = 0.894\n",
      "Epoch 777 Batch    0/96   train_loss = 0.880\n",
      "Epoch 777 Batch   32/96   train_loss = 0.943\n",
      "Epoch 777 Batch   64/96   train_loss = 0.919\n",
      "Epoch 778 Batch    0/96   train_loss = 0.882\n",
      "Epoch 778 Batch   32/96   train_loss = 0.938\n",
      "Epoch 778 Batch   64/96   train_loss = 0.897\n",
      "Epoch 779 Batch    0/96   train_loss = 0.899\n",
      "Epoch 779 Batch   32/96   train_loss = 0.938\n",
      "Epoch 779 Batch   64/96   train_loss = 0.895\n",
      "Epoch 780 Batch    0/96   train_loss = 0.887\n",
      "Epoch 780 Batch   32/96   train_loss = 0.934\n",
      "Epoch 780 Batch   64/96   train_loss = 0.897\n",
      "Epoch 781 Batch    0/96   train_loss = 0.881\n",
      "Epoch 781 Batch   32/96   train_loss = 0.931\n",
      "Epoch 781 Batch   64/96   train_loss = 0.893\n",
      "Epoch 782 Batch    0/96   train_loss = 0.883\n",
      "Epoch 782 Batch   32/96   train_loss = 0.950\n",
      "Epoch 782 Batch   64/96   train_loss = 0.923\n",
      "Epoch 783 Batch    0/96   train_loss = 0.878\n",
      "Epoch 783 Batch   32/96   train_loss = 0.934\n",
      "Epoch 783 Batch   64/96   train_loss = 0.894\n",
      "Epoch 784 Batch    0/96   train_loss = 0.880\n",
      "Epoch 784 Batch   32/96   train_loss = 0.944\n",
      "Epoch 784 Batch   64/96   train_loss = 0.895\n",
      "Epoch 785 Batch    0/96   train_loss = 0.869\n",
      "Epoch 785 Batch   32/96   train_loss = 0.935\n",
      "Epoch 785 Batch   64/96   train_loss = 0.911\n",
      "Epoch 786 Batch    0/96   train_loss = 0.878\n",
      "Epoch 786 Batch   32/96   train_loss = 0.937\n",
      "Epoch 786 Batch   64/96   train_loss = 0.913\n",
      "Epoch 787 Batch    0/96   train_loss = 0.894\n",
      "Epoch 787 Batch   32/96   train_loss = 0.924\n",
      "Epoch 787 Batch   64/96   train_loss = 0.901\n",
      "Epoch 788 Batch    0/96   train_loss = 0.893\n",
      "Epoch 788 Batch   32/96   train_loss = 0.928\n",
      "Epoch 788 Batch   64/96   train_loss = 0.919\n",
      "Epoch 789 Batch    0/96   train_loss = 0.888\n",
      "Epoch 789 Batch   32/96   train_loss = 0.927\n",
      "Epoch 789 Batch   64/96   train_loss = 0.896\n",
      "Epoch 790 Batch    0/96   train_loss = 0.877\n",
      "Epoch 790 Batch   32/96   train_loss = 0.927\n",
      "Epoch 790 Batch   64/96   train_loss = 0.904\n",
      "Epoch 791 Batch    0/96   train_loss = 0.891\n",
      "Epoch 791 Batch   32/96   train_loss = 0.921\n",
      "Epoch 791 Batch   64/96   train_loss = 0.907\n",
      "Epoch 792 Batch    0/96   train_loss = 0.888\n",
      "Epoch 792 Batch   32/96   train_loss = 0.937\n",
      "Epoch 792 Batch   64/96   train_loss = 0.904\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 793 Batch    0/96   train_loss = 0.882\n",
      "Epoch 793 Batch   32/96   train_loss = 0.927\n",
      "Epoch 793 Batch   64/96   train_loss = 0.896\n",
      "Epoch 794 Batch    0/96   train_loss = 0.874\n",
      "Epoch 794 Batch   32/96   train_loss = 0.926\n",
      "Epoch 794 Batch   64/96   train_loss = 0.898\n",
      "Epoch 795 Batch    0/96   train_loss = 0.884\n",
      "Epoch 795 Batch   32/96   train_loss = 0.938\n",
      "Epoch 795 Batch   64/96   train_loss = 0.896\n",
      "Epoch 796 Batch    0/96   train_loss = 0.903\n",
      "Epoch 796 Batch   32/96   train_loss = 0.939\n",
      "Epoch 796 Batch   64/96   train_loss = 0.904\n",
      "Epoch 797 Batch    0/96   train_loss = 0.889\n",
      "Epoch 797 Batch   32/96   train_loss = 0.930\n",
      "Epoch 797 Batch   64/96   train_loss = 0.896\n",
      "Epoch 798 Batch    0/96   train_loss = 0.896\n",
      "Epoch 798 Batch   32/96   train_loss = 0.940\n",
      "Epoch 798 Batch   64/96   train_loss = 0.903\n",
      "Epoch 799 Batch    0/96   train_loss = 0.894\n",
      "Epoch 799 Batch   32/96   train_loss = 0.937\n",
      "Epoch 799 Batch   64/96   train_loss = 0.900\n",
      "Epoch 800 Batch    0/96   train_loss = 0.890\n",
      "Epoch 800 Batch   32/96   train_loss = 0.940\n",
      "Epoch 800 Batch   64/96   train_loss = 0.906\n",
      "Epoch 801 Batch    0/96   train_loss = 0.886\n",
      "Epoch 801 Batch   32/96   train_loss = 0.939\n",
      "Epoch 801 Batch   64/96   train_loss = 0.904\n",
      "Epoch 802 Batch    0/96   train_loss = 0.893\n",
      "Epoch 802 Batch   32/96   train_loss = 0.941\n",
      "Epoch 802 Batch   64/96   train_loss = 0.919\n",
      "Epoch 803 Batch    0/96   train_loss = 0.898\n",
      "Epoch 803 Batch   32/96   train_loss = 0.945\n",
      "Epoch 803 Batch   64/96   train_loss = 0.905\n",
      "Epoch 804 Batch    0/96   train_loss = 0.895\n",
      "Epoch 804 Batch   32/96   train_loss = 0.931\n",
      "Epoch 804 Batch   64/96   train_loss = 0.903\n",
      "Epoch 805 Batch    0/96   train_loss = 0.887\n",
      "Epoch 805 Batch   32/96   train_loss = 0.937\n",
      "Epoch 805 Batch   64/96   train_loss = 0.910\n",
      "Epoch 806 Batch    0/96   train_loss = 0.899\n",
      "Epoch 806 Batch   32/96   train_loss = 0.944\n",
      "Epoch 806 Batch   64/96   train_loss = 0.918\n",
      "Epoch 807 Batch    0/96   train_loss = 0.892\n",
      "Epoch 807 Batch   32/96   train_loss = 0.953\n",
      "Epoch 807 Batch   64/96   train_loss = 0.905\n",
      "Epoch 808 Batch    0/96   train_loss = 0.894\n",
      "Epoch 808 Batch   32/96   train_loss = 0.963\n",
      "Epoch 808 Batch   64/96   train_loss = 0.906\n",
      "Epoch 809 Batch    0/96   train_loss = 0.897\n",
      "Epoch 809 Batch   32/96   train_loss = 0.961\n",
      "Epoch 809 Batch   64/96   train_loss = 0.918\n",
      "Epoch 810 Batch    0/96   train_loss = 0.901\n",
      "Epoch 810 Batch   32/96   train_loss = 0.953\n",
      "Epoch 810 Batch   64/96   train_loss = 0.914\n",
      "Epoch 811 Batch    0/96   train_loss = 0.902\n",
      "Epoch 811 Batch   32/96   train_loss = 0.959\n",
      "Epoch 811 Batch   64/96   train_loss = 0.909\n",
      "Epoch 812 Batch    0/96   train_loss = 0.890\n",
      "Epoch 812 Batch   32/96   train_loss = 0.950\n",
      "Epoch 812 Batch   64/96   train_loss = 0.927\n",
      "Epoch 813 Batch    0/96   train_loss = 0.898\n",
      "Epoch 813 Batch   32/96   train_loss = 0.946\n",
      "Epoch 813 Batch   64/96   train_loss = 0.920\n",
      "Epoch 814 Batch    0/96   train_loss = 0.907\n",
      "Epoch 814 Batch   32/96   train_loss = 0.944\n",
      "Epoch 814 Batch   64/96   train_loss = 0.916\n",
      "Epoch 815 Batch    0/96   train_loss = 0.892\n",
      "Epoch 815 Batch   32/96   train_loss = 0.949\n",
      "Epoch 815 Batch   64/96   train_loss = 0.901\n",
      "Epoch 816 Batch    0/96   train_loss = 0.890\n",
      "Epoch 816 Batch   32/96   train_loss = 0.954\n",
      "Epoch 816 Batch   64/96   train_loss = 0.904\n",
      "Epoch 817 Batch    0/96   train_loss = 0.885\n",
      "Epoch 817 Batch   32/96   train_loss = 0.939\n",
      "Epoch 817 Batch   64/96   train_loss = 0.915\n",
      "Epoch 818 Batch    0/96   train_loss = 0.878\n",
      "Epoch 818 Batch   32/96   train_loss = 0.946\n",
      "Epoch 818 Batch   64/96   train_loss = 0.908\n",
      "Epoch 819 Batch    0/96   train_loss = 0.885\n",
      "Epoch 819 Batch   32/96   train_loss = 0.950\n",
      "Epoch 819 Batch   64/96   train_loss = 0.910\n",
      "Epoch 820 Batch    0/96   train_loss = 0.882\n",
      "Epoch 820 Batch   32/96   train_loss = 0.939\n",
      "Epoch 820 Batch   64/96   train_loss = 0.913\n",
      "Epoch 821 Batch    0/96   train_loss = 0.878\n",
      "Epoch 821 Batch   32/96   train_loss = 0.926\n",
      "Epoch 821 Batch   64/96   train_loss = 0.898\n",
      "Epoch 822 Batch    0/96   train_loss = 0.873\n",
      "Epoch 822 Batch   32/96   train_loss = 0.936\n",
      "Epoch 822 Batch   64/96   train_loss = 0.909\n",
      "Epoch 823 Batch    0/96   train_loss = 0.875\n",
      "Epoch 823 Batch   32/96   train_loss = 0.936\n",
      "Epoch 823 Batch   64/96   train_loss = 0.904\n",
      "Epoch 824 Batch    0/96   train_loss = 0.872\n",
      "Epoch 824 Batch   32/96   train_loss = 0.943\n",
      "Epoch 824 Batch   64/96   train_loss = 0.906\n",
      "Epoch 825 Batch    0/96   train_loss = 0.877\n",
      "Epoch 825 Batch   32/96   train_loss = 0.942\n",
      "Epoch 825 Batch   64/96   train_loss = 0.912\n",
      "Epoch 826 Batch    0/96   train_loss = 0.871\n",
      "Epoch 826 Batch   32/96   train_loss = 0.945\n",
      "Epoch 826 Batch   64/96   train_loss = 0.911\n",
      "Epoch 827 Batch    0/96   train_loss = 0.883\n",
      "Epoch 827 Batch   32/96   train_loss = 0.945\n",
      "Epoch 827 Batch   64/96   train_loss = 0.930\n",
      "Epoch 828 Batch    0/96   train_loss = 0.877\n",
      "Epoch 828 Batch   32/96   train_loss = 0.946\n",
      "Epoch 828 Batch   64/96   train_loss = 0.914\n",
      "Epoch 829 Batch    0/96   train_loss = 0.882\n",
      "Epoch 829 Batch   32/96   train_loss = 0.942\n",
      "Epoch 829 Batch   64/96   train_loss = 0.910\n",
      "Epoch 830 Batch    0/96   train_loss = 0.894\n",
      "Epoch 830 Batch   32/96   train_loss = 0.939\n",
      "Epoch 830 Batch   64/96   train_loss = 0.909\n",
      "Epoch 831 Batch    0/96   train_loss = 0.888\n",
      "Epoch 831 Batch   32/96   train_loss = 0.941\n",
      "Epoch 831 Batch   64/96   train_loss = 0.913\n",
      "Epoch 832 Batch    0/96   train_loss = 0.892\n",
      "Epoch 832 Batch   32/96   train_loss = 0.946\n",
      "Epoch 832 Batch   64/96   train_loss = 0.918\n",
      "Epoch 833 Batch    0/96   train_loss = 0.886\n",
      "Epoch 833 Batch   32/96   train_loss = 0.938\n",
      "Epoch 833 Batch   64/96   train_loss = 0.914\n",
      "Epoch 834 Batch    0/96   train_loss = 0.890\n",
      "Epoch 834 Batch   32/96   train_loss = 0.938\n",
      "Epoch 834 Batch   64/96   train_loss = 0.913\n",
      "Epoch 835 Batch    0/96   train_loss = 0.896\n",
      "Epoch 835 Batch   32/96   train_loss = 0.950\n",
      "Epoch 835 Batch   64/96   train_loss = 0.939\n",
      "Epoch 836 Batch    0/96   train_loss = 0.901\n",
      "Epoch 836 Batch   32/96   train_loss = 0.965\n",
      "Epoch 836 Batch   64/96   train_loss = 0.924\n",
      "Epoch 837 Batch    0/96   train_loss = 0.907\n",
      "Epoch 837 Batch   32/96   train_loss = 0.960\n",
      "Epoch 837 Batch   64/96   train_loss = 0.926\n",
      "Epoch 838 Batch    0/96   train_loss = 0.924\n",
      "Epoch 838 Batch   32/96   train_loss = 0.957\n",
      "Epoch 838 Batch   64/96   train_loss = 0.926\n",
      "Epoch 839 Batch    0/96   train_loss = 0.900\n",
      "Epoch 839 Batch   32/96   train_loss = 0.958\n",
      "Epoch 839 Batch   64/96   train_loss = 0.930\n",
      "Epoch 840 Batch    0/96   train_loss = 0.897\n",
      "Epoch 840 Batch   32/96   train_loss = 0.949\n",
      "Epoch 840 Batch   64/96   train_loss = 0.923\n",
      "Epoch 841 Batch    0/96   train_loss = 0.900\n",
      "Epoch 841 Batch   32/96   train_loss = 0.961\n",
      "Epoch 841 Batch   64/96   train_loss = 0.924\n",
      "Epoch 842 Batch    0/96   train_loss = 0.895\n",
      "Epoch 842 Batch   32/96   train_loss = 0.950\n",
      "Epoch 842 Batch   64/96   train_loss = 0.923\n",
      "Epoch 843 Batch    0/96   train_loss = 0.891\n",
      "Epoch 843 Batch   32/96   train_loss = 0.960\n",
      "Epoch 843 Batch   64/96   train_loss = 0.900\n",
      "Epoch 844 Batch    0/96   train_loss = 0.908\n",
      "Epoch 844 Batch   32/96   train_loss = 0.951\n",
      "Epoch 844 Batch   64/96   train_loss = 0.906\n",
      "Epoch 845 Batch    0/96   train_loss = 0.897\n",
      "Epoch 845 Batch   32/96   train_loss = 0.955\n",
      "Epoch 845 Batch   64/96   train_loss = 0.904\n",
      "Epoch 846 Batch    0/96   train_loss = 0.892\n",
      "Epoch 846 Batch   32/96   train_loss = 0.944\n",
      "Epoch 846 Batch   64/96   train_loss = 0.909\n",
      "Epoch 847 Batch    0/96   train_loss = 0.889\n",
      "Epoch 847 Batch   32/96   train_loss = 0.947\n",
      "Epoch 847 Batch   64/96   train_loss = 0.904\n",
      "Epoch 848 Batch    0/96   train_loss = 0.882\n",
      "Epoch 848 Batch   32/96   train_loss = 0.951\n",
      "Epoch 848 Batch   64/96   train_loss = 0.908\n",
      "Epoch 849 Batch    0/96   train_loss = 0.878\n",
      "Epoch 849 Batch   32/96   train_loss = 0.949\n",
      "Epoch 849 Batch   64/96   train_loss = 0.908\n",
      "Epoch 850 Batch    0/96   train_loss = 0.882\n",
      "Epoch 850 Batch   32/96   train_loss = 0.948\n",
      "Epoch 850 Batch   64/96   train_loss = 0.910\n",
      "Epoch 851 Batch    0/96   train_loss = 0.877\n",
      "Epoch 851 Batch   32/96   train_loss = 0.942\n",
      "Epoch 851 Batch   64/96   train_loss = 0.915\n",
      "Epoch 852 Batch    0/96   train_loss = 0.893\n",
      "Epoch 852 Batch   32/96   train_loss = 0.948\n",
      "Epoch 852 Batch   64/96   train_loss = 0.908\n",
      "Epoch 853 Batch    0/96   train_loss = 0.897\n",
      "Epoch 853 Batch   32/96   train_loss = 0.947\n",
      "Epoch 853 Batch   64/96   train_loss = 0.913\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 854 Batch    0/96   train_loss = 0.897\n",
      "Epoch 854 Batch   32/96   train_loss = 0.956\n",
      "Epoch 854 Batch   64/96   train_loss = 0.920\n",
      "Epoch 855 Batch    0/96   train_loss = 0.897\n",
      "Epoch 855 Batch   32/96   train_loss = 0.948\n",
      "Epoch 855 Batch   64/96   train_loss = 0.899\n",
      "Epoch 856 Batch    0/96   train_loss = 0.893\n",
      "Epoch 856 Batch   32/96   train_loss = 0.957\n",
      "Epoch 856 Batch   64/96   train_loss = 0.896\n",
      "Epoch 857 Batch    0/96   train_loss = 0.886\n",
      "Epoch 857 Batch   32/96   train_loss = 0.946\n",
      "Epoch 857 Batch   64/96   train_loss = 0.903\n",
      "Epoch 858 Batch    0/96   train_loss = 0.880\n",
      "Epoch 858 Batch   32/96   train_loss = 0.942\n",
      "Epoch 858 Batch   64/96   train_loss = 0.908\n",
      "Epoch 859 Batch    0/96   train_loss = 0.884\n",
      "Epoch 859 Batch   32/96   train_loss = 0.951\n",
      "Epoch 859 Batch   64/96   train_loss = 0.904\n",
      "Epoch 860 Batch    0/96   train_loss = 0.890\n",
      "Epoch 860 Batch   32/96   train_loss = 0.952\n",
      "Epoch 860 Batch   64/96   train_loss = 0.895\n",
      "Epoch 861 Batch    0/96   train_loss = 0.883\n",
      "Epoch 861 Batch   32/96   train_loss = 0.959\n",
      "Epoch 861 Batch   64/96   train_loss = 0.904\n",
      "Epoch 862 Batch    0/96   train_loss = 0.883\n",
      "Epoch 862 Batch   32/96   train_loss = 0.955\n",
      "Epoch 862 Batch   64/96   train_loss = 0.915\n",
      "Epoch 863 Batch    0/96   train_loss = 0.887\n",
      "Epoch 863 Batch   32/96   train_loss = 0.941\n",
      "Epoch 863 Batch   64/96   train_loss = 0.905\n",
      "Epoch 864 Batch    0/96   train_loss = 0.876\n",
      "Epoch 864 Batch   32/96   train_loss = 0.934\n",
      "Epoch 864 Batch   64/96   train_loss = 0.910\n",
      "Epoch 865 Batch    0/96   train_loss = 0.889\n",
      "Epoch 865 Batch   32/96   train_loss = 0.945\n",
      "Epoch 865 Batch   64/96   train_loss = 0.912\n",
      "Epoch 866 Batch    0/96   train_loss = 0.896\n",
      "Epoch 866 Batch   32/96   train_loss = 0.942\n",
      "Epoch 866 Batch   64/96   train_loss = 0.910\n",
      "Epoch 867 Batch    0/96   train_loss = 0.881\n",
      "Epoch 867 Batch   32/96   train_loss = 0.939\n",
      "Epoch 867 Batch   64/96   train_loss = 0.921\n",
      "Epoch 868 Batch    0/96   train_loss = 0.889\n",
      "Epoch 868 Batch   32/96   train_loss = 0.941\n",
      "Epoch 868 Batch   64/96   train_loss = 0.896\n",
      "Epoch 869 Batch    0/96   train_loss = 0.874\n",
      "Epoch 869 Batch   32/96   train_loss = 0.941\n",
      "Epoch 869 Batch   64/96   train_loss = 0.903\n",
      "Epoch 870 Batch    0/96   train_loss = 0.902\n",
      "Epoch 870 Batch   32/96   train_loss = 0.946\n",
      "Epoch 870 Batch   64/96   train_loss = 0.909\n",
      "Epoch 871 Batch    0/96   train_loss = 0.896\n",
      "Epoch 871 Batch   32/96   train_loss = 0.954\n",
      "Epoch 871 Batch   64/96   train_loss = 0.909\n",
      "Epoch 872 Batch    0/96   train_loss = 0.896\n",
      "Epoch 872 Batch   32/96   train_loss = 0.946\n",
      "Epoch 872 Batch   64/96   train_loss = 0.903\n",
      "Epoch 873 Batch    0/96   train_loss = 0.895\n",
      "Epoch 873 Batch   32/96   train_loss = 0.943\n",
      "Epoch 873 Batch   64/96   train_loss = 0.926\n",
      "Epoch 874 Batch    0/96   train_loss = 0.902\n",
      "Epoch 874 Batch   32/96   train_loss = 0.947\n",
      "Epoch 874 Batch   64/96   train_loss = 0.910\n",
      "Epoch 875 Batch    0/96   train_loss = 0.899\n",
      "Epoch 875 Batch   32/96   train_loss = 0.959\n",
      "Epoch 875 Batch   64/96   train_loss = 0.921\n",
      "Epoch 876 Batch    0/96   train_loss = 0.890\n",
      "Epoch 876 Batch   32/96   train_loss = 0.938\n",
      "Epoch 876 Batch   64/96   train_loss = 0.915\n",
      "Epoch 877 Batch    0/96   train_loss = 0.881\n",
      "Epoch 877 Batch   32/96   train_loss = 0.947\n",
      "Epoch 877 Batch   64/96   train_loss = 0.917\n",
      "Epoch 878 Batch    0/96   train_loss = 0.895\n",
      "Epoch 878 Batch   32/96   train_loss = 0.934\n",
      "Epoch 878 Batch   64/96   train_loss = 0.906\n",
      "Epoch 879 Batch    0/96   train_loss = 0.895\n",
      "Epoch 879 Batch   32/96   train_loss = 0.952\n",
      "Epoch 879 Batch   64/96   train_loss = 0.894\n",
      "Epoch 880 Batch    0/96   train_loss = 0.887\n",
      "Epoch 880 Batch   32/96   train_loss = 0.945\n",
      "Epoch 880 Batch   64/96   train_loss = 0.909\n",
      "Epoch 881 Batch    0/96   train_loss = 0.896\n",
      "Epoch 881 Batch   32/96   train_loss = 0.946\n",
      "Epoch 881 Batch   64/96   train_loss = 0.898\n",
      "Epoch 882 Batch    0/96   train_loss = 0.891\n",
      "Epoch 882 Batch   32/96   train_loss = 0.946\n",
      "Epoch 882 Batch   64/96   train_loss = 0.912\n",
      "Epoch 883 Batch    0/96   train_loss = 0.888\n",
      "Epoch 883 Batch   32/96   train_loss = 0.944\n",
      "Epoch 883 Batch   64/96   train_loss = 0.895\n",
      "Epoch 884 Batch    0/96   train_loss = 0.876\n",
      "Epoch 884 Batch   32/96   train_loss = 0.942\n",
      "Epoch 884 Batch   64/96   train_loss = 0.897\n",
      "Epoch 885 Batch    0/96   train_loss = 0.872\n",
      "Epoch 885 Batch   32/96   train_loss = 0.945\n",
      "Epoch 885 Batch   64/96   train_loss = 0.889\n",
      "Epoch 886 Batch    0/96   train_loss = 0.881\n",
      "Epoch 886 Batch   32/96   train_loss = 0.936\n",
      "Epoch 886 Batch   64/96   train_loss = 0.890\n",
      "Epoch 887 Batch    0/96   train_loss = 0.878\n",
      "Epoch 887 Batch   32/96   train_loss = 0.934\n",
      "Epoch 887 Batch   64/96   train_loss = 0.903\n",
      "Epoch 888 Batch    0/96   train_loss = 0.899\n",
      "Epoch 888 Batch   32/96   train_loss = 0.945\n",
      "Epoch 888 Batch   64/96   train_loss = 0.908\n",
      "Epoch 889 Batch    0/96   train_loss = 0.885\n",
      "Epoch 889 Batch   32/96   train_loss = 0.939\n",
      "Epoch 889 Batch   64/96   train_loss = 0.905\n",
      "Epoch 890 Batch    0/96   train_loss = 0.882\n",
      "Epoch 890 Batch   32/96   train_loss = 0.940\n",
      "Epoch 890 Batch   64/96   train_loss = 0.918\n",
      "Epoch 891 Batch    0/96   train_loss = 0.898\n",
      "Epoch 891 Batch   32/96   train_loss = 0.940\n",
      "Epoch 891 Batch   64/96   train_loss = 0.923\n",
      "Epoch 892 Batch    0/96   train_loss = 0.886\n",
      "Epoch 892 Batch   32/96   train_loss = 0.939\n",
      "Epoch 892 Batch   64/96   train_loss = 0.901\n",
      "Epoch 893 Batch    0/96   train_loss = 0.880\n",
      "Epoch 893 Batch   32/96   train_loss = 0.932\n",
      "Epoch 893 Batch   64/96   train_loss = 0.907\n",
      "Epoch 894 Batch    0/96   train_loss = 0.888\n",
      "Epoch 894 Batch   32/96   train_loss = 0.946\n",
      "Epoch 894 Batch   64/96   train_loss = 0.917\n",
      "Epoch 895 Batch    0/96   train_loss = 0.888\n",
      "Epoch 895 Batch   32/96   train_loss = 0.935\n",
      "Epoch 895 Batch   64/96   train_loss = 0.912\n",
      "Epoch 896 Batch    0/96   train_loss = 0.883\n",
      "Epoch 896 Batch   32/96   train_loss = 0.940\n",
      "Epoch 896 Batch   64/96   train_loss = 0.902\n",
      "Epoch 897 Batch    0/96   train_loss = 0.883\n",
      "Epoch 897 Batch   32/96   train_loss = 0.940\n",
      "Epoch 897 Batch   64/96   train_loss = 0.909\n",
      "Epoch 898 Batch    0/96   train_loss = 0.882\n",
      "Epoch 898 Batch   32/96   train_loss = 0.936\n",
      "Epoch 898 Batch   64/96   train_loss = 0.917\n",
      "Epoch 899 Batch    0/96   train_loss = 0.884\n",
      "Epoch 899 Batch   32/96   train_loss = 0.935\n",
      "Epoch 899 Batch   64/96   train_loss = 0.909\n",
      "Epoch 900 Batch    0/96   train_loss = 0.886\n",
      "Epoch 900 Batch   32/96   train_loss = 0.932\n",
      "Epoch 900 Batch   64/96   train_loss = 0.899\n",
      "Epoch 901 Batch    0/96   train_loss = 0.881\n",
      "Epoch 901 Batch   32/96   train_loss = 0.945\n",
      "Epoch 901 Batch   64/96   train_loss = 0.908\n",
      "Epoch 902 Batch    0/96   train_loss = 0.881\n",
      "Epoch 902 Batch   32/96   train_loss = 0.945\n",
      "Epoch 902 Batch   64/96   train_loss = 0.912\n",
      "Epoch 903 Batch    0/96   train_loss = 0.892\n",
      "Epoch 903 Batch   32/96   train_loss = 0.940\n",
      "Epoch 903 Batch   64/96   train_loss = 0.928\n",
      "Epoch 904 Batch    0/96   train_loss = 0.894\n",
      "Epoch 904 Batch   32/96   train_loss = 0.949\n",
      "Epoch 904 Batch   64/96   train_loss = 0.912\n",
      "Epoch 905 Batch    0/96   train_loss = 0.906\n",
      "Epoch 905 Batch   32/96   train_loss = 0.949\n",
      "Epoch 905 Batch   64/96   train_loss = 0.922\n",
      "Epoch 906 Batch    0/96   train_loss = 0.901\n",
      "Epoch 906 Batch   32/96   train_loss = 0.958\n",
      "Epoch 906 Batch   64/96   train_loss = 0.910\n",
      "Epoch 907 Batch    0/96   train_loss = 0.898\n",
      "Epoch 907 Batch   32/96   train_loss = 0.951\n",
      "Epoch 907 Batch   64/96   train_loss = 0.905\n",
      "Epoch 908 Batch    0/96   train_loss = 0.890\n",
      "Epoch 908 Batch   32/96   train_loss = 0.945\n",
      "Epoch 908 Batch   64/96   train_loss = 0.908\n",
      "Epoch 909 Batch    0/96   train_loss = 0.885\n",
      "Epoch 909 Batch   32/96   train_loss = 0.938\n",
      "Epoch 909 Batch   64/96   train_loss = 0.905\n",
      "Epoch 910 Batch    0/96   train_loss = 0.899\n",
      "Epoch 910 Batch   32/96   train_loss = 0.948\n",
      "Epoch 910 Batch   64/96   train_loss = 0.920\n",
      "Epoch 911 Batch    0/96   train_loss = 0.879\n",
      "Epoch 911 Batch   32/96   train_loss = 0.948\n",
      "Epoch 911 Batch   64/96   train_loss = 0.927\n",
      "Epoch 912 Batch    0/96   train_loss = 0.898\n",
      "Epoch 912 Batch   32/96   train_loss = 0.934\n",
      "Epoch 912 Batch   64/96   train_loss = 0.921\n",
      "Epoch 913 Batch    0/96   train_loss = 0.899\n",
      "Epoch 913 Batch   32/96   train_loss = 0.952\n",
      "Epoch 913 Batch   64/96   train_loss = 0.913\n",
      "Epoch 914 Batch    0/96   train_loss = 0.896\n",
      "Epoch 914 Batch   32/96   train_loss = 0.956\n",
      "Epoch 914 Batch   64/96   train_loss = 0.907\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 915 Batch    0/96   train_loss = 0.894\n",
      "Epoch 915 Batch   32/96   train_loss = 0.955\n",
      "Epoch 915 Batch   64/96   train_loss = 0.918\n",
      "Epoch 916 Batch    0/96   train_loss = 0.889\n",
      "Epoch 916 Batch   32/96   train_loss = 0.948\n",
      "Epoch 916 Batch   64/96   train_loss = 0.916\n",
      "Epoch 917 Batch    0/96   train_loss = 0.886\n",
      "Epoch 917 Batch   32/96   train_loss = 0.932\n",
      "Epoch 917 Batch   64/96   train_loss = 0.903\n",
      "Epoch 918 Batch    0/96   train_loss = 0.883\n",
      "Epoch 918 Batch   32/96   train_loss = 0.933\n",
      "Epoch 918 Batch   64/96   train_loss = 0.901\n",
      "Epoch 919 Batch    0/96   train_loss = 0.882\n",
      "Epoch 919 Batch   32/96   train_loss = 0.945\n",
      "Epoch 919 Batch   64/96   train_loss = 0.920\n",
      "Epoch 920 Batch    0/96   train_loss = 0.884\n",
      "Epoch 920 Batch   32/96   train_loss = 0.940\n",
      "Epoch 920 Batch   64/96   train_loss = 0.912\n",
      "Epoch 921 Batch    0/96   train_loss = 0.880\n",
      "Epoch 921 Batch   32/96   train_loss = 0.937\n",
      "Epoch 921 Batch   64/96   train_loss = 0.911\n",
      "Epoch 922 Batch    0/96   train_loss = 0.880\n",
      "Epoch 922 Batch   32/96   train_loss = 0.934\n",
      "Epoch 922 Batch   64/96   train_loss = 0.913\n",
      "Epoch 923 Batch    0/96   train_loss = 0.871\n",
      "Epoch 923 Batch   32/96   train_loss = 0.935\n",
      "Epoch 923 Batch   64/96   train_loss = 0.900\n",
      "Epoch 924 Batch    0/96   train_loss = 0.878\n",
      "Epoch 924 Batch   32/96   train_loss = 0.951\n",
      "Epoch 924 Batch   64/96   train_loss = 0.898\n",
      "Epoch 925 Batch    0/96   train_loss = 0.879\n",
      "Epoch 925 Batch   32/96   train_loss = 0.946\n",
      "Epoch 925 Batch   64/96   train_loss = 0.894\n",
      "Epoch 926 Batch    0/96   train_loss = 0.871\n",
      "Epoch 926 Batch   32/96   train_loss = 0.938\n",
      "Epoch 926 Batch   64/96   train_loss = 0.897\n",
      "Epoch 927 Batch    0/96   train_loss = 0.882\n",
      "Epoch 927 Batch   32/96   train_loss = 0.928\n",
      "Epoch 927 Batch   64/96   train_loss = 0.899\n",
      "Epoch 928 Batch    0/96   train_loss = 0.864\n",
      "Epoch 928 Batch   32/96   train_loss = 0.931\n",
      "Epoch 928 Batch   64/96   train_loss = 0.897\n",
      "Epoch 929 Batch    0/96   train_loss = 0.868\n",
      "Epoch 929 Batch   32/96   train_loss = 0.935\n",
      "Epoch 929 Batch   64/96   train_loss = 0.902\n",
      "Epoch 930 Batch    0/96   train_loss = 0.862\n",
      "Epoch 930 Batch   32/96   train_loss = 0.918\n",
      "Epoch 930 Batch   64/96   train_loss = 0.894\n",
      "Epoch 931 Batch    0/96   train_loss = 0.880\n",
      "Epoch 931 Batch   32/96   train_loss = 0.939\n",
      "Epoch 931 Batch   64/96   train_loss = 0.901\n",
      "Epoch 932 Batch    0/96   train_loss = 0.882\n",
      "Epoch 932 Batch   32/96   train_loss = 0.936\n",
      "Epoch 932 Batch   64/96   train_loss = 0.895\n",
      "Epoch 933 Batch    0/96   train_loss = 0.885\n",
      "Epoch 933 Batch   32/96   train_loss = 0.930\n",
      "Epoch 933 Batch   64/96   train_loss = 0.891\n",
      "Epoch 934 Batch    0/96   train_loss = 0.871\n",
      "Epoch 934 Batch   32/96   train_loss = 0.929\n",
      "Epoch 934 Batch   64/96   train_loss = 0.898\n",
      "Epoch 935 Batch    0/96   train_loss = 0.871\n",
      "Epoch 935 Batch   32/96   train_loss = 0.938\n",
      "Epoch 935 Batch   64/96   train_loss = 0.901\n",
      "Epoch 936 Batch    0/96   train_loss = 0.879\n",
      "Epoch 936 Batch   32/96   train_loss = 0.938\n",
      "Epoch 936 Batch   64/96   train_loss = 0.902\n",
      "Epoch 937 Batch    0/96   train_loss = 0.884\n",
      "Epoch 937 Batch   32/96   train_loss = 0.940\n",
      "Epoch 937 Batch   64/96   train_loss = 0.903\n",
      "Epoch 938 Batch    0/96   train_loss = 0.879\n",
      "Epoch 938 Batch   32/96   train_loss = 0.929\n",
      "Epoch 938 Batch   64/96   train_loss = 0.898\n",
      "Epoch 939 Batch    0/96   train_loss = 0.879\n",
      "Epoch 939 Batch   32/96   train_loss = 0.929\n",
      "Epoch 939 Batch   64/96   train_loss = 0.909\n",
      "Epoch 940 Batch    0/96   train_loss = 0.885\n",
      "Epoch 940 Batch   32/96   train_loss = 0.949\n",
      "Epoch 940 Batch   64/96   train_loss = 0.903\n",
      "Epoch 941 Batch    0/96   train_loss = 0.873\n",
      "Epoch 941 Batch   32/96   train_loss = 0.932\n",
      "Epoch 941 Batch   64/96   train_loss = 0.896\n",
      "Epoch 942 Batch    0/96   train_loss = 0.871\n",
      "Epoch 942 Batch   32/96   train_loss = 0.931\n",
      "Epoch 942 Batch   64/96   train_loss = 0.908\n",
      "Epoch 943 Batch    0/96   train_loss = 0.886\n",
      "Epoch 943 Batch   32/96   train_loss = 0.934\n",
      "Epoch 943 Batch   64/96   train_loss = 0.903\n",
      "Epoch 944 Batch    0/96   train_loss = 0.886\n",
      "Epoch 944 Batch   32/96   train_loss = 0.944\n",
      "Epoch 944 Batch   64/96   train_loss = 0.906\n",
      "Epoch 945 Batch    0/96   train_loss = 0.883\n",
      "Epoch 945 Batch   32/96   train_loss = 0.942\n",
      "Epoch 945 Batch   64/96   train_loss = 0.897\n",
      "Epoch 946 Batch    0/96   train_loss = 0.882\n",
      "Epoch 946 Batch   32/96   train_loss = 0.935\n",
      "Epoch 946 Batch   64/96   train_loss = 0.892\n",
      "Epoch 947 Batch    0/96   train_loss = 0.877\n",
      "Epoch 947 Batch   32/96   train_loss = 0.933\n",
      "Epoch 947 Batch   64/96   train_loss = 0.886\n",
      "Epoch 948 Batch    0/96   train_loss = 0.874\n",
      "Epoch 948 Batch   32/96   train_loss = 0.924\n",
      "Epoch 948 Batch   64/96   train_loss = 0.909\n",
      "Epoch 949 Batch    0/96   train_loss = 0.868\n",
      "Epoch 949 Batch   32/96   train_loss = 0.932\n",
      "Epoch 949 Batch   64/96   train_loss = 0.905\n",
      "Epoch 950 Batch    0/96   train_loss = 0.871\n",
      "Epoch 950 Batch   32/96   train_loss = 0.922\n",
      "Epoch 950 Batch   64/96   train_loss = 0.886\n",
      "Epoch 951 Batch    0/96   train_loss = 0.869\n",
      "Epoch 951 Batch   32/96   train_loss = 0.928\n",
      "Epoch 951 Batch   64/96   train_loss = 0.895\n",
      "Epoch 952 Batch    0/96   train_loss = 0.873\n",
      "Epoch 952 Batch   32/96   train_loss = 0.945\n",
      "Epoch 952 Batch   64/96   train_loss = 0.889\n",
      "Epoch 953 Batch    0/96   train_loss = 0.863\n",
      "Epoch 953 Batch   32/96   train_loss = 0.930\n",
      "Epoch 953 Batch   64/96   train_loss = 0.902\n",
      "Epoch 954 Batch    0/96   train_loss = 0.886\n",
      "Epoch 954 Batch   32/96   train_loss = 0.943\n",
      "Epoch 954 Batch   64/96   train_loss = 0.913\n",
      "Epoch 955 Batch    0/96   train_loss = 0.890\n",
      "Epoch 955 Batch   32/96   train_loss = 0.936\n",
      "Epoch 955 Batch   64/96   train_loss = 0.903\n",
      "Epoch 956 Batch    0/96   train_loss = 0.888\n",
      "Epoch 956 Batch   32/96   train_loss = 0.937\n",
      "Epoch 956 Batch   64/96   train_loss = 0.890\n",
      "Epoch 957 Batch    0/96   train_loss = 0.890\n",
      "Epoch 957 Batch   32/96   train_loss = 0.939\n",
      "Epoch 957 Batch   64/96   train_loss = 0.899\n",
      "Epoch 958 Batch    0/96   train_loss = 0.881\n",
      "Epoch 958 Batch   32/96   train_loss = 0.930\n",
      "Epoch 958 Batch   64/96   train_loss = 0.897\n",
      "Epoch 959 Batch    0/96   train_loss = 0.884\n",
      "Epoch 959 Batch   32/96   train_loss = 0.925\n",
      "Epoch 959 Batch   64/96   train_loss = 0.892\n",
      "Epoch 960 Batch    0/96   train_loss = 0.893\n",
      "Epoch 960 Batch   32/96   train_loss = 0.925\n",
      "Epoch 960 Batch   64/96   train_loss = 0.891\n",
      "Epoch 961 Batch    0/96   train_loss = 0.887\n",
      "Epoch 961 Batch   32/96   train_loss = 0.938\n",
      "Epoch 961 Batch   64/96   train_loss = 0.891\n",
      "Epoch 962 Batch    0/96   train_loss = 0.873\n",
      "Epoch 962 Batch   32/96   train_loss = 0.934\n",
      "Epoch 962 Batch   64/96   train_loss = 0.903\n",
      "Epoch 963 Batch    0/96   train_loss = 0.890\n",
      "Epoch 963 Batch   32/96   train_loss = 0.925\n",
      "Epoch 963 Batch   64/96   train_loss = 0.903\n",
      "Epoch 964 Batch    0/96   train_loss = 0.905\n",
      "Epoch 964 Batch   32/96   train_loss = 0.954\n",
      "Epoch 964 Batch   64/96   train_loss = 0.900\n",
      "Epoch 965 Batch    0/96   train_loss = 0.893\n",
      "Epoch 965 Batch   32/96   train_loss = 0.944\n",
      "Epoch 965 Batch   64/96   train_loss = 0.903\n",
      "Epoch 966 Batch    0/96   train_loss = 0.891\n",
      "Epoch 966 Batch   32/96   train_loss = 0.942\n",
      "Epoch 966 Batch   64/96   train_loss = 0.909\n",
      "Epoch 967 Batch    0/96   train_loss = 0.881\n",
      "Epoch 967 Batch   32/96   train_loss = 0.932\n",
      "Epoch 967 Batch   64/96   train_loss = 0.899\n",
      "Epoch 968 Batch    0/96   train_loss = 0.875\n",
      "Epoch 968 Batch   32/96   train_loss = 0.937\n",
      "Epoch 968 Batch   64/96   train_loss = 0.894\n",
      "Epoch 969 Batch    0/96   train_loss = 0.883\n",
      "Epoch 969 Batch   32/96   train_loss = 0.925\n",
      "Epoch 969 Batch   64/96   train_loss = 0.905\n",
      "Epoch 970 Batch    0/96   train_loss = 0.879\n",
      "Epoch 970 Batch   32/96   train_loss = 0.931\n",
      "Epoch 970 Batch   64/96   train_loss = 0.904\n",
      "Epoch 971 Batch    0/96   train_loss = 0.880\n",
      "Epoch 971 Batch   32/96   train_loss = 0.946\n",
      "Epoch 971 Batch   64/96   train_loss = 0.903\n",
      "Epoch 972 Batch    0/96   train_loss = 0.883\n",
      "Epoch 972 Batch   32/96   train_loss = 0.932\n",
      "Epoch 972 Batch   64/96   train_loss = 0.887\n",
      "Epoch 973 Batch    0/96   train_loss = 0.880\n",
      "Epoch 973 Batch   32/96   train_loss = 0.929\n",
      "Epoch 973 Batch   64/96   train_loss = 0.890\n",
      "Epoch 974 Batch    0/96   train_loss = 0.883\n",
      "Epoch 974 Batch   32/96   train_loss = 0.930\n",
      "Epoch 974 Batch   64/96   train_loss = 0.900\n",
      "Epoch 975 Batch    0/96   train_loss = 0.877\n",
      "Epoch 975 Batch   32/96   train_loss = 0.933\n",
      "Epoch 975 Batch   64/96   train_loss = 0.887\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 976 Batch    0/96   train_loss = 0.879\n",
      "Epoch 976 Batch   32/96   train_loss = 0.935\n",
      "Epoch 976 Batch   64/96   train_loss = 0.899\n",
      "Epoch 977 Batch    0/96   train_loss = 0.884\n",
      "Epoch 977 Batch   32/96   train_loss = 0.938\n",
      "Epoch 977 Batch   64/96   train_loss = 0.911\n",
      "Epoch 978 Batch    0/96   train_loss = 0.888\n",
      "Epoch 978 Batch   32/96   train_loss = 0.946\n",
      "Epoch 978 Batch   64/96   train_loss = 0.898\n",
      "Epoch 979 Batch    0/96   train_loss = 0.882\n",
      "Epoch 979 Batch   32/96   train_loss = 0.940\n",
      "Epoch 979 Batch   64/96   train_loss = 0.899\n",
      "Epoch 980 Batch    0/96   train_loss = 0.890\n",
      "Epoch 980 Batch   32/96   train_loss = 0.953\n",
      "Epoch 980 Batch   64/96   train_loss = 0.907\n",
      "Epoch 981 Batch    0/96   train_loss = 0.893\n",
      "Epoch 981 Batch   32/96   train_loss = 0.961\n",
      "Epoch 981 Batch   64/96   train_loss = 0.914\n",
      "Epoch 982 Batch    0/96   train_loss = 0.894\n",
      "Epoch 982 Batch   32/96   train_loss = 0.948\n",
      "Epoch 982 Batch   64/96   train_loss = 0.910\n",
      "Epoch 983 Batch    0/96   train_loss = 0.887\n",
      "Epoch 983 Batch   32/96   train_loss = 0.950\n",
      "Epoch 983 Batch   64/96   train_loss = 0.911\n",
      "Epoch 984 Batch    0/96   train_loss = 0.888\n",
      "Epoch 984 Batch   32/96   train_loss = 0.948\n",
      "Epoch 984 Batch   64/96   train_loss = 0.903\n",
      "Epoch 985 Batch    0/96   train_loss = 0.885\n",
      "Epoch 985 Batch   32/96   train_loss = 0.938\n",
      "Epoch 985 Batch   64/96   train_loss = 0.899\n",
      "Epoch 986 Batch    0/96   train_loss = 0.882\n",
      "Epoch 986 Batch   32/96   train_loss = 0.950\n",
      "Epoch 986 Batch   64/96   train_loss = 0.901\n",
      "Epoch 987 Batch    0/96   train_loss = 0.890\n",
      "Epoch 987 Batch   32/96   train_loss = 0.956\n",
      "Epoch 987 Batch   64/96   train_loss = 0.918\n",
      "Epoch 988 Batch    0/96   train_loss = 0.897\n",
      "Epoch 988 Batch   32/96   train_loss = 0.951\n",
      "Epoch 988 Batch   64/96   train_loss = 0.907\n",
      "Epoch 989 Batch    0/96   train_loss = 0.891\n",
      "Epoch 989 Batch   32/96   train_loss = 0.949\n",
      "Epoch 989 Batch   64/96   train_loss = 0.899\n",
      "Epoch 990 Batch    0/96   train_loss = 0.891\n",
      "Epoch 990 Batch   32/96   train_loss = 0.958\n",
      "Epoch 990 Batch   64/96   train_loss = 0.899\n",
      "Epoch 991 Batch    0/96   train_loss = 0.900\n",
      "Epoch 991 Batch   32/96   train_loss = 0.965\n",
      "Epoch 991 Batch   64/96   train_loss = 0.893\n",
      "Epoch 992 Batch    0/96   train_loss = 0.888\n",
      "Epoch 992 Batch   32/96   train_loss = 0.955\n",
      "Epoch 992 Batch   64/96   train_loss = 0.913\n",
      "Epoch 993 Batch    0/96   train_loss = 0.887\n",
      "Epoch 993 Batch   32/96   train_loss = 0.939\n",
      "Epoch 993 Batch   64/96   train_loss = 0.905\n",
      "Epoch 994 Batch    0/96   train_loss = 0.895\n",
      "Epoch 994 Batch   32/96   train_loss = 0.943\n",
      "Epoch 994 Batch   64/96   train_loss = 0.903\n",
      "Epoch 995 Batch    0/96   train_loss = 0.887\n",
      "Epoch 995 Batch   32/96   train_loss = 0.936\n",
      "Epoch 995 Batch   64/96   train_loss = 0.910\n",
      "Epoch 996 Batch    0/96   train_loss = 0.878\n",
      "Epoch 996 Batch   32/96   train_loss = 0.948\n",
      "Epoch 996 Batch   64/96   train_loss = 0.916\n",
      "Epoch 997 Batch    0/96   train_loss = 0.877\n",
      "Epoch 997 Batch   32/96   train_loss = 0.933\n",
      "Epoch 997 Batch   64/96   train_loss = 0.897\n",
      "Epoch 998 Batch    0/96   train_loss = 0.869\n",
      "Epoch 998 Batch   32/96   train_loss = 0.935\n",
      "Epoch 998 Batch   64/96   train_loss = 0.897\n",
      "Epoch 999 Batch    0/96   train_loss = 0.866\n",
      "Epoch 999 Batch   32/96   train_loss = 0.942\n",
      "Epoch 999 Batch   64/96   train_loss = 0.896\n",
      "Epoch 1000 Batch    0/96   train_loss = 0.871\n",
      "Epoch 1000 Batch   32/96   train_loss = 0.947\n",
      "Epoch 1000 Batch   64/96   train_loss = 0.895\n",
      "Epoch 1001 Batch    0/96   train_loss = 0.880\n",
      "Epoch 1001 Batch   32/96   train_loss = 0.943\n",
      "Epoch 1001 Batch   64/96   train_loss = 0.897\n",
      "Epoch 1002 Batch    0/96   train_loss = 0.876\n",
      "Epoch 1002 Batch   32/96   train_loss = 0.949\n",
      "Epoch 1002 Batch   64/96   train_loss = 0.909\n",
      "Epoch 1003 Batch    0/96   train_loss = 0.874\n",
      "Epoch 1003 Batch   32/96   train_loss = 0.947\n",
      "Epoch 1003 Batch   64/96   train_loss = 0.916\n",
      "Epoch 1004 Batch    0/96   train_loss = 0.887\n",
      "Epoch 1004 Batch   32/96   train_loss = 0.955\n",
      "Epoch 1004 Batch   64/96   train_loss = 0.905\n",
      "Epoch 1005 Batch    0/96   train_loss = 0.878\n",
      "Epoch 1005 Batch   32/96   train_loss = 0.942\n",
      "Epoch 1005 Batch   64/96   train_loss = 0.899\n",
      "Epoch 1006 Batch    0/96   train_loss = 0.869\n",
      "Epoch 1006 Batch   32/96   train_loss = 0.952\n",
      "Epoch 1006 Batch   64/96   train_loss = 0.904\n",
      "Epoch 1007 Batch    0/96   train_loss = 0.893\n",
      "Epoch 1007 Batch   32/96   train_loss = 0.938\n",
      "Epoch 1007 Batch   64/96   train_loss = 0.908\n",
      "Epoch 1008 Batch    0/96   train_loss = 0.876\n",
      "Epoch 1008 Batch   32/96   train_loss = 0.934\n",
      "Epoch 1008 Batch   64/96   train_loss = 0.906\n",
      "Epoch 1009 Batch    0/96   train_loss = 0.870\n",
      "Epoch 1009 Batch   32/96   train_loss = 0.942\n",
      "Epoch 1009 Batch   64/96   train_loss = 0.902\n",
      "Epoch 1010 Batch    0/96   train_loss = 0.869\n",
      "Epoch 1010 Batch   32/96   train_loss = 0.940\n",
      "Epoch 1010 Batch   64/96   train_loss = 0.905\n",
      "Epoch 1011 Batch    0/96   train_loss = 0.879\n",
      "Epoch 1011 Batch   32/96   train_loss = 0.951\n",
      "Epoch 1011 Batch   64/96   train_loss = 0.905\n",
      "Epoch 1012 Batch    0/96   train_loss = 0.866\n",
      "Epoch 1012 Batch   32/96   train_loss = 0.940\n",
      "Epoch 1012 Batch   64/96   train_loss = 0.907\n",
      "Epoch 1013 Batch    0/96   train_loss = 0.875\n",
      "Epoch 1013 Batch   32/96   train_loss = 0.951\n",
      "Epoch 1013 Batch   64/96   train_loss = 0.902\n",
      "Epoch 1014 Batch    0/96   train_loss = 0.867\n",
      "Epoch 1014 Batch   32/96   train_loss = 0.961\n",
      "Epoch 1014 Batch   64/96   train_loss = 0.911\n",
      "Epoch 1015 Batch    0/96   train_loss = 0.885\n",
      "Epoch 1015 Batch   32/96   train_loss = 0.953\n",
      "Epoch 1015 Batch   64/96   train_loss = 0.912\n",
      "Epoch 1016 Batch    0/96   train_loss = 0.879\n",
      "Epoch 1016 Batch   32/96   train_loss = 0.937\n",
      "Epoch 1016 Batch   64/96   train_loss = 0.913\n",
      "Epoch 1017 Batch    0/96   train_loss = 0.875\n",
      "Epoch 1017 Batch   32/96   train_loss = 0.949\n",
      "Epoch 1017 Batch   64/96   train_loss = 0.910\n",
      "Epoch 1018 Batch    0/96   train_loss = 0.874\n",
      "Epoch 1018 Batch   32/96   train_loss = 0.945\n",
      "Epoch 1018 Batch   64/96   train_loss = 0.900\n",
      "Epoch 1019 Batch    0/96   train_loss = 0.875\n",
      "Epoch 1019 Batch   32/96   train_loss = 0.934\n",
      "Epoch 1019 Batch   64/96   train_loss = 0.902\n",
      "Epoch 1020 Batch    0/96   train_loss = 0.885\n",
      "Epoch 1020 Batch   32/96   train_loss = 0.952\n",
      "Epoch 1020 Batch   64/96   train_loss = 0.915\n",
      "Epoch 1021 Batch    0/96   train_loss = 0.881\n",
      "Epoch 1021 Batch   32/96   train_loss = 0.954\n",
      "Epoch 1021 Batch   64/96   train_loss = 0.909\n",
      "Epoch 1022 Batch    0/96   train_loss = 0.895\n",
      "Epoch 1022 Batch   32/96   train_loss = 0.950\n",
      "Epoch 1022 Batch   64/96   train_loss = 0.920\n",
      "Epoch 1023 Batch    0/96   train_loss = 0.886\n",
      "Epoch 1023 Batch   32/96   train_loss = 0.964\n",
      "Epoch 1023 Batch   64/96   train_loss = 0.907\n",
      "Epoch 1024 Batch    0/96   train_loss = 0.880\n",
      "Epoch 1024 Batch   32/96   train_loss = 0.958\n",
      "Epoch 1024 Batch   64/96   train_loss = 0.914\n",
      "Epoch 1025 Batch    0/96   train_loss = 0.874\n",
      "Epoch 1025 Batch   32/96   train_loss = 0.951\n",
      "Epoch 1025 Batch   64/96   train_loss = 0.893\n",
      "Epoch 1026 Batch    0/96   train_loss = 0.870\n",
      "Epoch 1026 Batch   32/96   train_loss = 0.948\n",
      "Epoch 1026 Batch   64/96   train_loss = 0.898\n",
      "Epoch 1027 Batch    0/96   train_loss = 0.882\n",
      "Epoch 1027 Batch   32/96   train_loss = 0.952\n",
      "Epoch 1027 Batch   64/96   train_loss = 0.911\n",
      "Epoch 1028 Batch    0/96   train_loss = 0.887\n",
      "Epoch 1028 Batch   32/96   train_loss = 0.944\n",
      "Epoch 1028 Batch   64/96   train_loss = 0.910\n",
      "Epoch 1029 Batch    0/96   train_loss = 0.879\n",
      "Epoch 1029 Batch   32/96   train_loss = 0.944\n",
      "Epoch 1029 Batch   64/96   train_loss = 0.895\n",
      "Epoch 1030 Batch    0/96   train_loss = 0.877\n",
      "Epoch 1030 Batch   32/96   train_loss = 0.942\n",
      "Epoch 1030 Batch   64/96   train_loss = 0.903\n",
      "Epoch 1031 Batch    0/96   train_loss = 0.877\n",
      "Epoch 1031 Batch   32/96   train_loss = 0.946\n",
      "Epoch 1031 Batch   64/96   train_loss = 0.906\n",
      "Epoch 1032 Batch    0/96   train_loss = 0.872\n",
      "Epoch 1032 Batch   32/96   train_loss = 0.936\n",
      "Epoch 1032 Batch   64/96   train_loss = 0.901\n",
      "Epoch 1033 Batch    0/96   train_loss = 0.865\n",
      "Epoch 1033 Batch   32/96   train_loss = 0.944\n",
      "Epoch 1033 Batch   64/96   train_loss = 0.909\n",
      "Epoch 1034 Batch    0/96   train_loss = 0.875\n",
      "Epoch 1034 Batch   32/96   train_loss = 0.942\n",
      "Epoch 1034 Batch   64/96   train_loss = 0.914\n",
      "Epoch 1035 Batch    0/96   train_loss = 0.866\n",
      "Epoch 1035 Batch   32/96   train_loss = 0.947\n",
      "Epoch 1035 Batch   64/96   train_loss = 0.906\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1036 Batch    0/96   train_loss = 0.869\n",
      "Epoch 1036 Batch   32/96   train_loss = 0.950\n",
      "Epoch 1036 Batch   64/96   train_loss = 0.911\n",
      "Epoch 1037 Batch    0/96   train_loss = 0.876\n",
      "Epoch 1037 Batch   32/96   train_loss = 0.962\n",
      "Epoch 1037 Batch   64/96   train_loss = 0.903\n",
      "Epoch 1038 Batch    0/96   train_loss = 0.873\n",
      "Epoch 1038 Batch   32/96   train_loss = 0.940\n",
      "Epoch 1038 Batch   64/96   train_loss = 0.891\n",
      "Epoch 1039 Batch    0/96   train_loss = 0.863\n",
      "Epoch 1039 Batch   32/96   train_loss = 0.931\n",
      "Epoch 1039 Batch   64/96   train_loss = 0.909\n",
      "Epoch 1040 Batch    0/96   train_loss = 0.870\n",
      "Epoch 1040 Batch   32/96   train_loss = 0.920\n",
      "Epoch 1040 Batch   64/96   train_loss = 0.896\n",
      "Epoch 1041 Batch    0/96   train_loss = 0.870\n",
      "Epoch 1041 Batch   32/96   train_loss = 0.917\n",
      "Epoch 1041 Batch   64/96   train_loss = 0.908\n",
      "Epoch 1042 Batch    0/96   train_loss = 0.866\n",
      "Epoch 1042 Batch   32/96   train_loss = 0.921\n",
      "Epoch 1042 Batch   64/96   train_loss = 0.916\n",
      "Epoch 1043 Batch    0/96   train_loss = 0.860\n",
      "Epoch 1043 Batch   32/96   train_loss = 0.924\n",
      "Epoch 1043 Batch   64/96   train_loss = 0.918\n",
      "Epoch 1044 Batch    0/96   train_loss = 0.855\n",
      "Epoch 1044 Batch   32/96   train_loss = 0.929\n",
      "Epoch 1044 Batch   64/96   train_loss = 0.900\n",
      "Epoch 1045 Batch    0/96   train_loss = 0.866\n",
      "Epoch 1045 Batch   32/96   train_loss = 0.944\n",
      "Epoch 1045 Batch   64/96   train_loss = 0.917\n",
      "Epoch 1046 Batch    0/96   train_loss = 0.866\n",
      "Epoch 1046 Batch   32/96   train_loss = 0.948\n",
      "Epoch 1046 Batch   64/96   train_loss = 0.908\n",
      "Epoch 1047 Batch    0/96   train_loss = 0.881\n",
      "Epoch 1047 Batch   32/96   train_loss = 0.935\n",
      "Epoch 1047 Batch   64/96   train_loss = 0.916\n",
      "Epoch 1048 Batch    0/96   train_loss = 0.873\n",
      "Epoch 1048 Batch   32/96   train_loss = 0.937\n",
      "Epoch 1048 Batch   64/96   train_loss = 0.906\n",
      "Epoch 1049 Batch    0/96   train_loss = 0.877\n",
      "Epoch 1049 Batch   32/96   train_loss = 0.929\n",
      "Epoch 1049 Batch   64/96   train_loss = 0.906\n",
      "Epoch 1050 Batch    0/96   train_loss = 0.878\n",
      "Epoch 1050 Batch   32/96   train_loss = 0.943\n",
      "Epoch 1050 Batch   64/96   train_loss = 0.904\n",
      "Epoch 1051 Batch    0/96   train_loss = 0.874\n",
      "Epoch 1051 Batch   32/96   train_loss = 0.942\n",
      "Epoch 1051 Batch   64/96   train_loss = 0.912\n",
      "Epoch 1052 Batch    0/96   train_loss = 0.885\n",
      "Epoch 1052 Batch   32/96   train_loss = 0.927\n",
      "Epoch 1052 Batch   64/96   train_loss = 0.909\n",
      "Epoch 1053 Batch    0/96   train_loss = 0.875\n",
      "Epoch 1053 Batch   32/96   train_loss = 0.942\n",
      "Epoch 1053 Batch   64/96   train_loss = 0.902\n",
      "Epoch 1054 Batch    0/96   train_loss = 0.869\n",
      "Epoch 1054 Batch   32/96   train_loss = 0.930\n",
      "Epoch 1054 Batch   64/96   train_loss = 0.905\n",
      "Epoch 1055 Batch    0/96   train_loss = 0.874\n",
      "Epoch 1055 Batch   32/96   train_loss = 0.937\n",
      "Epoch 1055 Batch   64/96   train_loss = 0.899\n",
      "Epoch 1056 Batch    0/96   train_loss = 0.859\n",
      "Epoch 1056 Batch   32/96   train_loss = 0.946\n",
      "Epoch 1056 Batch   64/96   train_loss = 0.894\n",
      "Epoch 1057 Batch    0/96   train_loss = 0.865\n",
      "Epoch 1057 Batch   32/96   train_loss = 0.932\n",
      "Epoch 1057 Batch   64/96   train_loss = 0.898\n",
      "Epoch 1058 Batch    0/96   train_loss = 0.863\n",
      "Epoch 1058 Batch   32/96   train_loss = 0.923\n",
      "Epoch 1058 Batch   64/96   train_loss = 0.893\n",
      "Epoch 1059 Batch    0/96   train_loss = 0.861\n",
      "Epoch 1059 Batch   32/96   train_loss = 0.929\n",
      "Epoch 1059 Batch   64/96   train_loss = 0.910\n",
      "Epoch 1060 Batch    0/96   train_loss = 0.865\n",
      "Epoch 1060 Batch   32/96   train_loss = 0.930\n",
      "Epoch 1060 Batch   64/96   train_loss = 0.905\n",
      "Epoch 1061 Batch    0/96   train_loss = 0.851\n",
      "Epoch 1061 Batch   32/96   train_loss = 0.945\n",
      "Epoch 1061 Batch   64/96   train_loss = 0.896\n",
      "Epoch 1062 Batch    0/96   train_loss = 0.859\n",
      "Epoch 1062 Batch   32/96   train_loss = 0.939\n",
      "Epoch 1062 Batch   64/96   train_loss = 0.888\n",
      "Epoch 1063 Batch    0/96   train_loss = 0.864\n",
      "Epoch 1063 Batch   32/96   train_loss = 0.934\n",
      "Epoch 1063 Batch   64/96   train_loss = 0.889\n",
      "Epoch 1064 Batch    0/96   train_loss = 0.854\n",
      "Epoch 1064 Batch   32/96   train_loss = 0.934\n",
      "Epoch 1064 Batch   64/96   train_loss = 0.896\n",
      "Epoch 1065 Batch    0/96   train_loss = 0.863\n",
      "Epoch 1065 Batch   32/96   train_loss = 0.938\n",
      "Epoch 1065 Batch   64/96   train_loss = 0.901\n",
      "Epoch 1066 Batch    0/96   train_loss = 0.866\n",
      "Epoch 1066 Batch   32/96   train_loss = 0.928\n",
      "Epoch 1066 Batch   64/96   train_loss = 0.900\n",
      "Epoch 1067 Batch    0/96   train_loss = 0.883\n",
      "Epoch 1067 Batch   32/96   train_loss = 0.935\n",
      "Epoch 1067 Batch   64/96   train_loss = 0.900\n",
      "Epoch 1068 Batch    0/96   train_loss = 0.876\n",
      "Epoch 1068 Batch   32/96   train_loss = 0.954\n",
      "Epoch 1068 Batch   64/96   train_loss = 0.914\n",
      "Epoch 1069 Batch    0/96   train_loss = 0.863\n",
      "Epoch 1069 Batch   32/96   train_loss = 0.930\n",
      "Epoch 1069 Batch   64/96   train_loss = 0.916\n",
      "Epoch 1070 Batch    0/96   train_loss = 0.861\n",
      "Epoch 1070 Batch   32/96   train_loss = 0.933\n",
      "Epoch 1070 Batch   64/96   train_loss = 0.935\n",
      "Epoch 1071 Batch    0/96   train_loss = 0.866\n",
      "Epoch 1071 Batch   32/96   train_loss = 0.941\n",
      "Epoch 1071 Batch   64/96   train_loss = 0.905\n",
      "Epoch 1072 Batch    0/96   train_loss = 0.868\n",
      "Epoch 1072 Batch   32/96   train_loss = 0.943\n",
      "Epoch 1072 Batch   64/96   train_loss = 0.913\n",
      "Epoch 1073 Batch    0/96   train_loss = 0.865\n",
      "Epoch 1073 Batch   32/96   train_loss = 0.948\n",
      "Epoch 1073 Batch   64/96   train_loss = 0.915\n",
      "Epoch 1074 Batch    0/96   train_loss = 0.861\n",
      "Epoch 1074 Batch   32/96   train_loss = 0.937\n",
      "Epoch 1074 Batch   64/96   train_loss = 0.912\n",
      "Epoch 1075 Batch    0/96   train_loss = 0.872\n",
      "Epoch 1075 Batch   32/96   train_loss = 0.943\n",
      "Epoch 1075 Batch   64/96   train_loss = 0.913\n",
      "Epoch 1076 Batch    0/96   train_loss = 0.870\n",
      "Epoch 1076 Batch   32/96   train_loss = 0.931\n",
      "Epoch 1076 Batch   64/96   train_loss = 0.917\n",
      "Epoch 1077 Batch    0/96   train_loss = 0.867\n",
      "Epoch 1077 Batch   32/96   train_loss = 0.934\n",
      "Epoch 1077 Batch   64/96   train_loss = 0.912\n",
      "Epoch 1078 Batch    0/96   train_loss = 0.870\n",
      "Epoch 1078 Batch   32/96   train_loss = 0.950\n",
      "Epoch 1078 Batch   64/96   train_loss = 0.902\n",
      "Epoch 1079 Batch    0/96   train_loss = 0.860\n",
      "Epoch 1079 Batch   32/96   train_loss = 0.926\n",
      "Epoch 1079 Batch   64/96   train_loss = 0.907\n",
      "Epoch 1080 Batch    0/96   train_loss = 0.870\n",
      "Epoch 1080 Batch   32/96   train_loss = 0.940\n",
      "Epoch 1080 Batch   64/96   train_loss = 0.914\n",
      "Epoch 1081 Batch    0/96   train_loss = 0.880\n",
      "Epoch 1081 Batch   32/96   train_loss = 0.938\n",
      "Epoch 1081 Batch   64/96   train_loss = 0.908\n",
      "Epoch 1082 Batch    0/96   train_loss = 0.884\n",
      "Epoch 1082 Batch   32/96   train_loss = 0.942\n",
      "Epoch 1082 Batch   64/96   train_loss = 0.917\n",
      "Epoch 1083 Batch    0/96   train_loss = 0.882\n",
      "Epoch 1083 Batch   32/96   train_loss = 0.931\n",
      "Epoch 1083 Batch   64/96   train_loss = 0.916\n",
      "Epoch 1084 Batch    0/96   train_loss = 0.879\n",
      "Epoch 1084 Batch   32/96   train_loss = 0.941\n",
      "Epoch 1084 Batch   64/96   train_loss = 0.912\n",
      "Epoch 1085 Batch    0/96   train_loss = 0.877\n",
      "Epoch 1085 Batch   32/96   train_loss = 0.942\n",
      "Epoch 1085 Batch   64/96   train_loss = 0.893\n",
      "Epoch 1086 Batch    0/96   train_loss = 0.869\n",
      "Epoch 1086 Batch   32/96   train_loss = 0.930\n",
      "Epoch 1086 Batch   64/96   train_loss = 0.905\n",
      "Epoch 1087 Batch    0/96   train_loss = 0.864\n",
      "Epoch 1087 Batch   32/96   train_loss = 0.951\n",
      "Epoch 1087 Batch   64/96   train_loss = 0.896\n",
      "Epoch 1088 Batch    0/96   train_loss = 0.852\n",
      "Epoch 1088 Batch   32/96   train_loss = 0.942\n",
      "Epoch 1088 Batch   64/96   train_loss = 0.904\n",
      "Epoch 1089 Batch    0/96   train_loss = 0.861\n",
      "Epoch 1089 Batch   32/96   train_loss = 0.941\n",
      "Epoch 1089 Batch   64/96   train_loss = 0.899\n",
      "Epoch 1090 Batch    0/96   train_loss = 0.861\n",
      "Epoch 1090 Batch   32/96   train_loss = 0.945\n",
      "Epoch 1090 Batch   64/96   train_loss = 0.969\n",
      "Epoch 1091 Batch    0/96   train_loss = 0.845\n",
      "Epoch 1091 Batch   32/96   train_loss = 0.939\n",
      "Epoch 1091 Batch   64/96   train_loss = 0.921\n",
      "Epoch 1092 Batch    0/96   train_loss = 0.858\n",
      "Epoch 1092 Batch   32/96   train_loss = 0.952\n",
      "Epoch 1092 Batch   64/96   train_loss = 0.917\n",
      "Epoch 1093 Batch    0/96   train_loss = 0.864\n",
      "Epoch 1093 Batch   32/96   train_loss = 0.945\n",
      "Epoch 1093 Batch   64/96   train_loss = 0.913\n",
      "Epoch 1094 Batch    0/96   train_loss = 0.852\n",
      "Epoch 1094 Batch   32/96   train_loss = 0.942\n",
      "Epoch 1094 Batch   64/96   train_loss = 0.899\n",
      "Epoch 1095 Batch    0/96   train_loss = 0.865\n",
      "Epoch 1095 Batch   32/96   train_loss = 0.941\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1095 Batch   64/96   train_loss = 0.907\n",
      "Epoch 1096 Batch    0/96   train_loss = 0.863\n",
      "Epoch 1096 Batch   32/96   train_loss = 0.935\n",
      "Epoch 1096 Batch   64/96   train_loss = 0.908\n",
      "Epoch 1097 Batch    0/96   train_loss = 0.878\n",
      "Epoch 1097 Batch   32/96   train_loss = 0.954\n",
      "Epoch 1097 Batch   64/96   train_loss = 0.898\n",
      "Epoch 1098 Batch    0/96   train_loss = 0.866\n",
      "Epoch 1098 Batch   32/96   train_loss = 0.935\n",
      "Epoch 1098 Batch   64/96   train_loss = 0.917\n",
      "Epoch 1099 Batch    0/96   train_loss = 0.884\n",
      "Epoch 1099 Batch   32/96   train_loss = 0.948\n",
      "Epoch 1099 Batch   64/96   train_loss = 0.902\n",
      "Epoch 1100 Batch    0/96   train_loss = 0.876\n",
      "Epoch 1100 Batch   32/96   train_loss = 0.938\n",
      "Epoch 1100 Batch   64/96   train_loss = 0.905\n",
      "Epoch 1101 Batch    0/96   train_loss = 0.872\n",
      "Epoch 1101 Batch   32/96   train_loss = 0.941\n",
      "Epoch 1101 Batch   64/96   train_loss = 0.914\n",
      "Epoch 1102 Batch    0/96   train_loss = 0.875\n",
      "Epoch 1102 Batch   32/96   train_loss = 0.940\n",
      "Epoch 1102 Batch   64/96   train_loss = 0.897\n",
      "Epoch 1103 Batch    0/96   train_loss = 0.856\n",
      "Epoch 1103 Batch   32/96   train_loss = 0.944\n",
      "Epoch 1103 Batch   64/96   train_loss = 0.895\n",
      "Epoch 1104 Batch    0/96   train_loss = 0.874\n",
      "Epoch 1104 Batch   32/96   train_loss = 0.935\n",
      "Epoch 1104 Batch   64/96   train_loss = 0.901\n",
      "Epoch 1105 Batch    0/96   train_loss = 0.869\n",
      "Epoch 1105 Batch   32/96   train_loss = 0.945\n",
      "Epoch 1105 Batch   64/96   train_loss = 0.909\n",
      "Epoch 1106 Batch    0/96   train_loss = 0.870\n",
      "Epoch 1106 Batch   32/96   train_loss = 0.940\n",
      "Epoch 1106 Batch   64/96   train_loss = 0.900\n",
      "Epoch 1107 Batch    0/96   train_loss = 0.873\n",
      "Epoch 1107 Batch   32/96   train_loss = 0.947\n",
      "Epoch 1107 Batch   64/96   train_loss = 0.899\n",
      "Epoch 1108 Batch    0/96   train_loss = 0.866\n",
      "Epoch 1108 Batch   32/96   train_loss = 0.935\n",
      "Epoch 1108 Batch   64/96   train_loss = 0.904\n",
      "Epoch 1109 Batch    0/96   train_loss = 0.874\n",
      "Epoch 1109 Batch   32/96   train_loss = 0.941\n",
      "Epoch 1109 Batch   64/96   train_loss = 0.915\n",
      "Epoch 1110 Batch    0/96   train_loss = 0.875\n",
      "Epoch 1110 Batch   32/96   train_loss = 0.934\n",
      "Epoch 1110 Batch   64/96   train_loss = 0.900\n",
      "Epoch 1111 Batch    0/96   train_loss = 0.873\n",
      "Epoch 1111 Batch   32/96   train_loss = 0.927\n",
      "Epoch 1111 Batch   64/96   train_loss = 0.918\n",
      "Epoch 1112 Batch    0/96   train_loss = 0.888\n",
      "Epoch 1112 Batch   32/96   train_loss = 0.947\n",
      "Epoch 1112 Batch   64/96   train_loss = 0.913\n",
      "Epoch 1113 Batch    0/96   train_loss = 0.884\n",
      "Epoch 1113 Batch   32/96   train_loss = 0.942\n",
      "Epoch 1113 Batch   64/96   train_loss = 0.912\n",
      "Epoch 1114 Batch    0/96   train_loss = 0.879\n",
      "Epoch 1114 Batch   32/96   train_loss = 0.940\n",
      "Epoch 1114 Batch   64/96   train_loss = 0.902\n",
      "Epoch 1115 Batch    0/96   train_loss = 0.891\n",
      "Epoch 1115 Batch   32/96   train_loss = 0.952\n",
      "Epoch 1115 Batch   64/96   train_loss = 0.905\n",
      "Epoch 1116 Batch    0/96   train_loss = 0.881\n",
      "Epoch 1116 Batch   32/96   train_loss = 0.964\n",
      "Epoch 1116 Batch   64/96   train_loss = 0.910\n",
      "Epoch 1117 Batch    0/96   train_loss = 0.880\n",
      "Epoch 1117 Batch   32/96   train_loss = 0.958\n",
      "Epoch 1117 Batch   64/96   train_loss = 0.920\n",
      "Epoch 1118 Batch    0/96   train_loss = 0.877\n",
      "Epoch 1118 Batch   32/96   train_loss = 0.952\n",
      "Epoch 1118 Batch   64/96   train_loss = 0.913\n",
      "Epoch 1119 Batch    0/96   train_loss = 0.884\n",
      "Epoch 1119 Batch   32/96   train_loss = 0.944\n",
      "Epoch 1119 Batch   64/96   train_loss = 0.905\n",
      "Epoch 1120 Batch    0/96   train_loss = 0.885\n",
      "Epoch 1120 Batch   32/96   train_loss = 0.948\n",
      "Epoch 1120 Batch   64/96   train_loss = 0.917\n",
      "Epoch 1121 Batch    0/96   train_loss = 0.879\n",
      "Epoch 1121 Batch   32/96   train_loss = 0.947\n",
      "Epoch 1121 Batch   64/96   train_loss = 0.913\n",
      "Epoch 1122 Batch    0/96   train_loss = 0.885\n",
      "Epoch 1122 Batch   32/96   train_loss = 0.952\n",
      "Epoch 1122 Batch   64/96   train_loss = 0.899\n",
      "Epoch 1123 Batch    0/96   train_loss = 0.870\n",
      "Epoch 1123 Batch   32/96   train_loss = 0.950\n",
      "Epoch 1123 Batch   64/96   train_loss = 0.915\n",
      "Epoch 1124 Batch    0/96   train_loss = 0.873\n",
      "Epoch 1124 Batch   32/96   train_loss = 0.950\n",
      "Epoch 1124 Batch   64/96   train_loss = 0.903\n",
      "Epoch 1125 Batch    0/96   train_loss = 0.875\n",
      "Epoch 1125 Batch   32/96   train_loss = 0.952\n",
      "Epoch 1125 Batch   64/96   train_loss = 0.895\n",
      "Epoch 1126 Batch    0/96   train_loss = 0.883\n",
      "Epoch 1126 Batch   32/96   train_loss = 0.952\n",
      "Epoch 1126 Batch   64/96   train_loss = 0.902\n",
      "Epoch 1127 Batch    0/96   train_loss = 0.873\n",
      "Epoch 1127 Batch   32/96   train_loss = 0.939\n",
      "Epoch 1127 Batch   64/96   train_loss = 0.900\n",
      "Epoch 1128 Batch    0/96   train_loss = 0.862\n",
      "Epoch 1128 Batch   32/96   train_loss = 0.948\n",
      "Epoch 1128 Batch   64/96   train_loss = 0.894\n",
      "Epoch 1129 Batch    0/96   train_loss = 0.870\n",
      "Epoch 1129 Batch   32/96   train_loss = 0.951\n",
      "Epoch 1129 Batch   64/96   train_loss = 0.908\n",
      "Epoch 1130 Batch    0/96   train_loss = 0.874\n",
      "Epoch 1130 Batch   32/96   train_loss = 0.946\n",
      "Epoch 1130 Batch   64/96   train_loss = 0.909\n",
      "Epoch 1131 Batch    0/96   train_loss = 0.885\n",
      "Epoch 1131 Batch   32/96   train_loss = 0.933\n",
      "Epoch 1131 Batch   64/96   train_loss = 0.904\n",
      "Epoch 1132 Batch    0/96   train_loss = 0.869\n",
      "Epoch 1132 Batch   32/96   train_loss = 0.951\n",
      "Epoch 1132 Batch   64/96   train_loss = 0.906\n",
      "Epoch 1133 Batch    0/96   train_loss = 0.873\n",
      "Epoch 1133 Batch   32/96   train_loss = 0.938\n",
      "Epoch 1133 Batch   64/96   train_loss = 0.909\n",
      "Epoch 1134 Batch    0/96   train_loss = 0.865\n",
      "Epoch 1134 Batch   32/96   train_loss = 0.941\n",
      "Epoch 1134 Batch   64/96   train_loss = 0.898\n",
      "Epoch 1135 Batch    0/96   train_loss = 0.869\n",
      "Epoch 1135 Batch   32/96   train_loss = 0.941\n",
      "Epoch 1135 Batch   64/96   train_loss = 0.889\n",
      "Epoch 1136 Batch    0/96   train_loss = 0.859\n",
      "Epoch 1136 Batch   32/96   train_loss = 0.964\n",
      "Epoch 1136 Batch   64/96   train_loss = 0.901\n",
      "Epoch 1137 Batch    0/96   train_loss = 0.858\n",
      "Epoch 1137 Batch   32/96   train_loss = 0.946\n",
      "Epoch 1137 Batch   64/96   train_loss = 0.886\n",
      "Epoch 1138 Batch    0/96   train_loss = 0.873\n",
      "Epoch 1138 Batch   32/96   train_loss = 0.934\n",
      "Epoch 1138 Batch   64/96   train_loss = 0.892\n",
      "Epoch 1139 Batch    0/96   train_loss = 0.874\n",
      "Epoch 1139 Batch   32/96   train_loss = 0.932\n",
      "Epoch 1139 Batch   64/96   train_loss = 0.900\n",
      "Epoch 1140 Batch    0/96   train_loss = 0.866\n",
      "Epoch 1140 Batch   32/96   train_loss = 0.945\n",
      "Epoch 1140 Batch   64/96   train_loss = 0.885\n",
      "Epoch 1141 Batch    0/96   train_loss = 0.879\n",
      "Epoch 1141 Batch   32/96   train_loss = 0.945\n",
      "Epoch 1141 Batch   64/96   train_loss = 0.892\n",
      "Epoch 1142 Batch    0/96   train_loss = 0.876\n",
      "Epoch 1142 Batch   32/96   train_loss = 0.958\n",
      "Epoch 1142 Batch   64/96   train_loss = 0.896\n",
      "Epoch 1143 Batch    0/96   train_loss = 0.865\n",
      "Epoch 1143 Batch   32/96   train_loss = 0.938\n",
      "Epoch 1143 Batch   64/96   train_loss = 0.887\n",
      "Epoch 1144 Batch    0/96   train_loss = 0.871\n",
      "Epoch 1144 Batch   32/96   train_loss = 0.937\n",
      "Epoch 1144 Batch   64/96   train_loss = 0.889\n",
      "Epoch 1145 Batch    0/96   train_loss = 0.870\n",
      "Epoch 1145 Batch   32/96   train_loss = 0.933\n",
      "Epoch 1145 Batch   64/96   train_loss = 0.903\n",
      "Epoch 1146 Batch    0/96   train_loss = 0.862\n",
      "Epoch 1146 Batch   32/96   train_loss = 0.946\n",
      "Epoch 1146 Batch   64/96   train_loss = 0.890\n",
      "Epoch 1147 Batch    0/96   train_loss = 0.872\n",
      "Epoch 1147 Batch   32/96   train_loss = 0.938\n",
      "Epoch 1147 Batch   64/96   train_loss = 0.894\n",
      "Epoch 1148 Batch    0/96   train_loss = 0.871\n",
      "Epoch 1148 Batch   32/96   train_loss = 0.943\n",
      "Epoch 1148 Batch   64/96   train_loss = 0.898\n",
      "Epoch 1149 Batch    0/96   train_loss = 0.870\n",
      "Epoch 1149 Batch   32/96   train_loss = 0.938\n",
      "Epoch 1149 Batch   64/96   train_loss = 0.893\n",
      "Epoch 1150 Batch    0/96   train_loss = 0.871\n",
      "Epoch 1150 Batch   32/96   train_loss = 0.941\n",
      "Epoch 1150 Batch   64/96   train_loss = 0.894\n",
      "Epoch 1151 Batch    0/96   train_loss = 0.879\n",
      "Epoch 1151 Batch   32/96   train_loss = 0.939\n",
      "Epoch 1151 Batch   64/96   train_loss = 0.904\n",
      "Epoch 1152 Batch    0/96   train_loss = 0.869\n",
      "Epoch 1152 Batch   32/96   train_loss = 0.937\n",
      "Epoch 1152 Batch   64/96   train_loss = 0.901\n",
      "Epoch 1153 Batch    0/96   train_loss = 0.878\n",
      "Epoch 1153 Batch   32/96   train_loss = 0.933\n",
      "Epoch 1153 Batch   64/96   train_loss = 0.898\n",
      "Epoch 1154 Batch    0/96   train_loss = 0.867\n",
      "Epoch 1154 Batch   32/96   train_loss = 0.931\n",
      "Epoch 1154 Batch   64/96   train_loss = 0.891\n",
      "Epoch 1155 Batch    0/96   train_loss = 0.860\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1155 Batch   32/96   train_loss = 0.921\n",
      "Epoch 1155 Batch   64/96   train_loss = 0.897\n",
      "Epoch 1156 Batch    0/96   train_loss = 0.865\n",
      "Epoch 1156 Batch   32/96   train_loss = 0.929\n",
      "Epoch 1156 Batch   64/96   train_loss = 0.901\n",
      "Epoch 1157 Batch    0/96   train_loss = 0.859\n",
      "Epoch 1157 Batch   32/96   train_loss = 0.949\n",
      "Epoch 1157 Batch   64/96   train_loss = 0.905\n",
      "Epoch 1158 Batch    0/96   train_loss = 0.862\n",
      "Epoch 1158 Batch   32/96   train_loss = 0.941\n",
      "Epoch 1158 Batch   64/96   train_loss = 0.908\n",
      "Epoch 1159 Batch    0/96   train_loss = 0.860\n",
      "Epoch 1159 Batch   32/96   train_loss = 0.928\n",
      "Epoch 1159 Batch   64/96   train_loss = 0.896\n",
      "Epoch 1160 Batch    0/96   train_loss = 0.851\n",
      "Epoch 1160 Batch   32/96   train_loss = 0.941\n",
      "Epoch 1160 Batch   64/96   train_loss = 0.906\n",
      "Epoch 1161 Batch    0/96   train_loss = 0.857\n",
      "Epoch 1161 Batch   32/96   train_loss = 0.933\n",
      "Epoch 1161 Batch   64/96   train_loss = 0.899\n",
      "Epoch 1162 Batch    0/96   train_loss = 0.850\n",
      "Epoch 1162 Batch   32/96   train_loss = 0.941\n",
      "Epoch 1162 Batch   64/96   train_loss = 0.905\n",
      "Epoch 1163 Batch    0/96   train_loss = 0.863\n",
      "Epoch 1163 Batch   32/96   train_loss = 0.937\n",
      "Epoch 1163 Batch   64/96   train_loss = 0.905\n",
      "Epoch 1164 Batch    0/96   train_loss = 0.866\n",
      "Epoch 1164 Batch   32/96   train_loss = 0.946\n",
      "Epoch 1164 Batch   64/96   train_loss = 0.899\n",
      "Epoch 1165 Batch    0/96   train_loss = 0.868\n",
      "Epoch 1165 Batch   32/96   train_loss = 0.937\n",
      "Epoch 1165 Batch   64/96   train_loss = 0.898\n",
      "Epoch 1166 Batch    0/96   train_loss = 0.860\n",
      "Epoch 1166 Batch   32/96   train_loss = 0.946\n",
      "Epoch 1166 Batch   64/96   train_loss = 0.894\n",
      "Epoch 1167 Batch    0/96   train_loss = 0.867\n",
      "Epoch 1167 Batch   32/96   train_loss = 0.944\n",
      "Epoch 1167 Batch   64/96   train_loss = 0.900\n",
      "Epoch 1168 Batch    0/96   train_loss = 0.866\n",
      "Epoch 1168 Batch   32/96   train_loss = 0.948\n",
      "Epoch 1168 Batch   64/96   train_loss = 0.891\n",
      "Epoch 1169 Batch    0/96   train_loss = 0.857\n",
      "Epoch 1169 Batch   32/96   train_loss = 0.939\n",
      "Epoch 1169 Batch   64/96   train_loss = 0.903\n",
      "Epoch 1170 Batch    0/96   train_loss = 0.849\n",
      "Epoch 1170 Batch   32/96   train_loss = 0.944\n",
      "Epoch 1170 Batch   64/96   train_loss = 0.897\n",
      "Epoch 1171 Batch    0/96   train_loss = 0.860\n",
      "Epoch 1171 Batch   32/96   train_loss = 0.954\n",
      "Epoch 1171 Batch   64/96   train_loss = 0.909\n",
      "Epoch 1172 Batch    0/96   train_loss = 0.869\n",
      "Epoch 1172 Batch   32/96   train_loss = 0.942\n",
      "Epoch 1172 Batch   64/96   train_loss = 0.907\n",
      "Epoch 1173 Batch    0/96   train_loss = 0.870\n",
      "Epoch 1173 Batch   32/96   train_loss = 0.953\n",
      "Epoch 1173 Batch   64/96   train_loss = 0.915\n",
      "Epoch 1174 Batch    0/96   train_loss = 0.866\n",
      "Epoch 1174 Batch   32/96   train_loss = 0.939\n",
      "Epoch 1174 Batch   64/96   train_loss = 0.900\n",
      "Epoch 1175 Batch    0/96   train_loss = 0.861\n",
      "Epoch 1175 Batch   32/96   train_loss = 0.940\n",
      "Epoch 1175 Batch   64/96   train_loss = 0.920\n",
      "Epoch 1176 Batch    0/96   train_loss = 0.886\n",
      "Epoch 1176 Batch   32/96   train_loss = 0.943\n",
      "Epoch 1176 Batch   64/96   train_loss = 0.913\n",
      "Epoch 1177 Batch    0/96   train_loss = 0.876\n",
      "Epoch 1177 Batch   32/96   train_loss = 0.935\n",
      "Epoch 1177 Batch   64/96   train_loss = 0.901\n",
      "Epoch 1178 Batch    0/96   train_loss = 0.865\n",
      "Epoch 1178 Batch   32/96   train_loss = 0.941\n",
      "Epoch 1178 Batch   64/96   train_loss = 0.917\n",
      "Epoch 1179 Batch    0/96   train_loss = 0.860\n",
      "Epoch 1179 Batch   32/96   train_loss = 0.926\n",
      "Epoch 1179 Batch   64/96   train_loss = 0.898\n",
      "Epoch 1180 Batch    0/96   train_loss = 0.854\n",
      "Epoch 1180 Batch   32/96   train_loss = 0.917\n",
      "Epoch 1180 Batch   64/96   train_loss = 0.891\n",
      "Epoch 1181 Batch    0/96   train_loss = 0.855\n",
      "Epoch 1181 Batch   32/96   train_loss = 0.940\n",
      "Epoch 1181 Batch   64/96   train_loss = 0.899\n",
      "Epoch 1182 Batch    0/96   train_loss = 0.872\n",
      "Epoch 1182 Batch   32/96   train_loss = 0.939\n",
      "Epoch 1182 Batch   64/96   train_loss = 0.903\n",
      "Epoch 1183 Batch    0/96   train_loss = 0.858\n",
      "Epoch 1183 Batch   32/96   train_loss = 0.926\n",
      "Epoch 1183 Batch   64/96   train_loss = 0.895\n",
      "Epoch 1184 Batch    0/96   train_loss = 0.857\n",
      "Epoch 1184 Batch   32/96   train_loss = 0.934\n",
      "Epoch 1184 Batch   64/96   train_loss = 0.891\n",
      "Epoch 1185 Batch    0/96   train_loss = 0.858\n",
      "Epoch 1185 Batch   32/96   train_loss = 0.925\n",
      "Epoch 1185 Batch   64/96   train_loss = 0.906\n",
      "Epoch 1186 Batch    0/96   train_loss = 0.875\n",
      "Epoch 1186 Batch   32/96   train_loss = 0.931\n",
      "Epoch 1186 Batch   64/96   train_loss = 0.930\n",
      "Epoch 1187 Batch    0/96   train_loss = 0.874\n",
      "Epoch 1187 Batch   32/96   train_loss = 0.927\n",
      "Epoch 1187 Batch   64/96   train_loss = 0.924\n",
      "Epoch 1188 Batch    0/96   train_loss = 0.871\n",
      "Epoch 1188 Batch   32/96   train_loss = 0.921\n",
      "Epoch 1188 Batch   64/96   train_loss = 0.911\n",
      "Epoch 1189 Batch    0/96   train_loss = 0.864\n",
      "Epoch 1189 Batch   32/96   train_loss = 0.934\n",
      "Epoch 1189 Batch   64/96   train_loss = 0.898\n",
      "Epoch 1190 Batch    0/96   train_loss = 0.876\n",
      "Epoch 1190 Batch   32/96   train_loss = 0.928\n",
      "Epoch 1190 Batch   64/96   train_loss = 0.909\n",
      "Epoch 1191 Batch    0/96   train_loss = 0.867\n",
      "Epoch 1191 Batch   32/96   train_loss = 0.928\n",
      "Epoch 1191 Batch   64/96   train_loss = 0.910\n",
      "Epoch 1192 Batch    0/96   train_loss = 0.866\n",
      "Epoch 1192 Batch   32/96   train_loss = 0.926\n",
      "Epoch 1192 Batch   64/96   train_loss = 0.899\n",
      "Epoch 1193 Batch    0/96   train_loss = 0.863\n",
      "Epoch 1193 Batch   32/96   train_loss = 0.926\n",
      "Epoch 1193 Batch   64/96   train_loss = 0.892\n",
      "Epoch 1194 Batch    0/96   train_loss = 0.866\n",
      "Epoch 1194 Batch   32/96   train_loss = 0.918\n",
      "Epoch 1194 Batch   64/96   train_loss = 0.909\n",
      "Epoch 1195 Batch    0/96   train_loss = 0.881\n",
      "Epoch 1195 Batch   32/96   train_loss = 0.934\n",
      "Epoch 1195 Batch   64/96   train_loss = 0.895\n",
      "Epoch 1196 Batch    0/96   train_loss = 0.863\n",
      "Epoch 1196 Batch   32/96   train_loss = 0.936\n",
      "Epoch 1196 Batch   64/96   train_loss = 0.896\n",
      "Epoch 1197 Batch    0/96   train_loss = 0.851\n",
      "Epoch 1197 Batch   32/96   train_loss = 0.930\n",
      "Epoch 1197 Batch   64/96   train_loss = 0.888\n",
      "Epoch 1198 Batch    0/96   train_loss = 0.854\n",
      "Epoch 1198 Batch   32/96   train_loss = 0.918\n",
      "Epoch 1198 Batch   64/96   train_loss = 0.889\n",
      "Epoch 1199 Batch    0/96   train_loss = 0.857\n",
      "Epoch 1199 Batch   32/96   train_loss = 0.934\n",
      "Epoch 1199 Batch   64/96   train_loss = 0.894\n",
      "Epoch 1200 Batch    0/96   train_loss = 0.873\n",
      "Epoch 1200 Batch   32/96   train_loss = 0.952\n",
      "Epoch 1200 Batch   64/96   train_loss = 0.905\n",
      "Epoch 1201 Batch    0/96   train_loss = 0.860\n",
      "Epoch 1201 Batch   32/96   train_loss = 0.936\n",
      "Epoch 1201 Batch   64/96   train_loss = 0.883\n",
      "Epoch 1202 Batch    0/96   train_loss = 0.868\n",
      "Epoch 1202 Batch   32/96   train_loss = 0.929\n",
      "Epoch 1202 Batch   64/96   train_loss = 0.902\n",
      "Epoch 1203 Batch    0/96   train_loss = 0.871\n",
      "Epoch 1203 Batch   32/96   train_loss = 0.945\n",
      "Epoch 1203 Batch   64/96   train_loss = 0.894\n",
      "Epoch 1204 Batch    0/96   train_loss = 0.866\n",
      "Epoch 1204 Batch   32/96   train_loss = 0.946\n",
      "Epoch 1204 Batch   64/96   train_loss = 0.901\n",
      "Epoch 1205 Batch    0/96   train_loss = 0.861\n",
      "Epoch 1205 Batch   32/96   train_loss = 0.938\n",
      "Epoch 1205 Batch   64/96   train_loss = 0.902\n",
      "Epoch 1206 Batch    0/96   train_loss = 0.866\n",
      "Epoch 1206 Batch   32/96   train_loss = 0.938\n",
      "Epoch 1206 Batch   64/96   train_loss = 0.903\n",
      "Epoch 1207 Batch    0/96   train_loss = 0.872\n",
      "Epoch 1207 Batch   32/96   train_loss = 0.929\n",
      "Epoch 1207 Batch   64/96   train_loss = 0.903\n",
      "Epoch 1208 Batch    0/96   train_loss = 0.866\n",
      "Epoch 1208 Batch   32/96   train_loss = 0.955\n",
      "Epoch 1208 Batch   64/96   train_loss = 0.900\n",
      "Epoch 1209 Batch    0/96   train_loss = 0.875\n",
      "Epoch 1209 Batch   32/96   train_loss = 0.930\n",
      "Epoch 1209 Batch   64/96   train_loss = 0.897\n",
      "Epoch 1210 Batch    0/96   train_loss = 0.874\n",
      "Epoch 1210 Batch   32/96   train_loss = 0.945\n",
      "Epoch 1210 Batch   64/96   train_loss = 0.897\n",
      "Epoch 1211 Batch    0/96   train_loss = 0.876\n",
      "Epoch 1211 Batch   32/96   train_loss = 0.939\n",
      "Epoch 1211 Batch   64/96   train_loss = 0.912\n",
      "Epoch 1212 Batch    0/96   train_loss = 0.877\n",
      "Epoch 1212 Batch   32/96   train_loss = 0.957\n",
      "Epoch 1212 Batch   64/96   train_loss = 0.914\n",
      "Epoch 1213 Batch    0/96   train_loss = 0.874\n",
      "Epoch 1213 Batch   32/96   train_loss = 0.934\n",
      "Epoch 1213 Batch   64/96   train_loss = 0.900\n",
      "Epoch 1214 Batch    0/96   train_loss = 0.853\n",
      "Epoch 1214 Batch   32/96   train_loss = 0.914\n",
      "Epoch 1214 Batch   64/96   train_loss = 0.901\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1215 Batch    0/96   train_loss = 0.854\n",
      "Epoch 1215 Batch   32/96   train_loss = 0.920\n",
      "Epoch 1215 Batch   64/96   train_loss = 0.894\n",
      "Epoch 1216 Batch    0/96   train_loss = 0.864\n",
      "Epoch 1216 Batch   32/96   train_loss = 0.932\n",
      "Epoch 1216 Batch   64/96   train_loss = 0.898\n",
      "Epoch 1217 Batch    0/96   train_loss = 0.860\n",
      "Epoch 1217 Batch   32/96   train_loss = 0.916\n",
      "Epoch 1217 Batch   64/96   train_loss = 0.893\n",
      "Epoch 1218 Batch    0/96   train_loss = 0.862\n",
      "Epoch 1218 Batch   32/96   train_loss = 0.931\n",
      "Epoch 1218 Batch   64/96   train_loss = 0.895\n",
      "Epoch 1219 Batch    0/96   train_loss = 0.864\n",
      "Epoch 1219 Batch   32/96   train_loss = 0.927\n",
      "Epoch 1219 Batch   64/96   train_loss = 0.889\n",
      "Epoch 1220 Batch    0/96   train_loss = 0.873\n",
      "Epoch 1220 Batch   32/96   train_loss = 0.936\n",
      "Epoch 1220 Batch   64/96   train_loss = 0.907\n",
      "Epoch 1221 Batch    0/96   train_loss = 0.862\n",
      "Epoch 1221 Batch   32/96   train_loss = 0.937\n",
      "Epoch 1221 Batch   64/96   train_loss = 0.903\n",
      "Epoch 1222 Batch    0/96   train_loss = 0.873\n",
      "Epoch 1222 Batch   32/96   train_loss = 0.935\n",
      "Epoch 1222 Batch   64/96   train_loss = 0.914\n",
      "Epoch 1223 Batch    0/96   train_loss = 0.858\n",
      "Epoch 1223 Batch   32/96   train_loss = 0.927\n",
      "Epoch 1223 Batch   64/96   train_loss = 0.907\n",
      "Epoch 1224 Batch    0/96   train_loss = 0.862\n",
      "Epoch 1224 Batch   32/96   train_loss = 0.924\n",
      "Epoch 1224 Batch   64/96   train_loss = 0.890\n",
      "Epoch 1225 Batch    0/96   train_loss = 0.863\n",
      "Epoch 1225 Batch   32/96   train_loss = 0.934\n",
      "Epoch 1225 Batch   64/96   train_loss = 0.894\n",
      "Epoch 1226 Batch    0/96   train_loss = 0.871\n",
      "Epoch 1226 Batch   32/96   train_loss = 0.924\n",
      "Epoch 1226 Batch   64/96   train_loss = 0.911\n",
      "Epoch 1227 Batch    0/96   train_loss = 0.861\n",
      "Epoch 1227 Batch   32/96   train_loss = 0.931\n",
      "Epoch 1227 Batch   64/96   train_loss = 0.899\n",
      "Epoch 1228 Batch    0/96   train_loss = 0.862\n",
      "Epoch 1228 Batch   32/96   train_loss = 0.924\n",
      "Epoch 1228 Batch   64/96   train_loss = 0.894\n",
      "Epoch 1229 Batch    0/96   train_loss = 0.869\n",
      "Epoch 1229 Batch   32/96   train_loss = 0.934\n",
      "Epoch 1229 Batch   64/96   train_loss = 0.907\n",
      "Epoch 1230 Batch    0/96   train_loss = 0.877\n",
      "Epoch 1230 Batch   32/96   train_loss = 0.925\n",
      "Epoch 1230 Batch   64/96   train_loss = 0.902\n",
      "Epoch 1231 Batch    0/96   train_loss = 0.883\n",
      "Epoch 1231 Batch   32/96   train_loss = 0.933\n",
      "Epoch 1231 Batch   64/96   train_loss = 0.902\n",
      "Epoch 1232 Batch    0/96   train_loss = 0.875\n",
      "Epoch 1232 Batch   32/96   train_loss = 0.937\n",
      "Epoch 1232 Batch   64/96   train_loss = 0.908\n",
      "Epoch 1233 Batch    0/96   train_loss = 0.878\n",
      "Epoch 1233 Batch   32/96   train_loss = 0.929\n",
      "Epoch 1233 Batch   64/96   train_loss = 0.912\n",
      "Epoch 1234 Batch    0/96   train_loss = 0.878\n",
      "Epoch 1234 Batch   32/96   train_loss = 0.923\n",
      "Epoch 1234 Batch   64/96   train_loss = 0.911\n",
      "Epoch 1235 Batch    0/96   train_loss = 0.868\n",
      "Epoch 1235 Batch   32/96   train_loss = 0.915\n",
      "Epoch 1235 Batch   64/96   train_loss = 0.897\n",
      "Epoch 1236 Batch    0/96   train_loss = 0.867\n",
      "Epoch 1236 Batch   32/96   train_loss = 0.937\n",
      "Epoch 1236 Batch   64/96   train_loss = 0.900\n",
      "Epoch 1237 Batch    0/96   train_loss = 0.873\n",
      "Epoch 1237 Batch   32/96   train_loss = 0.924\n",
      "Epoch 1237 Batch   64/96   train_loss = 0.888\n",
      "Epoch 1238 Batch    0/96   train_loss = 0.869\n",
      "Epoch 1238 Batch   32/96   train_loss = 0.926\n",
      "Epoch 1238 Batch   64/96   train_loss = 0.890\n",
      "Epoch 1239 Batch    0/96   train_loss = 0.878\n",
      "Epoch 1239 Batch   32/96   train_loss = 0.937\n",
      "Epoch 1239 Batch   64/96   train_loss = 0.890\n",
      "Epoch 1240 Batch    0/96   train_loss = 0.877\n",
      "Epoch 1240 Batch   32/96   train_loss = 0.934\n",
      "Epoch 1240 Batch   64/96   train_loss = 0.895\n",
      "Epoch 1241 Batch    0/96   train_loss = 0.872\n",
      "Epoch 1241 Batch   32/96   train_loss = 0.943\n",
      "Epoch 1241 Batch   64/96   train_loss = 0.893\n",
      "Epoch 1242 Batch    0/96   train_loss = 0.869\n",
      "Epoch 1242 Batch   32/96   train_loss = 0.932\n",
      "Epoch 1242 Batch   64/96   train_loss = 0.889\n",
      "Epoch 1243 Batch    0/96   train_loss = 0.873\n",
      "Epoch 1243 Batch   32/96   train_loss = 0.949\n",
      "Epoch 1243 Batch   64/96   train_loss = 0.895\n",
      "Epoch 1244 Batch    0/96   train_loss = 0.886\n",
      "Epoch 1244 Batch   32/96   train_loss = 0.961\n",
      "Epoch 1244 Batch   64/96   train_loss = 0.918\n",
      "Epoch 1245 Batch    0/96   train_loss = 0.878\n",
      "Epoch 1245 Batch   32/96   train_loss = 0.948\n",
      "Epoch 1245 Batch   64/96   train_loss = 0.915\n",
      "Epoch 1246 Batch    0/96   train_loss = 0.858\n",
      "Epoch 1246 Batch   32/96   train_loss = 0.941\n",
      "Epoch 1246 Batch   64/96   train_loss = 0.915\n",
      "Epoch 1247 Batch    0/96   train_loss = 0.864\n",
      "Epoch 1247 Batch   32/96   train_loss = 0.935\n",
      "Epoch 1247 Batch   64/96   train_loss = 0.899\n",
      "Epoch 1248 Batch    0/96   train_loss = 0.859\n",
      "Epoch 1248 Batch   32/96   train_loss = 0.926\n",
      "Epoch 1248 Batch   64/96   train_loss = 0.905\n",
      "Epoch 1249 Batch    0/96   train_loss = 0.853\n",
      "Epoch 1249 Batch   32/96   train_loss = 0.931\n",
      "Epoch 1249 Batch   64/96   train_loss = 0.914\n",
      "Epoch 1250 Batch    0/96   train_loss = 0.857\n",
      "Epoch 1250 Batch   32/96   train_loss = 0.922\n",
      "Epoch 1250 Batch   64/96   train_loss = 0.895\n",
      "Epoch 1251 Batch    0/96   train_loss = 0.871\n",
      "Epoch 1251 Batch   32/96   train_loss = 0.933\n",
      "Epoch 1251 Batch   64/96   train_loss = 0.889\n",
      "Epoch 1252 Batch    0/96   train_loss = 0.861\n",
      "Epoch 1252 Batch   32/96   train_loss = 0.925\n",
      "Epoch 1252 Batch   64/96   train_loss = 0.898\n",
      "Epoch 1253 Batch    0/96   train_loss = 0.853\n",
      "Epoch 1253 Batch   32/96   train_loss = 0.927\n",
      "Epoch 1253 Batch   64/96   train_loss = 0.904\n",
      "Epoch 1254 Batch    0/96   train_loss = 0.860\n",
      "Epoch 1254 Batch   32/96   train_loss = 0.925\n",
      "Epoch 1254 Batch   64/96   train_loss = 0.900\n",
      "Epoch 1255 Batch    0/96   train_loss = 0.863\n",
      "Epoch 1255 Batch   32/96   train_loss = 0.926\n",
      "Epoch 1255 Batch   64/96   train_loss = 0.894\n",
      "Epoch 1256 Batch    0/96   train_loss = 0.865\n",
      "Epoch 1256 Batch   32/96   train_loss = 0.919\n",
      "Epoch 1256 Batch   64/96   train_loss = 0.894\n",
      "Epoch 1257 Batch    0/96   train_loss = 0.864\n",
      "Epoch 1257 Batch   32/96   train_loss = 0.934\n",
      "Epoch 1257 Batch   64/96   train_loss = 0.904\n",
      "Epoch 1258 Batch    0/96   train_loss = 0.868\n",
      "Epoch 1258 Batch   32/96   train_loss = 0.946\n",
      "Epoch 1258 Batch   64/96   train_loss = 0.897\n",
      "Epoch 1259 Batch    0/96   train_loss = 0.878\n",
      "Epoch 1259 Batch   32/96   train_loss = 0.922\n",
      "Epoch 1259 Batch   64/96   train_loss = 0.904\n",
      "Epoch 1260 Batch    0/96   train_loss = 0.874\n",
      "Epoch 1260 Batch   32/96   train_loss = 0.939\n",
      "Epoch 1260 Batch   64/96   train_loss = 0.899\n",
      "Epoch 1261 Batch    0/96   train_loss = 0.861\n",
      "Epoch 1261 Batch   32/96   train_loss = 0.938\n",
      "Epoch 1261 Batch   64/96   train_loss = 0.900\n",
      "Epoch 1262 Batch    0/96   train_loss = 0.863\n",
      "Epoch 1262 Batch   32/96   train_loss = 0.938\n",
      "Epoch 1262 Batch   64/96   train_loss = 0.908\n",
      "Epoch 1263 Batch    0/96   train_loss = 0.857\n",
      "Epoch 1263 Batch   32/96   train_loss = 0.928\n",
      "Epoch 1263 Batch   64/96   train_loss = 0.906\n",
      "Epoch 1264 Batch    0/96   train_loss = 0.866\n",
      "Epoch 1264 Batch   32/96   train_loss = 0.928\n",
      "Epoch 1264 Batch   64/96   train_loss = 0.899\n",
      "Epoch 1265 Batch    0/96   train_loss = 0.858\n",
      "Epoch 1265 Batch   32/96   train_loss = 0.935\n",
      "Epoch 1265 Batch   64/96   train_loss = 0.900\n",
      "Epoch 1266 Batch    0/96   train_loss = 0.858\n",
      "Epoch 1266 Batch   32/96   train_loss = 0.917\n",
      "Epoch 1266 Batch   64/96   train_loss = 0.898\n",
      "Epoch 1267 Batch    0/96   train_loss = 0.873\n",
      "Epoch 1267 Batch   32/96   train_loss = 0.935\n",
      "Epoch 1267 Batch   64/96   train_loss = 0.902\n",
      "Epoch 1268 Batch    0/96   train_loss = 0.871\n",
      "Epoch 1268 Batch   32/96   train_loss = 0.921\n",
      "Epoch 1268 Batch   64/96   train_loss = 0.890\n",
      "Epoch 1269 Batch    0/96   train_loss = 0.874\n",
      "Epoch 1269 Batch   32/96   train_loss = 0.933\n",
      "Epoch 1269 Batch   64/96   train_loss = 0.899\n",
      "Epoch 1270 Batch    0/96   train_loss = 0.875\n",
      "Epoch 1270 Batch   32/96   train_loss = 0.937\n",
      "Epoch 1270 Batch   64/96   train_loss = 0.906\n",
      "Epoch 1271 Batch    0/96   train_loss = 0.881\n",
      "Epoch 1271 Batch   32/96   train_loss = 0.920\n",
      "Epoch 1271 Batch   64/96   train_loss = 0.901\n",
      "Epoch 1272 Batch    0/96   train_loss = 0.882\n",
      "Epoch 1272 Batch   32/96   train_loss = 0.932\n",
      "Epoch 1272 Batch   64/96   train_loss = 0.910\n",
      "Epoch 1273 Batch    0/96   train_loss = 0.863\n",
      "Epoch 1273 Batch   32/96   train_loss = 0.950\n",
      "Epoch 1273 Batch   64/96   train_loss = 0.922\n",
      "Epoch 1274 Batch    0/96   train_loss = 0.883\n",
      "Epoch 1274 Batch   32/96   train_loss = 0.955\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1274 Batch   64/96   train_loss = 0.902\n",
      "Epoch 1275 Batch    0/96   train_loss = 0.876\n",
      "Epoch 1275 Batch   32/96   train_loss = 0.940\n",
      "Epoch 1275 Batch   64/96   train_loss = 0.902\n",
      "Epoch 1276 Batch    0/96   train_loss = 0.869\n",
      "Epoch 1276 Batch   32/96   train_loss = 0.936\n",
      "Epoch 1276 Batch   64/96   train_loss = 0.912\n",
      "Epoch 1277 Batch    0/96   train_loss = 0.874\n",
      "Epoch 1277 Batch   32/96   train_loss = 0.942\n",
      "Epoch 1277 Batch   64/96   train_loss = 0.922\n",
      "Epoch 1278 Batch    0/96   train_loss = 0.882\n",
      "Epoch 1278 Batch   32/96   train_loss = 0.936\n",
      "Epoch 1278 Batch   64/96   train_loss = 0.900\n",
      "Epoch 1279 Batch    0/96   train_loss = 0.863\n",
      "Epoch 1279 Batch   32/96   train_loss = 0.936\n",
      "Epoch 1279 Batch   64/96   train_loss = 0.918\n",
      "Epoch 1280 Batch    0/96   train_loss = 0.879\n",
      "Epoch 1280 Batch   32/96   train_loss = 0.939\n",
      "Epoch 1280 Batch   64/96   train_loss = 0.895\n",
      "Epoch 1281 Batch    0/96   train_loss = 0.877\n",
      "Epoch 1281 Batch   32/96   train_loss = 0.932\n",
      "Epoch 1281 Batch   64/96   train_loss = 0.910\n",
      "Epoch 1282 Batch    0/96   train_loss = 0.863\n",
      "Epoch 1282 Batch   32/96   train_loss = 0.952\n",
      "Epoch 1282 Batch   64/96   train_loss = 0.899\n",
      "Epoch 1283 Batch    0/96   train_loss = 0.871\n",
      "Epoch 1283 Batch   32/96   train_loss = 0.942\n",
      "Epoch 1283 Batch   64/96   train_loss = 0.905\n",
      "Epoch 1284 Batch    0/96   train_loss = 0.874\n",
      "Epoch 1284 Batch   32/96   train_loss = 0.944\n",
      "Epoch 1284 Batch   64/96   train_loss = 0.908\n",
      "Epoch 1285 Batch    0/96   train_loss = 0.871\n",
      "Epoch 1285 Batch   32/96   train_loss = 0.946\n",
      "Epoch 1285 Batch   64/96   train_loss = 0.896\n",
      "Epoch 1286 Batch    0/96   train_loss = 0.861\n",
      "Epoch 1286 Batch   32/96   train_loss = 0.942\n",
      "Epoch 1286 Batch   64/96   train_loss = 0.902\n",
      "Epoch 1287 Batch    0/96   train_loss = 0.874\n",
      "Epoch 1287 Batch   32/96   train_loss = 0.945\n",
      "Epoch 1287 Batch   64/96   train_loss = 0.894\n",
      "Epoch 1288 Batch    0/96   train_loss = 0.876\n",
      "Epoch 1288 Batch   32/96   train_loss = 0.943\n",
      "Epoch 1288 Batch   64/96   train_loss = 0.884\n",
      "Epoch 1289 Batch    0/96   train_loss = 0.887\n",
      "Epoch 1289 Batch   32/96   train_loss = 0.941\n",
      "Epoch 1289 Batch   64/96   train_loss = 0.900\n",
      "Epoch 1290 Batch    0/96   train_loss = 0.872\n",
      "Epoch 1290 Batch   32/96   train_loss = 0.943\n",
      "Epoch 1290 Batch   64/96   train_loss = 0.911\n",
      "Epoch 1291 Batch    0/96   train_loss = 0.859\n",
      "Epoch 1291 Batch   32/96   train_loss = 0.942\n",
      "Epoch 1291 Batch   64/96   train_loss = 0.892\n",
      "Epoch 1292 Batch    0/96   train_loss = 0.872\n",
      "Epoch 1292 Batch   32/96   train_loss = 0.949\n",
      "Epoch 1292 Batch   64/96   train_loss = 0.906\n",
      "Epoch 1293 Batch    0/96   train_loss = 0.870\n",
      "Epoch 1293 Batch   32/96   train_loss = 0.942\n",
      "Epoch 1293 Batch   64/96   train_loss = 0.905\n",
      "Epoch 1294 Batch    0/96   train_loss = 0.878\n",
      "Epoch 1294 Batch   32/96   train_loss = 0.937\n",
      "Epoch 1294 Batch   64/96   train_loss = 0.889\n",
      "Epoch 1295 Batch    0/96   train_loss = 0.876\n",
      "Epoch 1295 Batch   32/96   train_loss = 0.936\n",
      "Epoch 1295 Batch   64/96   train_loss = 0.905\n",
      "Epoch 1296 Batch    0/96   train_loss = 0.881\n",
      "Epoch 1296 Batch   32/96   train_loss = 0.937\n",
      "Epoch 1296 Batch   64/96   train_loss = 0.892\n",
      "Epoch 1297 Batch    0/96   train_loss = 0.887\n",
      "Epoch 1297 Batch   32/96   train_loss = 0.945\n",
      "Epoch 1297 Batch   64/96   train_loss = 0.900\n",
      "Epoch 1298 Batch    0/96   train_loss = 0.875\n",
      "Epoch 1298 Batch   32/96   train_loss = 0.942\n",
      "Epoch 1298 Batch   64/96   train_loss = 0.901\n",
      "Epoch 1299 Batch    0/96   train_loss = 0.884\n",
      "Epoch 1299 Batch   32/96   train_loss = 0.942\n",
      "Epoch 1299 Batch   64/96   train_loss = 0.888\n",
      "Epoch 1300 Batch    0/96   train_loss = 0.876\n",
      "Epoch 1300 Batch   32/96   train_loss = 0.941\n",
      "Epoch 1300 Batch   64/96   train_loss = 0.893\n",
      "Epoch 1301 Batch    0/96   train_loss = 0.871\n",
      "Epoch 1301 Batch   32/96   train_loss = 0.932\n",
      "Epoch 1301 Batch   64/96   train_loss = 0.905\n",
      "Epoch 1302 Batch    0/96   train_loss = 0.862\n",
      "Epoch 1302 Batch   32/96   train_loss = 0.930\n",
      "Epoch 1302 Batch   64/96   train_loss = 0.891\n",
      "Epoch 1303 Batch    0/96   train_loss = 0.879\n",
      "Epoch 1303 Batch   32/96   train_loss = 0.940\n",
      "Epoch 1303 Batch   64/96   train_loss = 0.898\n",
      "Epoch 1304 Batch    0/96   train_loss = 0.874\n",
      "Epoch 1304 Batch   32/96   train_loss = 0.929\n",
      "Epoch 1304 Batch   64/96   train_loss = 0.895\n",
      "Epoch 1305 Batch    0/96   train_loss = 0.877\n",
      "Epoch 1305 Batch   32/96   train_loss = 0.922\n",
      "Epoch 1305 Batch   64/96   train_loss = 0.885\n",
      "Epoch 1306 Batch    0/96   train_loss = 0.878\n",
      "Epoch 1306 Batch   32/96   train_loss = 0.932\n",
      "Epoch 1306 Batch   64/96   train_loss = 0.889\n",
      "Epoch 1307 Batch    0/96   train_loss = 0.864\n",
      "Epoch 1307 Batch   32/96   train_loss = 0.933\n",
      "Epoch 1307 Batch   64/96   train_loss = 0.898\n",
      "Epoch 1308 Batch    0/96   train_loss = 0.884\n",
      "Epoch 1308 Batch   32/96   train_loss = 0.941\n",
      "Epoch 1308 Batch   64/96   train_loss = 0.899\n",
      "Epoch 1309 Batch    0/96   train_loss = 0.868\n",
      "Epoch 1309 Batch   32/96   train_loss = 0.940\n",
      "Epoch 1309 Batch   64/96   train_loss = 0.886\n",
      "Epoch 1310 Batch    0/96   train_loss = 0.865\n",
      "Epoch 1310 Batch   32/96   train_loss = 0.934\n",
      "Epoch 1310 Batch   64/96   train_loss = 0.896\n",
      "Epoch 1311 Batch    0/96   train_loss = 0.872\n",
      "Epoch 1311 Batch   32/96   train_loss = 0.929\n",
      "Epoch 1311 Batch   64/96   train_loss = 0.908\n",
      "Epoch 1312 Batch    0/96   train_loss = 0.869\n",
      "Epoch 1312 Batch   32/96   train_loss = 0.935\n",
      "Epoch 1312 Batch   64/96   train_loss = 0.900\n",
      "Epoch 1313 Batch    0/96   train_loss = 0.880\n",
      "Epoch 1313 Batch   32/96   train_loss = 0.920\n",
      "Epoch 1313 Batch   64/96   train_loss = 0.904\n",
      "Epoch 1314 Batch    0/96   train_loss = 0.869\n",
      "Epoch 1314 Batch   32/96   train_loss = 0.933\n",
      "Epoch 1314 Batch   64/96   train_loss = 0.895\n",
      "Epoch 1315 Batch    0/96   train_loss = 0.871\n",
      "Epoch 1315 Batch   32/96   train_loss = 0.940\n",
      "Epoch 1315 Batch   64/96   train_loss = 0.899\n",
      "Epoch 1316 Batch    0/96   train_loss = 0.891\n",
      "Epoch 1316 Batch   32/96   train_loss = 0.947\n",
      "Epoch 1316 Batch   64/96   train_loss = 0.903\n",
      "Epoch 1317 Batch    0/96   train_loss = 0.886\n",
      "Epoch 1317 Batch   32/96   train_loss = 0.924\n",
      "Epoch 1317 Batch   64/96   train_loss = 0.897\n",
      "Epoch 1318 Batch    0/96   train_loss = 0.877\n",
      "Epoch 1318 Batch   32/96   train_loss = 0.947\n",
      "Epoch 1318 Batch   64/96   train_loss = 0.892\n",
      "Epoch 1319 Batch    0/96   train_loss = 0.866\n",
      "Epoch 1319 Batch   32/96   train_loss = 0.926\n",
      "Epoch 1319 Batch   64/96   train_loss = 0.903\n",
      "Epoch 1320 Batch    0/96   train_loss = 0.859\n",
      "Epoch 1320 Batch   32/96   train_loss = 0.931\n",
      "Epoch 1320 Batch   64/96   train_loss = 0.890\n",
      "Epoch 1321 Batch    0/96   train_loss = 0.869\n",
      "Epoch 1321 Batch   32/96   train_loss = 0.938\n",
      "Epoch 1321 Batch   64/96   train_loss = 0.899\n",
      "Epoch 1322 Batch    0/96   train_loss = 0.858\n",
      "Epoch 1322 Batch   32/96   train_loss = 0.928\n",
      "Epoch 1322 Batch   64/96   train_loss = 0.889\n",
      "Epoch 1323 Batch    0/96   train_loss = 0.870\n",
      "Epoch 1323 Batch   32/96   train_loss = 0.944\n",
      "Epoch 1323 Batch   64/96   train_loss = 0.888\n",
      "Epoch 1324 Batch    0/96   train_loss = 0.857\n",
      "Epoch 1324 Batch   32/96   train_loss = 0.941\n",
      "Epoch 1324 Batch   64/96   train_loss = 0.893\n",
      "Epoch 1325 Batch    0/96   train_loss = 0.862\n",
      "Epoch 1325 Batch   32/96   train_loss = 0.924\n",
      "Epoch 1325 Batch   64/96   train_loss = 0.892\n",
      "Epoch 1326 Batch    0/96   train_loss = 0.867\n",
      "Epoch 1326 Batch   32/96   train_loss = 0.942\n",
      "Epoch 1326 Batch   64/96   train_loss = 0.881\n",
      "Epoch 1327 Batch    0/96   train_loss = 0.863\n",
      "Epoch 1327 Batch   32/96   train_loss = 0.931\n",
      "Epoch 1327 Batch   64/96   train_loss = 0.892\n",
      "Epoch 1328 Batch    0/96   train_loss = 0.874\n",
      "Epoch 1328 Batch   32/96   train_loss = 0.930\n",
      "Epoch 1328 Batch   64/96   train_loss = 0.899\n",
      "Epoch 1329 Batch    0/96   train_loss = 0.866\n",
      "Epoch 1329 Batch   32/96   train_loss = 0.946\n",
      "Epoch 1329 Batch   64/96   train_loss = 0.887\n",
      "Epoch 1330 Batch    0/96   train_loss = 0.871\n",
      "Epoch 1330 Batch   32/96   train_loss = 0.937\n",
      "Epoch 1330 Batch   64/96   train_loss = 0.883\n",
      "Epoch 1331 Batch    0/96   train_loss = 0.863\n",
      "Epoch 1331 Batch   32/96   train_loss = 0.947\n",
      "Epoch 1331 Batch   64/96   train_loss = 0.909\n",
      "Epoch 1332 Batch    0/96   train_loss = 0.858\n",
      "Epoch 1332 Batch   32/96   train_loss = 0.938\n",
      "Epoch 1332 Batch   64/96   train_loss = 0.892\n",
      "Epoch 1333 Batch    0/96   train_loss = 0.855\n",
      "Epoch 1333 Batch   32/96   train_loss = 0.929\n",
      "Epoch 1333 Batch   64/96   train_loss = 0.888\n",
      "Epoch 1334 Batch    0/96   train_loss = 0.847\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1334 Batch   32/96   train_loss = 0.924\n",
      "Epoch 1334 Batch   64/96   train_loss = 0.889\n",
      "Epoch 1335 Batch    0/96   train_loss = 0.857\n",
      "Epoch 1335 Batch   32/96   train_loss = 0.928\n",
      "Epoch 1335 Batch   64/96   train_loss = 0.900\n",
      "Epoch 1336 Batch    0/96   train_loss = 0.869\n",
      "Epoch 1336 Batch   32/96   train_loss = 0.936\n",
      "Epoch 1336 Batch   64/96   train_loss = 0.893\n",
      "Epoch 1337 Batch    0/96   train_loss = 0.855\n",
      "Epoch 1337 Batch   32/96   train_loss = 0.929\n",
      "Epoch 1337 Batch   64/96   train_loss = 0.905\n",
      "Epoch 1338 Batch    0/96   train_loss = 0.856\n",
      "Epoch 1338 Batch   32/96   train_loss = 0.917\n",
      "Epoch 1338 Batch   64/96   train_loss = 0.898\n",
      "Epoch 1339 Batch    0/96   train_loss = 0.862\n",
      "Epoch 1339 Batch   32/96   train_loss = 0.943\n",
      "Epoch 1339 Batch   64/96   train_loss = 0.883\n",
      "Epoch 1340 Batch    0/96   train_loss = 0.873\n",
      "Epoch 1340 Batch   32/96   train_loss = 0.929\n",
      "Epoch 1340 Batch   64/96   train_loss = 0.893\n",
      "Epoch 1341 Batch    0/96   train_loss = 0.867\n",
      "Epoch 1341 Batch   32/96   train_loss = 0.947\n",
      "Epoch 1341 Batch   64/96   train_loss = 0.895\n",
      "Epoch 1342 Batch    0/96   train_loss = 0.857\n",
      "Epoch 1342 Batch   32/96   train_loss = 0.924\n",
      "Epoch 1342 Batch   64/96   train_loss = 0.892\n",
      "Epoch 1343 Batch    0/96   train_loss = 0.871\n",
      "Epoch 1343 Batch   32/96   train_loss = 0.941\n",
      "Epoch 1343 Batch   64/96   train_loss = 0.878\n",
      "Epoch 1344 Batch    0/96   train_loss = 0.864\n",
      "Epoch 1344 Batch   32/96   train_loss = 0.946\n",
      "Epoch 1344 Batch   64/96   train_loss = 0.889\n",
      "Epoch 1345 Batch    0/96   train_loss = 0.860\n",
      "Epoch 1345 Batch   32/96   train_loss = 0.929\n",
      "Epoch 1345 Batch   64/96   train_loss = 0.899\n",
      "Epoch 1346 Batch    0/96   train_loss = 0.850\n",
      "Epoch 1346 Batch   32/96   train_loss = 0.932\n",
      "Epoch 1346 Batch   64/96   train_loss = 0.890\n",
      "Epoch 1347 Batch    0/96   train_loss = 0.866\n",
      "Epoch 1347 Batch   32/96   train_loss = 0.931\n",
      "Epoch 1347 Batch   64/96   train_loss = 0.904\n",
      "Epoch 1348 Batch    0/96   train_loss = 0.868\n",
      "Epoch 1348 Batch   32/96   train_loss = 0.946\n",
      "Epoch 1348 Batch   64/96   train_loss = 0.902\n",
      "Epoch 1349 Batch    0/96   train_loss = 0.858\n",
      "Epoch 1349 Batch   32/96   train_loss = 0.943\n",
      "Epoch 1349 Batch   64/96   train_loss = 0.911\n",
      "Epoch 1350 Batch    0/96   train_loss = 0.886\n",
      "Epoch 1350 Batch   32/96   train_loss = 0.954\n",
      "Epoch 1350 Batch   64/96   train_loss = 0.901\n",
      "Epoch 1351 Batch    0/96   train_loss = 0.864\n",
      "Epoch 1351 Batch   32/96   train_loss = 0.940\n",
      "Epoch 1351 Batch   64/96   train_loss = 0.898\n",
      "Epoch 1352 Batch    0/96   train_loss = 0.882\n",
      "Epoch 1352 Batch   32/96   train_loss = 0.939\n",
      "Epoch 1352 Batch   64/96   train_loss = 0.909\n",
      "Epoch 1353 Batch    0/96   train_loss = 0.870\n",
      "Epoch 1353 Batch   32/96   train_loss = 0.943\n",
      "Epoch 1353 Batch   64/96   train_loss = 0.907\n",
      "Epoch 1354 Batch    0/96   train_loss = 0.876\n",
      "Epoch 1354 Batch   32/96   train_loss = 0.951\n",
      "Epoch 1354 Batch   64/96   train_loss = 0.901\n",
      "Epoch 1355 Batch    0/96   train_loss = 0.872\n",
      "Epoch 1355 Batch   32/96   train_loss = 0.937\n",
      "Epoch 1355 Batch   64/96   train_loss = 0.902\n",
      "Epoch 1356 Batch    0/96   train_loss = 0.856\n",
      "Epoch 1356 Batch   32/96   train_loss = 0.939\n",
      "Epoch 1356 Batch   64/96   train_loss = 0.903\n",
      "Epoch 1357 Batch    0/96   train_loss = 0.853\n",
      "Epoch 1357 Batch   32/96   train_loss = 0.946\n",
      "Epoch 1357 Batch   64/96   train_loss = 0.897\n",
      "Epoch 1358 Batch    0/96   train_loss = 0.862\n",
      "Epoch 1358 Batch   32/96   train_loss = 0.951\n",
      "Epoch 1358 Batch   64/96   train_loss = 0.892\n",
      "Epoch 1359 Batch    0/96   train_loss = 0.860\n",
      "Epoch 1359 Batch   32/96   train_loss = 0.943\n",
      "Epoch 1359 Batch   64/96   train_loss = 0.907\n",
      "Epoch 1360 Batch    0/96   train_loss = 0.872\n",
      "Epoch 1360 Batch   32/96   train_loss = 0.945\n",
      "Epoch 1360 Batch   64/96   train_loss = 0.909\n",
      "Epoch 1361 Batch    0/96   train_loss = 0.854\n",
      "Epoch 1361 Batch   32/96   train_loss = 0.947\n",
      "Epoch 1361 Batch   64/96   train_loss = 0.906\n",
      "Epoch 1362 Batch    0/96   train_loss = 0.867\n",
      "Epoch 1362 Batch   32/96   train_loss = 0.935\n",
      "Epoch 1362 Batch   64/96   train_loss = 0.915\n",
      "Epoch 1363 Batch    0/96   train_loss = 0.856\n",
      "Epoch 1363 Batch   32/96   train_loss = 0.938\n",
      "Epoch 1363 Batch   64/96   train_loss = 0.908\n",
      "Epoch 1364 Batch    0/96   train_loss = 0.860\n",
      "Epoch 1364 Batch   32/96   train_loss = 0.928\n",
      "Epoch 1364 Batch   64/96   train_loss = 0.899\n",
      "Epoch 1365 Batch    0/96   train_loss = 0.866\n",
      "Epoch 1365 Batch   32/96   train_loss = 0.926\n",
      "Epoch 1365 Batch   64/96   train_loss = 0.888\n",
      "Epoch 1366 Batch    0/96   train_loss = 0.851\n",
      "Epoch 1366 Batch   32/96   train_loss = 0.925\n",
      "Epoch 1366 Batch   64/96   train_loss = 0.903\n",
      "Epoch 1367 Batch    0/96   train_loss = 0.865\n",
      "Epoch 1367 Batch   32/96   train_loss = 0.927\n",
      "Epoch 1367 Batch   64/96   train_loss = 0.912\n",
      "Epoch 1368 Batch    0/96   train_loss = 0.855\n",
      "Epoch 1368 Batch   32/96   train_loss = 0.947\n",
      "Epoch 1368 Batch   64/96   train_loss = 0.887\n",
      "Epoch 1369 Batch    0/96   train_loss = 0.862\n",
      "Epoch 1369 Batch   32/96   train_loss = 0.933\n",
      "Epoch 1369 Batch   64/96   train_loss = 0.901\n",
      "Epoch 1370 Batch    0/96   train_loss = 0.860\n",
      "Epoch 1370 Batch   32/96   train_loss = 0.934\n",
      "Epoch 1370 Batch   64/96   train_loss = 0.910\n",
      "Epoch 1371 Batch    0/96   train_loss = 0.880\n",
      "Epoch 1371 Batch   32/96   train_loss = 0.941\n",
      "Epoch 1371 Batch   64/96   train_loss = 0.897\n",
      "Epoch 1372 Batch    0/96   train_loss = 0.871\n",
      "Epoch 1372 Batch   32/96   train_loss = 0.952\n",
      "Epoch 1372 Batch   64/96   train_loss = 0.902\n",
      "Epoch 1373 Batch    0/96   train_loss = 0.869\n",
      "Epoch 1373 Batch   32/96   train_loss = 0.945\n",
      "Epoch 1373 Batch   64/96   train_loss = 0.905\n",
      "Epoch 1374 Batch    0/96   train_loss = 0.875\n",
      "Epoch 1374 Batch   32/96   train_loss = 0.952\n",
      "Epoch 1374 Batch   64/96   train_loss = 0.904\n",
      "Epoch 1375 Batch    0/96   train_loss = 0.863\n",
      "Epoch 1375 Batch   32/96   train_loss = 0.943\n",
      "Epoch 1375 Batch   64/96   train_loss = 0.910\n",
      "Epoch 1376 Batch    0/96   train_loss = 0.861\n",
      "Epoch 1376 Batch   32/96   train_loss = 0.926\n",
      "Epoch 1376 Batch   64/96   train_loss = 0.890\n",
      "Epoch 1377 Batch    0/96   train_loss = 0.863\n",
      "Epoch 1377 Batch   32/96   train_loss = 0.939\n",
      "Epoch 1377 Batch   64/96   train_loss = 0.904\n",
      "Epoch 1378 Batch    0/96   train_loss = 0.880\n",
      "Epoch 1378 Batch   32/96   train_loss = 0.942\n",
      "Epoch 1378 Batch   64/96   train_loss = 0.903\n",
      "Epoch 1379 Batch    0/96   train_loss = 0.876\n",
      "Epoch 1379 Batch   32/96   train_loss = 0.945\n",
      "Epoch 1379 Batch   64/96   train_loss = 0.901\n",
      "Epoch 1380 Batch    0/96   train_loss = 0.875\n",
      "Epoch 1380 Batch   32/96   train_loss = 0.940\n",
      "Epoch 1380 Batch   64/96   train_loss = 0.942\n",
      "Epoch 1381 Batch    0/96   train_loss = 0.881\n",
      "Epoch 1381 Batch   32/96   train_loss = 0.948\n",
      "Epoch 1381 Batch   64/96   train_loss = 0.924\n",
      "Epoch 1382 Batch    0/96   train_loss = 0.866\n",
      "Epoch 1382 Batch   32/96   train_loss = 0.951\n",
      "Epoch 1382 Batch   64/96   train_loss = 0.909\n",
      "Epoch 1383 Batch    0/96   train_loss = 0.866\n",
      "Epoch 1383 Batch   32/96   train_loss = 0.934\n",
      "Epoch 1383 Batch   64/96   train_loss = 0.905\n",
      "Epoch 1384 Batch    0/96   train_loss = 0.856\n",
      "Epoch 1384 Batch   32/96   train_loss = 0.935\n",
      "Epoch 1384 Batch   64/96   train_loss = 0.911\n",
      "Epoch 1385 Batch    0/96   train_loss = 0.883\n",
      "Epoch 1385 Batch   32/96   train_loss = 0.946\n",
      "Epoch 1385 Batch   64/96   train_loss = 0.919\n",
      "Epoch 1386 Batch    0/96   train_loss = 0.870\n",
      "Epoch 1386 Batch   32/96   train_loss = 0.949\n",
      "Epoch 1386 Batch   64/96   train_loss = 0.898\n",
      "Epoch 1387 Batch    0/96   train_loss = 0.873\n",
      "Epoch 1387 Batch   32/96   train_loss = 0.951\n",
      "Epoch 1387 Batch   64/96   train_loss = 0.919\n",
      "Epoch 1388 Batch    0/96   train_loss = 0.859\n",
      "Epoch 1388 Batch   32/96   train_loss = 0.950\n",
      "Epoch 1388 Batch   64/96   train_loss = 0.914\n",
      "Epoch 1389 Batch    0/96   train_loss = 0.865\n",
      "Epoch 1389 Batch   32/96   train_loss = 0.946\n",
      "Epoch 1389 Batch   64/96   train_loss = 0.903\n",
      "Epoch 1390 Batch    0/96   train_loss = 0.870\n",
      "Epoch 1390 Batch   32/96   train_loss = 0.934\n",
      "Epoch 1390 Batch   64/96   train_loss = 0.897\n",
      "Epoch 1391 Batch    0/96   train_loss = 0.867\n",
      "Epoch 1391 Batch   32/96   train_loss = 0.937\n",
      "Epoch 1391 Batch   64/96   train_loss = 0.887\n",
      "Epoch 1392 Batch    0/96   train_loss = 0.876\n",
      "Epoch 1392 Batch   32/96   train_loss = 0.927\n",
      "Epoch 1392 Batch   64/96   train_loss = 0.900\n",
      "Epoch 1393 Batch    0/96   train_loss = 0.864\n",
      "Epoch 1393 Batch   32/96   train_loss = 0.949\n",
      "Epoch 1393 Batch   64/96   train_loss = 0.909\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1394 Batch    0/96   train_loss = 0.882\n",
      "Epoch 1394 Batch   32/96   train_loss = 0.935\n",
      "Epoch 1394 Batch   64/96   train_loss = 0.906\n",
      "Epoch 1395 Batch    0/96   train_loss = 0.863\n",
      "Epoch 1395 Batch   32/96   train_loss = 0.933\n",
      "Epoch 1395 Batch   64/96   train_loss = 0.896\n",
      "Epoch 1396 Batch    0/96   train_loss = 0.861\n",
      "Epoch 1396 Batch   32/96   train_loss = 0.923\n",
      "Epoch 1396 Batch   64/96   train_loss = 0.887\n",
      "Epoch 1397 Batch    0/96   train_loss = 0.863\n",
      "Epoch 1397 Batch   32/96   train_loss = 0.929\n",
      "Epoch 1397 Batch   64/96   train_loss = 0.903\n",
      "Epoch 1398 Batch    0/96   train_loss = 0.866\n",
      "Epoch 1398 Batch   32/96   train_loss = 0.941\n",
      "Epoch 1398 Batch   64/96   train_loss = 0.901\n",
      "Epoch 1399 Batch    0/96   train_loss = 0.888\n",
      "Epoch 1399 Batch   32/96   train_loss = 0.932\n",
      "Epoch 1399 Batch   64/96   train_loss = 0.903\n",
      "Epoch 1400 Batch    0/96   train_loss = 0.876\n",
      "Epoch 1400 Batch   32/96   train_loss = 0.930\n",
      "Epoch 1400 Batch   64/96   train_loss = 0.891\n",
      "Epoch 1401 Batch    0/96   train_loss = 0.903\n",
      "Epoch 1401 Batch   32/96   train_loss = 0.952\n",
      "Epoch 1401 Batch   64/96   train_loss = 0.918\n",
      "Epoch 1402 Batch    0/96   train_loss = 0.874\n",
      "Epoch 1402 Batch   32/96   train_loss = 0.958\n",
      "Epoch 1402 Batch   64/96   train_loss = 0.916\n",
      "Epoch 1403 Batch    0/96   train_loss = 0.888\n",
      "Epoch 1403 Batch   32/96   train_loss = 0.950\n",
      "Epoch 1403 Batch   64/96   train_loss = 0.909\n",
      "Epoch 1404 Batch    0/96   train_loss = 0.883\n",
      "Epoch 1404 Batch   32/96   train_loss = 0.948\n",
      "Epoch 1404 Batch   64/96   train_loss = 0.916\n",
      "Epoch 1405 Batch    0/96   train_loss = 0.874\n",
      "Epoch 1405 Batch   32/96   train_loss = 0.948\n",
      "Epoch 1405 Batch   64/96   train_loss = 0.923\n",
      "Epoch 1406 Batch    0/96   train_loss = 0.875\n",
      "Epoch 1406 Batch   32/96   train_loss = 0.956\n",
      "Epoch 1406 Batch   64/96   train_loss = 0.928\n",
      "Epoch 1407 Batch    0/96   train_loss = 0.889\n",
      "Epoch 1407 Batch   32/96   train_loss = 0.965\n",
      "Epoch 1407 Batch   64/96   train_loss = 0.914\n",
      "Epoch 1408 Batch    0/96   train_loss = 0.896\n",
      "Epoch 1408 Batch   32/96   train_loss = 0.953\n",
      "Epoch 1408 Batch   64/96   train_loss = 0.924\n",
      "Epoch 1409 Batch    0/96   train_loss = 0.888\n",
      "Epoch 1409 Batch   32/96   train_loss = 0.948\n",
      "Epoch 1409 Batch   64/96   train_loss = 0.921\n",
      "Epoch 1410 Batch    0/96   train_loss = 0.884\n",
      "Epoch 1410 Batch   32/96   train_loss = 0.950\n",
      "Epoch 1410 Batch   64/96   train_loss = 0.900\n",
      "Epoch 1411 Batch    0/96   train_loss = 0.886\n",
      "Epoch 1411 Batch   32/96   train_loss = 0.947\n",
      "Epoch 1411 Batch   64/96   train_loss = 0.916\n",
      "Epoch 1412 Batch    0/96   train_loss = 0.893\n",
      "Epoch 1412 Batch   32/96   train_loss = 0.957\n",
      "Epoch 1412 Batch   64/96   train_loss = 0.912\n",
      "Epoch 1413 Batch    0/96   train_loss = 0.893\n",
      "Epoch 1413 Batch   32/96   train_loss = 0.965\n",
      "Epoch 1413 Batch   64/96   train_loss = 0.905\n",
      "Epoch 1414 Batch    0/96   train_loss = 0.890\n",
      "Epoch 1414 Batch   32/96   train_loss = 0.947\n",
      "Epoch 1414 Batch   64/96   train_loss = 0.908\n",
      "Epoch 1415 Batch    0/96   train_loss = 0.874\n",
      "Epoch 1415 Batch   32/96   train_loss = 0.949\n",
      "Epoch 1415 Batch   64/96   train_loss = 0.905\n",
      "Epoch 1416 Batch    0/96   train_loss = 0.876\n",
      "Epoch 1416 Batch   32/96   train_loss = 0.939\n",
      "Epoch 1416 Batch   64/96   train_loss = 0.900\n",
      "Epoch 1417 Batch    0/96   train_loss = 0.873\n",
      "Epoch 1417 Batch   32/96   train_loss = 0.937\n",
      "Epoch 1417 Batch   64/96   train_loss = 0.898\n",
      "Epoch 1418 Batch    0/96   train_loss = 0.876\n",
      "Epoch 1418 Batch   32/96   train_loss = 0.943\n",
      "Epoch 1418 Batch   64/96   train_loss = 0.919\n",
      "Epoch 1419 Batch    0/96   train_loss = 0.867\n",
      "Epoch 1419 Batch   32/96   train_loss = 0.927\n",
      "Epoch 1419 Batch   64/96   train_loss = 0.907\n",
      "Epoch 1420 Batch    0/96   train_loss = 0.864\n",
      "Epoch 1420 Batch   32/96   train_loss = 0.928\n",
      "Epoch 1420 Batch   64/96   train_loss = 0.911\n",
      "Epoch 1421 Batch    0/96   train_loss = 0.868\n",
      "Epoch 1421 Batch   32/96   train_loss = 0.923\n",
      "Epoch 1421 Batch   64/96   train_loss = 0.900\n",
      "Epoch 1422 Batch    0/96   train_loss = 0.866\n",
      "Epoch 1422 Batch   32/96   train_loss = 0.935\n",
      "Epoch 1422 Batch   64/96   train_loss = 0.914\n",
      "Epoch 1423 Batch    0/96   train_loss = 0.887\n",
      "Epoch 1423 Batch   32/96   train_loss = 0.939\n",
      "Epoch 1423 Batch   64/96   train_loss = 0.913\n",
      "Epoch 1424 Batch    0/96   train_loss = 0.895\n",
      "Epoch 1424 Batch   32/96   train_loss = 0.941\n",
      "Epoch 1424 Batch   64/96   train_loss = 0.914\n",
      "Epoch 1425 Batch    0/96   train_loss = 0.891\n",
      "Epoch 1425 Batch   32/96   train_loss = 0.942\n",
      "Epoch 1425 Batch   64/96   train_loss = 0.926\n",
      "Epoch 1426 Batch    0/96   train_loss = 0.881\n",
      "Epoch 1426 Batch   32/96   train_loss = 0.938\n",
      "Epoch 1426 Batch   64/96   train_loss = 0.914\n",
      "Epoch 1427 Batch    0/96   train_loss = 0.874\n",
      "Epoch 1427 Batch   32/96   train_loss = 0.951\n",
      "Epoch 1427 Batch   64/96   train_loss = 0.900\n",
      "Epoch 1428 Batch    0/96   train_loss = 0.865\n",
      "Epoch 1428 Batch   32/96   train_loss = 0.939\n",
      "Epoch 1428 Batch   64/96   train_loss = 0.895\n",
      "Epoch 1429 Batch    0/96   train_loss = 0.866\n",
      "Epoch 1429 Batch   32/96   train_loss = 0.944\n",
      "Epoch 1429 Batch   64/96   train_loss = 0.915\n",
      "Epoch 1430 Batch    0/96   train_loss = 0.892\n",
      "Epoch 1430 Batch   32/96   train_loss = 0.951\n",
      "Epoch 1430 Batch   64/96   train_loss = 0.901\n",
      "Epoch 1431 Batch    0/96   train_loss = 0.866\n",
      "Epoch 1431 Batch   32/96   train_loss = 0.947\n",
      "Epoch 1431 Batch   64/96   train_loss = 0.899\n",
      "Epoch 1432 Batch    0/96   train_loss = 0.862\n",
      "Epoch 1432 Batch   32/96   train_loss = 0.929\n",
      "Epoch 1432 Batch   64/96   train_loss = 0.904\n",
      "Epoch 1433 Batch    0/96   train_loss = 0.863\n",
      "Epoch 1433 Batch   32/96   train_loss = 0.948\n",
      "Epoch 1433 Batch   64/96   train_loss = 0.895\n",
      "Epoch 1434 Batch    0/96   train_loss = 0.856\n",
      "Epoch 1434 Batch   32/96   train_loss = 0.927\n",
      "Epoch 1434 Batch   64/96   train_loss = 0.903\n",
      "Epoch 1435 Batch    0/96   train_loss = 0.861\n",
      "Epoch 1435 Batch   32/96   train_loss = 0.935\n",
      "Epoch 1435 Batch   64/96   train_loss = 0.911\n",
      "Epoch 1436 Batch    0/96   train_loss = 0.861\n",
      "Epoch 1436 Batch   32/96   train_loss = 0.936\n",
      "Epoch 1436 Batch   64/96   train_loss = 0.893\n",
      "Epoch 1437 Batch    0/96   train_loss = 0.885\n",
      "Epoch 1437 Batch   32/96   train_loss = 0.950\n",
      "Epoch 1437 Batch   64/96   train_loss = 0.907\n",
      "Epoch 1438 Batch    0/96   train_loss = 0.875\n",
      "Epoch 1438 Batch   32/96   train_loss = 0.947\n",
      "Epoch 1438 Batch   64/96   train_loss = 0.925\n",
      "Epoch 1439 Batch    0/96   train_loss = 0.885\n",
      "Epoch 1439 Batch   32/96   train_loss = 0.949\n",
      "Epoch 1439 Batch   64/96   train_loss = 0.901\n",
      "Epoch 1440 Batch    0/96   train_loss = 0.877\n",
      "Epoch 1440 Batch   32/96   train_loss = 0.949\n",
      "Epoch 1440 Batch   64/96   train_loss = 0.902\n",
      "Epoch 1441 Batch    0/96   train_loss = 0.872\n",
      "Epoch 1441 Batch   32/96   train_loss = 0.949\n",
      "Epoch 1441 Batch   64/96   train_loss = 0.898\n",
      "Epoch 1442 Batch    0/96   train_loss = 0.873\n",
      "Epoch 1442 Batch   32/96   train_loss = 0.943\n",
      "Epoch 1442 Batch   64/96   train_loss = 0.900\n",
      "Epoch 1443 Batch    0/96   train_loss = 0.881\n",
      "Epoch 1443 Batch   32/96   train_loss = 0.950\n",
      "Epoch 1443 Batch   64/96   train_loss = 0.901\n",
      "Epoch 1444 Batch    0/96   train_loss = 0.867\n",
      "Epoch 1444 Batch   32/96   train_loss = 0.954\n",
      "Epoch 1444 Batch   64/96   train_loss = 0.910\n",
      "Epoch 1445 Batch    0/96   train_loss = 0.868\n",
      "Epoch 1445 Batch   32/96   train_loss = 0.955\n",
      "Epoch 1445 Batch   64/96   train_loss = 0.892\n",
      "Epoch 1446 Batch    0/96   train_loss = 0.878\n",
      "Epoch 1446 Batch   32/96   train_loss = 0.947\n",
      "Epoch 1446 Batch   64/96   train_loss = 0.906\n",
      "Epoch 1447 Batch    0/96   train_loss = 0.876\n",
      "Epoch 1447 Batch   32/96   train_loss = 0.966\n",
      "Epoch 1447 Batch   64/96   train_loss = 0.910\n",
      "Epoch 1448 Batch    0/96   train_loss = 0.887\n",
      "Epoch 1448 Batch   32/96   train_loss = 0.954\n",
      "Epoch 1448 Batch   64/96   train_loss = 0.905\n",
      "Epoch 1449 Batch    0/96   train_loss = 0.884\n",
      "Epoch 1449 Batch   32/96   train_loss = 0.944\n",
      "Epoch 1449 Batch   64/96   train_loss = 0.902\n",
      "Epoch 1450 Batch    0/96   train_loss = 0.876\n",
      "Epoch 1450 Batch   32/96   train_loss = 0.952\n",
      "Epoch 1450 Batch   64/96   train_loss = 0.896\n",
      "Epoch 1451 Batch    0/96   train_loss = 0.866\n",
      "Epoch 1451 Batch   32/96   train_loss = 0.951\n",
      "Epoch 1451 Batch   64/96   train_loss = 0.899\n",
      "Epoch 1452 Batch    0/96   train_loss = 0.868\n",
      "Epoch 1452 Batch   32/96   train_loss = 0.944\n",
      "Epoch 1452 Batch   64/96   train_loss = 0.909\n",
      "Epoch 1453 Batch    0/96   train_loss = 0.869\n",
      "Epoch 1453 Batch   32/96   train_loss = 0.943\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1453 Batch   64/96   train_loss = 0.907\n",
      "Epoch 1454 Batch    0/96   train_loss = 0.872\n",
      "Epoch 1454 Batch   32/96   train_loss = 0.945\n",
      "Epoch 1454 Batch   64/96   train_loss = 0.908\n",
      "Epoch 1455 Batch    0/96   train_loss = 0.893\n",
      "Epoch 1455 Batch   32/96   train_loss = 0.959\n",
      "Epoch 1455 Batch   64/96   train_loss = 0.896\n",
      "Epoch 1456 Batch    0/96   train_loss = 0.886\n",
      "Epoch 1456 Batch   32/96   train_loss = 0.955\n",
      "Epoch 1456 Batch   64/96   train_loss = 0.909\n",
      "Epoch 1457 Batch    0/96   train_loss = 0.875\n",
      "Epoch 1457 Batch   32/96   train_loss = 0.949\n",
      "Epoch 1457 Batch   64/96   train_loss = 0.910\n",
      "Epoch 1458 Batch    0/96   train_loss = 0.878\n",
      "Epoch 1458 Batch   32/96   train_loss = 0.943\n",
      "Epoch 1458 Batch   64/96   train_loss = 0.908\n",
      "Epoch 1459 Batch    0/96   train_loss = 0.869\n",
      "Epoch 1459 Batch   32/96   train_loss = 0.949\n",
      "Epoch 1459 Batch   64/96   train_loss = 0.895\n",
      "Epoch 1460 Batch    0/96   train_loss = 0.870\n",
      "Epoch 1460 Batch   32/96   train_loss = 0.944\n",
      "Epoch 1460 Batch   64/96   train_loss = 0.900\n",
      "Epoch 1461 Batch    0/96   train_loss = 0.868\n",
      "Epoch 1461 Batch   32/96   train_loss = 0.948\n",
      "Epoch 1461 Batch   64/96   train_loss = 0.890\n",
      "Epoch 1462 Batch    0/96   train_loss = 0.872\n",
      "Epoch 1462 Batch   32/96   train_loss = 0.948\n",
      "Epoch 1462 Batch   64/96   train_loss = 0.892\n",
      "Epoch 1463 Batch    0/96   train_loss = 0.871\n",
      "Epoch 1463 Batch   32/96   train_loss = 0.953\n",
      "Epoch 1463 Batch   64/96   train_loss = 0.902\n",
      "Epoch 1464 Batch    0/96   train_loss = 0.877\n",
      "Epoch 1464 Batch   32/96   train_loss = 0.947\n",
      "Epoch 1464 Batch   64/96   train_loss = 0.906\n",
      "Epoch 1465 Batch    0/96   train_loss = 0.887\n",
      "Epoch 1465 Batch   32/96   train_loss = 0.957\n",
      "Epoch 1465 Batch   64/96   train_loss = 0.905\n",
      "Epoch 1466 Batch    0/96   train_loss = 0.883\n",
      "Epoch 1466 Batch   32/96   train_loss = 0.957\n",
      "Epoch 1466 Batch   64/96   train_loss = 0.900\n",
      "Epoch 1467 Batch    0/96   train_loss = 0.888\n",
      "Epoch 1467 Batch   32/96   train_loss = 0.931\n",
      "Epoch 1467 Batch   64/96   train_loss = 0.892\n",
      "Epoch 1468 Batch    0/96   train_loss = 0.872\n",
      "Epoch 1468 Batch   32/96   train_loss = 0.936\n",
      "Epoch 1468 Batch   64/96   train_loss = 0.906\n",
      "Epoch 1469 Batch    0/96   train_loss = 0.874\n",
      "Epoch 1469 Batch   32/96   train_loss = 0.927\n",
      "Epoch 1469 Batch   64/96   train_loss = 0.896\n",
      "Epoch 1470 Batch    0/96   train_loss = 0.868\n",
      "Epoch 1470 Batch   32/96   train_loss = 0.930\n",
      "Epoch 1470 Batch   64/96   train_loss = 0.898\n",
      "Epoch 1471 Batch    0/96   train_loss = 0.877\n",
      "Epoch 1471 Batch   32/96   train_loss = 0.926\n",
      "Epoch 1471 Batch   64/96   train_loss = 0.909\n",
      "Epoch 1472 Batch    0/96   train_loss = 0.875\n",
      "Epoch 1472 Batch   32/96   train_loss = 0.941\n",
      "Epoch 1472 Batch   64/96   train_loss = 0.927\n",
      "Epoch 1473 Batch    0/96   train_loss = 0.885\n",
      "Epoch 1473 Batch   32/96   train_loss = 0.942\n",
      "Epoch 1473 Batch   64/96   train_loss = 0.907\n",
      "Epoch 1474 Batch    0/96   train_loss = 0.892\n",
      "Epoch 1474 Batch   32/96   train_loss = 0.948\n",
      "Epoch 1474 Batch   64/96   train_loss = 0.916\n",
      "Epoch 1475 Batch    0/96   train_loss = 0.866\n",
      "Epoch 1475 Batch   32/96   train_loss = 0.949\n",
      "Epoch 1475 Batch   64/96   train_loss = 0.913\n",
      "Epoch 1476 Batch    0/96   train_loss = 0.871\n",
      "Epoch 1476 Batch   32/96   train_loss = 0.936\n",
      "Epoch 1476 Batch   64/96   train_loss = 0.906\n",
      "Epoch 1477 Batch    0/96   train_loss = 0.878\n",
      "Epoch 1477 Batch   32/96   train_loss = 0.939\n",
      "Epoch 1477 Batch   64/96   train_loss = 0.918\n",
      "Epoch 1478 Batch    0/96   train_loss = 0.877\n",
      "Epoch 1478 Batch   32/96   train_loss = 0.935\n",
      "Epoch 1478 Batch   64/96   train_loss = 0.913\n",
      "Epoch 1479 Batch    0/96   train_loss = 0.877\n",
      "Epoch 1479 Batch   32/96   train_loss = 0.950\n",
      "Epoch 1479 Batch   64/96   train_loss = 0.908\n",
      "Epoch 1480 Batch    0/96   train_loss = 0.884\n",
      "Epoch 1480 Batch   32/96   train_loss = 0.935\n",
      "Epoch 1480 Batch   64/96   train_loss = 0.916\n",
      "Epoch 1481 Batch    0/96   train_loss = 0.872\n",
      "Epoch 1481 Batch   32/96   train_loss = 0.940\n",
      "Epoch 1481 Batch   64/96   train_loss = 0.912\n",
      "Epoch 1482 Batch    0/96   train_loss = 0.891\n",
      "Epoch 1482 Batch   32/96   train_loss = 0.948\n",
      "Epoch 1482 Batch   64/96   train_loss = 0.910\n",
      "Epoch 1483 Batch    0/96   train_loss = 0.886\n",
      "Epoch 1483 Batch   32/96   train_loss = 0.936\n",
      "Epoch 1483 Batch   64/96   train_loss = 0.916\n",
      "Epoch 1484 Batch    0/96   train_loss = 0.879\n",
      "Epoch 1484 Batch   32/96   train_loss = 0.962\n",
      "Epoch 1484 Batch   64/96   train_loss = 0.905\n",
      "Epoch 1485 Batch    0/96   train_loss = 0.890\n",
      "Epoch 1485 Batch   32/96   train_loss = 0.935\n",
      "Epoch 1485 Batch   64/96   train_loss = 0.925\n",
      "Epoch 1486 Batch    0/96   train_loss = 0.890\n",
      "Epoch 1486 Batch   32/96   train_loss = 0.939\n",
      "Epoch 1486 Batch   64/96   train_loss = 0.927\n",
      "Epoch 1487 Batch    0/96   train_loss = 0.913\n",
      "Epoch 1487 Batch   32/96   train_loss = 0.963\n",
      "Epoch 1487 Batch   64/96   train_loss = 0.927\n",
      "Epoch 1488 Batch    0/96   train_loss = 0.895\n",
      "Epoch 1488 Batch   32/96   train_loss = 0.957\n",
      "Epoch 1488 Batch   64/96   train_loss = 0.929\n",
      "Epoch 1489 Batch    0/96   train_loss = 0.891\n",
      "Epoch 1489 Batch   32/96   train_loss = 0.952\n",
      "Epoch 1489 Batch   64/96   train_loss = 0.921\n",
      "Epoch 1490 Batch    0/96   train_loss = 0.906\n",
      "Epoch 1490 Batch   32/96   train_loss = 0.964\n",
      "Epoch 1490 Batch   64/96   train_loss = 0.961\n",
      "Epoch 1491 Batch    0/96   train_loss = 0.899\n",
      "Epoch 1491 Batch   32/96   train_loss = 0.952\n",
      "Epoch 1491 Batch   64/96   train_loss = 0.921\n",
      "Epoch 1492 Batch    0/96   train_loss = 0.896\n",
      "Epoch 1492 Batch   32/96   train_loss = 0.963\n",
      "Epoch 1492 Batch   64/96   train_loss = 0.923\n",
      "Epoch 1493 Batch    0/96   train_loss = 0.906\n",
      "Epoch 1493 Batch   32/96   train_loss = 0.989\n",
      "Epoch 1493 Batch   64/96   train_loss = 0.931\n",
      "Epoch 1494 Batch    0/96   train_loss = 0.881\n",
      "Epoch 1494 Batch   32/96   train_loss = 0.976\n",
      "Epoch 1494 Batch   64/96   train_loss = 0.935\n",
      "Epoch 1495 Batch    0/96   train_loss = 0.901\n",
      "Epoch 1495 Batch   32/96   train_loss = 0.967\n",
      "Epoch 1495 Batch   64/96   train_loss = 0.923\n",
      "Epoch 1496 Batch    0/96   train_loss = 0.894\n",
      "Epoch 1496 Batch   32/96   train_loss = 0.954\n",
      "Epoch 1496 Batch   64/96   train_loss = 0.923\n",
      "Epoch 1497 Batch    0/96   train_loss = 0.884\n",
      "Epoch 1497 Batch   32/96   train_loss = 0.957\n",
      "Epoch 1497 Batch   64/96   train_loss = 0.937\n",
      "Epoch 1498 Batch    0/96   train_loss = 0.881\n",
      "Epoch 1498 Batch   32/96   train_loss = 0.954\n",
      "Epoch 1498 Batch   64/96   train_loss = 0.942\n",
      "Epoch 1499 Batch    0/96   train_loss = 0.896\n",
      "Epoch 1499 Batch   32/96   train_loss = 0.959\n",
      "Epoch 1499 Batch   64/96   train_loss = 0.937\n",
      "Epoch 1500 Batch    0/96   train_loss = 0.894\n",
      "Epoch 1500 Batch   32/96   train_loss = 0.962\n",
      "Epoch 1500 Batch   64/96   train_loss = 0.935\n",
      "Epoch 1501 Batch    0/96   train_loss = 0.892\n",
      "Epoch 1501 Batch   32/96   train_loss = 0.964\n",
      "Epoch 1501 Batch   64/96   train_loss = 0.927\n",
      "Epoch 1502 Batch    0/96   train_loss = 0.892\n",
      "Epoch 1502 Batch   32/96   train_loss = 0.951\n",
      "Epoch 1502 Batch   64/96   train_loss = 0.933\n",
      "Epoch 1503 Batch    0/96   train_loss = 0.888\n",
      "Epoch 1503 Batch   32/96   train_loss = 0.943\n",
      "Epoch 1503 Batch   64/96   train_loss = 0.929\n",
      "Epoch 1504 Batch    0/96   train_loss = 0.881\n",
      "Epoch 1504 Batch   32/96   train_loss = 0.959\n",
      "Epoch 1504 Batch   64/96   train_loss = 0.927\n",
      "Epoch 1505 Batch    0/96   train_loss = 0.893\n",
      "Epoch 1505 Batch   32/96   train_loss = 0.949\n",
      "Epoch 1505 Batch   64/96   train_loss = 0.928\n",
      "Epoch 1506 Batch    0/96   train_loss = 0.884\n",
      "Epoch 1506 Batch   32/96   train_loss = 0.953\n",
      "Epoch 1506 Batch   64/96   train_loss = 0.917\n",
      "Epoch 1507 Batch    0/96   train_loss = 0.869\n",
      "Epoch 1507 Batch   32/96   train_loss = 0.954\n",
      "Epoch 1507 Batch   64/96   train_loss = 0.919\n",
      "Epoch 1508 Batch    0/96   train_loss = 0.877\n",
      "Epoch 1508 Batch   32/96   train_loss = 0.939\n",
      "Epoch 1508 Batch   64/96   train_loss = 0.924\n",
      "Epoch 1509 Batch    0/96   train_loss = 0.870\n",
      "Epoch 1509 Batch   32/96   train_loss = 0.934\n",
      "Epoch 1509 Batch   64/96   train_loss = 0.907\n",
      "Epoch 1510 Batch    0/96   train_loss = 0.887\n",
      "Epoch 1510 Batch   32/96   train_loss = 0.940\n",
      "Epoch 1510 Batch   64/96   train_loss = 0.891\n",
      "Epoch 1511 Batch    0/96   train_loss = 0.863\n",
      "Epoch 1511 Batch   32/96   train_loss = 0.938\n",
      "Epoch 1511 Batch   64/96   train_loss = 0.882\n",
      "Epoch 1512 Batch    0/96   train_loss = 0.864\n",
      "Epoch 1512 Batch   32/96   train_loss = 0.925\n",
      "Epoch 1512 Batch   64/96   train_loss = 0.895\n",
      "Epoch 1513 Batch    0/96   train_loss = 0.861\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1513 Batch   32/96   train_loss = 0.946\n",
      "Epoch 1513 Batch   64/96   train_loss = 0.898\n",
      "Epoch 1514 Batch    0/96   train_loss = 0.865\n",
      "Epoch 1514 Batch   32/96   train_loss = 0.940\n",
      "Epoch 1514 Batch   64/96   train_loss = 0.907\n",
      "Epoch 1515 Batch    0/96   train_loss = 0.867\n",
      "Epoch 1515 Batch   32/96   train_loss = 0.923\n",
      "Epoch 1515 Batch   64/96   train_loss = 0.903\n",
      "Epoch 1516 Batch    0/96   train_loss = 0.855\n",
      "Epoch 1516 Batch   32/96   train_loss = 0.930\n",
      "Epoch 1516 Batch   64/96   train_loss = 0.907\n",
      "Epoch 1517 Batch    0/96   train_loss = 0.874\n",
      "Epoch 1517 Batch   32/96   train_loss = 0.941\n",
      "Epoch 1517 Batch   64/96   train_loss = 0.921\n",
      "Epoch 1518 Batch    0/96   train_loss = 0.870\n",
      "Epoch 1518 Batch   32/96   train_loss = 0.939\n",
      "Epoch 1518 Batch   64/96   train_loss = 0.910\n",
      "Epoch 1519 Batch    0/96   train_loss = 0.874\n",
      "Epoch 1519 Batch   32/96   train_loss = 0.953\n",
      "Epoch 1519 Batch   64/96   train_loss = 0.926\n",
      "Epoch 1520 Batch    0/96   train_loss = 0.886\n",
      "Epoch 1520 Batch   32/96   train_loss = 0.949\n",
      "Epoch 1520 Batch   64/96   train_loss = 0.918\n",
      "Epoch 1521 Batch    0/96   train_loss = 0.884\n",
      "Epoch 1521 Batch   32/96   train_loss = 0.959\n",
      "Epoch 1521 Batch   64/96   train_loss = 0.917\n",
      "Epoch 1522 Batch    0/96   train_loss = 0.901\n",
      "Epoch 1522 Batch   32/96   train_loss = 0.941\n",
      "Epoch 1522 Batch   64/96   train_loss = 0.912\n",
      "Epoch 1523 Batch    0/96   train_loss = 0.884\n",
      "Epoch 1523 Batch   32/96   train_loss = 0.975\n",
      "Epoch 1523 Batch   64/96   train_loss = 0.917\n",
      "Epoch 1524 Batch    0/96   train_loss = 0.901\n",
      "Epoch 1524 Batch   32/96   train_loss = 0.964\n",
      "Epoch 1524 Batch   64/96   train_loss = 0.922\n",
      "Epoch 1525 Batch    0/96   train_loss = 0.891\n",
      "Epoch 1525 Batch   32/96   train_loss = 0.954\n",
      "Epoch 1525 Batch   64/96   train_loss = 0.905\n",
      "Epoch 1526 Batch    0/96   train_loss = 0.889\n",
      "Epoch 1526 Batch   32/96   train_loss = 0.952\n",
      "Epoch 1526 Batch   64/96   train_loss = 0.914\n",
      "Epoch 1527 Batch    0/96   train_loss = 0.879\n",
      "Epoch 1527 Batch   32/96   train_loss = 0.958\n",
      "Epoch 1527 Batch   64/96   train_loss = 0.917\n",
      "Epoch 1528 Batch    0/96   train_loss = 0.899\n",
      "Epoch 1528 Batch   32/96   train_loss = 0.950\n",
      "Epoch 1528 Batch   64/96   train_loss = 0.916\n",
      "Epoch 1529 Batch    0/96   train_loss = 0.893\n",
      "Epoch 1529 Batch   32/96   train_loss = 0.958\n",
      "Epoch 1529 Batch   64/96   train_loss = 0.928\n",
      "Epoch 1530 Batch    0/96   train_loss = 0.879\n",
      "Epoch 1530 Batch   32/96   train_loss = 0.951\n",
      "Epoch 1530 Batch   64/96   train_loss = 0.918\n",
      "Epoch 1531 Batch    0/96   train_loss = 0.882\n",
      "Epoch 1531 Batch   32/96   train_loss = 0.966\n",
      "Epoch 1531 Batch   64/96   train_loss = 0.906\n",
      "Epoch 1532 Batch    0/96   train_loss = 0.875\n",
      "Epoch 1532 Batch   32/96   train_loss = 0.952\n",
      "Epoch 1532 Batch   64/96   train_loss = 0.901\n",
      "Epoch 1533 Batch    0/96   train_loss = 0.869\n",
      "Epoch 1533 Batch   32/96   train_loss = 0.934\n",
      "Epoch 1533 Batch   64/96   train_loss = 0.916\n",
      "Epoch 1534 Batch    0/96   train_loss = 0.874\n",
      "Epoch 1534 Batch   32/96   train_loss = 0.947\n",
      "Epoch 1534 Batch   64/96   train_loss = 0.903\n",
      "Epoch 1535 Batch    0/96   train_loss = 0.881\n",
      "Epoch 1535 Batch   32/96   train_loss = 0.936\n",
      "Epoch 1535 Batch   64/96   train_loss = 0.905\n",
      "Epoch 1536 Batch    0/96   train_loss = 0.881\n",
      "Epoch 1536 Batch   32/96   train_loss = 0.949\n",
      "Epoch 1536 Batch   64/96   train_loss = 0.921\n",
      "Epoch 1537 Batch    0/96   train_loss = 0.883\n",
      "Epoch 1537 Batch   32/96   train_loss = 0.950\n",
      "Epoch 1537 Batch   64/96   train_loss = 0.908\n",
      "Epoch 1538 Batch    0/96   train_loss = 0.890\n",
      "Epoch 1538 Batch   32/96   train_loss = 0.962\n",
      "Epoch 1538 Batch   64/96   train_loss = 0.922\n",
      "Epoch 1539 Batch    0/96   train_loss = 0.893\n",
      "Epoch 1539 Batch   32/96   train_loss = 0.952\n",
      "Epoch 1539 Batch   64/96   train_loss = 0.910\n",
      "Epoch 1540 Batch    0/96   train_loss = 0.880\n",
      "Epoch 1540 Batch   32/96   train_loss = 0.952\n",
      "Epoch 1540 Batch   64/96   train_loss = 0.932\n",
      "Epoch 1541 Batch    0/96   train_loss = 0.883\n",
      "Epoch 1541 Batch   32/96   train_loss = 0.953\n",
      "Epoch 1541 Batch   64/96   train_loss = 0.944\n",
      "Epoch 1542 Batch    0/96   train_loss = 0.883\n",
      "Epoch 1542 Batch   32/96   train_loss = 0.967\n",
      "Epoch 1542 Batch   64/96   train_loss = 0.934\n",
      "Epoch 1543 Batch    0/96   train_loss = 0.882\n",
      "Epoch 1543 Batch   32/96   train_loss = 0.953\n",
      "Epoch 1543 Batch   64/96   train_loss = 0.936\n",
      "Epoch 1544 Batch    0/96   train_loss = 0.889\n",
      "Epoch 1544 Batch   32/96   train_loss = 0.956\n",
      "Epoch 1544 Batch   64/96   train_loss = 0.924\n",
      "Epoch 1545 Batch    0/96   train_loss = 0.893\n",
      "Epoch 1545 Batch   32/96   train_loss = 0.942\n",
      "Epoch 1545 Batch   64/96   train_loss = 0.920\n",
      "Epoch 1546 Batch    0/96   train_loss = 0.909\n",
      "Epoch 1546 Batch   32/96   train_loss = 0.951\n",
      "Epoch 1546 Batch   64/96   train_loss = 0.915\n",
      "Epoch 1547 Batch    0/96   train_loss = 0.885\n",
      "Epoch 1547 Batch   32/96   train_loss = 0.951\n",
      "Epoch 1547 Batch   64/96   train_loss = 0.931\n",
      "Epoch 1548 Batch    0/96   train_loss = 0.900\n",
      "Epoch 1548 Batch   32/96   train_loss = 0.950\n",
      "Epoch 1548 Batch   64/96   train_loss = 0.922\n",
      "Epoch 1549 Batch    0/96   train_loss = 0.888\n",
      "Epoch 1549 Batch   32/96   train_loss = 0.953\n",
      "Epoch 1549 Batch   64/96   train_loss = 0.926\n",
      "Epoch 1550 Batch    0/96   train_loss = 0.879\n",
      "Epoch 1550 Batch   32/96   train_loss = 0.944\n",
      "Epoch 1550 Batch   64/96   train_loss = 0.917\n",
      "Epoch 1551 Batch    0/96   train_loss = 0.892\n",
      "Epoch 1551 Batch   32/96   train_loss = 0.946\n",
      "Epoch 1551 Batch   64/96   train_loss = 0.926\n",
      "Epoch 1552 Batch    0/96   train_loss = 0.886\n",
      "Epoch 1552 Batch   32/96   train_loss = 0.956\n",
      "Epoch 1552 Batch   64/96   train_loss = 0.908\n",
      "Epoch 1553 Batch    0/96   train_loss = 0.877\n",
      "Epoch 1553 Batch   32/96   train_loss = 0.957\n",
      "Epoch 1553 Batch   64/96   train_loss = 0.923\n",
      "Epoch 1554 Batch    0/96   train_loss = 0.874\n",
      "Epoch 1554 Batch   32/96   train_loss = 0.934\n",
      "Epoch 1554 Batch   64/96   train_loss = 0.914\n",
      "Epoch 1555 Batch    0/96   train_loss = 0.870\n",
      "Epoch 1555 Batch   32/96   train_loss = 0.951\n",
      "Epoch 1555 Batch   64/96   train_loss = 0.917\n",
      "Epoch 1556 Batch    0/96   train_loss = 0.882\n",
      "Epoch 1556 Batch   32/96   train_loss = 0.944\n",
      "Epoch 1556 Batch   64/96   train_loss = 0.912\n",
      "Epoch 1557 Batch    0/96   train_loss = 0.883\n",
      "Epoch 1557 Batch   32/96   train_loss = 0.943\n",
      "Epoch 1557 Batch   64/96   train_loss = 0.922\n",
      "Epoch 1558 Batch    0/96   train_loss = 0.885\n",
      "Epoch 1558 Batch   32/96   train_loss = 0.934\n",
      "Epoch 1558 Batch   64/96   train_loss = 0.913\n",
      "Epoch 1559 Batch    0/96   train_loss = 0.875\n",
      "Epoch 1559 Batch   32/96   train_loss = 0.935\n",
      "Epoch 1559 Batch   64/96   train_loss = 0.913\n",
      "Epoch 1560 Batch    0/96   train_loss = 0.870\n",
      "Epoch 1560 Batch   32/96   train_loss = 0.935\n",
      "Epoch 1560 Batch   64/96   train_loss = 0.914\n",
      "Epoch 1561 Batch    0/96   train_loss = 0.881\n",
      "Epoch 1561 Batch   32/96   train_loss = 0.940\n",
      "Epoch 1561 Batch   64/96   train_loss = 0.901\n",
      "Epoch 1562 Batch    0/96   train_loss = 0.880\n",
      "Epoch 1562 Batch   32/96   train_loss = 0.932\n",
      "Epoch 1562 Batch   64/96   train_loss = 0.899\n",
      "Epoch 1563 Batch    0/96   train_loss = 0.872\n",
      "Epoch 1563 Batch   32/96   train_loss = 0.932\n",
      "Epoch 1563 Batch   64/96   train_loss = 0.903\n",
      "Epoch 1564 Batch    0/96   train_loss = 0.893\n",
      "Epoch 1564 Batch   32/96   train_loss = 0.931\n",
      "Epoch 1564 Batch   64/96   train_loss = 0.919\n",
      "Epoch 1565 Batch    0/96   train_loss = 0.876\n",
      "Epoch 1565 Batch   32/96   train_loss = 0.932\n",
      "Epoch 1565 Batch   64/96   train_loss = 0.909\n",
      "Epoch 1566 Batch    0/96   train_loss = 0.885\n",
      "Epoch 1566 Batch   32/96   train_loss = 0.938\n",
      "Epoch 1566 Batch   64/96   train_loss = 0.913\n",
      "Epoch 1567 Batch    0/96   train_loss = 0.874\n",
      "Epoch 1567 Batch   32/96   train_loss = 0.931\n",
      "Epoch 1567 Batch   64/96   train_loss = 0.902\n",
      "Epoch 1568 Batch    0/96   train_loss = 0.876\n",
      "Epoch 1568 Batch   32/96   train_loss = 0.923\n",
      "Epoch 1568 Batch   64/96   train_loss = 0.916\n",
      "Epoch 1569 Batch    0/96   train_loss = 0.888\n",
      "Epoch 1569 Batch   32/96   train_loss = 0.937\n",
      "Epoch 1569 Batch   64/96   train_loss = 0.920\n",
      "Epoch 1570 Batch    0/96   train_loss = 0.882\n",
      "Epoch 1570 Batch   32/96   train_loss = 0.947\n",
      "Epoch 1570 Batch   64/96   train_loss = 0.930\n",
      "Epoch 1571 Batch    0/96   train_loss = 0.887\n",
      "Epoch 1571 Batch   32/96   train_loss = 0.943\n",
      "Epoch 1571 Batch   64/96   train_loss = 0.934\n",
      "Epoch 1572 Batch    0/96   train_loss = 0.881\n",
      "Epoch 1572 Batch   32/96   train_loss = 0.951\n",
      "Epoch 1572 Batch   64/96   train_loss = 0.908\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1573 Batch    0/96   train_loss = 0.864\n",
      "Epoch 1573 Batch   32/96   train_loss = 0.958\n",
      "Epoch 1573 Batch   64/96   train_loss = 0.912\n",
      "Epoch 1574 Batch    0/96   train_loss = 0.871\n",
      "Epoch 1574 Batch   32/96   train_loss = 0.946\n",
      "Epoch 1574 Batch   64/96   train_loss = 0.912\n",
      "Epoch 1575 Batch    0/96   train_loss = 0.874\n",
      "Epoch 1575 Batch   32/96   train_loss = 0.939\n",
      "Epoch 1575 Batch   64/96   train_loss = 0.914\n",
      "Epoch 1576 Batch    0/96   train_loss = 0.891\n",
      "Epoch 1576 Batch   32/96   train_loss = 0.939\n",
      "Epoch 1576 Batch   64/96   train_loss = 0.908\n",
      "Epoch 1577 Batch    0/96   train_loss = 0.883\n",
      "Epoch 1577 Batch   32/96   train_loss = 0.952\n",
      "Epoch 1577 Batch   64/96   train_loss = 0.910\n",
      "Epoch 1578 Batch    0/96   train_loss = 0.868\n",
      "Epoch 1578 Batch   32/96   train_loss = 0.938\n",
      "Epoch 1578 Batch   64/96   train_loss = 0.897\n",
      "Epoch 1579 Batch    0/96   train_loss = 0.857\n",
      "Epoch 1579 Batch   32/96   train_loss = 0.938\n",
      "Epoch 1579 Batch   64/96   train_loss = 0.901\n",
      "Epoch 1580 Batch    0/96   train_loss = 0.864\n",
      "Epoch 1580 Batch   32/96   train_loss = 0.935\n",
      "Epoch 1580 Batch   64/96   train_loss = 0.907\n",
      "Epoch 1581 Batch    0/96   train_loss = 0.865\n",
      "Epoch 1581 Batch   32/96   train_loss = 0.931\n",
      "Epoch 1581 Batch   64/96   train_loss = 0.899\n",
      "Epoch 1582 Batch    0/96   train_loss = 0.879\n",
      "Epoch 1582 Batch   32/96   train_loss = 0.943\n",
      "Epoch 1582 Batch   64/96   train_loss = 0.911\n",
      "Epoch 1583 Batch    0/96   train_loss = 0.882\n",
      "Epoch 1583 Batch   32/96   train_loss = 0.929\n",
      "Epoch 1583 Batch   64/96   train_loss = 0.919\n",
      "Epoch 1584 Batch    0/96   train_loss = 0.884\n",
      "Epoch 1584 Batch   32/96   train_loss = 0.933\n",
      "Epoch 1584 Batch   64/96   train_loss = 0.913\n",
      "Epoch 1585 Batch    0/96   train_loss = 0.889\n",
      "Epoch 1585 Batch   32/96   train_loss = 0.925\n",
      "Epoch 1585 Batch   64/96   train_loss = 0.909\n",
      "Epoch 1586 Batch    0/96   train_loss = 0.869\n",
      "Epoch 1586 Batch   32/96   train_loss = 0.935\n",
      "Epoch 1586 Batch   64/96   train_loss = 0.922\n",
      "Epoch 1587 Batch    0/96   train_loss = 0.867\n",
      "Epoch 1587 Batch   32/96   train_loss = 0.945\n",
      "Epoch 1587 Batch   64/96   train_loss = 0.925\n",
      "Epoch 1588 Batch    0/96   train_loss = 0.890\n",
      "Epoch 1588 Batch   32/96   train_loss = 0.949\n",
      "Epoch 1588 Batch   64/96   train_loss = 0.914\n",
      "Epoch 1589 Batch    0/96   train_loss = 0.881\n",
      "Epoch 1589 Batch   32/96   train_loss = 0.952\n",
      "Epoch 1589 Batch   64/96   train_loss = 0.902\n",
      "Epoch 1590 Batch    0/96   train_loss = 0.874\n",
      "Epoch 1590 Batch   32/96   train_loss = 0.949\n",
      "Epoch 1590 Batch   64/96   train_loss = 0.906\n",
      "Epoch 1591 Batch    0/96   train_loss = 0.878\n",
      "Epoch 1591 Batch   32/96   train_loss = 0.959\n",
      "Epoch 1591 Batch   64/96   train_loss = 0.912\n",
      "Epoch 1592 Batch    0/96   train_loss = 0.875\n",
      "Epoch 1592 Batch   32/96   train_loss = 0.956\n",
      "Epoch 1592 Batch   64/96   train_loss = 0.908\n",
      "Epoch 1593 Batch    0/96   train_loss = 0.887\n",
      "Epoch 1593 Batch   32/96   train_loss = 0.986\n",
      "Epoch 1593 Batch   64/96   train_loss = 0.926\n",
      "Epoch 1594 Batch    0/96   train_loss = 0.902\n",
      "Epoch 1594 Batch   32/96   train_loss = 0.965\n",
      "Epoch 1594 Batch   64/96   train_loss = 0.916\n",
      "Epoch 1595 Batch    0/96   train_loss = 0.886\n",
      "Epoch 1595 Batch   32/96   train_loss = 0.963\n",
      "Epoch 1595 Batch   64/96   train_loss = 0.902\n",
      "Epoch 1596 Batch    0/96   train_loss = 0.875\n",
      "Epoch 1596 Batch   32/96   train_loss = 0.947\n",
      "Epoch 1596 Batch   64/96   train_loss = 0.908\n",
      "Epoch 1597 Batch    0/96   train_loss = 0.876\n",
      "Epoch 1597 Batch   32/96   train_loss = 0.951\n",
      "Epoch 1597 Batch   64/96   train_loss = 0.909\n",
      "Epoch 1598 Batch    0/96   train_loss = 0.859\n",
      "Epoch 1598 Batch   32/96   train_loss = 0.946\n",
      "Epoch 1598 Batch   64/96   train_loss = 0.904\n",
      "Epoch 1599 Batch    0/96   train_loss = 0.866\n",
      "Epoch 1599 Batch   32/96   train_loss = 0.952\n",
      "Epoch 1599 Batch   64/96   train_loss = 0.905\n",
      "Epoch 1600 Batch    0/96   train_loss = 0.868\n",
      "Epoch 1600 Batch   32/96   train_loss = 0.941\n",
      "Epoch 1600 Batch   64/96   train_loss = 0.901\n",
      "Epoch 1601 Batch    0/96   train_loss = 0.856\n",
      "Epoch 1601 Batch   32/96   train_loss = 0.945\n",
      "Epoch 1601 Batch   64/96   train_loss = 0.891\n",
      "Epoch 1602 Batch    0/96   train_loss = 0.856\n",
      "Epoch 1602 Batch   32/96   train_loss = 0.933\n",
      "Epoch 1602 Batch   64/96   train_loss = 0.895\n",
      "Epoch 1603 Batch    0/96   train_loss = 0.854\n",
      "Epoch 1603 Batch   32/96   train_loss = 0.929\n",
      "Epoch 1603 Batch   64/96   train_loss = 0.892\n",
      "Epoch 1604 Batch    0/96   train_loss = 0.853\n",
      "Epoch 1604 Batch   32/96   train_loss = 0.926\n",
      "Epoch 1604 Batch   64/96   train_loss = 0.880\n",
      "Epoch 1605 Batch    0/96   train_loss = 0.841\n",
      "Epoch 1605 Batch   32/96   train_loss = 0.926\n",
      "Epoch 1605 Batch   64/96   train_loss = 0.891\n",
      "Epoch 1606 Batch    0/96   train_loss = 0.868\n",
      "Epoch 1606 Batch   32/96   train_loss = 0.948\n",
      "Epoch 1606 Batch   64/96   train_loss = 0.905\n",
      "Epoch 1607 Batch    0/96   train_loss = 0.859\n",
      "Epoch 1607 Batch   32/96   train_loss = 0.936\n",
      "Epoch 1607 Batch   64/96   train_loss = 0.888\n",
      "Epoch 1608 Batch    0/96   train_loss = 0.866\n",
      "Epoch 1608 Batch   32/96   train_loss = 0.947\n",
      "Epoch 1608 Batch   64/96   train_loss = 0.904\n",
      "Epoch 1609 Batch    0/96   train_loss = 0.861\n",
      "Epoch 1609 Batch   32/96   train_loss = 0.950\n",
      "Epoch 1609 Batch   64/96   train_loss = 0.897\n",
      "Epoch 1610 Batch    0/96   train_loss = 0.875\n",
      "Epoch 1610 Batch   32/96   train_loss = 0.950\n",
      "Epoch 1610 Batch   64/96   train_loss = 0.892\n",
      "Epoch 1611 Batch    0/96   train_loss = 0.881\n",
      "Epoch 1611 Batch   32/96   train_loss = 0.949\n",
      "Epoch 1611 Batch   64/96   train_loss = 0.894\n",
      "Epoch 1612 Batch    0/96   train_loss = 0.871\n",
      "Epoch 1612 Batch   32/96   train_loss = 0.953\n",
      "Epoch 1612 Batch   64/96   train_loss = 0.900\n",
      "Epoch 1613 Batch    0/96   train_loss = 0.880\n",
      "Epoch 1613 Batch   32/96   train_loss = 0.957\n",
      "Epoch 1613 Batch   64/96   train_loss = 0.900\n",
      "Epoch 1614 Batch    0/96   train_loss = 0.872\n",
      "Epoch 1614 Batch   32/96   train_loss = 0.951\n",
      "Epoch 1614 Batch   64/96   train_loss = 0.914\n",
      "Epoch 1615 Batch    0/96   train_loss = 0.877\n",
      "Epoch 1615 Batch   32/96   train_loss = 0.959\n",
      "Epoch 1615 Batch   64/96   train_loss = 0.905\n",
      "Epoch 1616 Batch    0/96   train_loss = 0.894\n",
      "Epoch 1616 Batch   32/96   train_loss = 0.959\n",
      "Epoch 1616 Batch   64/96   train_loss = 0.919\n",
      "Epoch 1617 Batch    0/96   train_loss = 0.888\n",
      "Epoch 1617 Batch   32/96   train_loss = 0.952\n",
      "Epoch 1617 Batch   64/96   train_loss = 0.916\n",
      "Epoch 1618 Batch    0/96   train_loss = 0.878\n",
      "Epoch 1618 Batch   32/96   train_loss = 0.958\n",
      "Epoch 1618 Batch   64/96   train_loss = 0.912\n",
      "Epoch 1619 Batch    0/96   train_loss = 0.877\n",
      "Epoch 1619 Batch   32/96   train_loss = 0.955\n",
      "Epoch 1619 Batch   64/96   train_loss = 0.902\n",
      "Epoch 1620 Batch    0/96   train_loss = 0.876\n",
      "Epoch 1620 Batch   32/96   train_loss = 0.964\n",
      "Epoch 1620 Batch   64/96   train_loss = 0.937\n",
      "Epoch 1621 Batch    0/96   train_loss = 0.882\n",
      "Epoch 1621 Batch   32/96   train_loss = 0.952\n",
      "Epoch 1621 Batch   64/96   train_loss = 0.911\n",
      "Epoch 1622 Batch    0/96   train_loss = 0.886\n",
      "Epoch 1622 Batch   32/96   train_loss = 0.947\n",
      "Epoch 1622 Batch   64/96   train_loss = 0.919\n",
      "Epoch 1623 Batch    0/96   train_loss = 0.882\n",
      "Epoch 1623 Batch   32/96   train_loss = 0.942\n",
      "Epoch 1623 Batch   64/96   train_loss = 0.919\n",
      "Epoch 1624 Batch    0/96   train_loss = 0.881\n",
      "Epoch 1624 Batch   32/96   train_loss = 0.951\n",
      "Epoch 1624 Batch   64/96   train_loss = 0.914\n",
      "Epoch 1625 Batch    0/96   train_loss = 0.888\n",
      "Epoch 1625 Batch   32/96   train_loss = 0.947\n",
      "Epoch 1625 Batch   64/96   train_loss = 0.909\n",
      "Epoch 1626 Batch    0/96   train_loss = 0.874\n",
      "Epoch 1626 Batch   32/96   train_loss = 0.944\n",
      "Epoch 1626 Batch   64/96   train_loss = 0.908\n",
      "Epoch 1627 Batch    0/96   train_loss = 0.879\n",
      "Epoch 1627 Batch   32/96   train_loss = 0.951\n",
      "Epoch 1627 Batch   64/96   train_loss = 0.918\n",
      "Epoch 1628 Batch    0/96   train_loss = 0.890\n",
      "Epoch 1628 Batch   32/96   train_loss = 0.962\n",
      "Epoch 1628 Batch   64/96   train_loss = 0.912\n",
      "Epoch 1629 Batch    0/96   train_loss = 0.904\n",
      "Epoch 1629 Batch   32/96   train_loss = 0.959\n",
      "Epoch 1629 Batch   64/96   train_loss = 0.917\n",
      "Epoch 1630 Batch    0/96   train_loss = 0.883\n",
      "Epoch 1630 Batch   32/96   train_loss = 0.961\n",
      "Epoch 1630 Batch   64/96   train_loss = 0.918\n",
      "Epoch 1631 Batch    0/96   train_loss = 0.872\n",
      "Epoch 1631 Batch   32/96   train_loss = 0.947\n",
      "Epoch 1631 Batch   64/96   train_loss = 0.925\n",
      "Epoch 1632 Batch    0/96   train_loss = 0.879\n",
      "Epoch 1632 Batch   32/96   train_loss = 0.961\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1632 Batch   64/96   train_loss = 0.930\n",
      "Epoch 1633 Batch    0/96   train_loss = 0.891\n",
      "Epoch 1633 Batch   32/96   train_loss = 0.962\n",
      "Epoch 1633 Batch   64/96   train_loss = 0.940\n",
      "Epoch 1634 Batch    0/96   train_loss = 0.892\n",
      "Epoch 1634 Batch   32/96   train_loss = 0.946\n",
      "Epoch 1634 Batch   64/96   train_loss = 0.924\n",
      "Epoch 1635 Batch    0/96   train_loss = 0.883\n",
      "Epoch 1635 Batch   32/96   train_loss = 0.967\n",
      "Epoch 1635 Batch   64/96   train_loss = 0.917\n",
      "Epoch 1636 Batch    0/96   train_loss = 0.878\n",
      "Epoch 1636 Batch   32/96   train_loss = 0.958\n",
      "Epoch 1636 Batch   64/96   train_loss = 0.915\n",
      "Epoch 1637 Batch    0/96   train_loss = 0.887\n",
      "Epoch 1637 Batch   32/96   train_loss = 0.958\n",
      "Epoch 1637 Batch   64/96   train_loss = 0.910\n",
      "Epoch 1638 Batch    0/96   train_loss = 0.886\n",
      "Epoch 1638 Batch   32/96   train_loss = 0.967\n",
      "Epoch 1638 Batch   64/96   train_loss = 0.919\n",
      "Epoch 1639 Batch    0/96   train_loss = 0.887\n",
      "Epoch 1639 Batch   32/96   train_loss = 0.970\n",
      "Epoch 1639 Batch   64/96   train_loss = 0.909\n",
      "Epoch 1640 Batch    0/96   train_loss = 0.873\n",
      "Epoch 1640 Batch   32/96   train_loss = 0.947\n",
      "Epoch 1640 Batch   64/96   train_loss = 0.898\n",
      "Epoch 1641 Batch    0/96   train_loss = 0.880\n",
      "Epoch 1641 Batch   32/96   train_loss = 0.953\n",
      "Epoch 1641 Batch   64/96   train_loss = 0.903\n",
      "Epoch 1642 Batch    0/96   train_loss = 0.876\n",
      "Epoch 1642 Batch   32/96   train_loss = 0.947\n",
      "Epoch 1642 Batch   64/96   train_loss = 0.907\n",
      "Epoch 1643 Batch    0/96   train_loss = 0.892\n",
      "Epoch 1643 Batch   32/96   train_loss = 0.942\n",
      "Epoch 1643 Batch   64/96   train_loss = 0.908\n",
      "Epoch 1644 Batch    0/96   train_loss = 0.875\n",
      "Epoch 1644 Batch   32/96   train_loss = 0.957\n",
      "Epoch 1644 Batch   64/96   train_loss = 0.908\n",
      "Epoch 1645 Batch    0/96   train_loss = 0.874\n",
      "Epoch 1645 Batch   32/96   train_loss = 0.945\n",
      "Epoch 1645 Batch   64/96   train_loss = 0.913\n",
      "Epoch 1646 Batch    0/96   train_loss = 0.899\n",
      "Epoch 1646 Batch   32/96   train_loss = 0.961\n",
      "Epoch 1646 Batch   64/96   train_loss = 0.911\n",
      "Epoch 1647 Batch    0/96   train_loss = 0.894\n",
      "Epoch 1647 Batch   32/96   train_loss = 0.957\n",
      "Epoch 1647 Batch   64/96   train_loss = 0.914\n",
      "Epoch 1648 Batch    0/96   train_loss = 0.896\n",
      "Epoch 1648 Batch   32/96   train_loss = 0.939\n",
      "Epoch 1648 Batch   64/96   train_loss = 0.902\n",
      "Epoch 1649 Batch    0/96   train_loss = 0.890\n",
      "Epoch 1649 Batch   32/96   train_loss = 0.939\n",
      "Epoch 1649 Batch   64/96   train_loss = 0.919\n",
      "Epoch 1650 Batch    0/96   train_loss = 0.893\n",
      "Epoch 1650 Batch   32/96   train_loss = 0.955\n",
      "Epoch 1650 Batch   64/96   train_loss = 0.911\n",
      "Epoch 1651 Batch    0/96   train_loss = 0.906\n",
      "Epoch 1651 Batch   32/96   train_loss = 0.948\n",
      "Epoch 1651 Batch   64/96   train_loss = 0.914\n",
      "Epoch 1652 Batch    0/96   train_loss = 0.888\n",
      "Epoch 1652 Batch   32/96   train_loss = 0.948\n",
      "Epoch 1652 Batch   64/96   train_loss = 0.912\n",
      "Epoch 1653 Batch    0/96   train_loss = 0.900\n",
      "Epoch 1653 Batch   32/96   train_loss = 0.961\n",
      "Epoch 1653 Batch   64/96   train_loss = 0.918\n",
      "Epoch 1654 Batch    0/96   train_loss = 0.923\n",
      "Epoch 1654 Batch   32/96   train_loss = 0.949\n",
      "Epoch 1654 Batch   64/96   train_loss = 0.912\n",
      "Epoch 1655 Batch    0/96   train_loss = 0.894\n",
      "Epoch 1655 Batch   32/96   train_loss = 0.973\n",
      "Epoch 1655 Batch   64/96   train_loss = 0.903\n",
      "Epoch 1656 Batch    0/96   train_loss = 0.887\n",
      "Epoch 1656 Batch   32/96   train_loss = 0.946\n",
      "Epoch 1656 Batch   64/96   train_loss = 0.903\n",
      "Epoch 1657 Batch    0/96   train_loss = 0.885\n",
      "Epoch 1657 Batch   32/96   train_loss = 0.947\n",
      "Epoch 1657 Batch   64/96   train_loss = 0.899\n",
      "Epoch 1658 Batch    0/96   train_loss = 0.868\n",
      "Epoch 1658 Batch   32/96   train_loss = 0.934\n",
      "Epoch 1658 Batch   64/96   train_loss = 0.890\n",
      "Epoch 1659 Batch    0/96   train_loss = 0.871\n",
      "Epoch 1659 Batch   32/96   train_loss = 0.952\n",
      "Epoch 1659 Batch   64/96   train_loss = 0.906\n",
      "Epoch 1660 Batch    0/96   train_loss = 0.875\n",
      "Epoch 1660 Batch   32/96   train_loss = 0.936\n",
      "Epoch 1660 Batch   64/96   train_loss = 0.907\n",
      "Epoch 1661 Batch    0/96   train_loss = 0.884\n",
      "Epoch 1661 Batch   32/96   train_loss = 0.934\n",
      "Epoch 1661 Batch   64/96   train_loss = 0.899\n",
      "Epoch 1662 Batch    0/96   train_loss = 0.880\n",
      "Epoch 1662 Batch   32/96   train_loss = 0.936\n",
      "Epoch 1662 Batch   64/96   train_loss = 0.928\n",
      "Epoch 1663 Batch    0/96   train_loss = 0.895\n",
      "Epoch 1663 Batch   32/96   train_loss = 0.950\n",
      "Epoch 1663 Batch   64/96   train_loss = 0.917\n",
      "Epoch 1664 Batch    0/96   train_loss = 0.899\n",
      "Epoch 1664 Batch   32/96   train_loss = 0.939\n",
      "Epoch 1664 Batch   64/96   train_loss = 0.910\n",
      "Epoch 1665 Batch    0/96   train_loss = 0.887\n",
      "Epoch 1665 Batch   32/96   train_loss = 0.947\n",
      "Epoch 1665 Batch   64/96   train_loss = 0.910\n",
      "Epoch 1666 Batch    0/96   train_loss = 0.883\n",
      "Epoch 1666 Batch   32/96   train_loss = 0.942\n",
      "Epoch 1666 Batch   64/96   train_loss = 0.911\n",
      "Epoch 1667 Batch    0/96   train_loss = 0.874\n",
      "Epoch 1667 Batch   32/96   train_loss = 0.942\n",
      "Epoch 1667 Batch   64/96   train_loss = 0.919\n",
      "Epoch 1668 Batch    0/96   train_loss = 0.870\n",
      "Epoch 1668 Batch   32/96   train_loss = 0.939\n",
      "Epoch 1668 Batch   64/96   train_loss = 0.907\n",
      "Epoch 1669 Batch    0/96   train_loss = 0.876\n",
      "Epoch 1669 Batch   32/96   train_loss = 0.954\n",
      "Epoch 1669 Batch   64/96   train_loss = 0.911\n",
      "Epoch 1670 Batch    0/96   train_loss = 0.876\n",
      "Epoch 1670 Batch   32/96   train_loss = 0.935\n",
      "Epoch 1670 Batch   64/96   train_loss = 0.916\n",
      "Epoch 1671 Batch    0/96   train_loss = 0.869\n",
      "Epoch 1671 Batch   32/96   train_loss = 0.929\n",
      "Epoch 1671 Batch   64/96   train_loss = 0.904\n",
      "Epoch 1672 Batch    0/96   train_loss = 0.868\n",
      "Epoch 1672 Batch   32/96   train_loss = 0.947\n",
      "Epoch 1672 Batch   64/96   train_loss = 0.918\n",
      "Epoch 1673 Batch    0/96   train_loss = 0.891\n",
      "Epoch 1673 Batch   32/96   train_loss = 0.950\n",
      "Epoch 1673 Batch   64/96   train_loss = 0.931\n",
      "Epoch 1674 Batch    0/96   train_loss = 0.887\n",
      "Epoch 1674 Batch   32/96   train_loss = 0.946\n",
      "Epoch 1674 Batch   64/96   train_loss = 0.934\n",
      "Epoch 1675 Batch    0/96   train_loss = 0.880\n",
      "Epoch 1675 Batch   32/96   train_loss = 0.937\n",
      "Epoch 1675 Batch   64/96   train_loss = 0.921\n",
      "Epoch 1676 Batch    0/96   train_loss = 0.893\n",
      "Epoch 1676 Batch   32/96   train_loss = 0.943\n",
      "Epoch 1676 Batch   64/96   train_loss = 0.923\n",
      "Epoch 1677 Batch    0/96   train_loss = 0.874\n",
      "Epoch 1677 Batch   32/96   train_loss = 0.939\n",
      "Epoch 1677 Batch   64/96   train_loss = 0.909\n",
      "Epoch 1678 Batch    0/96   train_loss = 0.867\n",
      "Epoch 1678 Batch   32/96   train_loss = 0.935\n",
      "Epoch 1678 Batch   64/96   train_loss = 0.903\n",
      "Epoch 1679 Batch    0/96   train_loss = 0.864\n",
      "Epoch 1679 Batch   32/96   train_loss = 0.941\n",
      "Epoch 1679 Batch   64/96   train_loss = 0.909\n",
      "Epoch 1680 Batch    0/96   train_loss = 0.872\n",
      "Epoch 1680 Batch   32/96   train_loss = 0.944\n",
      "Epoch 1680 Batch   64/96   train_loss = 0.914\n",
      "Epoch 1681 Batch    0/96   train_loss = 0.871\n",
      "Epoch 1681 Batch   32/96   train_loss = 0.954\n",
      "Epoch 1681 Batch   64/96   train_loss = 0.915\n",
      "Epoch 1682 Batch    0/96   train_loss = 0.874\n",
      "Epoch 1682 Batch   32/96   train_loss = 0.933\n",
      "Epoch 1682 Batch   64/96   train_loss = 0.901\n",
      "Epoch 1683 Batch    0/96   train_loss = 0.872\n",
      "Epoch 1683 Batch   32/96   train_loss = 0.949\n",
      "Epoch 1683 Batch   64/96   train_loss = 0.912\n",
      "Epoch 1684 Batch    0/96   train_loss = 0.877\n",
      "Epoch 1684 Batch   32/96   train_loss = 0.948\n",
      "Epoch 1684 Batch   64/96   train_loss = 0.920\n",
      "Epoch 1685 Batch    0/96   train_loss = 0.890\n",
      "Epoch 1685 Batch   32/96   train_loss = 0.935\n",
      "Epoch 1685 Batch   64/96   train_loss = 0.932\n",
      "Epoch 1686 Batch    0/96   train_loss = 0.881\n",
      "Epoch 1686 Batch   32/96   train_loss = 0.948\n",
      "Epoch 1686 Batch   64/96   train_loss = 0.922\n",
      "Epoch 1687 Batch    0/96   train_loss = 0.888\n",
      "Epoch 1687 Batch   32/96   train_loss = 0.947\n",
      "Epoch 1687 Batch   64/96   train_loss = 0.922\n",
      "Epoch 1688 Batch    0/96   train_loss = 0.878\n",
      "Epoch 1688 Batch   32/96   train_loss = 0.951\n",
      "Epoch 1688 Batch   64/96   train_loss = 0.916\n",
      "Epoch 1689 Batch    0/96   train_loss = 0.872\n",
      "Epoch 1689 Batch   32/96   train_loss = 0.933\n",
      "Epoch 1689 Batch   64/96   train_loss = 0.909\n",
      "Epoch 1690 Batch    0/96   train_loss = 0.891\n",
      "Epoch 1690 Batch   32/96   train_loss = 0.947\n",
      "Epoch 1690 Batch   64/96   train_loss = 0.916\n",
      "Epoch 1691 Batch    0/96   train_loss = 0.875\n",
      "Epoch 1691 Batch   32/96   train_loss = 0.946\n",
      "Epoch 1691 Batch   64/96   train_loss = 0.901\n",
      "Epoch 1692 Batch    0/96   train_loss = 0.882\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1692 Batch   32/96   train_loss = 0.941\n",
      "Epoch 1692 Batch   64/96   train_loss = 0.914\n",
      "Epoch 1693 Batch    0/96   train_loss = 0.882\n",
      "Epoch 1693 Batch   32/96   train_loss = 0.944\n",
      "Epoch 1693 Batch   64/96   train_loss = 0.928\n",
      "Epoch 1694 Batch    0/96   train_loss = 0.885\n",
      "Epoch 1694 Batch   32/96   train_loss = 0.954\n",
      "Epoch 1694 Batch   64/96   train_loss = 0.923\n",
      "Epoch 1695 Batch    0/96   train_loss = 0.898\n",
      "Epoch 1695 Batch   32/96   train_loss = 0.959\n",
      "Epoch 1695 Batch   64/96   train_loss = 0.917\n",
      "Epoch 1696 Batch    0/96   train_loss = 0.916\n",
      "Epoch 1696 Batch   32/96   train_loss = 0.951\n",
      "Epoch 1696 Batch   64/96   train_loss = 0.931\n",
      "Epoch 1697 Batch    0/96   train_loss = 0.903\n",
      "Epoch 1697 Batch   32/96   train_loss = 0.941\n",
      "Epoch 1697 Batch   64/96   train_loss = 0.919\n",
      "Epoch 1698 Batch    0/96   train_loss = 0.892\n",
      "Epoch 1698 Batch   32/96   train_loss = 0.955\n",
      "Epoch 1698 Batch   64/96   train_loss = 0.928\n",
      "Epoch 1699 Batch    0/96   train_loss = 0.894\n",
      "Epoch 1699 Batch   32/96   train_loss = 0.972\n",
      "Epoch 1699 Batch   64/96   train_loss = 0.922\n",
      "Epoch 1700 Batch    0/96   train_loss = 0.879\n",
      "Epoch 1700 Batch   32/96   train_loss = 0.933\n",
      "Epoch 1700 Batch   64/96   train_loss = 0.932\n",
      "Epoch 1701 Batch    0/96   train_loss = 0.876\n",
      "Epoch 1701 Batch   32/96   train_loss = 0.939\n",
      "Epoch 1701 Batch   64/96   train_loss = 0.907\n",
      "Epoch 1702 Batch    0/96   train_loss = 0.869\n",
      "Epoch 1702 Batch   32/96   train_loss = 0.932\n",
      "Epoch 1702 Batch   64/96   train_loss = 0.913\n",
      "Epoch 1703 Batch    0/96   train_loss = 0.885\n",
      "Epoch 1703 Batch   32/96   train_loss = 0.950\n",
      "Epoch 1703 Batch   64/96   train_loss = 0.908\n",
      "Epoch 1704 Batch    0/96   train_loss = 0.881\n",
      "Epoch 1704 Batch   32/96   train_loss = 0.960\n",
      "Epoch 1704 Batch   64/96   train_loss = 0.908\n",
      "Epoch 1705 Batch    0/96   train_loss = 0.894\n",
      "Epoch 1705 Batch   32/96   train_loss = 0.974\n",
      "Epoch 1705 Batch   64/96   train_loss = 0.921\n",
      "Epoch 1706 Batch    0/96   train_loss = 0.888\n",
      "Epoch 1706 Batch   32/96   train_loss = 0.949\n",
      "Epoch 1706 Batch   64/96   train_loss = 0.909\n",
      "Epoch 1707 Batch    0/96   train_loss = 0.879\n",
      "Epoch 1707 Batch   32/96   train_loss = 0.951\n",
      "Epoch 1707 Batch   64/96   train_loss = 0.896\n",
      "Epoch 1708 Batch    0/96   train_loss = 0.886\n",
      "Epoch 1708 Batch   32/96   train_loss = 0.937\n",
      "Epoch 1708 Batch   64/96   train_loss = 0.910\n",
      "Epoch 1709 Batch    0/96   train_loss = 0.884\n",
      "Epoch 1709 Batch   32/96   train_loss = 0.962\n",
      "Epoch 1709 Batch   64/96   train_loss = 0.903\n",
      "Epoch 1710 Batch    0/96   train_loss = 0.874\n",
      "Epoch 1710 Batch   32/96   train_loss = 0.949\n",
      "Epoch 1710 Batch   64/96   train_loss = 0.907\n",
      "Epoch 1711 Batch    0/96   train_loss = 0.882\n",
      "Epoch 1711 Batch   32/96   train_loss = 0.932\n",
      "Epoch 1711 Batch   64/96   train_loss = 0.916\n",
      "Epoch 1712 Batch    0/96   train_loss = 0.891\n",
      "Epoch 1712 Batch   32/96   train_loss = 0.949\n",
      "Epoch 1712 Batch   64/96   train_loss = 0.918\n",
      "Epoch 1713 Batch    0/96   train_loss = 0.888\n",
      "Epoch 1713 Batch   32/96   train_loss = 0.955\n",
      "Epoch 1713 Batch   64/96   train_loss = 0.926\n",
      "Epoch 1714 Batch    0/96   train_loss = 0.886\n",
      "Epoch 1714 Batch   32/96   train_loss = 0.941\n",
      "Epoch 1714 Batch   64/96   train_loss = 0.930\n",
      "Epoch 1715 Batch    0/96   train_loss = 0.895\n",
      "Epoch 1715 Batch   32/96   train_loss = 0.942\n",
      "Epoch 1715 Batch   64/96   train_loss = 0.912\n",
      "Epoch 1716 Batch    0/96   train_loss = 0.906\n",
      "Epoch 1716 Batch   32/96   train_loss = 0.949\n",
      "Epoch 1716 Batch   64/96   train_loss = 0.909\n",
      "Epoch 1717 Batch    0/96   train_loss = 0.892\n",
      "Epoch 1717 Batch   32/96   train_loss = 0.948\n",
      "Epoch 1717 Batch   64/96   train_loss = 0.928\n",
      "Epoch 1718 Batch    0/96   train_loss = 0.888\n",
      "Epoch 1718 Batch   32/96   train_loss = 0.953\n",
      "Epoch 1718 Batch   64/96   train_loss = 0.919\n",
      "Epoch 1719 Batch    0/96   train_loss = 0.888\n",
      "Epoch 1719 Batch   32/96   train_loss = 0.949\n",
      "Epoch 1719 Batch   64/96   train_loss = 0.922\n",
      "Epoch 1720 Batch    0/96   train_loss = 0.878\n",
      "Epoch 1720 Batch   32/96   train_loss = 0.942\n",
      "Epoch 1720 Batch   64/96   train_loss = 0.932\n",
      "Epoch 1721 Batch    0/96   train_loss = 0.879\n",
      "Epoch 1721 Batch   32/96   train_loss = 0.950\n",
      "Epoch 1721 Batch   64/96   train_loss = 0.919\n",
      "Epoch 1722 Batch    0/96   train_loss = 0.896\n",
      "Epoch 1722 Batch   32/96   train_loss = 0.929\n",
      "Epoch 1722 Batch   64/96   train_loss = 0.927\n",
      "Epoch 1723 Batch    0/96   train_loss = 0.896\n",
      "Epoch 1723 Batch   32/96   train_loss = 0.942\n",
      "Epoch 1723 Batch   64/96   train_loss = 0.931\n",
      "Epoch 1724 Batch    0/96   train_loss = 0.897\n",
      "Epoch 1724 Batch   32/96   train_loss = 0.935\n",
      "Epoch 1724 Batch   64/96   train_loss = 0.937\n",
      "Epoch 1725 Batch    0/96   train_loss = 0.906\n",
      "Epoch 1725 Batch   32/96   train_loss = 0.951\n",
      "Epoch 1725 Batch   64/96   train_loss = 0.914\n",
      "Epoch 1726 Batch    0/96   train_loss = 0.900\n",
      "Epoch 1726 Batch   32/96   train_loss = 0.948\n",
      "Epoch 1726 Batch   64/96   train_loss = 0.913\n",
      "Epoch 1727 Batch    0/96   train_loss = 0.899\n",
      "Epoch 1727 Batch   32/96   train_loss = 0.964\n",
      "Epoch 1727 Batch   64/96   train_loss = 0.926\n",
      "Epoch 1728 Batch    0/96   train_loss = 0.890\n",
      "Epoch 1728 Batch   32/96   train_loss = 0.951\n",
      "Epoch 1728 Batch   64/96   train_loss = 0.932\n",
      "Epoch 1729 Batch    0/96   train_loss = 0.916\n",
      "Epoch 1729 Batch   32/96   train_loss = 0.962\n",
      "Epoch 1729 Batch   64/96   train_loss = 0.927\n",
      "Epoch 1730 Batch    0/96   train_loss = 0.890\n",
      "Epoch 1730 Batch   32/96   train_loss = 0.952\n",
      "Epoch 1730 Batch   64/96   train_loss = 0.912\n",
      "Epoch 1731 Batch    0/96   train_loss = 0.890\n",
      "Epoch 1731 Batch   32/96   train_loss = 0.943\n",
      "Epoch 1731 Batch   64/96   train_loss = 0.919\n",
      "Epoch 1732 Batch    0/96   train_loss = 0.896\n",
      "Epoch 1732 Batch   32/96   train_loss = 0.940\n",
      "Epoch 1732 Batch   64/96   train_loss = 0.913\n",
      "Epoch 1733 Batch    0/96   train_loss = 0.890\n",
      "Epoch 1733 Batch   32/96   train_loss = 0.932\n",
      "Epoch 1733 Batch   64/96   train_loss = 0.935\n",
      "Epoch 1734 Batch    0/96   train_loss = 0.881\n",
      "Epoch 1734 Batch   32/96   train_loss = 0.926\n",
      "Epoch 1734 Batch   64/96   train_loss = 0.921\n",
      "Epoch 1735 Batch    0/96   train_loss = 0.896\n",
      "Epoch 1735 Batch   32/96   train_loss = 0.939\n",
      "Epoch 1735 Batch   64/96   train_loss = 0.918\n",
      "Epoch 1736 Batch    0/96   train_loss = 0.898\n",
      "Epoch 1736 Batch   32/96   train_loss = 0.935\n",
      "Epoch 1736 Batch   64/96   train_loss = 0.922\n",
      "Epoch 1737 Batch    0/96   train_loss = 0.877\n",
      "Epoch 1737 Batch   32/96   train_loss = 0.958\n",
      "Epoch 1737 Batch   64/96   train_loss = 0.932\n",
      "Epoch 1738 Batch    0/96   train_loss = 0.890\n",
      "Epoch 1738 Batch   32/96   train_loss = 0.954\n",
      "Epoch 1738 Batch   64/96   train_loss = 0.921\n",
      "Epoch 1739 Batch    0/96   train_loss = 0.875\n",
      "Epoch 1739 Batch   32/96   train_loss = 0.955\n",
      "Epoch 1739 Batch   64/96   train_loss = 0.935\n",
      "Epoch 1740 Batch    0/96   train_loss = 0.892\n",
      "Epoch 1740 Batch   32/96   train_loss = 0.960\n",
      "Epoch 1740 Batch   64/96   train_loss = 0.922\n",
      "Epoch 1741 Batch    0/96   train_loss = 0.881\n",
      "Epoch 1741 Batch   32/96   train_loss = 0.962\n",
      "Epoch 1741 Batch   64/96   train_loss = 0.921\n",
      "Epoch 1742 Batch    0/96   train_loss = 0.897\n",
      "Epoch 1742 Batch   32/96   train_loss = 0.972\n",
      "Epoch 1742 Batch   64/96   train_loss = 0.935\n",
      "Epoch 1743 Batch    0/96   train_loss = 0.898\n",
      "Epoch 1743 Batch   32/96   train_loss = 0.966\n",
      "Epoch 1743 Batch   64/96   train_loss = 0.917\n",
      "Epoch 1744 Batch    0/96   train_loss = 0.891\n",
      "Epoch 1744 Batch   32/96   train_loss = 0.949\n",
      "Epoch 1744 Batch   64/96   train_loss = 0.922\n",
      "Epoch 1745 Batch    0/96   train_loss = 0.877\n",
      "Epoch 1745 Batch   32/96   train_loss = 0.937\n",
      "Epoch 1745 Batch   64/96   train_loss = 0.936\n",
      "Epoch 1746 Batch    0/96   train_loss = 0.876\n",
      "Epoch 1746 Batch   32/96   train_loss = 0.957\n",
      "Epoch 1746 Batch   64/96   train_loss = 0.960\n",
      "Epoch 1747 Batch    0/96   train_loss = 0.920\n",
      "Epoch 1747 Batch   32/96   train_loss = 0.953\n",
      "Epoch 1747 Batch   64/96   train_loss = 0.944\n",
      "Epoch 1748 Batch    0/96   train_loss = 0.893\n",
      "Epoch 1748 Batch   32/96   train_loss = 0.948\n",
      "Epoch 1748 Batch   64/96   train_loss = 0.938\n",
      "Epoch 1749 Batch    0/96   train_loss = 0.895\n",
      "Epoch 1749 Batch   32/96   train_loss = 0.929\n",
      "Epoch 1749 Batch   64/96   train_loss = 0.951\n",
      "Epoch 1750 Batch    0/96   train_loss = 0.875\n",
      "Epoch 1750 Batch   32/96   train_loss = 0.937\n",
      "Epoch 1750 Batch   64/96   train_loss = 0.959\n",
      "Epoch 1751 Batch    0/96   train_loss = 0.873\n",
      "Epoch 1751 Batch   32/96   train_loss = 0.935\n",
      "Epoch 1751 Batch   64/96   train_loss = 0.930\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1752 Batch    0/96   train_loss = 0.878\n",
      "Epoch 1752 Batch   32/96   train_loss = 0.934\n",
      "Epoch 1752 Batch   64/96   train_loss = 0.917\n",
      "Epoch 1753 Batch    0/96   train_loss = 0.879\n",
      "Epoch 1753 Batch   32/96   train_loss = 0.942\n",
      "Epoch 1753 Batch   64/96   train_loss = 0.943\n",
      "Epoch 1754 Batch    0/96   train_loss = 0.897\n",
      "Epoch 1754 Batch   32/96   train_loss = 0.933\n",
      "Epoch 1754 Batch   64/96   train_loss = 0.937\n",
      "Epoch 1755 Batch    0/96   train_loss = 0.887\n",
      "Epoch 1755 Batch   32/96   train_loss = 0.941\n",
      "Epoch 1755 Batch   64/96   train_loss = 0.946\n",
      "Epoch 1756 Batch    0/96   train_loss = 0.876\n",
      "Epoch 1756 Batch   32/96   train_loss = 0.937\n",
      "Epoch 1756 Batch   64/96   train_loss = 0.941\n",
      "Epoch 1757 Batch    0/96   train_loss = 0.885\n",
      "Epoch 1757 Batch   32/96   train_loss = 0.945\n",
      "Epoch 1757 Batch   64/96   train_loss = 0.937\n",
      "Epoch 1758 Batch    0/96   train_loss = 0.882\n",
      "Epoch 1758 Batch   32/96   train_loss = 0.936\n",
      "Epoch 1758 Batch   64/96   train_loss = 0.931\n",
      "Epoch 1759 Batch    0/96   train_loss = 0.916\n",
      "Epoch 1759 Batch   32/96   train_loss = 0.954\n",
      "Epoch 1759 Batch   64/96   train_loss = 0.942\n",
      "Epoch 1760 Batch    0/96   train_loss = 0.893\n",
      "Epoch 1760 Batch   32/96   train_loss = 0.943\n",
      "Epoch 1760 Batch   64/96   train_loss = 0.957\n",
      "Epoch 1761 Batch    0/96   train_loss = 0.907\n",
      "Epoch 1761 Batch   32/96   train_loss = 0.953\n",
      "Epoch 1761 Batch   64/96   train_loss = 0.951\n",
      "Epoch 1762 Batch    0/96   train_loss = 0.902\n",
      "Epoch 1762 Batch   32/96   train_loss = 0.979\n",
      "Epoch 1762 Batch   64/96   train_loss = 0.943\n",
      "Epoch 1763 Batch    0/96   train_loss = 0.899\n",
      "Epoch 1763 Batch   32/96   train_loss = 0.957\n",
      "Epoch 1763 Batch   64/96   train_loss = 0.917\n",
      "Epoch 1764 Batch    0/96   train_loss = 0.888\n",
      "Epoch 1764 Batch   32/96   train_loss = 0.945\n",
      "Epoch 1764 Batch   64/96   train_loss = 0.925\n",
      "Epoch 1765 Batch    0/96   train_loss = 0.887\n",
      "Epoch 1765 Batch   32/96   train_loss = 0.953\n",
      "Epoch 1765 Batch   64/96   train_loss = 0.932\n",
      "Epoch 1766 Batch    0/96   train_loss = 0.872\n",
      "Epoch 1766 Batch   32/96   train_loss = 0.948\n",
      "Epoch 1766 Batch   64/96   train_loss = 0.916\n",
      "Epoch 1767 Batch    0/96   train_loss = 0.882\n",
      "Epoch 1767 Batch   32/96   train_loss = 0.936\n",
      "Epoch 1767 Batch   64/96   train_loss = 0.912\n",
      "Epoch 1768 Batch    0/96   train_loss = 0.868\n",
      "Epoch 1768 Batch   32/96   train_loss = 0.943\n",
      "Epoch 1768 Batch   64/96   train_loss = 0.915\n",
      "Epoch 1769 Batch    0/96   train_loss = 0.868\n",
      "Epoch 1769 Batch   32/96   train_loss = 0.928\n",
      "Epoch 1769 Batch   64/96   train_loss = 0.901\n",
      "Epoch 1770 Batch    0/96   train_loss = 0.876\n",
      "Epoch 1770 Batch   32/96   train_loss = 0.937\n",
      "Epoch 1770 Batch   64/96   train_loss = 0.924\n",
      "Epoch 1771 Batch    0/96   train_loss = 0.881\n",
      "Epoch 1771 Batch   32/96   train_loss = 0.938\n",
      "Epoch 1771 Batch   64/96   train_loss = 0.923\n",
      "Epoch 1772 Batch    0/96   train_loss = 0.900\n",
      "Epoch 1772 Batch   32/96   train_loss = 0.945\n",
      "Epoch 1772 Batch   64/96   train_loss = 0.917\n",
      "Epoch 1773 Batch    0/96   train_loss = 0.873\n",
      "Epoch 1773 Batch   32/96   train_loss = 0.952\n",
      "Epoch 1773 Batch   64/96   train_loss = 0.922\n",
      "Epoch 1774 Batch    0/96   train_loss = 0.888\n",
      "Epoch 1774 Batch   32/96   train_loss = 0.954\n",
      "Epoch 1774 Batch   64/96   train_loss = 0.916\n",
      "Epoch 1775 Batch    0/96   train_loss = 0.894\n",
      "Epoch 1775 Batch   32/96   train_loss = 0.947\n",
      "Epoch 1775 Batch   64/96   train_loss = 0.939\n",
      "Epoch 1776 Batch    0/96   train_loss = 0.892\n",
      "Epoch 1776 Batch   32/96   train_loss = 0.955\n",
      "Epoch 1776 Batch   64/96   train_loss = 0.918\n",
      "Epoch 1777 Batch    0/96   train_loss = 0.891\n",
      "Epoch 1777 Batch   32/96   train_loss = 0.952\n",
      "Epoch 1777 Batch   64/96   train_loss = 0.922\n",
      "Epoch 1778 Batch    0/96   train_loss = 0.892\n",
      "Epoch 1778 Batch   32/96   train_loss = 0.950\n",
      "Epoch 1778 Batch   64/96   train_loss = 0.939\n",
      "Epoch 1779 Batch    0/96   train_loss = 0.894\n",
      "Epoch 1779 Batch   32/96   train_loss = 0.945\n",
      "Epoch 1779 Batch   64/96   train_loss = 0.927\n",
      "Epoch 1780 Batch    0/96   train_loss = 0.885\n",
      "Epoch 1780 Batch   32/96   train_loss = 0.952\n",
      "Epoch 1780 Batch   64/96   train_loss = 0.948\n",
      "Epoch 1781 Batch    0/96   train_loss = 0.890\n",
      "Epoch 1781 Batch   32/96   train_loss = 0.944\n",
      "Epoch 1781 Batch   64/96   train_loss = 0.915\n",
      "Epoch 1782 Batch    0/96   train_loss = 0.880\n",
      "Epoch 1782 Batch   32/96   train_loss = 0.944\n",
      "Epoch 1782 Batch   64/96   train_loss = 0.939\n",
      "Epoch 1783 Batch    0/96   train_loss = 0.886\n",
      "Epoch 1783 Batch   32/96   train_loss = 0.956\n",
      "Epoch 1783 Batch   64/96   train_loss = 0.918\n",
      "Epoch 1784 Batch    0/96   train_loss = 0.874\n",
      "Epoch 1784 Batch   32/96   train_loss = 0.942\n",
      "Epoch 1784 Batch   64/96   train_loss = 0.910\n",
      "Epoch 1785 Batch    0/96   train_loss = 0.883\n",
      "Epoch 1785 Batch   32/96   train_loss = 0.927\n",
      "Epoch 1785 Batch   64/96   train_loss = 0.928\n",
      "Epoch 1786 Batch    0/96   train_loss = 0.877\n",
      "Epoch 1786 Batch   32/96   train_loss = 0.954\n",
      "Epoch 1786 Batch   64/96   train_loss = 0.920\n",
      "Epoch 1787 Batch    0/96   train_loss = 0.881\n",
      "Epoch 1787 Batch   32/96   train_loss = 0.945\n",
      "Epoch 1787 Batch   64/96   train_loss = 0.920\n",
      "Epoch 1788 Batch    0/96   train_loss = 0.889\n",
      "Epoch 1788 Batch   32/96   train_loss = 0.938\n",
      "Epoch 1788 Batch   64/96   train_loss = 0.909\n",
      "Epoch 1789 Batch    0/96   train_loss = 0.877\n",
      "Epoch 1789 Batch   32/96   train_loss = 0.936\n",
      "Epoch 1789 Batch   64/96   train_loss = 0.928\n",
      "Epoch 1790 Batch    0/96   train_loss = 0.869\n",
      "Epoch 1790 Batch   32/96   train_loss = 0.924\n",
      "Epoch 1790 Batch   64/96   train_loss = 0.893\n",
      "Epoch 1791 Batch    0/96   train_loss = 0.872\n",
      "Epoch 1791 Batch   32/96   train_loss = 0.938\n",
      "Epoch 1791 Batch   64/96   train_loss = 0.918\n",
      "Epoch 1792 Batch    0/96   train_loss = 0.886\n",
      "Epoch 1792 Batch   32/96   train_loss = 0.932\n",
      "Epoch 1792 Batch   64/96   train_loss = 0.921\n",
      "Epoch 1793 Batch    0/96   train_loss = 0.873\n",
      "Epoch 1793 Batch   32/96   train_loss = 0.931\n",
      "Epoch 1793 Batch   64/96   train_loss = 0.912\n",
      "Epoch 1794 Batch    0/96   train_loss = 0.854\n",
      "Epoch 1794 Batch   32/96   train_loss = 0.931\n",
      "Epoch 1794 Batch   64/96   train_loss = 0.902\n",
      "Epoch 1795 Batch    0/96   train_loss = 0.864\n",
      "Epoch 1795 Batch   32/96   train_loss = 0.917\n",
      "Epoch 1795 Batch   64/96   train_loss = 0.902\n",
      "Epoch 1796 Batch    0/96   train_loss = 0.872\n",
      "Epoch 1796 Batch   32/96   train_loss = 0.921\n",
      "Epoch 1796 Batch   64/96   train_loss = 0.919\n",
      "Epoch 1797 Batch    0/96   train_loss = 0.886\n",
      "Epoch 1797 Batch   32/96   train_loss = 0.925\n",
      "Epoch 1797 Batch   64/96   train_loss = 0.911\n",
      "Epoch 1798 Batch    0/96   train_loss = 0.870\n",
      "Epoch 1798 Batch   32/96   train_loss = 0.910\n",
      "Epoch 1798 Batch   64/96   train_loss = 0.909\n",
      "Epoch 1799 Batch    0/96   train_loss = 0.869\n",
      "Epoch 1799 Batch   32/96   train_loss = 0.932\n",
      "Epoch 1799 Batch   64/96   train_loss = 0.910\n",
      "Epoch 1800 Batch    0/96   train_loss = 0.883\n",
      "Epoch 1800 Batch   32/96   train_loss = 0.935\n",
      "Epoch 1800 Batch   64/96   train_loss = 0.908\n",
      "Epoch 1801 Batch    0/96   train_loss = 0.879\n",
      "Epoch 1801 Batch   32/96   train_loss = 0.937\n",
      "Epoch 1801 Batch   64/96   train_loss = 0.923\n",
      "Epoch 1802 Batch    0/96   train_loss = 0.892\n",
      "Epoch 1802 Batch   32/96   train_loss = 0.938\n",
      "Epoch 1802 Batch   64/96   train_loss = 0.931\n",
      "Epoch 1803 Batch    0/96   train_loss = 0.901\n",
      "Epoch 1803 Batch   32/96   train_loss = 0.949\n",
      "Epoch 1803 Batch   64/96   train_loss = 0.934\n",
      "Epoch 1804 Batch    0/96   train_loss = 0.903\n",
      "Epoch 1804 Batch   32/96   train_loss = 0.958\n",
      "Epoch 1804 Batch   64/96   train_loss = 0.924\n",
      "Epoch 1805 Batch    0/96   train_loss = 0.896\n",
      "Epoch 1805 Batch   32/96   train_loss = 0.944\n",
      "Epoch 1805 Batch   64/96   train_loss = 0.921\n",
      "Epoch 1806 Batch    0/96   train_loss = 0.901\n",
      "Epoch 1806 Batch   32/96   train_loss = 0.953\n",
      "Epoch 1806 Batch   64/96   train_loss = 0.916\n",
      "Epoch 1807 Batch    0/96   train_loss = 0.882\n",
      "Epoch 1807 Batch   32/96   train_loss = 0.934\n",
      "Epoch 1807 Batch   64/96   train_loss = 0.914\n",
      "Epoch 1808 Batch    0/96   train_loss = 0.890\n",
      "Epoch 1808 Batch   32/96   train_loss = 0.941\n",
      "Epoch 1808 Batch   64/96   train_loss = 0.921\n",
      "Epoch 1809 Batch    0/96   train_loss = 0.884\n",
      "Epoch 1809 Batch   32/96   train_loss = 0.947\n",
      "Epoch 1809 Batch   64/96   train_loss = 0.938\n",
      "Epoch 1810 Batch    0/96   train_loss = 0.880\n",
      "Epoch 1810 Batch   32/96   train_loss = 0.930\n",
      "Epoch 1810 Batch   64/96   train_loss = 0.926\n",
      "Epoch 1811 Batch    0/96   train_loss = 0.889\n",
      "Epoch 1811 Batch   32/96   train_loss = 0.925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1811 Batch   64/96   train_loss = 0.922\n",
      "Epoch 1812 Batch    0/96   train_loss = 0.885\n",
      "Epoch 1812 Batch   32/96   train_loss = 0.930\n",
      "Epoch 1812 Batch   64/96   train_loss = 0.930\n",
      "Epoch 1813 Batch    0/96   train_loss = 0.876\n",
      "Epoch 1813 Batch   32/96   train_loss = 0.937\n",
      "Epoch 1813 Batch   64/96   train_loss = 0.911\n",
      "Epoch 1814 Batch    0/96   train_loss = 0.875\n",
      "Epoch 1814 Batch   32/96   train_loss = 0.934\n",
      "Epoch 1814 Batch   64/96   train_loss = 0.912\n",
      "Epoch 1815 Batch    0/96   train_loss = 0.884\n",
      "Epoch 1815 Batch   32/96   train_loss = 0.943\n",
      "Epoch 1815 Batch   64/96   train_loss = 0.915\n",
      "Epoch 1816 Batch    0/96   train_loss = 0.875\n",
      "Epoch 1816 Batch   32/96   train_loss = 0.974\n",
      "Epoch 1816 Batch   64/96   train_loss = 0.932\n",
      "Epoch 1817 Batch    0/96   train_loss = 0.890\n",
      "Epoch 1817 Batch   32/96   train_loss = 0.945\n",
      "Epoch 1817 Batch   64/96   train_loss = 0.939\n",
      "Epoch 1818 Batch    0/96   train_loss = 0.883\n",
      "Epoch 1818 Batch   32/96   train_loss = 0.951\n",
      "Epoch 1818 Batch   64/96   train_loss = 0.931\n",
      "Epoch 1819 Batch    0/96   train_loss = 0.881\n",
      "Epoch 1819 Batch   32/96   train_loss = 0.946\n",
      "Epoch 1819 Batch   64/96   train_loss = 0.928\n",
      "Epoch 1820 Batch    0/96   train_loss = 0.869\n",
      "Epoch 1820 Batch   32/96   train_loss = 0.937\n",
      "Epoch 1820 Batch   64/96   train_loss = 0.940\n",
      "Epoch 1821 Batch    0/96   train_loss = 0.879\n",
      "Epoch 1821 Batch   32/96   train_loss = 0.947\n",
      "Epoch 1821 Batch   64/96   train_loss = 0.924\n",
      "Epoch 1822 Batch    0/96   train_loss = 0.885\n",
      "Epoch 1822 Batch   32/96   train_loss = 0.935\n",
      "Epoch 1822 Batch   64/96   train_loss = 0.931\n",
      "Epoch 1823 Batch    0/96   train_loss = 0.884\n",
      "Epoch 1823 Batch   32/96   train_loss = 0.935\n",
      "Epoch 1823 Batch   64/96   train_loss = 0.942\n",
      "Epoch 1824 Batch    0/96   train_loss = 0.885\n",
      "Epoch 1824 Batch   32/96   train_loss = 0.928\n",
      "Epoch 1824 Batch   64/96   train_loss = 0.941\n",
      "Epoch 1825 Batch    0/96   train_loss = 0.894\n",
      "Epoch 1825 Batch   32/96   train_loss = 0.925\n",
      "Epoch 1825 Batch   64/96   train_loss = 0.940\n",
      "Epoch 1826 Batch    0/96   train_loss = 0.881\n",
      "Epoch 1826 Batch   32/96   train_loss = 0.933\n",
      "Epoch 1826 Batch   64/96   train_loss = 0.925\n",
      "Epoch 1827 Batch    0/96   train_loss = 0.879\n",
      "Epoch 1827 Batch   32/96   train_loss = 0.934\n",
      "Epoch 1827 Batch   64/96   train_loss = 0.918\n",
      "Epoch 1828 Batch    0/96   train_loss = 0.873\n",
      "Epoch 1828 Batch   32/96   train_loss = 0.927\n",
      "Epoch 1828 Batch   64/96   train_loss = 0.914\n",
      "Epoch 1829 Batch    0/96   train_loss = 0.878\n",
      "Epoch 1829 Batch   32/96   train_loss = 0.923\n",
      "Epoch 1829 Batch   64/96   train_loss = 0.914\n",
      "Epoch 1830 Batch    0/96   train_loss = 0.874\n",
      "Epoch 1830 Batch   32/96   train_loss = 0.930\n",
      "Epoch 1830 Batch   64/96   train_loss = 0.916\n",
      "Epoch 1831 Batch    0/96   train_loss = 0.863\n",
      "Epoch 1831 Batch   32/96   train_loss = 0.941\n",
      "Epoch 1831 Batch   64/96   train_loss = 0.918\n",
      "Epoch 1832 Batch    0/96   train_loss = 0.875\n",
      "Epoch 1832 Batch   32/96   train_loss = 0.937\n",
      "Epoch 1832 Batch   64/96   train_loss = 0.919\n",
      "Epoch 1833 Batch    0/96   train_loss = 0.888\n",
      "Epoch 1833 Batch   32/96   train_loss = 0.936\n",
      "Epoch 1833 Batch   64/96   train_loss = 0.927\n",
      "Epoch 1834 Batch    0/96   train_loss = 0.884\n",
      "Epoch 1834 Batch   32/96   train_loss = 0.922\n",
      "Epoch 1834 Batch   64/96   train_loss = 0.926\n",
      "Epoch 1835 Batch    0/96   train_loss = 0.874\n",
      "Epoch 1835 Batch   32/96   train_loss = 0.933\n",
      "Epoch 1835 Batch   64/96   train_loss = 0.921\n",
      "Epoch 1836 Batch    0/96   train_loss = 0.885\n",
      "Epoch 1836 Batch   32/96   train_loss = 0.946\n",
      "Epoch 1836 Batch   64/96   train_loss = 0.935\n",
      "Epoch 1837 Batch    0/96   train_loss = 0.876\n",
      "Epoch 1837 Batch   32/96   train_loss = 0.947\n",
      "Epoch 1837 Batch   64/96   train_loss = 0.943\n",
      "Epoch 1838 Batch    0/96   train_loss = 0.880\n",
      "Epoch 1838 Batch   32/96   train_loss = 0.940\n",
      "Epoch 1838 Batch   64/96   train_loss = 0.941\n",
      "Epoch 1839 Batch    0/96   train_loss = 0.879\n",
      "Epoch 1839 Batch   32/96   train_loss = 0.943\n",
      "Epoch 1839 Batch   64/96   train_loss = 0.921\n",
      "Epoch 1840 Batch    0/96   train_loss = 0.887\n",
      "Epoch 1840 Batch   32/96   train_loss = 0.948\n",
      "Epoch 1840 Batch   64/96   train_loss = 0.925\n",
      "Epoch 1841 Batch    0/96   train_loss = 0.884\n",
      "Epoch 1841 Batch   32/96   train_loss = 0.946\n",
      "Epoch 1841 Batch   64/96   train_loss = 0.925\n",
      "Epoch 1842 Batch    0/96   train_loss = 0.876\n",
      "Epoch 1842 Batch   32/96   train_loss = 0.946\n",
      "Epoch 1842 Batch   64/96   train_loss = 0.910\n",
      "Epoch 1843 Batch    0/96   train_loss = 0.876\n",
      "Epoch 1843 Batch   32/96   train_loss = 0.940\n",
      "Epoch 1843 Batch   64/96   train_loss = 0.929\n",
      "Epoch 1844 Batch    0/96   train_loss = 0.874\n",
      "Epoch 1844 Batch   32/96   train_loss = 0.948\n",
      "Epoch 1844 Batch   64/96   train_loss = 0.916\n",
      "Epoch 1845 Batch    0/96   train_loss = 0.866\n",
      "Epoch 1845 Batch   32/96   train_loss = 0.950\n",
      "Epoch 1845 Batch   64/96   train_loss = 0.939\n",
      "Epoch 1846 Batch    0/96   train_loss = 0.885\n",
      "Epoch 1846 Batch   32/96   train_loss = 0.958\n",
      "Epoch 1846 Batch   64/96   train_loss = 0.937\n",
      "Epoch 1847 Batch    0/96   train_loss = 0.873\n",
      "Epoch 1847 Batch   32/96   train_loss = 0.945\n",
      "Epoch 1847 Batch   64/96   train_loss = 0.926\n",
      "Epoch 1848 Batch    0/96   train_loss = 0.886\n",
      "Epoch 1848 Batch   32/96   train_loss = 0.950\n",
      "Epoch 1848 Batch   64/96   train_loss = 0.920\n",
      "Epoch 1849 Batch    0/96   train_loss = 0.879\n",
      "Epoch 1849 Batch   32/96   train_loss = 0.940\n",
      "Epoch 1849 Batch   64/96   train_loss = 0.926\n",
      "Epoch 1850 Batch    0/96   train_loss = 0.875\n",
      "Epoch 1850 Batch   32/96   train_loss = 0.950\n",
      "Epoch 1850 Batch   64/96   train_loss = 0.907\n",
      "Epoch 1851 Batch    0/96   train_loss = 0.883\n",
      "Epoch 1851 Batch   32/96   train_loss = 0.941\n",
      "Epoch 1851 Batch   64/96   train_loss = 0.934\n",
      "Epoch 1852 Batch    0/96   train_loss = 0.886\n",
      "Epoch 1852 Batch   32/96   train_loss = 0.945\n",
      "Epoch 1852 Batch   64/96   train_loss = 0.911\n",
      "Epoch 1853 Batch    0/96   train_loss = 0.873\n",
      "Epoch 1853 Batch   32/96   train_loss = 0.952\n",
      "Epoch 1853 Batch   64/96   train_loss = 0.920\n",
      "Epoch 1854 Batch    0/96   train_loss = 0.885\n",
      "Epoch 1854 Batch   32/96   train_loss = 0.939\n",
      "Epoch 1854 Batch   64/96   train_loss = 0.934\n",
      "Epoch 1855 Batch    0/96   train_loss = 0.879\n",
      "Epoch 1855 Batch   32/96   train_loss = 0.938\n",
      "Epoch 1855 Batch   64/96   train_loss = 0.916\n",
      "Epoch 1856 Batch    0/96   train_loss = 0.900\n",
      "Epoch 1856 Batch   32/96   train_loss = 0.953\n",
      "Epoch 1856 Batch   64/96   train_loss = 0.933\n",
      "Epoch 1857 Batch    0/96   train_loss = 0.888\n",
      "Epoch 1857 Batch   32/96   train_loss = 0.947\n",
      "Epoch 1857 Batch   64/96   train_loss = 0.919\n",
      "Epoch 1858 Batch    0/96   train_loss = 0.908\n",
      "Epoch 1858 Batch   32/96   train_loss = 0.952\n",
      "Epoch 1858 Batch   64/96   train_loss = 0.943\n",
      "Epoch 1859 Batch    0/96   train_loss = 0.889\n",
      "Epoch 1859 Batch   32/96   train_loss = 0.946\n",
      "Epoch 1859 Batch   64/96   train_loss = 0.929\n",
      "Epoch 1860 Batch    0/96   train_loss = 0.889\n",
      "Epoch 1860 Batch   32/96   train_loss = 0.956\n",
      "Epoch 1860 Batch   64/96   train_loss = 0.926\n",
      "Epoch 1861 Batch    0/96   train_loss = 0.890\n",
      "Epoch 1861 Batch   32/96   train_loss = 0.952\n",
      "Epoch 1861 Batch   64/96   train_loss = 0.914\n",
      "Epoch 1862 Batch    0/96   train_loss = 0.900\n",
      "Epoch 1862 Batch   32/96   train_loss = 0.945\n",
      "Epoch 1862 Batch   64/96   train_loss = 0.919\n",
      "Epoch 1863 Batch    0/96   train_loss = 0.898\n",
      "Epoch 1863 Batch   32/96   train_loss = 0.939\n",
      "Epoch 1863 Batch   64/96   train_loss = 0.937\n",
      "Epoch 1864 Batch    0/96   train_loss = 0.889\n",
      "Epoch 1864 Batch   32/96   train_loss = 0.955\n",
      "Epoch 1864 Batch   64/96   train_loss = 0.929\n",
      "Epoch 1865 Batch    0/96   train_loss = 0.897\n",
      "Epoch 1865 Batch   32/96   train_loss = 0.945\n",
      "Epoch 1865 Batch   64/96   train_loss = 0.920\n",
      "Epoch 1866 Batch    0/96   train_loss = 0.906\n",
      "Epoch 1866 Batch   32/96   train_loss = 0.951\n",
      "Epoch 1866 Batch   64/96   train_loss = 0.935\n",
      "Epoch 1867 Batch    0/96   train_loss = 0.932\n",
      "Epoch 1867 Batch   32/96   train_loss = 0.970\n",
      "Epoch 1867 Batch   64/96   train_loss = 0.945\n",
      "Epoch 1868 Batch    0/96   train_loss = 0.910\n",
      "Epoch 1868 Batch   32/96   train_loss = 0.962\n",
      "Epoch 1868 Batch   64/96   train_loss = 0.928\n",
      "Epoch 1869 Batch    0/96   train_loss = 0.892\n",
      "Epoch 1869 Batch   32/96   train_loss = 0.953\n",
      "Epoch 1869 Batch   64/96   train_loss = 0.925\n",
      "Epoch 1870 Batch    0/96   train_loss = 0.893\n",
      "Epoch 1870 Batch   32/96   train_loss = 0.953\n",
      "Epoch 1870 Batch   64/96   train_loss = 0.914\n",
      "Epoch 1871 Batch    0/96   train_loss = 0.883\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1871 Batch   32/96   train_loss = 0.945\n",
      "Epoch 1871 Batch   64/96   train_loss = 0.918\n",
      "Epoch 1872 Batch    0/96   train_loss = 0.876\n",
      "Epoch 1872 Batch   32/96   train_loss = 0.955\n",
      "Epoch 1872 Batch   64/96   train_loss = 0.924\n",
      "Epoch 1873 Batch    0/96   train_loss = 0.878\n",
      "Epoch 1873 Batch   32/96   train_loss = 0.955\n",
      "Epoch 1873 Batch   64/96   train_loss = 0.924\n",
      "Epoch 1874 Batch    0/96   train_loss = 0.878\n",
      "Epoch 1874 Batch   32/96   train_loss = 0.968\n",
      "Epoch 1874 Batch   64/96   train_loss = 0.920\n",
      "Epoch 1875 Batch    0/96   train_loss = 0.876\n",
      "Epoch 1875 Batch   32/96   train_loss = 0.955\n",
      "Epoch 1875 Batch   64/96   train_loss = 0.921\n",
      "Epoch 1876 Batch    0/96   train_loss = 0.886\n",
      "Epoch 1876 Batch   32/96   train_loss = 0.944\n",
      "Epoch 1876 Batch   64/96   train_loss = 0.933\n",
      "Epoch 1877 Batch    0/96   train_loss = 0.884\n",
      "Epoch 1877 Batch   32/96   train_loss = 0.940\n",
      "Epoch 1877 Batch   64/96   train_loss = 0.932\n",
      "Epoch 1878 Batch    0/96   train_loss = 0.873\n",
      "Epoch 1878 Batch   32/96   train_loss = 0.944\n",
      "Epoch 1878 Batch   64/96   train_loss = 0.928\n",
      "Epoch 1879 Batch    0/96   train_loss = 0.887\n",
      "Epoch 1879 Batch   32/96   train_loss = 0.963\n",
      "Epoch 1879 Batch   64/96   train_loss = 0.930\n",
      "Epoch 1880 Batch    0/96   train_loss = 0.893\n",
      "Epoch 1880 Batch   32/96   train_loss = 0.934\n",
      "Epoch 1880 Batch   64/96   train_loss = 0.914\n",
      "Epoch 1881 Batch    0/96   train_loss = 0.874\n",
      "Epoch 1881 Batch   32/96   train_loss = 0.941\n",
      "Epoch 1881 Batch   64/96   train_loss = 0.910\n",
      "Epoch 1882 Batch    0/96   train_loss = 0.880\n",
      "Epoch 1882 Batch   32/96   train_loss = 0.936\n",
      "Epoch 1882 Batch   64/96   train_loss = 0.916\n",
      "Epoch 1883 Batch    0/96   train_loss = 0.872\n",
      "Epoch 1883 Batch   32/96   train_loss = 0.941\n",
      "Epoch 1883 Batch   64/96   train_loss = 0.921\n",
      "Epoch 1884 Batch    0/96   train_loss = 0.869\n",
      "Epoch 1884 Batch   32/96   train_loss = 0.945\n",
      "Epoch 1884 Batch   64/96   train_loss = 0.914\n",
      "Epoch 1885 Batch    0/96   train_loss = 0.865\n",
      "Epoch 1885 Batch   32/96   train_loss = 0.951\n",
      "Epoch 1885 Batch   64/96   train_loss = 0.924\n",
      "Epoch 1886 Batch    0/96   train_loss = 0.872\n",
      "Epoch 1886 Batch   32/96   train_loss = 0.949\n",
      "Epoch 1886 Batch   64/96   train_loss = 0.904\n",
      "Epoch 1887 Batch    0/96   train_loss = 0.868\n",
      "Epoch 1887 Batch   32/96   train_loss = 0.927\n",
      "Epoch 1887 Batch   64/96   train_loss = 0.899\n",
      "Epoch 1888 Batch    0/96   train_loss = 0.873\n",
      "Epoch 1888 Batch   32/96   train_loss = 0.941\n",
      "Epoch 1888 Batch   64/96   train_loss = 0.907\n",
      "Epoch 1889 Batch    0/96   train_loss = 0.879\n",
      "Epoch 1889 Batch   32/96   train_loss = 0.938\n",
      "Epoch 1889 Batch   64/96   train_loss = 0.907\n",
      "Epoch 1890 Batch    0/96   train_loss = 0.883\n",
      "Epoch 1890 Batch   32/96   train_loss = 0.935\n",
      "Epoch 1890 Batch   64/96   train_loss = 0.923\n",
      "Epoch 1891 Batch    0/96   train_loss = 0.884\n",
      "Epoch 1891 Batch   32/96   train_loss = 0.936\n",
      "Epoch 1891 Batch   64/96   train_loss = 0.906\n",
      "Epoch 1892 Batch    0/96   train_loss = 0.878\n",
      "Epoch 1892 Batch   32/96   train_loss = 0.931\n",
      "Epoch 1892 Batch   64/96   train_loss = 0.914\n",
      "Epoch 1893 Batch    0/96   train_loss = 0.867\n",
      "Epoch 1893 Batch   32/96   train_loss = 0.929\n",
      "Epoch 1893 Batch   64/96   train_loss = 0.913\n",
      "Epoch 1894 Batch    0/96   train_loss = 0.884\n",
      "Epoch 1894 Batch   32/96   train_loss = 0.930\n",
      "Epoch 1894 Batch   64/96   train_loss = 0.926\n",
      "Epoch 1895 Batch    0/96   train_loss = 0.881\n",
      "Epoch 1895 Batch   32/96   train_loss = 0.942\n",
      "Epoch 1895 Batch   64/96   train_loss = 0.925\n",
      "Epoch 1896 Batch    0/96   train_loss = 0.889\n",
      "Epoch 1896 Batch   32/96   train_loss = 0.948\n",
      "Epoch 1896 Batch   64/96   train_loss = 0.930\n",
      "Epoch 1897 Batch    0/96   train_loss = 0.895\n",
      "Epoch 1897 Batch   32/96   train_loss = 0.936\n",
      "Epoch 1897 Batch   64/96   train_loss = 0.927\n",
      "Epoch 1898 Batch    0/96   train_loss = 0.891\n",
      "Epoch 1898 Batch   32/96   train_loss = 0.940\n",
      "Epoch 1898 Batch   64/96   train_loss = 0.920\n",
      "Epoch 1899 Batch    0/96   train_loss = 0.882\n",
      "Epoch 1899 Batch   32/96   train_loss = 0.970\n",
      "Epoch 1899 Batch   64/96   train_loss = 0.943\n",
      "Epoch 1900 Batch    0/96   train_loss = 0.887\n",
      "Epoch 1900 Batch   32/96   train_loss = 0.944\n",
      "Epoch 1900 Batch   64/96   train_loss = 0.926\n",
      "Epoch 1901 Batch    0/96   train_loss = 0.881\n",
      "Epoch 1901 Batch   32/96   train_loss = 0.947\n",
      "Epoch 1901 Batch   64/96   train_loss = 0.921\n",
      "Epoch 1902 Batch    0/96   train_loss = 0.881\n",
      "Epoch 1902 Batch   32/96   train_loss = 0.943\n",
      "Epoch 1902 Batch   64/96   train_loss = 0.911\n",
      "Epoch 1903 Batch    0/96   train_loss = 0.871\n",
      "Epoch 1903 Batch   32/96   train_loss = 0.948\n",
      "Epoch 1903 Batch   64/96   train_loss = 0.921\n",
      "Epoch 1904 Batch    0/96   train_loss = 0.872\n",
      "Epoch 1904 Batch   32/96   train_loss = 0.936\n",
      "Epoch 1904 Batch   64/96   train_loss = 0.922\n",
      "Epoch 1905 Batch    0/96   train_loss = 0.892\n",
      "Epoch 1905 Batch   32/96   train_loss = 0.937\n",
      "Epoch 1905 Batch   64/96   train_loss = 0.897\n",
      "Epoch 1906 Batch    0/96   train_loss = 0.865\n",
      "Epoch 1906 Batch   32/96   train_loss = 0.924\n",
      "Epoch 1906 Batch   64/96   train_loss = 0.910\n",
      "Epoch 1907 Batch    0/96   train_loss = 0.865\n",
      "Epoch 1907 Batch   32/96   train_loss = 0.940\n",
      "Epoch 1907 Batch   64/96   train_loss = 0.927\n",
      "Epoch 1908 Batch    0/96   train_loss = 0.879\n",
      "Epoch 1908 Batch   32/96   train_loss = 0.950\n",
      "Epoch 1908 Batch   64/96   train_loss = 0.917\n",
      "Epoch 1909 Batch    0/96   train_loss = 0.899\n",
      "Epoch 1909 Batch   32/96   train_loss = 0.945\n",
      "Epoch 1909 Batch   64/96   train_loss = 0.919\n",
      "Epoch 1910 Batch    0/96   train_loss = 0.890\n",
      "Epoch 1910 Batch   32/96   train_loss = 0.941\n",
      "Epoch 1910 Batch   64/96   train_loss = 0.926\n",
      "Epoch 1911 Batch    0/96   train_loss = 0.877\n",
      "Epoch 1911 Batch   32/96   train_loss = 0.929\n",
      "Epoch 1911 Batch   64/96   train_loss = 0.938\n",
      "Epoch 1912 Batch    0/96   train_loss = 0.878\n",
      "Epoch 1912 Batch   32/96   train_loss = 0.950\n",
      "Epoch 1912 Batch   64/96   train_loss = 0.930\n",
      "Epoch 1913 Batch    0/96   train_loss = 0.871\n",
      "Epoch 1913 Batch   32/96   train_loss = 0.943\n",
      "Epoch 1913 Batch   64/96   train_loss = 0.941\n",
      "Epoch 1914 Batch    0/96   train_loss = 0.890\n",
      "Epoch 1914 Batch   32/96   train_loss = 0.957\n",
      "Epoch 1914 Batch   64/96   train_loss = 0.917\n",
      "Epoch 1915 Batch    0/96   train_loss = 0.884\n",
      "Epoch 1915 Batch   32/96   train_loss = 0.941\n",
      "Epoch 1915 Batch   64/96   train_loss = 0.934\n",
      "Epoch 1916 Batch    0/96   train_loss = 0.875\n",
      "Epoch 1916 Batch   32/96   train_loss = 0.945\n",
      "Epoch 1916 Batch   64/96   train_loss = 0.935\n",
      "Epoch 1917 Batch    0/96   train_loss = 0.873\n",
      "Epoch 1917 Batch   32/96   train_loss = 0.942\n",
      "Epoch 1917 Batch   64/96   train_loss = 0.916\n",
      "Epoch 1918 Batch    0/96   train_loss = 0.884\n",
      "Epoch 1918 Batch   32/96   train_loss = 0.939\n",
      "Epoch 1918 Batch   64/96   train_loss = 0.910\n",
      "Epoch 1919 Batch    0/96   train_loss = 0.887\n",
      "Epoch 1919 Batch   32/96   train_loss = 0.942\n",
      "Epoch 1919 Batch   64/96   train_loss = 0.930\n",
      "Epoch 1920 Batch    0/96   train_loss = 0.919\n",
      "Epoch 1920 Batch   32/96   train_loss = 0.943\n",
      "Epoch 1920 Batch   64/96   train_loss = 0.926\n",
      "Epoch 1921 Batch    0/96   train_loss = 0.878\n",
      "Epoch 1921 Batch   32/96   train_loss = 0.949\n",
      "Epoch 1921 Batch   64/96   train_loss = 0.944\n",
      "Epoch 1922 Batch    0/96   train_loss = 0.883\n",
      "Epoch 1922 Batch   32/96   train_loss = 0.953\n",
      "Epoch 1922 Batch   64/96   train_loss = 0.926\n",
      "Epoch 1923 Batch    0/96   train_loss = 0.893\n",
      "Epoch 1923 Batch   32/96   train_loss = 0.938\n",
      "Epoch 1923 Batch   64/96   train_loss = 0.917\n",
      "Epoch 1924 Batch    0/96   train_loss = 0.887\n",
      "Epoch 1924 Batch   32/96   train_loss = 0.942\n",
      "Epoch 1924 Batch   64/96   train_loss = 0.922\n",
      "Epoch 1925 Batch    0/96   train_loss = 0.887\n",
      "Epoch 1925 Batch   32/96   train_loss = 0.960\n",
      "Epoch 1925 Batch   64/96   train_loss = 0.932\n",
      "Epoch 1926 Batch    0/96   train_loss = 0.894\n",
      "Epoch 1926 Batch   32/96   train_loss = 0.958\n",
      "Epoch 1926 Batch   64/96   train_loss = 0.929\n",
      "Epoch 1927 Batch    0/96   train_loss = 0.876\n",
      "Epoch 1927 Batch   32/96   train_loss = 0.953\n",
      "Epoch 1927 Batch   64/96   train_loss = 0.931\n",
      "Epoch 1928 Batch    0/96   train_loss = 0.889\n",
      "Epoch 1928 Batch   32/96   train_loss = 0.950\n",
      "Epoch 1928 Batch   64/96   train_loss = 0.920\n",
      "Epoch 1929 Batch    0/96   train_loss = 0.887\n",
      "Epoch 1929 Batch   32/96   train_loss = 0.941\n",
      "Epoch 1929 Batch   64/96   train_loss = 0.923\n",
      "Epoch 1930 Batch    0/96   train_loss = 0.879\n",
      "Epoch 1930 Batch   32/96   train_loss = 0.940\n",
      "Epoch 1930 Batch   64/96   train_loss = 0.916\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1931 Batch    0/96   train_loss = 0.891\n",
      "Epoch 1931 Batch   32/96   train_loss = 0.941\n",
      "Epoch 1931 Batch   64/96   train_loss = 0.914\n",
      "Epoch 1932 Batch    0/96   train_loss = 0.903\n",
      "Epoch 1932 Batch   32/96   train_loss = 0.936\n",
      "Epoch 1932 Batch   64/96   train_loss = 0.922\n",
      "Epoch 1933 Batch    0/96   train_loss = 0.891\n",
      "Epoch 1933 Batch   32/96   train_loss = 0.941\n",
      "Epoch 1933 Batch   64/96   train_loss = 0.943\n",
      "Epoch 1934 Batch    0/96   train_loss = 0.885\n",
      "Epoch 1934 Batch   32/96   train_loss = 0.942\n",
      "Epoch 1934 Batch   64/96   train_loss = 0.930\n",
      "Epoch 1935 Batch    0/96   train_loss = 0.889\n",
      "Epoch 1935 Batch   32/96   train_loss = 0.949\n",
      "Epoch 1935 Batch   64/96   train_loss = 0.915\n",
      "Epoch 1936 Batch    0/96   train_loss = 0.895\n",
      "Epoch 1936 Batch   32/96   train_loss = 0.943\n",
      "Epoch 1936 Batch   64/96   train_loss = 0.927\n",
      "Epoch 1937 Batch    0/96   train_loss = 0.883\n",
      "Epoch 1937 Batch   32/96   train_loss = 0.939\n",
      "Epoch 1937 Batch   64/96   train_loss = 0.925\n",
      "Epoch 1938 Batch    0/96   train_loss = 0.871\n",
      "Epoch 1938 Batch   32/96   train_loss = 0.921\n",
      "Epoch 1938 Batch   64/96   train_loss = 0.926\n",
      "Epoch 1939 Batch    0/96   train_loss = 0.881\n",
      "Epoch 1939 Batch   32/96   train_loss = 0.940\n",
      "Epoch 1939 Batch   64/96   train_loss = 0.918\n",
      "Epoch 1940 Batch    0/96   train_loss = 0.871\n",
      "Epoch 1940 Batch   32/96   train_loss = 0.933\n",
      "Epoch 1940 Batch   64/96   train_loss = 0.916\n",
      "Epoch 1941 Batch    0/96   train_loss = 0.889\n",
      "Epoch 1941 Batch   32/96   train_loss = 0.927\n",
      "Epoch 1941 Batch   64/96   train_loss = 0.903\n",
      "Epoch 1942 Batch    0/96   train_loss = 0.898\n",
      "Epoch 1942 Batch   32/96   train_loss = 0.957\n",
      "Epoch 1942 Batch   64/96   train_loss = 0.934\n",
      "Epoch 1943 Batch    0/96   train_loss = 0.909\n",
      "Epoch 1943 Batch   32/96   train_loss = 0.947\n",
      "Epoch 1943 Batch   64/96   train_loss = 0.920\n",
      "Epoch 1944 Batch    0/96   train_loss = 0.886\n",
      "Epoch 1944 Batch   32/96   train_loss = 0.934\n",
      "Epoch 1944 Batch   64/96   train_loss = 0.943\n",
      "Epoch 1945 Batch    0/96   train_loss = 0.887\n",
      "Epoch 1945 Batch   32/96   train_loss = 0.938\n",
      "Epoch 1945 Batch   64/96   train_loss = 0.918\n",
      "Epoch 1946 Batch    0/96   train_loss = 0.886\n",
      "Epoch 1946 Batch   32/96   train_loss = 0.939\n",
      "Epoch 1946 Batch   64/96   train_loss = 0.927\n",
      "Epoch 1947 Batch    0/96   train_loss = 0.876\n",
      "Epoch 1947 Batch   32/96   train_loss = 0.935\n",
      "Epoch 1947 Batch   64/96   train_loss = 0.902\n",
      "Epoch 1948 Batch    0/96   train_loss = 0.867\n",
      "Epoch 1948 Batch   32/96   train_loss = 0.930\n",
      "Epoch 1948 Batch   64/96   train_loss = 0.919\n",
      "Epoch 1949 Batch    0/96   train_loss = 0.878\n",
      "Epoch 1949 Batch   32/96   train_loss = 0.950\n",
      "Epoch 1949 Batch   64/96   train_loss = 0.919\n",
      "Epoch 1950 Batch    0/96   train_loss = 0.864\n",
      "Epoch 1950 Batch   32/96   train_loss = 0.954\n",
      "Epoch 1950 Batch   64/96   train_loss = 0.912\n",
      "Epoch 1951 Batch    0/96   train_loss = 0.871\n",
      "Epoch 1951 Batch   32/96   train_loss = 0.934\n",
      "Epoch 1951 Batch   64/96   train_loss = 0.905\n",
      "Epoch 1952 Batch    0/96   train_loss = 0.863\n",
      "Epoch 1952 Batch   32/96   train_loss = 0.947\n",
      "Epoch 1952 Batch   64/96   train_loss = 0.911\n",
      "Epoch 1953 Batch    0/96   train_loss = 0.890\n",
      "Epoch 1953 Batch   32/96   train_loss = 0.941\n",
      "Epoch 1953 Batch   64/96   train_loss = 0.926\n",
      "Epoch 1954 Batch    0/96   train_loss = 0.874\n",
      "Epoch 1954 Batch   32/96   train_loss = 0.939\n",
      "Epoch 1954 Batch   64/96   train_loss = 0.941\n",
      "Epoch 1955 Batch    0/96   train_loss = 0.880\n",
      "Epoch 1955 Batch   32/96   train_loss = 0.939\n",
      "Epoch 1955 Batch   64/96   train_loss = 0.920\n",
      "Epoch 1956 Batch    0/96   train_loss = 0.887\n",
      "Epoch 1956 Batch   32/96   train_loss = 0.943\n",
      "Epoch 1956 Batch   64/96   train_loss = 0.928\n",
      "Epoch 1957 Batch    0/96   train_loss = 0.879\n",
      "Epoch 1957 Batch   32/96   train_loss = 0.949\n",
      "Epoch 1957 Batch   64/96   train_loss = 0.929\n",
      "Epoch 1958 Batch    0/96   train_loss = 0.871\n",
      "Epoch 1958 Batch   32/96   train_loss = 0.954\n",
      "Epoch 1958 Batch   64/96   train_loss = 0.918\n",
      "Epoch 1959 Batch    0/96   train_loss = 0.891\n",
      "Epoch 1959 Batch   32/96   train_loss = 0.943\n",
      "Epoch 1959 Batch   64/96   train_loss = 0.931\n",
      "Epoch 1960 Batch    0/96   train_loss = 0.882\n",
      "Epoch 1960 Batch   32/96   train_loss = 0.934\n",
      "Epoch 1960 Batch   64/96   train_loss = 0.910\n",
      "Epoch 1961 Batch    0/96   train_loss = 0.871\n",
      "Epoch 1961 Batch   32/96   train_loss = 0.954\n",
      "Epoch 1961 Batch   64/96   train_loss = 0.914\n",
      "Epoch 1962 Batch    0/96   train_loss = 0.897\n",
      "Epoch 1962 Batch   32/96   train_loss = 0.962\n",
      "Epoch 1962 Batch   64/96   train_loss = 0.915\n",
      "Epoch 1963 Batch    0/96   train_loss = 0.899\n",
      "Epoch 1963 Batch   32/96   train_loss = 0.955\n",
      "Epoch 1963 Batch   64/96   train_loss = 0.922\n",
      "Epoch 1964 Batch    0/96   train_loss = 0.891\n",
      "Epoch 1964 Batch   32/96   train_loss = 0.955\n",
      "Epoch 1964 Batch   64/96   train_loss = 0.920\n",
      "Epoch 1965 Batch    0/96   train_loss = 0.881\n",
      "Epoch 1965 Batch   32/96   train_loss = 0.947\n",
      "Epoch 1965 Batch   64/96   train_loss = 0.919\n",
      "Epoch 1966 Batch    0/96   train_loss = 0.887\n",
      "Epoch 1966 Batch   32/96   train_loss = 0.946\n",
      "Epoch 1966 Batch   64/96   train_loss = 0.914\n",
      "Epoch 1967 Batch    0/96   train_loss = 0.880\n",
      "Epoch 1967 Batch   32/96   train_loss = 0.939\n",
      "Epoch 1967 Batch   64/96   train_loss = 0.907\n",
      "Epoch 1968 Batch    0/96   train_loss = 0.876\n",
      "Epoch 1968 Batch   32/96   train_loss = 0.948\n",
      "Epoch 1968 Batch   64/96   train_loss = 0.921\n",
      "Epoch 1969 Batch    0/96   train_loss = 0.869\n",
      "Epoch 1969 Batch   32/96   train_loss = 0.946\n",
      "Epoch 1969 Batch   64/96   train_loss = 0.903\n",
      "Epoch 1970 Batch    0/96   train_loss = 0.880\n",
      "Epoch 1970 Batch   32/96   train_loss = 0.946\n",
      "Epoch 1970 Batch   64/96   train_loss = 0.932\n",
      "Epoch 1971 Batch    0/96   train_loss = 0.894\n",
      "Epoch 1971 Batch   32/96   train_loss = 0.940\n",
      "Epoch 1971 Batch   64/96   train_loss = 0.905\n",
      "Epoch 1972 Batch    0/96   train_loss = 0.875\n",
      "Epoch 1972 Batch   32/96   train_loss = 0.934\n",
      "Epoch 1972 Batch   64/96   train_loss = 0.919\n",
      "Epoch 1973 Batch    0/96   train_loss = 0.885\n",
      "Epoch 1973 Batch   32/96   train_loss = 0.939\n",
      "Epoch 1973 Batch   64/96   train_loss = 0.927\n",
      "Epoch 1974 Batch    0/96   train_loss = 0.894\n",
      "Epoch 1974 Batch   32/96   train_loss = 0.944\n",
      "Epoch 1974 Batch   64/96   train_loss = 0.911\n",
      "Epoch 1975 Batch    0/96   train_loss = 0.874\n",
      "Epoch 1975 Batch   32/96   train_loss = 0.925\n",
      "Epoch 1975 Batch   64/96   train_loss = 0.898\n",
      "Epoch 1976 Batch    0/96   train_loss = 0.867\n",
      "Epoch 1976 Batch   32/96   train_loss = 0.917\n",
      "Epoch 1976 Batch   64/96   train_loss = 0.898\n",
      "Epoch 1977 Batch    0/96   train_loss = 0.864\n",
      "Epoch 1977 Batch   32/96   train_loss = 0.923\n",
      "Epoch 1977 Batch   64/96   train_loss = 0.903\n",
      "Epoch 1978 Batch    0/96   train_loss = 0.869\n",
      "Epoch 1978 Batch   32/96   train_loss = 0.934\n",
      "Epoch 1978 Batch   64/96   train_loss = 0.919\n",
      "Epoch 1979 Batch    0/96   train_loss = 0.871\n",
      "Epoch 1979 Batch   32/96   train_loss = 0.939\n",
      "Epoch 1979 Batch   64/96   train_loss = 0.895\n",
      "Epoch 1980 Batch    0/96   train_loss = 0.880\n",
      "Epoch 1980 Batch   32/96   train_loss = 0.938\n",
      "Epoch 1980 Batch   64/96   train_loss = 0.912\n",
      "Epoch 1981 Batch    0/96   train_loss = 0.872\n",
      "Epoch 1981 Batch   32/96   train_loss = 0.935\n",
      "Epoch 1981 Batch   64/96   train_loss = 0.917\n",
      "Epoch 1982 Batch    0/96   train_loss = 0.878\n",
      "Epoch 1982 Batch   32/96   train_loss = 0.926\n",
      "Epoch 1982 Batch   64/96   train_loss = 0.910\n",
      "Epoch 1983 Batch    0/96   train_loss = 0.858\n",
      "Epoch 1983 Batch   32/96   train_loss = 0.921\n",
      "Epoch 1983 Batch   64/96   train_loss = 0.914\n",
      "Epoch 1984 Batch    0/96   train_loss = 0.855\n",
      "Epoch 1984 Batch   32/96   train_loss = 0.928\n",
      "Epoch 1984 Batch   64/96   train_loss = 0.929\n",
      "Epoch 1985 Batch    0/96   train_loss = 0.869\n",
      "Epoch 1985 Batch   32/96   train_loss = 0.957\n",
      "Epoch 1985 Batch   64/96   train_loss = 0.926\n",
      "Epoch 1986 Batch    0/96   train_loss = 0.881\n",
      "Epoch 1986 Batch   32/96   train_loss = 0.951\n",
      "Epoch 1986 Batch   64/96   train_loss = 0.927\n",
      "Epoch 1987 Batch    0/96   train_loss = 0.885\n",
      "Epoch 1987 Batch   32/96   train_loss = 0.962\n",
      "Epoch 1987 Batch   64/96   train_loss = 0.930\n",
      "Epoch 1988 Batch    0/96   train_loss = 0.893\n",
      "Epoch 1988 Batch   32/96   train_loss = 0.956\n",
      "Epoch 1988 Batch   64/96   train_loss = 0.948\n",
      "Epoch 1989 Batch    0/96   train_loss = 0.881\n",
      "Epoch 1989 Batch   32/96   train_loss = 0.957\n",
      "Epoch 1989 Batch   64/96   train_loss = 0.944\n",
      "Epoch 1990 Batch    0/96   train_loss = 0.897\n",
      "Epoch 1990 Batch   32/96   train_loss = 0.962\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1990 Batch   64/96   train_loss = 0.968\n",
      "Epoch 1991 Batch    0/96   train_loss = 0.908\n",
      "Epoch 1991 Batch   32/96   train_loss = 0.959\n",
      "Epoch 1991 Batch   64/96   train_loss = 0.965\n",
      "Epoch 1992 Batch    0/96   train_loss = 0.911\n",
      "Epoch 1992 Batch   32/96   train_loss = 0.947\n",
      "Epoch 1992 Batch   64/96   train_loss = 0.945\n",
      "Epoch 1993 Batch    0/96   train_loss = 0.891\n",
      "Epoch 1993 Batch   32/96   train_loss = 0.949\n",
      "Epoch 1993 Batch   64/96   train_loss = 0.944\n",
      "Epoch 1994 Batch    0/96   train_loss = 0.903\n",
      "Epoch 1994 Batch   32/96   train_loss = 0.963\n",
      "Epoch 1994 Batch   64/96   train_loss = 0.956\n",
      "Epoch 1995 Batch    0/96   train_loss = 0.902\n",
      "Epoch 1995 Batch   32/96   train_loss = 0.966\n",
      "Epoch 1995 Batch   64/96   train_loss = 0.941\n",
      "Epoch 1996 Batch    0/96   train_loss = 0.902\n",
      "Epoch 1996 Batch   32/96   train_loss = 0.952\n",
      "Epoch 1996 Batch   64/96   train_loss = 0.957\n",
      "Epoch 1997 Batch    0/96   train_loss = 0.907\n",
      "Epoch 1997 Batch   32/96   train_loss = 0.948\n",
      "Epoch 1997 Batch   64/96   train_loss = 0.941\n",
      "Epoch 1998 Batch    0/96   train_loss = 0.889\n",
      "Epoch 1998 Batch   32/96   train_loss = 0.943\n",
      "Epoch 1998 Batch   64/96   train_loss = 0.944\n",
      "Epoch 1999 Batch    0/96   train_loss = 0.899\n",
      "Epoch 1999 Batch   32/96   train_loss = 0.968\n",
      "Epoch 1999 Batch   64/96   train_loss = 0.937\n",
      "Epoch 2000 Batch    0/96   train_loss = 0.895\n",
      "Epoch 2000 Batch   32/96   train_loss = 0.946\n",
      "Epoch 2000 Batch   64/96   train_loss = 0.942\n",
      "Epoch 2001 Batch    0/96   train_loss = 0.888\n",
      "Epoch 2001 Batch   32/96   train_loss = 0.945\n",
      "Epoch 2001 Batch   64/96   train_loss = 0.942\n",
      "Epoch 2002 Batch    0/96   train_loss = 0.899\n",
      "Epoch 2002 Batch   32/96   train_loss = 0.953\n",
      "Epoch 2002 Batch   64/96   train_loss = 0.964\n",
      "Epoch 2003 Batch    0/96   train_loss = 0.878\n",
      "Epoch 2003 Batch   32/96   train_loss = 0.945\n",
      "Epoch 2003 Batch   64/96   train_loss = 0.923\n",
      "Epoch 2004 Batch    0/96   train_loss = 0.879\n",
      "Epoch 2004 Batch   32/96   train_loss = 0.953\n",
      "Epoch 2004 Batch   64/96   train_loss = 0.939\n",
      "Epoch 2005 Batch    0/96   train_loss = 0.887\n",
      "Epoch 2005 Batch   32/96   train_loss = 0.955\n",
      "Epoch 2005 Batch   64/96   train_loss = 0.929\n",
      "Epoch 2006 Batch    0/96   train_loss = 0.890\n",
      "Epoch 2006 Batch   32/96   train_loss = 0.944\n",
      "Epoch 2006 Batch   64/96   train_loss = 0.933\n",
      "Epoch 2007 Batch    0/96   train_loss = 0.868\n",
      "Epoch 2007 Batch   32/96   train_loss = 0.932\n",
      "Epoch 2007 Batch   64/96   train_loss = 0.923\n",
      "Epoch 2008 Batch    0/96   train_loss = 0.874\n",
      "Epoch 2008 Batch   32/96   train_loss = 0.931\n",
      "Epoch 2008 Batch   64/96   train_loss = 0.921\n",
      "Epoch 2009 Batch    0/96   train_loss = 0.872\n",
      "Epoch 2009 Batch   32/96   train_loss = 0.931\n",
      "Epoch 2009 Batch   64/96   train_loss = 0.924\n",
      "Epoch 2010 Batch    0/96   train_loss = 0.870\n",
      "Epoch 2010 Batch   32/96   train_loss = 0.931\n",
      "Epoch 2010 Batch   64/96   train_loss = 0.917\n",
      "Epoch 2011 Batch    0/96   train_loss = 0.875\n",
      "Epoch 2011 Batch   32/96   train_loss = 0.926\n",
      "Epoch 2011 Batch   64/96   train_loss = 0.923\n",
      "Epoch 2012 Batch    0/96   train_loss = 0.870\n",
      "Epoch 2012 Batch   32/96   train_loss = 0.937\n",
      "Epoch 2012 Batch   64/96   train_loss = 0.905\n",
      "Epoch 2013 Batch    0/96   train_loss = 0.881\n",
      "Epoch 2013 Batch   32/96   train_loss = 0.932\n",
      "Epoch 2013 Batch   64/96   train_loss = 0.908\n",
      "Epoch 2014 Batch    0/96   train_loss = 0.877\n",
      "Epoch 2014 Batch   32/96   train_loss = 0.935\n",
      "Epoch 2014 Batch   64/96   train_loss = 0.904\n",
      "Epoch 2015 Batch    0/96   train_loss = 0.874\n",
      "Epoch 2015 Batch   32/96   train_loss = 0.943\n",
      "Epoch 2015 Batch   64/96   train_loss = 0.907\n",
      "Epoch 2016 Batch    0/96   train_loss = 0.882\n",
      "Epoch 2016 Batch   32/96   train_loss = 0.931\n",
      "Epoch 2016 Batch   64/96   train_loss = 0.925\n",
      "Epoch 2017 Batch    0/96   train_loss = 0.876\n",
      "Epoch 2017 Batch   32/96   train_loss = 0.922\n",
      "Epoch 2017 Batch   64/96   train_loss = 0.911\n",
      "Epoch 2018 Batch    0/96   train_loss = 0.870\n",
      "Epoch 2018 Batch   32/96   train_loss = 0.931\n",
      "Epoch 2018 Batch   64/96   train_loss = 0.919\n",
      "Epoch 2019 Batch    0/96   train_loss = 0.870\n",
      "Epoch 2019 Batch   32/96   train_loss = 0.934\n",
      "Epoch 2019 Batch   64/96   train_loss = 0.918\n",
      "Epoch 2020 Batch    0/96   train_loss = 0.869\n",
      "Epoch 2020 Batch   32/96   train_loss = 0.937\n",
      "Epoch 2020 Batch   64/96   train_loss = 0.916\n",
      "Epoch 2021 Batch    0/96   train_loss = 0.871\n",
      "Epoch 2021 Batch   32/96   train_loss = 0.926\n",
      "Epoch 2021 Batch   64/96   train_loss = 0.907\n",
      "Epoch 2022 Batch    0/96   train_loss = 0.873\n",
      "Epoch 2022 Batch   32/96   train_loss = 0.932\n",
      "Epoch 2022 Batch   64/96   train_loss = 0.934\n",
      "Epoch 2023 Batch    0/96   train_loss = 0.889\n",
      "Epoch 2023 Batch   32/96   train_loss = 0.931\n",
      "Epoch 2023 Batch   64/96   train_loss = 0.926\n",
      "Epoch 2024 Batch    0/96   train_loss = 0.878\n",
      "Epoch 2024 Batch   32/96   train_loss = 0.942\n",
      "Epoch 2024 Batch   64/96   train_loss = 0.928\n",
      "Epoch 2025 Batch    0/96   train_loss = 0.878\n",
      "Epoch 2025 Batch   32/96   train_loss = 0.944\n",
      "Epoch 2025 Batch   64/96   train_loss = 0.931\n",
      "Epoch 2026 Batch    0/96   train_loss = 0.880\n",
      "Epoch 2026 Batch   32/96   train_loss = 0.934\n",
      "Epoch 2026 Batch   64/96   train_loss = 0.941\n",
      "Epoch 2027 Batch    0/96   train_loss = 0.898\n",
      "Epoch 2027 Batch   32/96   train_loss = 0.952\n",
      "Epoch 2027 Batch   64/96   train_loss = 0.925\n",
      "Epoch 2028 Batch    0/96   train_loss = 0.877\n",
      "Epoch 2028 Batch   32/96   train_loss = 0.950\n",
      "Epoch 2028 Batch   64/96   train_loss = 0.922\n",
      "Epoch 2029 Batch    0/96   train_loss = 0.879\n",
      "Epoch 2029 Batch   32/96   train_loss = 0.950\n",
      "Epoch 2029 Batch   64/96   train_loss = 0.924\n",
      "Epoch 2030 Batch    0/96   train_loss = 0.873\n",
      "Epoch 2030 Batch   32/96   train_loss = 0.957\n",
      "Epoch 2030 Batch   64/96   train_loss = 0.921\n",
      "Epoch 2031 Batch    0/96   train_loss = 0.893\n",
      "Epoch 2031 Batch   32/96   train_loss = 0.941\n",
      "Epoch 2031 Batch   64/96   train_loss = 0.940\n",
      "Epoch 2032 Batch    0/96   train_loss = 0.900\n",
      "Epoch 2032 Batch   32/96   train_loss = 0.950\n",
      "Epoch 2032 Batch   64/96   train_loss = 0.938\n",
      "Epoch 2033 Batch    0/96   train_loss = 0.886\n",
      "Epoch 2033 Batch   32/96   train_loss = 0.965\n",
      "Epoch 2033 Batch   64/96   train_loss = 0.929\n",
      "Epoch 2034 Batch    0/96   train_loss = 0.910\n",
      "Epoch 2034 Batch   32/96   train_loss = 0.976\n",
      "Epoch 2034 Batch   64/96   train_loss = 0.950\n",
      "Epoch 2035 Batch    0/96   train_loss = 0.896\n",
      "Epoch 2035 Batch   32/96   train_loss = 0.970\n",
      "Epoch 2035 Batch   64/96   train_loss = 0.930\n",
      "Epoch 2036 Batch    0/96   train_loss = 0.895\n",
      "Epoch 2036 Batch   32/96   train_loss = 0.966\n",
      "Epoch 2036 Batch   64/96   train_loss = 0.926\n",
      "Epoch 2037 Batch    0/96   train_loss = 0.887\n",
      "Epoch 2037 Batch   32/96   train_loss = 0.959\n",
      "Epoch 2037 Batch   64/96   train_loss = 0.926\n",
      "Epoch 2038 Batch    0/96   train_loss = 0.891\n",
      "Epoch 2038 Batch   32/96   train_loss = 0.957\n",
      "Epoch 2038 Batch   64/96   train_loss = 0.921\n",
      "Epoch 2039 Batch    0/96   train_loss = 0.865\n",
      "Epoch 2039 Batch   32/96   train_loss = 0.950\n",
      "Epoch 2039 Batch   64/96   train_loss = 0.915\n",
      "Epoch 2040 Batch    0/96   train_loss = 0.890\n",
      "Epoch 2040 Batch   32/96   train_loss = 0.955\n",
      "Epoch 2040 Batch   64/96   train_loss = 0.910\n",
      "Epoch 2041 Batch    0/96   train_loss = 0.873\n",
      "Epoch 2041 Batch   32/96   train_loss = 0.952\n",
      "Epoch 2041 Batch   64/96   train_loss = 0.918\n",
      "Epoch 2042 Batch    0/96   train_loss = 0.861\n",
      "Epoch 2042 Batch   32/96   train_loss = 0.959\n",
      "Epoch 2042 Batch   64/96   train_loss = 0.912\n",
      "Epoch 2043 Batch    0/96   train_loss = 0.899\n",
      "Epoch 2043 Batch   32/96   train_loss = 0.942\n",
      "Epoch 2043 Batch   64/96   train_loss = 0.909\n",
      "Epoch 2044 Batch    0/96   train_loss = 0.885\n",
      "Epoch 2044 Batch   32/96   train_loss = 0.952\n",
      "Epoch 2044 Batch   64/96   train_loss = 0.918\n",
      "Epoch 2045 Batch    0/96   train_loss = 0.880\n",
      "Epoch 2045 Batch   32/96   train_loss = 0.946\n",
      "Epoch 2045 Batch   64/96   train_loss = 0.907\n",
      "Epoch 2046 Batch    0/96   train_loss = 0.881\n",
      "Epoch 2046 Batch   32/96   train_loss = 0.952\n",
      "Epoch 2046 Batch   64/96   train_loss = 0.915\n",
      "Epoch 2047 Batch    0/96   train_loss = 0.883\n",
      "Epoch 2047 Batch   32/96   train_loss = 0.961\n",
      "Epoch 2047 Batch   64/96   train_loss = 0.927\n",
      "Epoch 2048 Batch    0/96   train_loss = 0.895\n",
      "Epoch 2048 Batch   32/96   train_loss = 0.953\n",
      "Epoch 2048 Batch   64/96   train_loss = 0.906\n",
      "Epoch 2049 Batch    0/96   train_loss = 0.876\n",
      "Epoch 2049 Batch   32/96   train_loss = 0.937\n",
      "Epoch 2049 Batch   64/96   train_loss = 0.936\n",
      "Epoch 2050 Batch    0/96   train_loss = 0.888\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2050 Batch   32/96   train_loss = 0.944\n",
      "Epoch 2050 Batch   64/96   train_loss = 0.920\n",
      "Epoch 2051 Batch    0/96   train_loss = 0.879\n",
      "Epoch 2051 Batch   32/96   train_loss = 0.942\n",
      "Epoch 2051 Batch   64/96   train_loss = 0.917\n",
      "Epoch 2052 Batch    0/96   train_loss = 0.875\n",
      "Epoch 2052 Batch   32/96   train_loss = 0.946\n",
      "Epoch 2052 Batch   64/96   train_loss = 0.913\n",
      "Epoch 2053 Batch    0/96   train_loss = 0.878\n",
      "Epoch 2053 Batch   32/96   train_loss = 0.927\n",
      "Epoch 2053 Batch   64/96   train_loss = 0.923\n",
      "Epoch 2054 Batch    0/96   train_loss = 0.884\n",
      "Epoch 2054 Batch   32/96   train_loss = 0.935\n",
      "Epoch 2054 Batch   64/96   train_loss = 0.921\n",
      "Epoch 2055 Batch    0/96   train_loss = 0.905\n",
      "Epoch 2055 Batch   32/96   train_loss = 0.952\n",
      "Epoch 2055 Batch   64/96   train_loss = 0.914\n",
      "Epoch 2056 Batch    0/96   train_loss = 0.884\n",
      "Epoch 2056 Batch   32/96   train_loss = 0.938\n",
      "Epoch 2056 Batch   64/96   train_loss = 0.932\n",
      "Epoch 2057 Batch    0/96   train_loss = 0.893\n",
      "Epoch 2057 Batch   32/96   train_loss = 0.932\n",
      "Epoch 2057 Batch   64/96   train_loss = 0.924\n",
      "Epoch 2058 Batch    0/96   train_loss = 0.886\n",
      "Epoch 2058 Batch   32/96   train_loss = 0.958\n",
      "Epoch 2058 Batch   64/96   train_loss = 0.937\n",
      "Epoch 2059 Batch    0/96   train_loss = 0.896\n",
      "Epoch 2059 Batch   32/96   train_loss = 0.973\n",
      "Epoch 2059 Batch   64/96   train_loss = 0.943\n",
      "Epoch 2060 Batch    0/96   train_loss = 0.892\n",
      "Epoch 2060 Batch   32/96   train_loss = 0.962\n",
      "Epoch 2060 Batch   64/96   train_loss = 0.936\n",
      "Epoch 2061 Batch    0/96   train_loss = 0.902\n",
      "Epoch 2061 Batch   32/96   train_loss = 0.952\n",
      "Epoch 2061 Batch   64/96   train_loss = 0.934\n",
      "Epoch 2062 Batch    0/96   train_loss = 0.889\n",
      "Epoch 2062 Batch   32/96   train_loss = 0.965\n",
      "Epoch 2062 Batch   64/96   train_loss = 0.927\n",
      "Epoch 2063 Batch    0/96   train_loss = 0.872\n",
      "Epoch 2063 Batch   32/96   train_loss = 0.961\n",
      "Epoch 2063 Batch   64/96   train_loss = 0.923\n",
      "Epoch 2064 Batch    0/96   train_loss = 0.874\n",
      "Epoch 2064 Batch   32/96   train_loss = 0.949\n",
      "Epoch 2064 Batch   64/96   train_loss = 0.918\n",
      "Epoch 2065 Batch    0/96   train_loss = 0.873\n",
      "Epoch 2065 Batch   32/96   train_loss = 0.943\n",
      "Epoch 2065 Batch   64/96   train_loss = 0.923\n",
      "Epoch 2066 Batch    0/96   train_loss = 0.878\n",
      "Epoch 2066 Batch   32/96   train_loss = 0.942\n",
      "Epoch 2066 Batch   64/96   train_loss = 0.925\n",
      "Epoch 2067 Batch    0/96   train_loss = 0.868\n",
      "Epoch 2067 Batch   32/96   train_loss = 0.924\n",
      "Epoch 2067 Batch   64/96   train_loss = 0.908\n",
      "Epoch 2068 Batch    0/96   train_loss = 0.865\n",
      "Epoch 2068 Batch   32/96   train_loss = 0.932\n",
      "Epoch 2068 Batch   64/96   train_loss = 0.898\n",
      "Epoch 2069 Batch    0/96   train_loss = 0.868\n",
      "Epoch 2069 Batch   32/96   train_loss = 0.927\n",
      "Epoch 2069 Batch   64/96   train_loss = 0.902\n",
      "Epoch 2070 Batch    0/96   train_loss = 0.864\n",
      "Epoch 2070 Batch   32/96   train_loss = 0.913\n",
      "Epoch 2070 Batch   64/96   train_loss = 0.912\n",
      "Epoch 2071 Batch    0/96   train_loss = 0.857\n",
      "Epoch 2071 Batch   32/96   train_loss = 0.923\n",
      "Epoch 2071 Batch   64/96   train_loss = 0.895\n",
      "Epoch 2072 Batch    0/96   train_loss = 0.861\n",
      "Epoch 2072 Batch   32/96   train_loss = 0.927\n",
      "Epoch 2072 Batch   64/96   train_loss = 0.897\n",
      "Epoch 2073 Batch    0/96   train_loss = 0.860\n",
      "Epoch 2073 Batch   32/96   train_loss = 0.947\n",
      "Epoch 2073 Batch   64/96   train_loss = 0.891\n",
      "Epoch 2074 Batch    0/96   train_loss = 0.867\n",
      "Epoch 2074 Batch   32/96   train_loss = 0.929\n",
      "Epoch 2074 Batch   64/96   train_loss = 0.912\n",
      "Epoch 2075 Batch    0/96   train_loss = 0.877\n",
      "Epoch 2075 Batch   32/96   train_loss = 0.929\n",
      "Epoch 2075 Batch   64/96   train_loss = 0.910\n",
      "Epoch 2076 Batch    0/96   train_loss = 0.874\n",
      "Epoch 2076 Batch   32/96   train_loss = 0.933\n",
      "Epoch 2076 Batch   64/96   train_loss = 0.920\n",
      "Epoch 2077 Batch    0/96   train_loss = 0.868\n",
      "Epoch 2077 Batch   32/96   train_loss = 0.929\n",
      "Epoch 2077 Batch   64/96   train_loss = 0.921\n",
      "Epoch 2078 Batch    0/96   train_loss = 0.873\n",
      "Epoch 2078 Batch   32/96   train_loss = 0.941\n",
      "Epoch 2078 Batch   64/96   train_loss = 0.924\n",
      "Epoch 2079 Batch    0/96   train_loss = 0.868\n",
      "Epoch 2079 Batch   32/96   train_loss = 0.930\n",
      "Epoch 2079 Batch   64/96   train_loss = 0.914\n",
      "Epoch 2080 Batch    0/96   train_loss = 0.874\n",
      "Epoch 2080 Batch   32/96   train_loss = 0.931\n",
      "Epoch 2080 Batch   64/96   train_loss = 0.922\n",
      "Epoch 2081 Batch    0/96   train_loss = 0.879\n",
      "Epoch 2081 Batch   32/96   train_loss = 0.934\n",
      "Epoch 2081 Batch   64/96   train_loss = 0.925\n",
      "Epoch 2082 Batch    0/96   train_loss = 0.876\n",
      "Epoch 2082 Batch   32/96   train_loss = 0.920\n",
      "Epoch 2082 Batch   64/96   train_loss = 0.931\n",
      "Epoch 2083 Batch    0/96   train_loss = 0.881\n",
      "Epoch 2083 Batch   32/96   train_loss = 0.941\n",
      "Epoch 2083 Batch   64/96   train_loss = 0.931\n",
      "Epoch 2084 Batch    0/96   train_loss = 0.876\n",
      "Epoch 2084 Batch   32/96   train_loss = 0.947\n",
      "Epoch 2084 Batch   64/96   train_loss = 0.937\n",
      "Epoch 2085 Batch    0/96   train_loss = 0.881\n",
      "Epoch 2085 Batch   32/96   train_loss = 0.952\n",
      "Epoch 2085 Batch   64/96   train_loss = 0.952\n",
      "Epoch 2086 Batch    0/96   train_loss = 0.896\n",
      "Epoch 2086 Batch   32/96   train_loss = 0.943\n",
      "Epoch 2086 Batch   64/96   train_loss = 0.927\n",
      "Epoch 2087 Batch    0/96   train_loss = 0.894\n",
      "Epoch 2087 Batch   32/96   train_loss = 0.947\n",
      "Epoch 2087 Batch   64/96   train_loss = 0.928\n",
      "Epoch 2088 Batch    0/96   train_loss = 0.892\n",
      "Epoch 2088 Batch   32/96   train_loss = 0.949\n",
      "Epoch 2088 Batch   64/96   train_loss = 0.925\n",
      "Epoch 2089 Batch    0/96   train_loss = 0.879\n",
      "Epoch 2089 Batch   32/96   train_loss = 0.930\n",
      "Epoch 2089 Batch   64/96   train_loss = 0.926\n",
      "Epoch 2090 Batch    0/96   train_loss = 0.876\n",
      "Epoch 2090 Batch   32/96   train_loss = 0.951\n",
      "Epoch 2090 Batch   64/96   train_loss = 0.948\n",
      "Epoch 2091 Batch    0/96   train_loss = 0.889\n",
      "Epoch 2091 Batch   32/96   train_loss = 0.937\n",
      "Epoch 2091 Batch   64/96   train_loss = 0.954\n",
      "Epoch 2092 Batch    0/96   train_loss = 0.897\n",
      "Epoch 2092 Batch   32/96   train_loss = 0.948\n",
      "Epoch 2092 Batch   64/96   train_loss = 0.943\n",
      "Epoch 2093 Batch    0/96   train_loss = 0.877\n",
      "Epoch 2093 Batch   32/96   train_loss = 0.947\n",
      "Epoch 2093 Batch   64/96   train_loss = 0.927\n",
      "Epoch 2094 Batch    0/96   train_loss = 0.898\n",
      "Epoch 2094 Batch   32/96   train_loss = 0.952\n",
      "Epoch 2094 Batch   64/96   train_loss = 0.924\n",
      "Epoch 2095 Batch    0/96   train_loss = 0.905\n",
      "Epoch 2095 Batch   32/96   train_loss = 0.949\n",
      "Epoch 2095 Batch   64/96   train_loss = 0.955\n",
      "Epoch 2096 Batch    0/96   train_loss = 0.897\n",
      "Epoch 2096 Batch   32/96   train_loss = 0.948\n",
      "Epoch 2096 Batch   64/96   train_loss = 0.934\n",
      "Epoch 2097 Batch    0/96   train_loss = 0.897\n",
      "Epoch 2097 Batch   32/96   train_loss = 0.950\n",
      "Epoch 2097 Batch   64/96   train_loss = 0.942\n",
      "Epoch 2098 Batch    0/96   train_loss = 0.903\n",
      "Epoch 2098 Batch   32/96   train_loss = 0.956\n",
      "Epoch 2098 Batch   64/96   train_loss = 0.936\n",
      "Epoch 2099 Batch    0/96   train_loss = 0.920\n",
      "Epoch 2099 Batch   32/96   train_loss = 0.962\n",
      "Epoch 2099 Batch   64/96   train_loss = 0.917\n",
      "Epoch 2100 Batch    0/96   train_loss = 0.897\n",
      "Epoch 2100 Batch   32/96   train_loss = 0.946\n",
      "Epoch 2100 Batch   64/96   train_loss = 0.922\n",
      "Epoch 2101 Batch    0/96   train_loss = 0.887\n",
      "Epoch 2101 Batch   32/96   train_loss = 0.964\n",
      "Epoch 2101 Batch   64/96   train_loss = 0.929\n",
      "Epoch 2102 Batch    0/96   train_loss = 0.898\n",
      "Epoch 2102 Batch   32/96   train_loss = 0.961\n",
      "Epoch 2102 Batch   64/96   train_loss = 0.915\n",
      "Epoch 2103 Batch    0/96   train_loss = 0.910\n",
      "Epoch 2103 Batch   32/96   train_loss = 0.952\n",
      "Epoch 2103 Batch   64/96   train_loss = 0.933\n",
      "Epoch 2104 Batch    0/96   train_loss = 0.883\n",
      "Epoch 2104 Batch   32/96   train_loss = 0.937\n",
      "Epoch 2104 Batch   64/96   train_loss = 0.911\n",
      "Epoch 2105 Batch    0/96   train_loss = 0.890\n",
      "Epoch 2105 Batch   32/96   train_loss = 0.939\n",
      "Epoch 2105 Batch   64/96   train_loss = 0.919\n",
      "Epoch 2106 Batch    0/96   train_loss = 0.898\n",
      "Epoch 2106 Batch   32/96   train_loss = 0.946\n",
      "Epoch 2106 Batch   64/96   train_loss = 0.920\n",
      "Epoch 2107 Batch    0/96   train_loss = 0.889\n",
      "Epoch 2107 Batch   32/96   train_loss = 0.950\n",
      "Epoch 2107 Batch   64/96   train_loss = 0.909\n",
      "Epoch 2108 Batch    0/96   train_loss = 0.879\n",
      "Epoch 2108 Batch   32/96   train_loss = 0.938\n",
      "Epoch 2108 Batch   64/96   train_loss = 0.930\n",
      "Epoch 2109 Batch    0/96   train_loss = 0.894\n",
      "Epoch 2109 Batch   32/96   train_loss = 0.950\n",
      "Epoch 2109 Batch   64/96   train_loss = 0.916\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2110 Batch    0/96   train_loss = 0.889\n",
      "Epoch 2110 Batch   32/96   train_loss = 0.934\n",
      "Epoch 2110 Batch   64/96   train_loss = 0.909\n",
      "Epoch 2111 Batch    0/96   train_loss = 0.903\n",
      "Epoch 2111 Batch   32/96   train_loss = 0.940\n",
      "Epoch 2111 Batch   64/96   train_loss = 0.933\n",
      "Epoch 2112 Batch    0/96   train_loss = 0.890\n",
      "Epoch 2112 Batch   32/96   train_loss = 0.942\n",
      "Epoch 2112 Batch   64/96   train_loss = 0.940\n",
      "Epoch 2113 Batch    0/96   train_loss = 0.884\n",
      "Epoch 2113 Batch   32/96   train_loss = 0.961\n",
      "Epoch 2113 Batch   64/96   train_loss = 0.933\n",
      "Epoch 2114 Batch    0/96   train_loss = 0.885\n",
      "Epoch 2114 Batch   32/96   train_loss = 0.968\n",
      "Epoch 2114 Batch   64/96   train_loss = 0.938\n",
      "Epoch 2115 Batch    0/96   train_loss = 0.894\n",
      "Epoch 2115 Batch   32/96   train_loss = 0.953\n",
      "Epoch 2115 Batch   64/96   train_loss = 0.943\n",
      "Epoch 2116 Batch    0/96   train_loss = 0.900\n",
      "Epoch 2116 Batch   32/96   train_loss = 0.950\n",
      "Epoch 2116 Batch   64/96   train_loss = 0.939\n",
      "Epoch 2117 Batch    0/96   train_loss = 0.903\n",
      "Epoch 2117 Batch   32/96   train_loss = 0.963\n",
      "Epoch 2117 Batch   64/96   train_loss = 0.957\n",
      "Epoch 2118 Batch    0/96   train_loss = 0.913\n",
      "Epoch 2118 Batch   32/96   train_loss = 0.956\n",
      "Epoch 2118 Batch   64/96   train_loss = 0.960\n",
      "Epoch 2119 Batch    0/96   train_loss = 0.893\n",
      "Epoch 2119 Batch   32/96   train_loss = 0.958\n",
      "Epoch 2119 Batch   64/96   train_loss = 0.939\n",
      "Epoch 2120 Batch    0/96   train_loss = 0.902\n",
      "Epoch 2120 Batch   32/96   train_loss = 0.949\n",
      "Epoch 2120 Batch   64/96   train_loss = 0.932\n",
      "Epoch 2121 Batch    0/96   train_loss = 0.914\n",
      "Epoch 2121 Batch   32/96   train_loss = 0.973\n",
      "Epoch 2121 Batch   64/96   train_loss = 0.930\n",
      "Epoch 2122 Batch    0/96   train_loss = 0.900\n",
      "Epoch 2122 Batch   32/96   train_loss = 0.972\n",
      "Epoch 2122 Batch   64/96   train_loss = 0.922\n",
      "Epoch 2123 Batch    0/96   train_loss = 0.903\n",
      "Epoch 2123 Batch   32/96   train_loss = 0.966\n",
      "Epoch 2123 Batch   64/96   train_loss = 0.927\n",
      "Epoch 2124 Batch    0/96   train_loss = 0.899\n",
      "Epoch 2124 Batch   32/96   train_loss = 0.966\n",
      "Epoch 2124 Batch   64/96   train_loss = 0.922\n",
      "Epoch 2125 Batch    0/96   train_loss = 0.904\n",
      "Epoch 2125 Batch   32/96   train_loss = 0.960\n",
      "Epoch 2125 Batch   64/96   train_loss = 0.928\n",
      "Epoch 2126 Batch    0/96   train_loss = 0.905\n",
      "Epoch 2126 Batch   32/96   train_loss = 0.969\n",
      "Epoch 2126 Batch   64/96   train_loss = 0.926\n",
      "Epoch 2127 Batch    0/96   train_loss = 0.924\n",
      "Epoch 2127 Batch   32/96   train_loss = 0.958\n",
      "Epoch 2127 Batch   64/96   train_loss = 0.933\n",
      "Epoch 2128 Batch    0/96   train_loss = 0.903\n",
      "Epoch 2128 Batch   32/96   train_loss = 0.977\n",
      "Epoch 2128 Batch   64/96   train_loss = 0.933\n",
      "Epoch 2129 Batch    0/96   train_loss = 0.900\n",
      "Epoch 2129 Batch   32/96   train_loss = 0.960\n",
      "Epoch 2129 Batch   64/96   train_loss = 0.932\n",
      "Epoch 2130 Batch    0/96   train_loss = 0.913\n",
      "Epoch 2130 Batch   32/96   train_loss = 0.959\n",
      "Epoch 2130 Batch   64/96   train_loss = 0.929\n",
      "Epoch 2131 Batch    0/96   train_loss = 0.913\n",
      "Epoch 2131 Batch   32/96   train_loss = 0.979\n",
      "Epoch 2131 Batch   64/96   train_loss = 0.979\n",
      "Epoch 2132 Batch    0/96   train_loss = 0.926\n",
      "Epoch 2132 Batch   32/96   train_loss = 0.981\n",
      "Epoch 2132 Batch   64/96   train_loss = 0.956\n",
      "Epoch 2133 Batch    0/96   train_loss = 0.931\n",
      "Epoch 2133 Batch   32/96   train_loss = 0.979\n",
      "Epoch 2133 Batch   64/96   train_loss = 0.960\n",
      "Epoch 2134 Batch    0/96   train_loss = 0.919\n",
      "Epoch 2134 Batch   32/96   train_loss = 0.973\n",
      "Epoch 2134 Batch   64/96   train_loss = 0.972\n",
      "Epoch 2135 Batch    0/96   train_loss = 0.909\n",
      "Epoch 2135 Batch   32/96   train_loss = 0.984\n",
      "Epoch 2135 Batch   64/96   train_loss = 0.953\n",
      "Epoch 2136 Batch    0/96   train_loss = 0.940\n",
      "Epoch 2136 Batch   32/96   train_loss = 0.973\n",
      "Epoch 2136 Batch   64/96   train_loss = 0.964\n",
      "Epoch 2137 Batch    0/96   train_loss = 0.924\n",
      "Epoch 2137 Batch   32/96   train_loss = 0.970\n",
      "Epoch 2137 Batch   64/96   train_loss = 0.953\n",
      "Epoch 2138 Batch    0/96   train_loss = 0.908\n",
      "Epoch 2138 Batch   32/96   train_loss = 0.975\n",
      "Epoch 2138 Batch   64/96   train_loss = 0.955\n",
      "Epoch 2139 Batch    0/96   train_loss = 0.892\n",
      "Epoch 2139 Batch   32/96   train_loss = 0.937\n",
      "Epoch 2139 Batch   64/96   train_loss = 0.947\n",
      "Epoch 2140 Batch    0/96   train_loss = 0.909\n",
      "Epoch 2140 Batch   32/96   train_loss = 0.961\n",
      "Epoch 2140 Batch   64/96   train_loss = 0.946\n",
      "Epoch 2141 Batch    0/96   train_loss = 0.898\n",
      "Epoch 2141 Batch   32/96   train_loss = 0.944\n",
      "Epoch 2141 Batch   64/96   train_loss = 0.924\n",
      "Epoch 2142 Batch    0/96   train_loss = 0.881\n",
      "Epoch 2142 Batch   32/96   train_loss = 0.930\n",
      "Epoch 2142 Batch   64/96   train_loss = 0.922\n",
      "Epoch 2143 Batch    0/96   train_loss = 0.868\n",
      "Epoch 2143 Batch   32/96   train_loss = 0.936\n",
      "Epoch 2143 Batch   64/96   train_loss = 0.930\n",
      "Epoch 2144 Batch    0/96   train_loss = 0.893\n",
      "Epoch 2144 Batch   32/96   train_loss = 0.938\n",
      "Epoch 2144 Batch   64/96   train_loss = 0.913\n",
      "Epoch 2145 Batch    0/96   train_loss = 0.870\n",
      "Epoch 2145 Batch   32/96   train_loss = 0.937\n",
      "Epoch 2145 Batch   64/96   train_loss = 0.917\n",
      "Epoch 2146 Batch    0/96   train_loss = 0.877\n",
      "Epoch 2146 Batch   32/96   train_loss = 0.946\n",
      "Epoch 2146 Batch   64/96   train_loss = 0.914\n",
      "Epoch 2147 Batch    0/96   train_loss = 0.877\n",
      "Epoch 2147 Batch   32/96   train_loss = 0.941\n",
      "Epoch 2147 Batch   64/96   train_loss = 0.916\n",
      "Epoch 2148 Batch    0/96   train_loss = 0.885\n",
      "Epoch 2148 Batch   32/96   train_loss = 0.943\n",
      "Epoch 2148 Batch   64/96   train_loss = 0.926\n",
      "Epoch 2149 Batch    0/96   train_loss = 0.889\n",
      "Epoch 2149 Batch   32/96   train_loss = 0.932\n",
      "Epoch 2149 Batch   64/96   train_loss = 0.922\n",
      "Epoch 2150 Batch    0/96   train_loss = 0.891\n",
      "Epoch 2150 Batch   32/96   train_loss = 0.948\n",
      "Epoch 2150 Batch   64/96   train_loss = 0.924\n",
      "Epoch 2151 Batch    0/96   train_loss = 0.883\n",
      "Epoch 2151 Batch   32/96   train_loss = 0.981\n",
      "Epoch 2151 Batch   64/96   train_loss = 0.931\n",
      "Epoch 2152 Batch    0/96   train_loss = 0.893\n",
      "Epoch 2152 Batch   32/96   train_loss = 0.963\n",
      "Epoch 2152 Batch   64/96   train_loss = 0.924\n",
      "Epoch 2153 Batch    0/96   train_loss = 0.887\n",
      "Epoch 2153 Batch   32/96   train_loss = 0.958\n",
      "Epoch 2153 Batch   64/96   train_loss = 0.928\n",
      "Epoch 2154 Batch    0/96   train_loss = 0.877\n",
      "Epoch 2154 Batch   32/96   train_loss = 0.953\n",
      "Epoch 2154 Batch   64/96   train_loss = 0.936\n",
      "Epoch 2155 Batch    0/96   train_loss = 0.886\n",
      "Epoch 2155 Batch   32/96   train_loss = 0.954\n",
      "Epoch 2155 Batch   64/96   train_loss = 0.928\n",
      "Epoch 2156 Batch    0/96   train_loss = 0.897\n",
      "Epoch 2156 Batch   32/96   train_loss = 0.961\n",
      "Epoch 2156 Batch   64/96   train_loss = 0.922\n",
      "Epoch 2157 Batch    0/96   train_loss = 0.878\n",
      "Epoch 2157 Batch   32/96   train_loss = 0.954\n",
      "Epoch 2157 Batch   64/96   train_loss = 0.911\n",
      "Epoch 2158 Batch    0/96   train_loss = 0.883\n",
      "Epoch 2158 Batch   32/96   train_loss = 0.941\n",
      "Epoch 2158 Batch   64/96   train_loss = 0.919\n",
      "Epoch 2159 Batch    0/96   train_loss = 0.888\n",
      "Epoch 2159 Batch   32/96   train_loss = 0.943\n",
      "Epoch 2159 Batch   64/96   train_loss = 0.918\n",
      "Epoch 2160 Batch    0/96   train_loss = 0.897\n",
      "Epoch 2160 Batch   32/96   train_loss = 0.946\n",
      "Epoch 2160 Batch   64/96   train_loss = 0.929\n",
      "Epoch 2161 Batch    0/96   train_loss = 0.887\n",
      "Epoch 2161 Batch   32/96   train_loss = 0.954\n",
      "Epoch 2161 Batch   64/96   train_loss = 0.929\n",
      "Epoch 2162 Batch    0/96   train_loss = 0.906\n",
      "Epoch 2162 Batch   32/96   train_loss = 0.964\n",
      "Epoch 2162 Batch   64/96   train_loss = 0.930\n",
      "Epoch 2163 Batch    0/96   train_loss = 0.884\n",
      "Epoch 2163 Batch   32/96   train_loss = 0.939\n",
      "Epoch 2163 Batch   64/96   train_loss = 0.928\n",
      "Epoch 2164 Batch    0/96   train_loss = 0.895\n",
      "Epoch 2164 Batch   32/96   train_loss = 0.947\n",
      "Epoch 2164 Batch   64/96   train_loss = 0.933\n",
      "Epoch 2165 Batch    0/96   train_loss = 0.885\n",
      "Epoch 2165 Batch   32/96   train_loss = 0.953\n",
      "Epoch 2165 Batch   64/96   train_loss = 0.930\n",
      "Epoch 2166 Batch    0/96   train_loss = 0.903\n",
      "Epoch 2166 Batch   32/96   train_loss = 0.941\n",
      "Epoch 2166 Batch   64/96   train_loss = 0.927\n",
      "Epoch 2167 Batch    0/96   train_loss = 0.892\n",
      "Epoch 2167 Batch   32/96   train_loss = 0.953\n",
      "Epoch 2167 Batch   64/96   train_loss = 0.931\n",
      "Epoch 2168 Batch    0/96   train_loss = 0.889\n",
      "Epoch 2168 Batch   32/96   train_loss = 0.950\n",
      "Epoch 2168 Batch   64/96   train_loss = 0.927\n",
      "Epoch 2169 Batch    0/96   train_loss = 0.895\n",
      "Epoch 2169 Batch   32/96   train_loss = 0.957\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2169 Batch   64/96   train_loss = 0.929\n",
      "Epoch 2170 Batch    0/96   train_loss = 0.897\n",
      "Epoch 2170 Batch   32/96   train_loss = 0.968\n",
      "Epoch 2170 Batch   64/96   train_loss = 0.930\n",
      "Epoch 2171 Batch    0/96   train_loss = 0.893\n",
      "Epoch 2171 Batch   32/96   train_loss = 0.967\n",
      "Epoch 2171 Batch   64/96   train_loss = 0.942\n",
      "Epoch 2172 Batch    0/96   train_loss = 0.915\n",
      "Epoch 2172 Batch   32/96   train_loss = 0.979\n",
      "Epoch 2172 Batch   64/96   train_loss = 0.927\n",
      "Epoch 2173 Batch    0/96   train_loss = 0.907\n",
      "Epoch 2173 Batch   32/96   train_loss = 0.970\n",
      "Epoch 2173 Batch   64/96   train_loss = 0.947\n",
      "Epoch 2174 Batch    0/96   train_loss = 0.887\n",
      "Epoch 2174 Batch   32/96   train_loss = 0.961\n",
      "Epoch 2174 Batch   64/96   train_loss = 0.924\n",
      "Epoch 2175 Batch    0/96   train_loss = 0.909\n",
      "Epoch 2175 Batch   32/96   train_loss = 0.960\n",
      "Epoch 2175 Batch   64/96   train_loss = 0.941\n",
      "Epoch 2176 Batch    0/96   train_loss = 0.906\n",
      "Epoch 2176 Batch   32/96   train_loss = 0.962\n",
      "Epoch 2176 Batch   64/96   train_loss = 0.919\n",
      "Epoch 2177 Batch    0/96   train_loss = 0.893\n",
      "Epoch 2177 Batch   32/96   train_loss = 0.972\n",
      "Epoch 2177 Batch   64/96   train_loss = 0.939\n",
      "Epoch 2178 Batch    0/96   train_loss = 0.909\n",
      "Epoch 2178 Batch   32/96   train_loss = 0.950\n",
      "Epoch 2178 Batch   64/96   train_loss = 0.924\n",
      "Epoch 2179 Batch    0/96   train_loss = 0.901\n",
      "Epoch 2179 Batch   32/96   train_loss = 0.956\n",
      "Epoch 2179 Batch   64/96   train_loss = 0.918\n",
      "Epoch 2180 Batch    0/96   train_loss = 0.901\n",
      "Epoch 2180 Batch   32/96   train_loss = 0.941\n",
      "Epoch 2180 Batch   64/96   train_loss = 0.919\n",
      "Epoch 2181 Batch    0/96   train_loss = 0.893\n",
      "Epoch 2181 Batch   32/96   train_loss = 0.974\n",
      "Epoch 2181 Batch   64/96   train_loss = 0.937\n",
      "Epoch 2182 Batch    0/96   train_loss = 0.905\n",
      "Epoch 2182 Batch   32/96   train_loss = 0.967\n",
      "Epoch 2182 Batch   64/96   train_loss = 0.944\n",
      "Epoch 2183 Batch    0/96   train_loss = 0.904\n",
      "Epoch 2183 Batch   32/96   train_loss = 0.968\n",
      "Epoch 2183 Batch   64/96   train_loss = 0.949\n",
      "Epoch 2184 Batch    0/96   train_loss = 0.915\n",
      "Epoch 2184 Batch   32/96   train_loss = 0.975\n",
      "Epoch 2184 Batch   64/96   train_loss = 0.943\n",
      "Epoch 2185 Batch    0/96   train_loss = 0.891\n",
      "Epoch 2185 Batch   32/96   train_loss = 0.976\n",
      "Epoch 2185 Batch   64/96   train_loss = 0.949\n",
      "Epoch 2186 Batch    0/96   train_loss = 0.910\n",
      "Epoch 2186 Batch   32/96   train_loss = 0.985\n",
      "Epoch 2186 Batch   64/96   train_loss = 0.955\n",
      "Epoch 2187 Batch    0/96   train_loss = 0.912\n",
      "Epoch 2187 Batch   32/96   train_loss = 0.959\n",
      "Epoch 2187 Batch   64/96   train_loss = 0.956\n",
      "Epoch 2188 Batch    0/96   train_loss = 0.911\n",
      "Epoch 2188 Batch   32/96   train_loss = 0.956\n",
      "Epoch 2188 Batch   64/96   train_loss = 0.935\n",
      "Epoch 2189 Batch    0/96   train_loss = 0.914\n",
      "Epoch 2189 Batch   32/96   train_loss = 0.973\n",
      "Epoch 2189 Batch   64/96   train_loss = 0.938\n",
      "Epoch 2190 Batch    0/96   train_loss = 0.911\n",
      "Epoch 2190 Batch   32/96   train_loss = 0.975\n",
      "Epoch 2190 Batch   64/96   train_loss = 0.940\n",
      "Epoch 2191 Batch    0/96   train_loss = 0.909\n",
      "Epoch 2191 Batch   32/96   train_loss = 0.954\n",
      "Epoch 2191 Batch   64/96   train_loss = 0.934\n",
      "Epoch 2192 Batch    0/96   train_loss = 0.899\n",
      "Epoch 2192 Batch   32/96   train_loss = 0.940\n",
      "Epoch 2192 Batch   64/96   train_loss = 0.929\n",
      "Epoch 2193 Batch    0/96   train_loss = 0.903\n",
      "Epoch 2193 Batch   32/96   train_loss = 0.956\n",
      "Epoch 2193 Batch   64/96   train_loss = 0.945\n",
      "Epoch 2194 Batch    0/96   train_loss = 0.925\n",
      "Epoch 2194 Batch   32/96   train_loss = 0.964\n",
      "Epoch 2194 Batch   64/96   train_loss = 0.934\n",
      "Epoch 2195 Batch    0/96   train_loss = 0.894\n",
      "Epoch 2195 Batch   32/96   train_loss = 0.936\n",
      "Epoch 2195 Batch   64/96   train_loss = 0.930\n",
      "Epoch 2196 Batch    0/96   train_loss = 0.902\n",
      "Epoch 2196 Batch   32/96   train_loss = 0.966\n",
      "Epoch 2196 Batch   64/96   train_loss = 0.938\n",
      "Epoch 2197 Batch    0/96   train_loss = 0.915\n",
      "Epoch 2197 Batch   32/96   train_loss = 0.980\n",
      "Epoch 2197 Batch   64/96   train_loss = 0.956\n",
      "Epoch 2198 Batch    0/96   train_loss = 0.922\n",
      "Epoch 2198 Batch   32/96   train_loss = 0.967\n",
      "Epoch 2198 Batch   64/96   train_loss = 0.953\n",
      "Epoch 2199 Batch    0/96   train_loss = 0.898\n",
      "Epoch 2199 Batch   32/96   train_loss = 0.957\n",
      "Epoch 2199 Batch   64/96   train_loss = 0.948\n",
      "Epoch 2200 Batch    0/96   train_loss = 0.900\n",
      "Epoch 2200 Batch   32/96   train_loss = 0.951\n",
      "Epoch 2200 Batch   64/96   train_loss = 0.931\n",
      "Epoch 2201 Batch    0/96   train_loss = 0.900\n",
      "Epoch 2201 Batch   32/96   train_loss = 0.953\n",
      "Epoch 2201 Batch   64/96   train_loss = 0.935\n",
      "Epoch 2202 Batch    0/96   train_loss = 0.901\n",
      "Epoch 2202 Batch   32/96   train_loss = 0.953\n",
      "Epoch 2202 Batch   64/96   train_loss = 0.938\n",
      "Epoch 2203 Batch    0/96   train_loss = 0.899\n",
      "Epoch 2203 Batch   32/96   train_loss = 0.946\n",
      "Epoch 2203 Batch   64/96   train_loss = 0.923\n",
      "Epoch 2204 Batch    0/96   train_loss = 0.883\n",
      "Epoch 2204 Batch   32/96   train_loss = 0.938\n",
      "Epoch 2204 Batch   64/96   train_loss = 0.929\n",
      "Epoch 2205 Batch    0/96   train_loss = 0.887\n",
      "Epoch 2205 Batch   32/96   train_loss = 0.929\n",
      "Epoch 2205 Batch   64/96   train_loss = 0.922\n",
      "Epoch 2206 Batch    0/96   train_loss = 0.885\n",
      "Epoch 2206 Batch   32/96   train_loss = 0.949\n",
      "Epoch 2206 Batch   64/96   train_loss = 0.936\n",
      "Epoch 2207 Batch    0/96   train_loss = 0.899\n",
      "Epoch 2207 Batch   32/96   train_loss = 0.954\n",
      "Epoch 2207 Batch   64/96   train_loss = 0.945\n",
      "Epoch 2208 Batch    0/96   train_loss = 0.911\n",
      "Epoch 2208 Batch   32/96   train_loss = 0.961\n",
      "Epoch 2208 Batch   64/96   train_loss = 0.948\n",
      "Epoch 2209 Batch    0/96   train_loss = 0.902\n",
      "Epoch 2209 Batch   32/96   train_loss = 0.958\n",
      "Epoch 2209 Batch   64/96   train_loss = 0.939\n",
      "Epoch 2210 Batch    0/96   train_loss = 0.885\n",
      "Epoch 2210 Batch   32/96   train_loss = 0.957\n",
      "Epoch 2210 Batch   64/96   train_loss = 0.925\n",
      "Epoch 2211 Batch    0/96   train_loss = 0.888\n",
      "Epoch 2211 Batch   32/96   train_loss = 0.941\n",
      "Epoch 2211 Batch   64/96   train_loss = 0.938\n",
      "Epoch 2212 Batch    0/96   train_loss = 0.908\n",
      "Epoch 2212 Batch   32/96   train_loss = 0.949\n",
      "Epoch 2212 Batch   64/96   train_loss = 0.937\n",
      "Epoch 2213 Batch    0/96   train_loss = 0.902\n",
      "Epoch 2213 Batch   32/96   train_loss = 0.937\n",
      "Epoch 2213 Batch   64/96   train_loss = 0.938\n",
      "Epoch 2214 Batch    0/96   train_loss = 0.907\n",
      "Epoch 2214 Batch   32/96   train_loss = 0.948\n",
      "Epoch 2214 Batch   64/96   train_loss = 0.919\n",
      "Epoch 2215 Batch    0/96   train_loss = 0.889\n",
      "Epoch 2215 Batch   32/96   train_loss = 0.940\n",
      "Epoch 2215 Batch   64/96   train_loss = 0.932\n",
      "Epoch 2216 Batch    0/96   train_loss = 0.878\n",
      "Epoch 2216 Batch   32/96   train_loss = 0.939\n",
      "Epoch 2216 Batch   64/96   train_loss = 0.915\n",
      "Epoch 2217 Batch    0/96   train_loss = 0.902\n",
      "Epoch 2217 Batch   32/96   train_loss = 0.932\n",
      "Epoch 2217 Batch   64/96   train_loss = 0.928\n",
      "Epoch 2218 Batch    0/96   train_loss = 0.887\n",
      "Epoch 2218 Batch   32/96   train_loss = 0.939\n",
      "Epoch 2218 Batch   64/96   train_loss = 0.930\n",
      "Epoch 2219 Batch    0/96   train_loss = 0.883\n",
      "Epoch 2219 Batch   32/96   train_loss = 0.942\n",
      "Epoch 2219 Batch   64/96   train_loss = 0.934\n",
      "Epoch 2220 Batch    0/96   train_loss = 0.884\n",
      "Epoch 2220 Batch   32/96   train_loss = 0.933\n",
      "Epoch 2220 Batch   64/96   train_loss = 0.922\n",
      "Epoch 2221 Batch    0/96   train_loss = 0.900\n",
      "Epoch 2221 Batch   32/96   train_loss = 0.946\n",
      "Epoch 2221 Batch   64/96   train_loss = 0.932\n",
      "Epoch 2222 Batch    0/96   train_loss = 0.873\n",
      "Epoch 2222 Batch   32/96   train_loss = 0.936\n",
      "Epoch 2222 Batch   64/96   train_loss = 0.918\n",
      "Epoch 2223 Batch    0/96   train_loss = 0.881\n",
      "Epoch 2223 Batch   32/96   train_loss = 0.931\n",
      "Epoch 2223 Batch   64/96   train_loss = 0.916\n",
      "Epoch 2224 Batch    0/96   train_loss = 0.889\n",
      "Epoch 2224 Batch   32/96   train_loss = 0.946\n",
      "Epoch 2224 Batch   64/96   train_loss = 0.908\n",
      "Epoch 2225 Batch    0/96   train_loss = 0.899\n",
      "Epoch 2225 Batch   32/96   train_loss = 0.935\n",
      "Epoch 2225 Batch   64/96   train_loss = 0.895\n",
      "Epoch 2226 Batch    0/96   train_loss = 0.887\n",
      "Epoch 2226 Batch   32/96   train_loss = 0.930\n",
      "Epoch 2226 Batch   64/96   train_loss = 0.912\n",
      "Epoch 2227 Batch    0/96   train_loss = 0.897\n",
      "Epoch 2227 Batch   32/96   train_loss = 0.942\n",
      "Epoch 2227 Batch   64/96   train_loss = 0.914\n",
      "Epoch 2228 Batch    0/96   train_loss = 0.890\n",
      "Epoch 2228 Batch   32/96   train_loss = 0.946\n",
      "Epoch 2228 Batch   64/96   train_loss = 0.929\n",
      "Epoch 2229 Batch    0/96   train_loss = 0.897\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2229 Batch   32/96   train_loss = 0.949\n",
      "Epoch 2229 Batch   64/96   train_loss = 0.919\n",
      "Epoch 2230 Batch    0/96   train_loss = 0.907\n",
      "Epoch 2230 Batch   32/96   train_loss = 0.952\n",
      "Epoch 2230 Batch   64/96   train_loss = 0.932\n",
      "Epoch 2231 Batch    0/96   train_loss = 0.913\n",
      "Epoch 2231 Batch   32/96   train_loss = 0.968\n",
      "Epoch 2231 Batch   64/96   train_loss = 0.937\n",
      "Epoch 2232 Batch    0/96   train_loss = 0.902\n",
      "Epoch 2232 Batch   32/96   train_loss = 0.966\n",
      "Epoch 2232 Batch   64/96   train_loss = 0.950\n",
      "Epoch 2233 Batch    0/96   train_loss = 0.915\n",
      "Epoch 2233 Batch   32/96   train_loss = 0.974\n",
      "Epoch 2233 Batch   64/96   train_loss = 0.944\n",
      "Epoch 2234 Batch    0/96   train_loss = 0.912\n",
      "Epoch 2234 Batch   32/96   train_loss = 0.959\n",
      "Epoch 2234 Batch   64/96   train_loss = 0.935\n",
      "Epoch 2235 Batch    0/96   train_loss = 0.901\n",
      "Epoch 2235 Batch   32/96   train_loss = 0.960\n",
      "Epoch 2235 Batch   64/96   train_loss = 0.940\n",
      "Epoch 2236 Batch    0/96   train_loss = 0.886\n",
      "Epoch 2236 Batch   32/96   train_loss = 0.949\n",
      "Epoch 2236 Batch   64/96   train_loss = 0.934\n",
      "Epoch 2237 Batch    0/96   train_loss = 0.897\n",
      "Epoch 2237 Batch   32/96   train_loss = 0.980\n",
      "Epoch 2237 Batch   64/96   train_loss = 0.932\n",
      "Epoch 2238 Batch    0/96   train_loss = 0.892\n",
      "Epoch 2238 Batch   32/96   train_loss = 0.961\n",
      "Epoch 2238 Batch   64/96   train_loss = 0.937\n",
      "Epoch 2239 Batch    0/96   train_loss = 0.891\n",
      "Epoch 2239 Batch   32/96   train_loss = 0.939\n",
      "Epoch 2239 Batch   64/96   train_loss = 0.929\n",
      "Epoch 2240 Batch    0/96   train_loss = 0.896\n",
      "Epoch 2240 Batch   32/96   train_loss = 0.954\n",
      "Epoch 2240 Batch   64/96   train_loss = 0.910\n",
      "Epoch 2241 Batch    0/96   train_loss = 0.891\n",
      "Epoch 2241 Batch   32/96   train_loss = 0.961\n",
      "Epoch 2241 Batch   64/96   train_loss = 0.928\n",
      "Epoch 2242 Batch    0/96   train_loss = 0.901\n",
      "Epoch 2242 Batch   32/96   train_loss = 0.955\n",
      "Epoch 2242 Batch   64/96   train_loss = 0.934\n",
      "Epoch 2243 Batch    0/96   train_loss = 0.897\n",
      "Epoch 2243 Batch   32/96   train_loss = 0.953\n",
      "Epoch 2243 Batch   64/96   train_loss = 0.925\n",
      "Epoch 2244 Batch    0/96   train_loss = 0.900\n",
      "Epoch 2244 Batch   32/96   train_loss = 0.950\n",
      "Epoch 2244 Batch   64/96   train_loss = 0.928\n",
      "Epoch 2245 Batch    0/96   train_loss = 0.903\n",
      "Epoch 2245 Batch   32/96   train_loss = 0.944\n",
      "Epoch 2245 Batch   64/96   train_loss = 0.930\n",
      "Epoch 2246 Batch    0/96   train_loss = 0.898\n",
      "Epoch 2246 Batch   32/96   train_loss = 0.949\n",
      "Epoch 2246 Batch   64/96   train_loss = 0.916\n",
      "Epoch 2247 Batch    0/96   train_loss = 0.905\n",
      "Epoch 2247 Batch   32/96   train_loss = 0.956\n",
      "Epoch 2247 Batch   64/96   train_loss = 0.935\n",
      "Epoch 2248 Batch    0/96   train_loss = 0.908\n",
      "Epoch 2248 Batch   32/96   train_loss = 0.969\n",
      "Epoch 2248 Batch   64/96   train_loss = 0.941\n",
      "Epoch 2249 Batch    0/96   train_loss = 0.897\n",
      "Epoch 2249 Batch   32/96   train_loss = 0.953\n",
      "Epoch 2249 Batch   64/96   train_loss = 0.935\n",
      "Epoch 2250 Batch    0/96   train_loss = 0.893\n",
      "Epoch 2250 Batch   32/96   train_loss = 0.966\n",
      "Epoch 2250 Batch   64/96   train_loss = 0.934\n",
      "Epoch 2251 Batch    0/96   train_loss = 0.894\n",
      "Epoch 2251 Batch   32/96   train_loss = 0.971\n",
      "Epoch 2251 Batch   64/96   train_loss = 0.946\n",
      "Epoch 2252 Batch    0/96   train_loss = 0.902\n",
      "Epoch 2252 Batch   32/96   train_loss = 0.952\n",
      "Epoch 2252 Batch   64/96   train_loss = 0.947\n",
      "Epoch 2253 Batch    0/96   train_loss = 0.900\n",
      "Epoch 2253 Batch   32/96   train_loss = 0.986\n",
      "Epoch 2253 Batch   64/96   train_loss = 0.953\n",
      "Epoch 2254 Batch    0/96   train_loss = 0.925\n",
      "Epoch 2254 Batch   32/96   train_loss = 0.977\n",
      "Epoch 2254 Batch   64/96   train_loss = 0.980\n",
      "Epoch 2255 Batch    0/96   train_loss = 0.909\n",
      "Epoch 2255 Batch   32/96   train_loss = 0.967\n",
      "Epoch 2255 Batch   64/96   train_loss = 0.964\n",
      "Epoch 2256 Batch    0/96   train_loss = 0.922\n",
      "Epoch 2256 Batch   32/96   train_loss = 0.971\n",
      "Epoch 2256 Batch   64/96   train_loss = 0.944\n",
      "Epoch 2257 Batch    0/96   train_loss = 0.910\n",
      "Epoch 2257 Batch   32/96   train_loss = 0.981\n",
      "Epoch 2257 Batch   64/96   train_loss = 0.939\n",
      "Epoch 2258 Batch    0/96   train_loss = 0.905\n",
      "Epoch 2258 Batch   32/96   train_loss = 0.980\n",
      "Epoch 2258 Batch   64/96   train_loss = 0.947\n",
      "Epoch 2259 Batch    0/96   train_loss = 0.896\n",
      "Epoch 2259 Batch   32/96   train_loss = 0.953\n",
      "Epoch 2259 Batch   64/96   train_loss = 0.939\n",
      "Epoch 2260 Batch    0/96   train_loss = 0.897\n",
      "Epoch 2260 Batch   32/96   train_loss = 0.958\n",
      "Epoch 2260 Batch   64/96   train_loss = 0.928\n",
      "Epoch 2261 Batch    0/96   train_loss = 0.889\n",
      "Epoch 2261 Batch   32/96   train_loss = 0.963\n",
      "Epoch 2261 Batch   64/96   train_loss = 0.918\n",
      "Epoch 2262 Batch    0/96   train_loss = 0.888\n",
      "Epoch 2262 Batch   32/96   train_loss = 0.949\n",
      "Epoch 2262 Batch   64/96   train_loss = 0.941\n",
      "Epoch 2263 Batch    0/96   train_loss = 0.901\n",
      "Epoch 2263 Batch   32/96   train_loss = 0.949\n",
      "Epoch 2263 Batch   64/96   train_loss = 0.932\n",
      "Epoch 2264 Batch    0/96   train_loss = 0.900\n",
      "Epoch 2264 Batch   32/96   train_loss = 0.950\n",
      "Epoch 2264 Batch   64/96   train_loss = 0.953\n",
      "Epoch 2265 Batch    0/96   train_loss = 0.910\n",
      "Epoch 2265 Batch   32/96   train_loss = 0.958\n",
      "Epoch 2265 Batch   64/96   train_loss = 0.928\n",
      "Epoch 2266 Batch    0/96   train_loss = 0.900\n",
      "Epoch 2266 Batch   32/96   train_loss = 0.970\n",
      "Epoch 2266 Batch   64/96   train_loss = 0.933\n",
      "Epoch 2267 Batch    0/96   train_loss = 0.901\n",
      "Epoch 2267 Batch   32/96   train_loss = 0.949\n",
      "Epoch 2267 Batch   64/96   train_loss = 0.945\n",
      "Epoch 2268 Batch    0/96   train_loss = 0.902\n",
      "Epoch 2268 Batch   32/96   train_loss = 0.981\n",
      "Epoch 2268 Batch   64/96   train_loss = 0.938\n",
      "Epoch 2269 Batch    0/96   train_loss = 0.908\n",
      "Epoch 2269 Batch   32/96   train_loss = 0.964\n",
      "Epoch 2269 Batch   64/96   train_loss = 0.960\n",
      "Epoch 2270 Batch    0/96   train_loss = 0.914\n",
      "Epoch 2270 Batch   32/96   train_loss = 0.975\n",
      "Epoch 2270 Batch   64/96   train_loss = 0.955\n",
      "Epoch 2271 Batch    0/96   train_loss = 0.917\n",
      "Epoch 2271 Batch   32/96   train_loss = 0.975\n",
      "Epoch 2271 Batch   64/96   train_loss = 0.938\n",
      "Epoch 2272 Batch    0/96   train_loss = 0.903\n",
      "Epoch 2272 Batch   32/96   train_loss = 0.964\n",
      "Epoch 2272 Batch   64/96   train_loss = 0.946\n",
      "Epoch 2273 Batch    0/96   train_loss = 0.905\n",
      "Epoch 2273 Batch   32/96   train_loss = 0.975\n",
      "Epoch 2273 Batch   64/96   train_loss = 0.950\n",
      "Epoch 2274 Batch    0/96   train_loss = 0.899\n",
      "Epoch 2274 Batch   32/96   train_loss = 0.954\n",
      "Epoch 2274 Batch   64/96   train_loss = 0.935\n",
      "Epoch 2275 Batch    0/96   train_loss = 0.888\n",
      "Epoch 2275 Batch   32/96   train_loss = 0.971\n",
      "Epoch 2275 Batch   64/96   train_loss = 0.949\n",
      "Epoch 2276 Batch    0/96   train_loss = 0.911\n",
      "Epoch 2276 Batch   32/96   train_loss = 0.949\n",
      "Epoch 2276 Batch   64/96   train_loss = 0.934\n",
      "Epoch 2277 Batch    0/96   train_loss = 0.891\n",
      "Epoch 2277 Batch   32/96   train_loss = 0.952\n",
      "Epoch 2277 Batch   64/96   train_loss = 0.948\n",
      "Epoch 2278 Batch    0/96   train_loss = 0.904\n",
      "Epoch 2278 Batch   32/96   train_loss = 0.969\n",
      "Epoch 2278 Batch   64/96   train_loss = 0.932\n",
      "Epoch 2279 Batch    0/96   train_loss = 0.896\n",
      "Epoch 2279 Batch   32/96   train_loss = 0.953\n",
      "Epoch 2279 Batch   64/96   train_loss = 0.925\n",
      "Epoch 2280 Batch    0/96   train_loss = 0.894\n",
      "Epoch 2280 Batch   32/96   train_loss = 0.952\n",
      "Epoch 2280 Batch   64/96   train_loss = 0.925\n",
      "Epoch 2281 Batch    0/96   train_loss = 0.879\n",
      "Epoch 2281 Batch   32/96   train_loss = 0.947\n",
      "Epoch 2281 Batch   64/96   train_loss = 0.921\n",
      "Epoch 2282 Batch    0/96   train_loss = 0.887\n",
      "Epoch 2282 Batch   32/96   train_loss = 0.954\n",
      "Epoch 2282 Batch   64/96   train_loss = 0.925\n",
      "Epoch 2283 Batch    0/96   train_loss = 0.888\n",
      "Epoch 2283 Batch   32/96   train_loss = 0.950\n",
      "Epoch 2283 Batch   64/96   train_loss = 0.920\n",
      "Epoch 2284 Batch    0/96   train_loss = 0.880\n",
      "Epoch 2284 Batch   32/96   train_loss = 0.956\n",
      "Epoch 2284 Batch   64/96   train_loss = 0.932\n",
      "Epoch 2285 Batch    0/96   train_loss = 0.896\n",
      "Epoch 2285 Batch   32/96   train_loss = 0.946\n",
      "Epoch 2285 Batch   64/96   train_loss = 0.937\n",
      "Epoch 2286 Batch    0/96   train_loss = 0.888\n",
      "Epoch 2286 Batch   32/96   train_loss = 0.939\n",
      "Epoch 2286 Batch   64/96   train_loss = 0.946\n",
      "Epoch 2287 Batch    0/96   train_loss = 0.881\n",
      "Epoch 2287 Batch   32/96   train_loss = 0.955\n",
      "Epoch 2287 Batch   64/96   train_loss = 0.943\n",
      "Epoch 2288 Batch    0/96   train_loss = 0.909\n",
      "Epoch 2288 Batch   32/96   train_loss = 0.976\n",
      "Epoch 2288 Batch   64/96   train_loss = 0.957\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2289 Batch    0/96   train_loss = 0.907\n",
      "Epoch 2289 Batch   32/96   train_loss = 0.959\n",
      "Epoch 2289 Batch   64/96   train_loss = 0.944\n",
      "Epoch 2290 Batch    0/96   train_loss = 0.908\n",
      "Epoch 2290 Batch   32/96   train_loss = 0.963\n",
      "Epoch 2290 Batch   64/96   train_loss = 0.932\n",
      "Epoch 2291 Batch    0/96   train_loss = 0.898\n",
      "Epoch 2291 Batch   32/96   train_loss = 0.965\n",
      "Epoch 2291 Batch   64/96   train_loss = 0.936\n",
      "Epoch 2292 Batch    0/96   train_loss = 0.896\n",
      "Epoch 2292 Batch   32/96   train_loss = 0.960\n",
      "Epoch 2292 Batch   64/96   train_loss = 0.945\n",
      "Epoch 2293 Batch    0/96   train_loss = 0.893\n",
      "Epoch 2293 Batch   32/96   train_loss = 0.961\n",
      "Epoch 2293 Batch   64/96   train_loss = 0.955\n",
      "Epoch 2294 Batch    0/96   train_loss = 0.890\n",
      "Epoch 2294 Batch   32/96   train_loss = 0.954\n",
      "Epoch 2294 Batch   64/96   train_loss = 0.949\n",
      "Epoch 2295 Batch    0/96   train_loss = 0.894\n",
      "Epoch 2295 Batch   32/96   train_loss = 0.963\n",
      "Epoch 2295 Batch   64/96   train_loss = 0.944\n",
      "Epoch 2296 Batch    0/96   train_loss = 0.902\n",
      "Epoch 2296 Batch   32/96   train_loss = 0.975\n",
      "Epoch 2296 Batch   64/96   train_loss = 0.952\n",
      "Epoch 2297 Batch    0/96   train_loss = 0.893\n",
      "Epoch 2297 Batch   32/96   train_loss = 0.973\n",
      "Epoch 2297 Batch   64/96   train_loss = 0.959\n",
      "Epoch 2298 Batch    0/96   train_loss = 0.900\n",
      "Epoch 2298 Batch   32/96   train_loss = 0.968\n",
      "Epoch 2298 Batch   64/96   train_loss = 0.943\n",
      "Epoch 2299 Batch    0/96   train_loss = 0.903\n",
      "Epoch 2299 Batch   32/96   train_loss = 0.974\n",
      "Epoch 2299 Batch   64/96   train_loss = 0.955\n",
      "Epoch 2300 Batch    0/96   train_loss = 0.900\n",
      "Epoch 2300 Batch   32/96   train_loss = 0.969\n",
      "Epoch 2300 Batch   64/96   train_loss = 0.950\n",
      "Epoch 2301 Batch    0/96   train_loss = 0.905\n",
      "Epoch 2301 Batch   32/96   train_loss = 0.975\n",
      "Epoch 2301 Batch   64/96   train_loss = 0.956\n",
      "Epoch 2302 Batch    0/96   train_loss = 0.897\n",
      "Epoch 2302 Batch   32/96   train_loss = 0.973\n",
      "Epoch 2302 Batch   64/96   train_loss = 0.942\n",
      "Epoch 2303 Batch    0/96   train_loss = 0.904\n",
      "Epoch 2303 Batch   32/96   train_loss = 0.969\n",
      "Epoch 2303 Batch   64/96   train_loss = 0.958\n",
      "Epoch 2304 Batch    0/96   train_loss = 0.896\n",
      "Epoch 2304 Batch   32/96   train_loss = 0.958\n",
      "Epoch 2304 Batch   64/96   train_loss = 0.946\n",
      "Epoch 2305 Batch    0/96   train_loss = 0.909\n",
      "Epoch 2305 Batch   32/96   train_loss = 0.964\n",
      "Epoch 2305 Batch   64/96   train_loss = 0.944\n",
      "Epoch 2306 Batch    0/96   train_loss = 0.898\n",
      "Epoch 2306 Batch   32/96   train_loss = 0.965\n",
      "Epoch 2306 Batch   64/96   train_loss = 0.924\n",
      "Epoch 2307 Batch    0/96   train_loss = 0.890\n",
      "Epoch 2307 Batch   32/96   train_loss = 0.944\n",
      "Epoch 2307 Batch   64/96   train_loss = 0.906\n",
      "Epoch 2308 Batch    0/96   train_loss = 0.883\n",
      "Epoch 2308 Batch   32/96   train_loss = 0.956\n",
      "Epoch 2308 Batch   64/96   train_loss = 0.923\n",
      "Epoch 2309 Batch    0/96   train_loss = 0.878\n",
      "Epoch 2309 Batch   32/96   train_loss = 0.962\n",
      "Epoch 2309 Batch   64/96   train_loss = 0.945\n",
      "Epoch 2310 Batch    0/96   train_loss = 0.885\n",
      "Epoch 2310 Batch   32/96   train_loss = 0.955\n",
      "Epoch 2310 Batch   64/96   train_loss = 0.929\n",
      "Epoch 2311 Batch    0/96   train_loss = 0.877\n",
      "Epoch 2311 Batch   32/96   train_loss = 0.979\n",
      "Epoch 2311 Batch   64/96   train_loss = 0.934\n",
      "Epoch 2312 Batch    0/96   train_loss = 0.890\n",
      "Epoch 2312 Batch   32/96   train_loss = 0.978\n",
      "Epoch 2312 Batch   64/96   train_loss = 0.938\n",
      "Epoch 2313 Batch    0/96   train_loss = 0.894\n",
      "Epoch 2313 Batch   32/96   train_loss = 0.968\n",
      "Epoch 2313 Batch   64/96   train_loss = 0.952\n",
      "Epoch 2314 Batch    0/96   train_loss = 0.907\n",
      "Epoch 2314 Batch   32/96   train_loss = 0.970\n",
      "Epoch 2314 Batch   64/96   train_loss = 0.952\n",
      "Epoch 2315 Batch    0/96   train_loss = 0.931\n",
      "Epoch 2315 Batch   32/96   train_loss = 0.966\n",
      "Epoch 2315 Batch   64/96   train_loss = 0.960\n",
      "Epoch 2316 Batch    0/96   train_loss = 0.932\n",
      "Epoch 2316 Batch   32/96   train_loss = 0.972\n",
      "Epoch 2316 Batch   64/96   train_loss = 0.963\n",
      "Epoch 2317 Batch    0/96   train_loss = 0.925\n",
      "Epoch 2317 Batch   32/96   train_loss = 0.976\n",
      "Epoch 2317 Batch   64/96   train_loss = 0.975\n",
      "Epoch 2318 Batch    0/96   train_loss = 0.931\n",
      "Epoch 2318 Batch   32/96   train_loss = 0.969\n",
      "Epoch 2318 Batch   64/96   train_loss = 0.947\n",
      "Epoch 2319 Batch    0/96   train_loss = 0.906\n",
      "Epoch 2319 Batch   32/96   train_loss = 0.957\n",
      "Epoch 2319 Batch   64/96   train_loss = 0.937\n",
      "Epoch 2320 Batch    0/96   train_loss = 0.898\n",
      "Epoch 2320 Batch   32/96   train_loss = 0.962\n",
      "Epoch 2320 Batch   64/96   train_loss = 0.931\n",
      "Epoch 2321 Batch    0/96   train_loss = 0.900\n",
      "Epoch 2321 Batch   32/96   train_loss = 0.953\n",
      "Epoch 2321 Batch   64/96   train_loss = 0.944\n",
      "Epoch 2322 Batch    0/96   train_loss = 0.899\n",
      "Epoch 2322 Batch   32/96   train_loss = 0.965\n",
      "Epoch 2322 Batch   64/96   train_loss = 0.939\n",
      "Epoch 2323 Batch    0/96   train_loss = 0.915\n",
      "Epoch 2323 Batch   32/96   train_loss = 0.956\n",
      "Epoch 2323 Batch   64/96   train_loss = 0.944\n",
      "Epoch 2324 Batch    0/96   train_loss = 0.899\n",
      "Epoch 2324 Batch   32/96   train_loss = 0.964\n",
      "Epoch 2324 Batch   64/96   train_loss = 0.947\n",
      "Epoch 2325 Batch    0/96   train_loss = 0.909\n",
      "Epoch 2325 Batch   32/96   train_loss = 0.960\n",
      "Epoch 2325 Batch   64/96   train_loss = 0.959\n",
      "Epoch 2326 Batch    0/96   train_loss = 0.902\n",
      "Epoch 2326 Batch   32/96   train_loss = 0.969\n",
      "Epoch 2326 Batch   64/96   train_loss = 0.940\n",
      "Epoch 2327 Batch    0/96   train_loss = 0.896\n",
      "Epoch 2327 Batch   32/96   train_loss = 0.973\n",
      "Epoch 2327 Batch   64/96   train_loss = 0.936\n",
      "Epoch 2328 Batch    0/96   train_loss = 0.897\n",
      "Epoch 2328 Batch   32/96   train_loss = 0.964\n",
      "Epoch 2328 Batch   64/96   train_loss = 0.940\n",
      "Epoch 2329 Batch    0/96   train_loss = 0.898\n",
      "Epoch 2329 Batch   32/96   train_loss = 0.974\n",
      "Epoch 2329 Batch   64/96   train_loss = 0.933\n",
      "Epoch 2330 Batch    0/96   train_loss = 0.880\n",
      "Epoch 2330 Batch   32/96   train_loss = 0.960\n",
      "Epoch 2330 Batch   64/96   train_loss = 0.934\n",
      "Epoch 2331 Batch    0/96   train_loss = 0.886\n",
      "Epoch 2331 Batch   32/96   train_loss = 0.952\n",
      "Epoch 2331 Batch   64/96   train_loss = 0.940\n",
      "Epoch 2332 Batch    0/96   train_loss = 0.876\n",
      "Epoch 2332 Batch   32/96   train_loss = 0.950\n",
      "Epoch 2332 Batch   64/96   train_loss = 0.936\n",
      "Epoch 2333 Batch    0/96   train_loss = 0.881\n",
      "Epoch 2333 Batch   32/96   train_loss = 0.938\n",
      "Epoch 2333 Batch   64/96   train_loss = 0.928\n",
      "Epoch 2334 Batch    0/96   train_loss = 0.874\n",
      "Epoch 2334 Batch   32/96   train_loss = 0.950\n",
      "Epoch 2334 Batch   64/96   train_loss = 0.929\n",
      "Epoch 2335 Batch    0/96   train_loss = 0.891\n",
      "Epoch 2335 Batch   32/96   train_loss = 0.944\n",
      "Epoch 2335 Batch   64/96   train_loss = 0.922\n",
      "Epoch 2336 Batch    0/96   train_loss = 0.898\n",
      "Epoch 2336 Batch   32/96   train_loss = 0.953\n",
      "Epoch 2336 Batch   64/96   train_loss = 0.936\n",
      "Epoch 2337 Batch    0/96   train_loss = 0.886\n",
      "Epoch 2337 Batch   32/96   train_loss = 0.969\n",
      "Epoch 2337 Batch   64/96   train_loss = 0.946\n",
      "Epoch 2338 Batch    0/96   train_loss = 0.882\n",
      "Epoch 2338 Batch   32/96   train_loss = 0.962\n",
      "Epoch 2338 Batch   64/96   train_loss = 0.923\n",
      "Epoch 2339 Batch    0/96   train_loss = 0.903\n",
      "Epoch 2339 Batch   32/96   train_loss = 0.970\n",
      "Epoch 2339 Batch   64/96   train_loss = 0.936\n",
      "Epoch 2340 Batch    0/96   train_loss = 0.890\n",
      "Epoch 2340 Batch   32/96   train_loss = 0.963\n",
      "Epoch 2340 Batch   64/96   train_loss = 0.956\n",
      "Epoch 2341 Batch    0/96   train_loss = 0.896\n",
      "Epoch 2341 Batch   32/96   train_loss = 0.975\n",
      "Epoch 2341 Batch   64/96   train_loss = 0.940\n",
      "Epoch 2342 Batch    0/96   train_loss = 0.900\n",
      "Epoch 2342 Batch   32/96   train_loss = 0.957\n",
      "Epoch 2342 Batch   64/96   train_loss = 0.927\n",
      "Epoch 2343 Batch    0/96   train_loss = 0.902\n",
      "Epoch 2343 Batch   32/96   train_loss = 0.952\n",
      "Epoch 2343 Batch   64/96   train_loss = 0.921\n",
      "Epoch 2344 Batch    0/96   train_loss = 0.900\n",
      "Epoch 2344 Batch   32/96   train_loss = 0.984\n",
      "Epoch 2344 Batch   64/96   train_loss = 0.949\n",
      "Epoch 2345 Batch    0/96   train_loss = 0.932\n",
      "Epoch 2345 Batch   32/96   train_loss = 0.985\n",
      "Epoch 2345 Batch   64/96   train_loss = 0.951\n",
      "Epoch 2346 Batch    0/96   train_loss = 0.905\n",
      "Epoch 2346 Batch   32/96   train_loss = 0.969\n",
      "Epoch 2346 Batch   64/96   train_loss = 0.947\n",
      "Epoch 2347 Batch    0/96   train_loss = 0.899\n",
      "Epoch 2347 Batch   32/96   train_loss = 0.966\n",
      "Epoch 2347 Batch   64/96   train_loss = 0.945\n",
      "Epoch 2348 Batch    0/96   train_loss = 0.912\n",
      "Epoch 2348 Batch   32/96   train_loss = 0.974\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2348 Batch   64/96   train_loss = 0.952\n",
      "Epoch 2349 Batch    0/96   train_loss = 0.910\n",
      "Epoch 2349 Batch   32/96   train_loss = 0.991\n",
      "Epoch 2349 Batch   64/96   train_loss = 0.944\n",
      "Epoch 2350 Batch    0/96   train_loss = 0.906\n",
      "Epoch 2350 Batch   32/96   train_loss = 0.958\n",
      "Epoch 2350 Batch   64/96   train_loss = 0.929\n",
      "Epoch 2351 Batch    0/96   train_loss = 0.912\n",
      "Epoch 2351 Batch   32/96   train_loss = 0.970\n",
      "Epoch 2351 Batch   64/96   train_loss = 0.933\n",
      "Epoch 2352 Batch    0/96   train_loss = 0.915\n",
      "Epoch 2352 Batch   32/96   train_loss = 0.977\n",
      "Epoch 2352 Batch   64/96   train_loss = 0.945\n",
      "Epoch 2353 Batch    0/96   train_loss = 0.909\n",
      "Epoch 2353 Batch   32/96   train_loss = 0.972\n",
      "Epoch 2353 Batch   64/96   train_loss = 0.943\n",
      "Epoch 2354 Batch    0/96   train_loss = 0.917\n",
      "Epoch 2354 Batch   32/96   train_loss = 0.962\n",
      "Epoch 2354 Batch   64/96   train_loss = 0.940\n",
      "Epoch 2355 Batch    0/96   train_loss = 0.940\n",
      "Epoch 2355 Batch   32/96   train_loss = 0.982\n",
      "Epoch 2355 Batch   64/96   train_loss = 0.949\n",
      "Epoch 2356 Batch    0/96   train_loss = 0.924\n",
      "Epoch 2356 Batch   32/96   train_loss = 0.956\n",
      "Epoch 2356 Batch   64/96   train_loss = 0.946\n",
      "Epoch 2357 Batch    0/96   train_loss = 0.903\n",
      "Epoch 2357 Batch   32/96   train_loss = 0.957\n",
      "Epoch 2357 Batch   64/96   train_loss = 0.950\n",
      "Epoch 2358 Batch    0/96   train_loss = 0.908\n",
      "Epoch 2358 Batch   32/96   train_loss = 0.954\n",
      "Epoch 2358 Batch   64/96   train_loss = 0.926\n",
      "Epoch 2359 Batch    0/96   train_loss = 0.921\n",
      "Epoch 2359 Batch   32/96   train_loss = 0.970\n",
      "Epoch 2359 Batch   64/96   train_loss = 0.936\n",
      "Epoch 2360 Batch    0/96   train_loss = 0.907\n",
      "Epoch 2360 Batch   32/96   train_loss = 0.963\n",
      "Epoch 2360 Batch   64/96   train_loss = 0.958\n",
      "Epoch 2361 Batch    0/96   train_loss = 0.923\n",
      "Epoch 2361 Batch   32/96   train_loss = 0.960\n",
      "Epoch 2361 Batch   64/96   train_loss = 0.935\n",
      "Epoch 2362 Batch    0/96   train_loss = 0.911\n",
      "Epoch 2362 Batch   32/96   train_loss = 0.944\n",
      "Epoch 2362 Batch   64/96   train_loss = 0.925\n",
      "Epoch 2363 Batch    0/96   train_loss = 0.898\n",
      "Epoch 2363 Batch   32/96   train_loss = 0.946\n",
      "Epoch 2363 Batch   64/96   train_loss = 0.920\n",
      "Epoch 2364 Batch    0/96   train_loss = 0.907\n",
      "Epoch 2364 Batch   32/96   train_loss = 0.954\n",
      "Epoch 2364 Batch   64/96   train_loss = 0.926\n",
      "Epoch 2365 Batch    0/96   train_loss = 0.904\n",
      "Epoch 2365 Batch   32/96   train_loss = 0.955\n",
      "Epoch 2365 Batch   64/96   train_loss = 0.918\n",
      "Epoch 2366 Batch    0/96   train_loss = 0.904\n",
      "Epoch 2366 Batch   32/96   train_loss = 0.957\n",
      "Epoch 2366 Batch   64/96   train_loss = 0.917\n",
      "Epoch 2367 Batch    0/96   train_loss = 0.905\n",
      "Epoch 2367 Batch   32/96   train_loss = 0.960\n",
      "Epoch 2367 Batch   64/96   train_loss = 0.929\n",
      "Epoch 2368 Batch    0/96   train_loss = 0.911\n",
      "Epoch 2368 Batch   32/96   train_loss = 0.958\n",
      "Epoch 2368 Batch   64/96   train_loss = 0.936\n",
      "Epoch 2369 Batch    0/96   train_loss = 0.916\n",
      "Epoch 2369 Batch   32/96   train_loss = 0.963\n",
      "Epoch 2369 Batch   64/96   train_loss = 0.946\n",
      "Epoch 2370 Batch    0/96   train_loss = 0.915\n",
      "Epoch 2370 Batch   32/96   train_loss = 0.984\n",
      "Epoch 2370 Batch   64/96   train_loss = 0.957\n",
      "Epoch 2371 Batch    0/96   train_loss = 0.911\n",
      "Epoch 2371 Batch   32/96   train_loss = 0.953\n",
      "Epoch 2371 Batch   64/96   train_loss = 0.925\n",
      "Epoch 2372 Batch    0/96   train_loss = 0.908\n",
      "Epoch 2372 Batch   32/96   train_loss = 0.960\n",
      "Epoch 2372 Batch   64/96   train_loss = 0.952\n",
      "Epoch 2373 Batch    0/96   train_loss = 0.913\n",
      "Epoch 2373 Batch   32/96   train_loss = 0.957\n",
      "Epoch 2373 Batch   64/96   train_loss = 0.940\n",
      "Epoch 2374 Batch    0/96   train_loss = 0.899\n",
      "Epoch 2374 Batch   32/96   train_loss = 0.953\n",
      "Epoch 2374 Batch   64/96   train_loss = 0.920\n",
      "Epoch 2375 Batch    0/96   train_loss = 0.905\n",
      "Epoch 2375 Batch   32/96   train_loss = 0.958\n",
      "Epoch 2375 Batch   64/96   train_loss = 0.922\n",
      "Epoch 2376 Batch    0/96   train_loss = 0.901\n",
      "Epoch 2376 Batch   32/96   train_loss = 0.959\n",
      "Epoch 2376 Batch   64/96   train_loss = 0.934\n",
      "Epoch 2377 Batch    0/96   train_loss = 0.893\n",
      "Epoch 2377 Batch   32/96   train_loss = 0.953\n",
      "Epoch 2377 Batch   64/96   train_loss = 0.921\n",
      "Epoch 2378 Batch    0/96   train_loss = 0.893\n",
      "Epoch 2378 Batch   32/96   train_loss = 0.958\n",
      "Epoch 2378 Batch   64/96   train_loss = 0.919\n",
      "Epoch 2379 Batch    0/96   train_loss = 0.888\n",
      "Epoch 2379 Batch   32/96   train_loss = 0.960\n",
      "Epoch 2379 Batch   64/96   train_loss = 0.925\n",
      "Epoch 2380 Batch    0/96   train_loss = 0.936\n",
      "Epoch 2380 Batch   32/96   train_loss = 0.973\n",
      "Epoch 2380 Batch   64/96   train_loss = 0.941\n",
      "Epoch 2381 Batch    0/96   train_loss = 0.901\n",
      "Epoch 2381 Batch   32/96   train_loss = 0.964\n",
      "Epoch 2381 Batch   64/96   train_loss = 0.930\n",
      "Epoch 2382 Batch    0/96   train_loss = 0.910\n",
      "Epoch 2382 Batch   32/96   train_loss = 0.973\n",
      "Epoch 2382 Batch   64/96   train_loss = 0.958\n",
      "Epoch 2383 Batch    0/96   train_loss = 0.907\n",
      "Epoch 2383 Batch   32/96   train_loss = 0.966\n",
      "Epoch 2383 Batch   64/96   train_loss = 0.942\n",
      "Epoch 2384 Batch    0/96   train_loss = 0.907\n",
      "Epoch 2384 Batch   32/96   train_loss = 0.974\n",
      "Epoch 2384 Batch   64/96   train_loss = 0.939\n",
      "Epoch 2385 Batch    0/96   train_loss = 0.905\n",
      "Epoch 2385 Batch   32/96   train_loss = 0.980\n",
      "Epoch 2385 Batch   64/96   train_loss = 0.947\n",
      "Epoch 2386 Batch    0/96   train_loss = 0.918\n",
      "Epoch 2386 Batch   32/96   train_loss = 0.962\n",
      "Epoch 2386 Batch   64/96   train_loss = 0.947\n",
      "Epoch 2387 Batch    0/96   train_loss = 0.909\n",
      "Epoch 2387 Batch   32/96   train_loss = 0.970\n",
      "Epoch 2387 Batch   64/96   train_loss = 0.953\n",
      "Epoch 2388 Batch    0/96   train_loss = 0.901\n",
      "Epoch 2388 Batch   32/96   train_loss = 0.977\n",
      "Epoch 2388 Batch   64/96   train_loss = 0.937\n",
      "Epoch 2389 Batch    0/96   train_loss = 0.893\n",
      "Epoch 2389 Batch   32/96   train_loss = 0.973\n",
      "Epoch 2389 Batch   64/96   train_loss = 0.942\n",
      "Epoch 2390 Batch    0/96   train_loss = 0.922\n",
      "Epoch 2390 Batch   32/96   train_loss = 0.973\n",
      "Epoch 2390 Batch   64/96   train_loss = 0.950\n",
      "Epoch 2391 Batch    0/96   train_loss = 0.909\n",
      "Epoch 2391 Batch   32/96   train_loss = 0.959\n",
      "Epoch 2391 Batch   64/96   train_loss = 0.944\n",
      "Epoch 2392 Batch    0/96   train_loss = 0.912\n",
      "Epoch 2392 Batch   32/96   train_loss = 0.963\n",
      "Epoch 2392 Batch   64/96   train_loss = 0.959\n",
      "Epoch 2393 Batch    0/96   train_loss = 0.924\n",
      "Epoch 2393 Batch   32/96   train_loss = 0.972\n",
      "Epoch 2393 Batch   64/96   train_loss = 0.934\n",
      "Epoch 2394 Batch    0/96   train_loss = 0.898\n",
      "Epoch 2394 Batch   32/96   train_loss = 0.975\n",
      "Epoch 2394 Batch   64/96   train_loss = 0.946\n",
      "Epoch 2395 Batch    0/96   train_loss = 0.914\n",
      "Epoch 2395 Batch   32/96   train_loss = 0.979\n",
      "Epoch 2395 Batch   64/96   train_loss = 0.954\n",
      "Epoch 2396 Batch    0/96   train_loss = 0.909\n",
      "Epoch 2396 Batch   32/96   train_loss = 0.973\n",
      "Epoch 2396 Batch   64/96   train_loss = 0.949\n",
      "Epoch 2397 Batch    0/96   train_loss = 0.895\n",
      "Epoch 2397 Batch   32/96   train_loss = 0.975\n",
      "Epoch 2397 Batch   64/96   train_loss = 0.955\n",
      "Epoch 2398 Batch    0/96   train_loss = 0.911\n",
      "Epoch 2398 Batch   32/96   train_loss = 0.959\n",
      "Epoch 2398 Batch   64/96   train_loss = 0.940\n",
      "Epoch 2399 Batch    0/96   train_loss = 0.900\n",
      "Epoch 2399 Batch   32/96   train_loss = 0.964\n",
      "Epoch 2399 Batch   64/96   train_loss = 0.943\n",
      "Epoch 2400 Batch    0/96   train_loss = 0.900\n",
      "Epoch 2400 Batch   32/96   train_loss = 0.970\n",
      "Epoch 2400 Batch   64/96   train_loss = 0.936\n",
      "Epoch 2401 Batch    0/96   train_loss = 0.897\n",
      "Epoch 2401 Batch   32/96   train_loss = 0.973\n",
      "Epoch 2401 Batch   64/96   train_loss = 0.954\n",
      "Epoch 2402 Batch    0/96   train_loss = 0.911\n",
      "Epoch 2402 Batch   32/96   train_loss = 0.974\n",
      "Epoch 2402 Batch   64/96   train_loss = 0.947\n",
      "Epoch 2403 Batch    0/96   train_loss = 0.903\n",
      "Epoch 2403 Batch   32/96   train_loss = 0.988\n",
      "Epoch 2403 Batch   64/96   train_loss = 0.938\n",
      "Epoch 2404 Batch    0/96   train_loss = 0.885\n",
      "Epoch 2404 Batch   32/96   train_loss = 0.968\n",
      "Epoch 2404 Batch   64/96   train_loss = 0.929\n",
      "Epoch 2405 Batch    0/96   train_loss = 0.891\n",
      "Epoch 2405 Batch   32/96   train_loss = 0.954\n",
      "Epoch 2405 Batch   64/96   train_loss = 0.930\n",
      "Epoch 2406 Batch    0/96   train_loss = 0.882\n",
      "Epoch 2406 Batch   32/96   train_loss = 0.963\n",
      "Epoch 2406 Batch   64/96   train_loss = 0.939\n",
      "Epoch 2407 Batch    0/96   train_loss = 0.886\n",
      "Epoch 2407 Batch   32/96   train_loss = 0.964\n",
      "Epoch 2407 Batch   64/96   train_loss = 0.950\n",
      "Epoch 2408 Batch    0/96   train_loss = 0.908\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2408 Batch   32/96   train_loss = 0.955\n",
      "Epoch 2408 Batch   64/96   train_loss = 0.937\n",
      "Epoch 2409 Batch    0/96   train_loss = 0.919\n",
      "Epoch 2409 Batch   32/96   train_loss = 0.968\n",
      "Epoch 2409 Batch   64/96   train_loss = 0.960\n",
      "Epoch 2410 Batch    0/96   train_loss = 0.906\n",
      "Epoch 2410 Batch   32/96   train_loss = 0.975\n",
      "Epoch 2410 Batch   64/96   train_loss = 0.952\n",
      "Epoch 2411 Batch    0/96   train_loss = 0.904\n",
      "Epoch 2411 Batch   32/96   train_loss = 0.988\n",
      "Epoch 2411 Batch   64/96   train_loss = 0.964\n",
      "Epoch 2412 Batch    0/96   train_loss = 0.908\n",
      "Epoch 2412 Batch   32/96   train_loss = 0.973\n",
      "Epoch 2412 Batch   64/96   train_loss = 0.941\n",
      "Epoch 2413 Batch    0/96   train_loss = 0.907\n",
      "Epoch 2413 Batch   32/96   train_loss = 0.971\n",
      "Epoch 2413 Batch   64/96   train_loss = 0.940\n",
      "Epoch 2414 Batch    0/96   train_loss = 0.903\n",
      "Epoch 2414 Batch   32/96   train_loss = 0.964\n",
      "Epoch 2414 Batch   64/96   train_loss = 0.944\n",
      "Epoch 2415 Batch    0/96   train_loss = 0.899\n",
      "Epoch 2415 Batch   32/96   train_loss = 0.971\n",
      "Epoch 2415 Batch   64/96   train_loss = 0.943\n",
      "Epoch 2416 Batch    0/96   train_loss = 0.893\n",
      "Epoch 2416 Batch   32/96   train_loss = 0.959\n",
      "Epoch 2416 Batch   64/96   train_loss = 0.936\n",
      "Epoch 2417 Batch    0/96   train_loss = 0.914\n",
      "Epoch 2417 Batch   32/96   train_loss = 0.968\n",
      "Epoch 2417 Batch   64/96   train_loss = 0.933\n",
      "Epoch 2418 Batch    0/96   train_loss = 0.900\n",
      "Epoch 2418 Batch   32/96   train_loss = 0.978\n",
      "Epoch 2418 Batch   64/96   train_loss = 0.945\n",
      "Epoch 2419 Batch    0/96   train_loss = 0.899\n",
      "Epoch 2419 Batch   32/96   train_loss = 0.979\n",
      "Epoch 2419 Batch   64/96   train_loss = 0.946\n",
      "Epoch 2420 Batch    0/96   train_loss = 0.898\n",
      "Epoch 2420 Batch   32/96   train_loss = 0.980\n",
      "Epoch 2420 Batch   64/96   train_loss = 0.938\n",
      "Epoch 2421 Batch    0/96   train_loss = 0.894\n",
      "Epoch 2421 Batch   32/96   train_loss = 0.962\n",
      "Epoch 2421 Batch   64/96   train_loss = 0.928\n",
      "Epoch 2422 Batch    0/96   train_loss = 0.918\n",
      "Epoch 2422 Batch   32/96   train_loss = 0.968\n",
      "Epoch 2422 Batch   64/96   train_loss = 0.940\n",
      "Epoch 2423 Batch    0/96   train_loss = 0.900\n",
      "Epoch 2423 Batch   32/96   train_loss = 0.959\n",
      "Epoch 2423 Batch   64/96   train_loss = 0.933\n",
      "Epoch 2424 Batch    0/96   train_loss = 0.920\n",
      "Epoch 2424 Batch   32/96   train_loss = 0.957\n",
      "Epoch 2424 Batch   64/96   train_loss = 0.942\n",
      "Epoch 2425 Batch    0/96   train_loss = 0.905\n",
      "Epoch 2425 Batch   32/96   train_loss = 0.970\n",
      "Epoch 2425 Batch   64/96   train_loss = 0.934\n",
      "Epoch 2426 Batch    0/96   train_loss = 0.908\n",
      "Epoch 2426 Batch   32/96   train_loss = 0.968\n",
      "Epoch 2426 Batch   64/96   train_loss = 0.949\n",
      "Epoch 2427 Batch    0/96   train_loss = 0.911\n",
      "Epoch 2427 Batch   32/96   train_loss = 0.968\n",
      "Epoch 2427 Batch   64/96   train_loss = 0.955\n",
      "Epoch 2428 Batch    0/96   train_loss = 0.909\n",
      "Epoch 2428 Batch   32/96   train_loss = 0.975\n",
      "Epoch 2428 Batch   64/96   train_loss = 0.940\n",
      "Epoch 2429 Batch    0/96   train_loss = 0.906\n",
      "Epoch 2429 Batch   32/96   train_loss = 0.963\n",
      "Epoch 2429 Batch   64/96   train_loss = 0.947\n",
      "Epoch 2430 Batch    0/96   train_loss = 0.910\n",
      "Epoch 2430 Batch   32/96   train_loss = 0.971\n",
      "Epoch 2430 Batch   64/96   train_loss = 0.956\n",
      "Epoch 2431 Batch    0/96   train_loss = 0.914\n",
      "Epoch 2431 Batch   32/96   train_loss = 0.963\n",
      "Epoch 2431 Batch   64/96   train_loss = 0.942\n",
      "Epoch 2432 Batch    0/96   train_loss = 0.909\n",
      "Epoch 2432 Batch   32/96   train_loss = 0.960\n",
      "Epoch 2432 Batch   64/96   train_loss = 0.930\n",
      "Epoch 2433 Batch    0/96   train_loss = 0.911\n",
      "Epoch 2433 Batch   32/96   train_loss = 0.962\n",
      "Epoch 2433 Batch   64/96   train_loss = 0.943\n",
      "Epoch 2434 Batch    0/96   train_loss = 0.900\n",
      "Epoch 2434 Batch   32/96   train_loss = 0.955\n",
      "Epoch 2434 Batch   64/96   train_loss = 0.949\n",
      "Epoch 2435 Batch    0/96   train_loss = 0.906\n",
      "Epoch 2435 Batch   32/96   train_loss = 0.975\n",
      "Epoch 2435 Batch   64/96   train_loss = 0.946\n",
      "Epoch 2436 Batch    0/96   train_loss = 0.893\n",
      "Epoch 2436 Batch   32/96   train_loss = 0.980\n",
      "Epoch 2436 Batch   64/96   train_loss = 0.952\n",
      "Epoch 2437 Batch    0/96   train_loss = 0.894\n",
      "Epoch 2437 Batch   32/96   train_loss = 0.984\n",
      "Epoch 2437 Batch   64/96   train_loss = 0.942\n",
      "Epoch 2438 Batch    0/96   train_loss = 0.912\n",
      "Epoch 2438 Batch   32/96   train_loss = 0.989\n",
      "Epoch 2438 Batch   64/96   train_loss = 0.952\n",
      "Epoch 2439 Batch    0/96   train_loss = 0.901\n",
      "Epoch 2439 Batch   32/96   train_loss = 0.978\n",
      "Epoch 2439 Batch   64/96   train_loss = 0.935\n",
      "Epoch 2440 Batch    0/96   train_loss = 0.890\n",
      "Epoch 2440 Batch   32/96   train_loss = 0.959\n",
      "Epoch 2440 Batch   64/96   train_loss = 0.930\n",
      "Epoch 2441 Batch    0/96   train_loss = 0.897\n",
      "Epoch 2441 Batch   32/96   train_loss = 0.956\n",
      "Epoch 2441 Batch   64/96   train_loss = 0.933\n",
      "Epoch 2442 Batch    0/96   train_loss = 0.912\n",
      "Epoch 2442 Batch   32/96   train_loss = 0.962\n",
      "Epoch 2442 Batch   64/96   train_loss = 0.943\n",
      "Epoch 2443 Batch    0/96   train_loss = 0.916\n",
      "Epoch 2443 Batch   32/96   train_loss = 0.977\n",
      "Epoch 2443 Batch   64/96   train_loss = 0.944\n",
      "Epoch 2444 Batch    0/96   train_loss = 0.909\n",
      "Epoch 2444 Batch   32/96   train_loss = 0.974\n",
      "Epoch 2444 Batch   64/96   train_loss = 0.963\n",
      "Epoch 2445 Batch    0/96   train_loss = 0.908\n",
      "Epoch 2445 Batch   32/96   train_loss = 0.963\n",
      "Epoch 2445 Batch   64/96   train_loss = 0.950\n",
      "Epoch 2446 Batch    0/96   train_loss = 0.903\n",
      "Epoch 2446 Batch   32/96   train_loss = 0.988\n",
      "Epoch 2446 Batch   64/96   train_loss = 0.960\n",
      "Epoch 2447 Batch    0/96   train_loss = 0.904\n",
      "Epoch 2447 Batch   32/96   train_loss = 0.970\n",
      "Epoch 2447 Batch   64/96   train_loss = 0.946\n",
      "Epoch 2448 Batch    0/96   train_loss = 0.900\n",
      "Epoch 2448 Batch   32/96   train_loss = 0.965\n",
      "Epoch 2448 Batch   64/96   train_loss = 0.934\n",
      "Epoch 2449 Batch    0/96   train_loss = 0.891\n",
      "Epoch 2449 Batch   32/96   train_loss = 0.969\n",
      "Epoch 2449 Batch   64/96   train_loss = 0.962\n",
      "Epoch 2450 Batch    0/96   train_loss = 0.905\n",
      "Epoch 2450 Batch   32/96   train_loss = 0.972\n",
      "Epoch 2450 Batch   64/96   train_loss = 0.949\n",
      "Epoch 2451 Batch    0/96   train_loss = 0.905\n",
      "Epoch 2451 Batch   32/96   train_loss = 0.966\n",
      "Epoch 2451 Batch   64/96   train_loss = 0.949\n",
      "Epoch 2452 Batch    0/96   train_loss = 0.895\n",
      "Epoch 2452 Batch   32/96   train_loss = 0.964\n",
      "Epoch 2452 Batch   64/96   train_loss = 0.943\n",
      "Epoch 2453 Batch    0/96   train_loss = 0.895\n",
      "Epoch 2453 Batch   32/96   train_loss = 0.964\n",
      "Epoch 2453 Batch   64/96   train_loss = 0.931\n",
      "Epoch 2454 Batch    0/96   train_loss = 0.898\n",
      "Epoch 2454 Batch   32/96   train_loss = 0.955\n",
      "Epoch 2454 Batch   64/96   train_loss = 0.930\n",
      "Epoch 2455 Batch    0/96   train_loss = 0.899\n",
      "Epoch 2455 Batch   32/96   train_loss = 0.967\n",
      "Epoch 2455 Batch   64/96   train_loss = 0.949\n",
      "Epoch 2456 Batch    0/96   train_loss = 0.889\n",
      "Epoch 2456 Batch   32/96   train_loss = 0.961\n",
      "Epoch 2456 Batch   64/96   train_loss = 0.946\n",
      "Epoch 2457 Batch    0/96   train_loss = 0.895\n",
      "Epoch 2457 Batch   32/96   train_loss = 0.958\n",
      "Epoch 2457 Batch   64/96   train_loss = 0.947\n",
      "Epoch 2458 Batch    0/96   train_loss = 0.898\n",
      "Epoch 2458 Batch   32/96   train_loss = 0.933\n",
      "Epoch 2458 Batch   64/96   train_loss = 0.945\n",
      "Epoch 2459 Batch    0/96   train_loss = 0.890\n",
      "Epoch 2459 Batch   32/96   train_loss = 0.939\n",
      "Epoch 2459 Batch   64/96   train_loss = 0.938\n",
      "Epoch 2460 Batch    0/96   train_loss = 0.879\n",
      "Epoch 2460 Batch   32/96   train_loss = 0.945\n",
      "Epoch 2460 Batch   64/96   train_loss = 0.944\n",
      "Epoch 2461 Batch    0/96   train_loss = 0.885\n",
      "Epoch 2461 Batch   32/96   train_loss = 0.945\n",
      "Epoch 2461 Batch   64/96   train_loss = 0.936\n",
      "Epoch 2462 Batch    0/96   train_loss = 0.868\n",
      "Epoch 2462 Batch   32/96   train_loss = 0.950\n",
      "Epoch 2462 Batch   64/96   train_loss = 0.946\n",
      "Epoch 2463 Batch    0/96   train_loss = 0.883\n",
      "Epoch 2463 Batch   32/96   train_loss = 0.953\n",
      "Epoch 2463 Batch   64/96   train_loss = 0.946\n",
      "Epoch 2464 Batch    0/96   train_loss = 0.888\n",
      "Epoch 2464 Batch   32/96   train_loss = 0.945\n",
      "Epoch 2464 Batch   64/96   train_loss = 0.925\n",
      "Epoch 2465 Batch    0/96   train_loss = 0.880\n",
      "Epoch 2465 Batch   32/96   train_loss = 0.950\n",
      "Epoch 2465 Batch   64/96   train_loss = 0.920\n",
      "Epoch 2466 Batch    0/96   train_loss = 0.897\n",
      "Epoch 2466 Batch   32/96   train_loss = 0.957\n",
      "Epoch 2466 Batch   64/96   train_loss = 0.935\n",
      "Epoch 2467 Batch    0/96   train_loss = 0.902\n",
      "Epoch 2467 Batch   32/96   train_loss = 0.958\n",
      "Epoch 2467 Batch   64/96   train_loss = 0.940\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2468 Batch    0/96   train_loss = 0.887\n",
      "Epoch 2468 Batch   32/96   train_loss = 0.950\n",
      "Epoch 2468 Batch   64/96   train_loss = 0.936\n",
      "Epoch 2469 Batch    0/96   train_loss = 0.892\n",
      "Epoch 2469 Batch   32/96   train_loss = 0.984\n",
      "Epoch 2469 Batch   64/96   train_loss = 0.950\n",
      "Epoch 2470 Batch    0/96   train_loss = 0.890\n",
      "Epoch 2470 Batch   32/96   train_loss = 0.979\n",
      "Epoch 2470 Batch   64/96   train_loss = 0.947\n",
      "Epoch 2471 Batch    0/96   train_loss = 0.898\n",
      "Epoch 2471 Batch   32/96   train_loss = 0.973\n",
      "Epoch 2471 Batch   64/96   train_loss = 0.966\n",
      "Epoch 2472 Batch    0/96   train_loss = 0.904\n",
      "Epoch 2472 Batch   32/96   train_loss = 0.957\n",
      "Epoch 2472 Batch   64/96   train_loss = 0.968\n",
      "Epoch 2473 Batch    0/96   train_loss = 0.907\n",
      "Epoch 2473 Batch   32/96   train_loss = 0.972\n",
      "Epoch 2473 Batch   64/96   train_loss = 0.965\n",
      "Epoch 2474 Batch    0/96   train_loss = 0.916\n",
      "Epoch 2474 Batch   32/96   train_loss = 0.969\n",
      "Epoch 2474 Batch   64/96   train_loss = 0.958\n",
      "Epoch 2475 Batch    0/96   train_loss = 0.913\n",
      "Epoch 2475 Batch   32/96   train_loss = 0.981\n",
      "Epoch 2475 Batch   64/96   train_loss = 0.968\n",
      "Epoch 2476 Batch    0/96   train_loss = 0.924\n",
      "Epoch 2476 Batch   32/96   train_loss = 0.970\n",
      "Epoch 2476 Batch   64/96   train_loss = 0.951\n",
      "Epoch 2477 Batch    0/96   train_loss = 0.904\n",
      "Epoch 2477 Batch   32/96   train_loss = 0.968\n",
      "Epoch 2477 Batch   64/96   train_loss = 0.956\n",
      "Epoch 2478 Batch    0/96   train_loss = 0.926\n",
      "Epoch 2478 Batch   32/96   train_loss = 0.987\n",
      "Epoch 2478 Batch   64/96   train_loss = 0.957\n",
      "Epoch 2479 Batch    0/96   train_loss = 0.921\n",
      "Epoch 2479 Batch   32/96   train_loss = 0.989\n",
      "Epoch 2479 Batch   64/96   train_loss = 0.973\n",
      "Epoch 2480 Batch    0/96   train_loss = 0.937\n",
      "Epoch 2480 Batch   32/96   train_loss = 0.992\n",
      "Epoch 2480 Batch   64/96   train_loss = 0.976\n",
      "Epoch 2481 Batch    0/96   train_loss = 0.926\n",
      "Epoch 2481 Batch   32/96   train_loss = 0.992\n",
      "Epoch 2481 Batch   64/96   train_loss = 0.972\n",
      "Epoch 2482 Batch    0/96   train_loss = 0.913\n",
      "Epoch 2482 Batch   32/96   train_loss = 0.978\n",
      "Epoch 2482 Batch   64/96   train_loss = 0.977\n",
      "Epoch 2483 Batch    0/96   train_loss = 0.908\n",
      "Epoch 2483 Batch   32/96   train_loss = 0.990\n",
      "Epoch 2483 Batch   64/96   train_loss = 0.960\n",
      "Epoch 2484 Batch    0/96   train_loss = 0.923\n",
      "Epoch 2484 Batch   32/96   train_loss = 0.974\n",
      "Epoch 2484 Batch   64/96   train_loss = 0.963\n",
      "Epoch 2485 Batch    0/96   train_loss = 0.919\n",
      "Epoch 2485 Batch   32/96   train_loss = 0.969\n",
      "Epoch 2485 Batch   64/96   train_loss = 0.960\n",
      "Epoch 2486 Batch    0/96   train_loss = 0.909\n",
      "Epoch 2486 Batch   32/96   train_loss = 0.983\n",
      "Epoch 2486 Batch   64/96   train_loss = 0.961\n",
      "Epoch 2487 Batch    0/96   train_loss = 0.913\n",
      "Epoch 2487 Batch   32/96   train_loss = 0.982\n",
      "Epoch 2487 Batch   64/96   train_loss = 0.970\n",
      "Epoch 2488 Batch    0/96   train_loss = 0.912\n",
      "Epoch 2488 Batch   32/96   train_loss = 0.968\n",
      "Epoch 2488 Batch   64/96   train_loss = 0.961\n",
      "Epoch 2489 Batch    0/96   train_loss = 0.898\n",
      "Epoch 2489 Batch   32/96   train_loss = 0.963\n",
      "Epoch 2489 Batch   64/96   train_loss = 0.961\n",
      "Epoch 2490 Batch    0/96   train_loss = 0.904\n",
      "Epoch 2490 Batch   32/96   train_loss = 0.963\n",
      "Epoch 2490 Batch   64/96   train_loss = 0.940\n",
      "Epoch 2491 Batch    0/96   train_loss = 0.918\n",
      "Epoch 2491 Batch   32/96   train_loss = 0.972\n",
      "Epoch 2491 Batch   64/96   train_loss = 0.954\n",
      "Epoch 2492 Batch    0/96   train_loss = 0.920\n",
      "Epoch 2492 Batch   32/96   train_loss = 0.961\n",
      "Epoch 2492 Batch   64/96   train_loss = 0.938\n",
      "Epoch 2493 Batch    0/96   train_loss = 0.902\n",
      "Epoch 2493 Batch   32/96   train_loss = 0.965\n",
      "Epoch 2493 Batch   64/96   train_loss = 0.951\n",
      "Epoch 2494 Batch    0/96   train_loss = 0.894\n",
      "Epoch 2494 Batch   32/96   train_loss = 0.961\n",
      "Epoch 2494 Batch   64/96   train_loss = 0.934\n",
      "Epoch 2495 Batch    0/96   train_loss = 0.887\n",
      "Epoch 2495 Batch   32/96   train_loss = 0.964\n",
      "Epoch 2495 Batch   64/96   train_loss = 0.943\n",
      "Epoch 2496 Batch    0/96   train_loss = 0.901\n",
      "Epoch 2496 Batch   32/96   train_loss = 0.959\n",
      "Epoch 2496 Batch   64/96   train_loss = 0.930\n",
      "Epoch 2497 Batch    0/96   train_loss = 0.891\n",
      "Epoch 2497 Batch   32/96   train_loss = 0.966\n",
      "Epoch 2497 Batch   64/96   train_loss = 0.930\n",
      "Epoch 2498 Batch    0/96   train_loss = 0.912\n",
      "Epoch 2498 Batch   32/96   train_loss = 0.969\n",
      "Epoch 2498 Batch   64/96   train_loss = 0.946\n",
      "Epoch 2499 Batch    0/96   train_loss = 0.895\n",
      "Epoch 2499 Batch   32/96   train_loss = 0.968\n",
      "Epoch 2499 Batch   64/96   train_loss = 0.939\n",
      "Epoch 2500 Batch    0/96   train_loss = 0.898\n",
      "Epoch 2500 Batch   32/96   train_loss = 0.957\n",
      "Epoch 2500 Batch   64/96   train_loss = 0.935\n",
      "Epoch 2501 Batch    0/96   train_loss = 0.888\n",
      "Epoch 2501 Batch   32/96   train_loss = 1.002\n",
      "Epoch 2501 Batch   64/96   train_loss = 0.955\n",
      "Epoch 2502 Batch    0/96   train_loss = 0.927\n",
      "Epoch 2502 Batch   32/96   train_loss = 0.984\n",
      "Epoch 2502 Batch   64/96   train_loss = 0.964\n",
      "Epoch 2503 Batch    0/96   train_loss = 0.926\n",
      "Epoch 2503 Batch   32/96   train_loss = 0.983\n",
      "Epoch 2503 Batch   64/96   train_loss = 0.968\n",
      "Epoch 2504 Batch    0/96   train_loss = 0.911\n",
      "Epoch 2504 Batch   32/96   train_loss = 1.002\n",
      "Epoch 2504 Batch   64/96   train_loss = 0.991\n",
      "Epoch 2505 Batch    0/96   train_loss = 0.920\n",
      "Epoch 2505 Batch   32/96   train_loss = 0.990\n",
      "Epoch 2505 Batch   64/96   train_loss = 0.985\n",
      "Epoch 2506 Batch    0/96   train_loss = 0.910\n",
      "Epoch 2506 Batch   32/96   train_loss = 0.985\n",
      "Epoch 2506 Batch   64/96   train_loss = 0.965\n",
      "Epoch 2507 Batch    0/96   train_loss = 0.911\n",
      "Epoch 2507 Batch   32/96   train_loss = 0.992\n",
      "Epoch 2507 Batch   64/96   train_loss = 0.969\n",
      "Epoch 2508 Batch    0/96   train_loss = 0.907\n",
      "Epoch 2508 Batch   32/96   train_loss = 0.999\n",
      "Epoch 2508 Batch   64/96   train_loss = 0.968\n",
      "Epoch 2509 Batch    0/96   train_loss = 0.914\n",
      "Epoch 2509 Batch   32/96   train_loss = 0.987\n",
      "Epoch 2509 Batch   64/96   train_loss = 0.964\n",
      "Epoch 2510 Batch    0/96   train_loss = 0.909\n",
      "Epoch 2510 Batch   32/96   train_loss = 0.977\n",
      "Epoch 2510 Batch   64/96   train_loss = 0.976\n",
      "Epoch 2511 Batch    0/96   train_loss = 0.909\n",
      "Epoch 2511 Batch   32/96   train_loss = 0.979\n",
      "Epoch 2511 Batch   64/96   train_loss = 0.962\n",
      "Epoch 2512 Batch    0/96   train_loss = 0.894\n",
      "Epoch 2512 Batch   32/96   train_loss = 0.989\n",
      "Epoch 2512 Batch   64/96   train_loss = 0.945\n",
      "Epoch 2513 Batch    0/96   train_loss = 0.898\n",
      "Epoch 2513 Batch   32/96   train_loss = 0.973\n",
      "Epoch 2513 Batch   64/96   train_loss = 0.944\n",
      "Epoch 2514 Batch    0/96   train_loss = 0.904\n",
      "Epoch 2514 Batch   32/96   train_loss = 0.979\n",
      "Epoch 2514 Batch   64/96   train_loss = 0.940\n",
      "Epoch 2515 Batch    0/96   train_loss = 0.898\n",
      "Epoch 2515 Batch   32/96   train_loss = 0.960\n",
      "Epoch 2515 Batch   64/96   train_loss = 0.939\n",
      "Epoch 2516 Batch    0/96   train_loss = 0.902\n",
      "Epoch 2516 Batch   32/96   train_loss = 0.969\n",
      "Epoch 2516 Batch   64/96   train_loss = 0.927\n",
      "Epoch 2517 Batch    0/96   train_loss = 0.900\n",
      "Epoch 2517 Batch   32/96   train_loss = 0.967\n",
      "Epoch 2517 Batch   64/96   train_loss = 0.940\n",
      "Epoch 2518 Batch    0/96   train_loss = 0.907\n",
      "Epoch 2518 Batch   32/96   train_loss = 0.973\n",
      "Epoch 2518 Batch   64/96   train_loss = 0.947\n",
      "Epoch 2519 Batch    0/96   train_loss = 0.905\n",
      "Epoch 2519 Batch   32/96   train_loss = 0.982\n",
      "Epoch 2519 Batch   64/96   train_loss = 0.957\n",
      "Epoch 2520 Batch    0/96   train_loss = 0.912\n",
      "Epoch 2520 Batch   32/96   train_loss = 0.966\n",
      "Epoch 2520 Batch   64/96   train_loss = 0.961\n",
      "Epoch 2521 Batch    0/96   train_loss = 0.906\n",
      "Epoch 2521 Batch   32/96   train_loss = 0.960\n",
      "Epoch 2521 Batch   64/96   train_loss = 0.938\n",
      "Epoch 2522 Batch    0/96   train_loss = 0.901\n",
      "Epoch 2522 Batch   32/96   train_loss = 0.953\n",
      "Epoch 2522 Batch   64/96   train_loss = 0.934\n",
      "Epoch 2523 Batch    0/96   train_loss = 0.884\n",
      "Epoch 2523 Batch   32/96   train_loss = 0.952\n",
      "Epoch 2523 Batch   64/96   train_loss = 0.938\n",
      "Epoch 2524 Batch    0/96   train_loss = 0.892\n",
      "Epoch 2524 Batch   32/96   train_loss = 0.972\n",
      "Epoch 2524 Batch   64/96   train_loss = 0.941\n",
      "Epoch 2525 Batch    0/96   train_loss = 0.905\n",
      "Epoch 2525 Batch   32/96   train_loss = 0.970\n",
      "Epoch 2525 Batch   64/96   train_loss = 0.953\n",
      "Epoch 2526 Batch    0/96   train_loss = 0.932\n",
      "Epoch 2526 Batch   32/96   train_loss = 0.974\n",
      "Epoch 2526 Batch   64/96   train_loss = 0.962\n",
      "Epoch 2527 Batch    0/96   train_loss = 0.904\n",
      "Epoch 2527 Batch   32/96   train_loss = 0.972\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2527 Batch   64/96   train_loss = 0.970\n",
      "Epoch 2528 Batch    0/96   train_loss = 0.904\n",
      "Epoch 2528 Batch   32/96   train_loss = 0.969\n",
      "Epoch 2528 Batch   64/96   train_loss = 0.948\n",
      "Epoch 2529 Batch    0/96   train_loss = 0.919\n",
      "Epoch 2529 Batch   32/96   train_loss = 0.963\n",
      "Epoch 2529 Batch   64/96   train_loss = 0.960\n",
      "Epoch 2530 Batch    0/96   train_loss = 0.901\n",
      "Epoch 2530 Batch   32/96   train_loss = 0.974\n",
      "Epoch 2530 Batch   64/96   train_loss = 0.955\n",
      "Epoch 2531 Batch    0/96   train_loss = 0.892\n",
      "Epoch 2531 Batch   32/96   train_loss = 0.971\n",
      "Epoch 2531 Batch   64/96   train_loss = 0.940\n",
      "Epoch 2532 Batch    0/96   train_loss = 0.905\n",
      "Epoch 2532 Batch   32/96   train_loss = 0.980\n",
      "Epoch 2532 Batch   64/96   train_loss = 0.947\n",
      "Epoch 2533 Batch    0/96   train_loss = 0.912\n",
      "Epoch 2533 Batch   32/96   train_loss = 0.973\n",
      "Epoch 2533 Batch   64/96   train_loss = 0.954\n",
      "Epoch 2534 Batch    0/96   train_loss = 0.898\n",
      "Epoch 2534 Batch   32/96   train_loss = 0.966\n",
      "Epoch 2534 Batch   64/96   train_loss = 0.966\n",
      "Epoch 2535 Batch    0/96   train_loss = 0.919\n",
      "Epoch 2535 Batch   32/96   train_loss = 0.980\n",
      "Epoch 2535 Batch   64/96   train_loss = 0.960\n",
      "Epoch 2536 Batch    0/96   train_loss = 0.907\n",
      "Epoch 2536 Batch   32/96   train_loss = 0.974\n",
      "Epoch 2536 Batch   64/96   train_loss = 0.949\n",
      "Epoch 2537 Batch    0/96   train_loss = 0.915\n",
      "Epoch 2537 Batch   32/96   train_loss = 0.970\n",
      "Epoch 2537 Batch   64/96   train_loss = 0.947\n",
      "Epoch 2538 Batch    0/96   train_loss = 0.924\n",
      "Epoch 2538 Batch   32/96   train_loss = 0.989\n",
      "Epoch 2538 Batch   64/96   train_loss = 0.947\n",
      "Epoch 2539 Batch    0/96   train_loss = 0.921\n",
      "Epoch 2539 Batch   32/96   train_loss = 1.003\n",
      "Epoch 2539 Batch   64/96   train_loss = 0.961\n",
      "Epoch 2540 Batch    0/96   train_loss = 0.914\n",
      "Epoch 2540 Batch   32/96   train_loss = 0.987\n",
      "Epoch 2540 Batch   64/96   train_loss = 0.958\n",
      "Epoch 2541 Batch    0/96   train_loss = 0.900\n",
      "Epoch 2541 Batch   32/96   train_loss = 0.978\n",
      "Epoch 2541 Batch   64/96   train_loss = 0.944\n",
      "Epoch 2542 Batch    0/96   train_loss = 0.898\n",
      "Epoch 2542 Batch   32/96   train_loss = 0.973\n",
      "Epoch 2542 Batch   64/96   train_loss = 0.975\n",
      "Epoch 2543 Batch    0/96   train_loss = 0.897\n",
      "Epoch 2543 Batch   32/96   train_loss = 0.976\n",
      "Epoch 2543 Batch   64/96   train_loss = 0.953\n",
      "Epoch 2544 Batch    0/96   train_loss = 0.909\n",
      "Epoch 2544 Batch   32/96   train_loss = 0.974\n",
      "Epoch 2544 Batch   64/96   train_loss = 0.953\n",
      "Epoch 2545 Batch    0/96   train_loss = 0.905\n",
      "Epoch 2545 Batch   32/96   train_loss = 0.965\n",
      "Epoch 2545 Batch   64/96   train_loss = 0.938\n",
      "Epoch 2546 Batch    0/96   train_loss = 0.913\n",
      "Epoch 2546 Batch   32/96   train_loss = 0.978\n",
      "Epoch 2546 Batch   64/96   train_loss = 0.958\n",
      "Epoch 2547 Batch    0/96   train_loss = 0.904\n",
      "Epoch 2547 Batch   32/96   train_loss = 0.961\n",
      "Epoch 2547 Batch   64/96   train_loss = 0.948\n",
      "Epoch 2548 Batch    0/96   train_loss = 0.890\n",
      "Epoch 2548 Batch   32/96   train_loss = 0.963\n",
      "Epoch 2548 Batch   64/96   train_loss = 0.933\n",
      "Epoch 2549 Batch    0/96   train_loss = 0.898\n",
      "Epoch 2549 Batch   32/96   train_loss = 0.960\n",
      "Epoch 2549 Batch   64/96   train_loss = 0.935\n",
      "Epoch 2550 Batch    0/96   train_loss = 0.896\n",
      "Epoch 2550 Batch   32/96   train_loss = 0.956\n",
      "Epoch 2550 Batch   64/96   train_loss = 0.945\n",
      "Epoch 2551 Batch    0/96   train_loss = 0.906\n",
      "Epoch 2551 Batch   32/96   train_loss = 0.955\n",
      "Epoch 2551 Batch   64/96   train_loss = 0.926\n",
      "Epoch 2552 Batch    0/96   train_loss = 0.883\n",
      "Epoch 2552 Batch   32/96   train_loss = 0.953\n",
      "Epoch 2552 Batch   64/96   train_loss = 0.933\n",
      "Epoch 2553 Batch    0/96   train_loss = 0.892\n",
      "Epoch 2553 Batch   32/96   train_loss = 0.946\n",
      "Epoch 2553 Batch   64/96   train_loss = 0.941\n",
      "Epoch 2554 Batch    0/96   train_loss = 0.885\n",
      "Epoch 2554 Batch   32/96   train_loss = 0.947\n",
      "Epoch 2554 Batch   64/96   train_loss = 0.932\n",
      "Epoch 2555 Batch    0/96   train_loss = 0.887\n",
      "Epoch 2555 Batch   32/96   train_loss = 0.953\n",
      "Epoch 2555 Batch   64/96   train_loss = 0.939\n",
      "Epoch 2556 Batch    0/96   train_loss = 0.894\n",
      "Epoch 2556 Batch   32/96   train_loss = 0.943\n",
      "Epoch 2556 Batch   64/96   train_loss = 0.941\n",
      "Epoch 2557 Batch    0/96   train_loss = 0.893\n",
      "Epoch 2557 Batch   32/96   train_loss = 0.954\n",
      "Epoch 2557 Batch   64/96   train_loss = 0.944\n",
      "Epoch 2558 Batch    0/96   train_loss = 0.891\n",
      "Epoch 2558 Batch   32/96   train_loss = 0.949\n",
      "Epoch 2558 Batch   64/96   train_loss = 0.917\n",
      "Epoch 2559 Batch    0/96   train_loss = 0.897\n",
      "Epoch 2559 Batch   32/96   train_loss = 0.960\n",
      "Epoch 2559 Batch   64/96   train_loss = 0.944\n",
      "Epoch 2560 Batch    0/96   train_loss = 0.892\n",
      "Epoch 2560 Batch   32/96   train_loss = 0.971\n",
      "Epoch 2560 Batch   64/96   train_loss = 0.960\n",
      "Epoch 2561 Batch    0/96   train_loss = 0.900\n",
      "Epoch 2561 Batch   32/96   train_loss = 0.970\n",
      "Epoch 2561 Batch   64/96   train_loss = 0.945\n",
      "Epoch 2562 Batch    0/96   train_loss = 0.917\n",
      "Epoch 2562 Batch   32/96   train_loss = 1.006\n",
      "Epoch 2562 Batch   64/96   train_loss = 0.969\n",
      "Epoch 2563 Batch    0/96   train_loss = 0.936\n",
      "Epoch 2563 Batch   32/96   train_loss = 0.989\n",
      "Epoch 2563 Batch   64/96   train_loss = 0.968\n",
      "Epoch 2564 Batch    0/96   train_loss = 0.925\n",
      "Epoch 2564 Batch   32/96   train_loss = 0.991\n",
      "Epoch 2564 Batch   64/96   train_loss = 0.961\n",
      "Epoch 2565 Batch    0/96   train_loss = 0.901\n",
      "Epoch 2565 Batch   32/96   train_loss = 0.983\n",
      "Epoch 2565 Batch   64/96   train_loss = 0.957\n",
      "Epoch 2566 Batch    0/96   train_loss = 0.908\n",
      "Epoch 2566 Batch   32/96   train_loss = 0.975\n",
      "Epoch 2566 Batch   64/96   train_loss = 0.950\n",
      "Epoch 2567 Batch    0/96   train_loss = 0.898\n",
      "Epoch 2567 Batch   32/96   train_loss = 0.974\n",
      "Epoch 2567 Batch   64/96   train_loss = 0.970\n",
      "Epoch 2568 Batch    0/96   train_loss = 0.911\n",
      "Epoch 2568 Batch   32/96   train_loss = 0.981\n",
      "Epoch 2568 Batch   64/96   train_loss = 0.947\n",
      "Epoch 2569 Batch    0/96   train_loss = 0.910\n",
      "Epoch 2569 Batch   32/96   train_loss = 0.992\n",
      "Epoch 2569 Batch   64/96   train_loss = 0.953\n",
      "Epoch 2570 Batch    0/96   train_loss = 0.903\n",
      "Epoch 2570 Batch   32/96   train_loss = 0.985\n",
      "Epoch 2570 Batch   64/96   train_loss = 0.958\n",
      "Epoch 2571 Batch    0/96   train_loss = 0.908\n",
      "Epoch 2571 Batch   32/96   train_loss = 0.983\n",
      "Epoch 2571 Batch   64/96   train_loss = 0.949\n",
      "Epoch 2572 Batch    0/96   train_loss = 0.912\n",
      "Epoch 2572 Batch   32/96   train_loss = 0.975\n",
      "Epoch 2572 Batch   64/96   train_loss = 0.947\n",
      "Epoch 2573 Batch    0/96   train_loss = 0.909\n",
      "Epoch 2573 Batch   32/96   train_loss = 0.976\n",
      "Epoch 2573 Batch   64/96   train_loss = 0.941\n",
      "Epoch 2574 Batch    0/96   train_loss = 0.912\n",
      "Epoch 2574 Batch   32/96   train_loss = 0.966\n",
      "Epoch 2574 Batch   64/96   train_loss = 0.936\n",
      "Epoch 2575 Batch    0/96   train_loss = 0.897\n",
      "Epoch 2575 Batch   32/96   train_loss = 0.960\n",
      "Epoch 2575 Batch   64/96   train_loss = 0.923\n",
      "Epoch 2576 Batch    0/96   train_loss = 0.887\n",
      "Epoch 2576 Batch   32/96   train_loss = 0.967\n",
      "Epoch 2576 Batch   64/96   train_loss = 0.924\n",
      "Epoch 2577 Batch    0/96   train_loss = 0.895\n",
      "Epoch 2577 Batch   32/96   train_loss = 0.952\n",
      "Epoch 2577 Batch   64/96   train_loss = 0.937\n",
      "Epoch 2578 Batch    0/96   train_loss = 0.898\n",
      "Epoch 2578 Batch   32/96   train_loss = 0.964\n",
      "Epoch 2578 Batch   64/96   train_loss = 0.937\n",
      "Epoch 2579 Batch    0/96   train_loss = 0.892\n",
      "Epoch 2579 Batch   32/96   train_loss = 0.965\n",
      "Epoch 2579 Batch   64/96   train_loss = 0.936\n",
      "Epoch 2580 Batch    0/96   train_loss = 0.896\n",
      "Epoch 2580 Batch   32/96   train_loss = 0.982\n",
      "Epoch 2580 Batch   64/96   train_loss = 0.951\n",
      "Epoch 2581 Batch    0/96   train_loss = 0.896\n",
      "Epoch 2581 Batch   32/96   train_loss = 0.976\n",
      "Epoch 2581 Batch   64/96   train_loss = 0.956\n",
      "Epoch 2582 Batch    0/96   train_loss = 0.903\n",
      "Epoch 2582 Batch   32/96   train_loss = 0.974\n",
      "Epoch 2582 Batch   64/96   train_loss = 0.954\n",
      "Epoch 2583 Batch    0/96   train_loss = 0.900\n",
      "Epoch 2583 Batch   32/96   train_loss = 0.977\n",
      "Epoch 2583 Batch   64/96   train_loss = 0.941\n",
      "Epoch 2584 Batch    0/96   train_loss = 0.912\n",
      "Epoch 2584 Batch   32/96   train_loss = 0.969\n",
      "Epoch 2584 Batch   64/96   train_loss = 0.956\n",
      "Epoch 2585 Batch    0/96   train_loss = 0.888\n",
      "Epoch 2585 Batch   32/96   train_loss = 0.974\n",
      "Epoch 2585 Batch   64/96   train_loss = 0.953\n",
      "Epoch 2586 Batch    0/96   train_loss = 0.902\n",
      "Epoch 2586 Batch   32/96   train_loss = 0.988\n",
      "Epoch 2586 Batch   64/96   train_loss = 0.967\n",
      "Epoch 2587 Batch    0/96   train_loss = 0.920\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2587 Batch   32/96   train_loss = 0.982\n",
      "Epoch 2587 Batch   64/96   train_loss = 0.956\n",
      "Epoch 2588 Batch    0/96   train_loss = 0.910\n",
      "Epoch 2588 Batch   32/96   train_loss = 0.990\n",
      "Epoch 2588 Batch   64/96   train_loss = 0.946\n",
      "Epoch 2589 Batch    0/96   train_loss = 0.907\n",
      "Epoch 2589 Batch   32/96   train_loss = 0.991\n",
      "Epoch 2589 Batch   64/96   train_loss = 0.962\n",
      "Epoch 2590 Batch    0/96   train_loss = 0.917\n",
      "Epoch 2590 Batch   32/96   train_loss = 0.991\n",
      "Epoch 2590 Batch   64/96   train_loss = 0.989\n",
      "Epoch 2591 Batch    0/96   train_loss = 0.918\n",
      "Epoch 2591 Batch   32/96   train_loss = 1.001\n",
      "Epoch 2591 Batch   64/96   train_loss = 0.969\n",
      "Epoch 2592 Batch    0/96   train_loss = 0.921\n",
      "Epoch 2592 Batch   32/96   train_loss = 1.000\n",
      "Epoch 2592 Batch   64/96   train_loss = 0.967\n",
      "Epoch 2593 Batch    0/96   train_loss = 0.925\n",
      "Epoch 2593 Batch   32/96   train_loss = 0.986\n",
      "Epoch 2593 Batch   64/96   train_loss = 0.985\n",
      "Epoch 2594 Batch    0/96   train_loss = 0.928\n",
      "Epoch 2594 Batch   32/96   train_loss = 0.992\n",
      "Epoch 2594 Batch   64/96   train_loss = 0.978\n",
      "Epoch 2595 Batch    0/96   train_loss = 0.939\n",
      "Epoch 2595 Batch   32/96   train_loss = 0.986\n",
      "Epoch 2595 Batch   64/96   train_loss = 0.969\n",
      "Epoch 2596 Batch    0/96   train_loss = 0.920\n",
      "Epoch 2596 Batch   32/96   train_loss = 0.991\n",
      "Epoch 2596 Batch   64/96   train_loss = 0.973\n",
      "Epoch 2597 Batch    0/96   train_loss = 0.944\n",
      "Epoch 2597 Batch   32/96   train_loss = 1.005\n",
      "Epoch 2597 Batch   64/96   train_loss = 0.981\n",
      "Epoch 2598 Batch    0/96   train_loss = 0.930\n",
      "Epoch 2598 Batch   32/96   train_loss = 1.005\n",
      "Epoch 2598 Batch   64/96   train_loss = 0.971\n",
      "Epoch 2599 Batch    0/96   train_loss = 0.931\n",
      "Epoch 2599 Batch   32/96   train_loss = 0.999\n",
      "Epoch 2599 Batch   64/96   train_loss = 0.964\n",
      "Epoch 2600 Batch    0/96   train_loss = 0.919\n",
      "Epoch 2600 Batch   32/96   train_loss = 0.989\n",
      "Epoch 2600 Batch   64/96   train_loss = 0.953\n",
      "Epoch 2601 Batch    0/96   train_loss = 0.921\n",
      "Epoch 2601 Batch   32/96   train_loss = 0.973\n",
      "Epoch 2601 Batch   64/96   train_loss = 0.966\n",
      "Epoch 2602 Batch    0/96   train_loss = 0.917\n",
      "Epoch 2602 Batch   32/96   train_loss = 0.987\n",
      "Epoch 2602 Batch   64/96   train_loss = 0.962\n",
      "Epoch 2603 Batch    0/96   train_loss = 0.930\n",
      "Epoch 2603 Batch   32/96   train_loss = 0.998\n",
      "Epoch 2603 Batch   64/96   train_loss = 0.980\n",
      "Epoch 2604 Batch    0/96   train_loss = 0.936\n",
      "Epoch 2604 Batch   32/96   train_loss = 0.988\n",
      "Epoch 2604 Batch   64/96   train_loss = 0.964\n",
      "Epoch 2605 Batch    0/96   train_loss = 0.928\n",
      "Epoch 2605 Batch   32/96   train_loss = 0.984\n",
      "Epoch 2605 Batch   64/96   train_loss = 0.962\n",
      "Epoch 2606 Batch    0/96   train_loss = 0.918\n",
      "Epoch 2606 Batch   32/96   train_loss = 0.979\n",
      "Epoch 2606 Batch   64/96   train_loss = 0.949\n",
      "Epoch 2607 Batch    0/96   train_loss = 0.917\n",
      "Epoch 2607 Batch   32/96   train_loss = 0.968\n",
      "Epoch 2607 Batch   64/96   train_loss = 0.966\n",
      "Epoch 2608 Batch    0/96   train_loss = 0.925\n",
      "Epoch 2608 Batch   32/96   train_loss = 0.962\n",
      "Epoch 2608 Batch   64/96   train_loss = 0.951\n",
      "Epoch 2609 Batch    0/96   train_loss = 0.908\n",
      "Epoch 2609 Batch   32/96   train_loss = 0.972\n",
      "Epoch 2609 Batch   64/96   train_loss = 0.947\n",
      "Epoch 2610 Batch    0/96   train_loss = 0.914\n",
      "Epoch 2610 Batch   32/96   train_loss = 0.958\n",
      "Epoch 2610 Batch   64/96   train_loss = 0.945\n",
      "Epoch 2611 Batch    0/96   train_loss = 0.911\n",
      "Epoch 2611 Batch   32/96   train_loss = 0.952\n",
      "Epoch 2611 Batch   64/96   train_loss = 0.939\n",
      "Epoch 2612 Batch    0/96   train_loss = 0.904\n",
      "Epoch 2612 Batch   32/96   train_loss = 0.970\n",
      "Epoch 2612 Batch   64/96   train_loss = 0.937\n",
      "Epoch 2613 Batch    0/96   train_loss = 0.893\n",
      "Epoch 2613 Batch   32/96   train_loss = 0.954\n",
      "Epoch 2613 Batch   64/96   train_loss = 0.923\n",
      "Epoch 2614 Batch    0/96   train_loss = 0.908\n",
      "Epoch 2614 Batch   32/96   train_loss = 0.959\n",
      "Epoch 2614 Batch   64/96   train_loss = 0.926\n",
      "Epoch 2615 Batch    0/96   train_loss = 0.902\n",
      "Epoch 2615 Batch   32/96   train_loss = 0.958\n",
      "Epoch 2615 Batch   64/96   train_loss = 0.937\n",
      "Epoch 2616 Batch    0/96   train_loss = 0.890\n",
      "Epoch 2616 Batch   32/96   train_loss = 0.955\n",
      "Epoch 2616 Batch   64/96   train_loss = 0.930\n",
      "Epoch 2617 Batch    0/96   train_loss = 0.907\n",
      "Epoch 2617 Batch   32/96   train_loss = 0.953\n",
      "Epoch 2617 Batch   64/96   train_loss = 0.937\n",
      "Epoch 2618 Batch    0/96   train_loss = 0.882\n",
      "Epoch 2618 Batch   32/96   train_loss = 0.957\n",
      "Epoch 2618 Batch   64/96   train_loss = 0.946\n",
      "Epoch 2619 Batch    0/96   train_loss = 0.904\n",
      "Epoch 2619 Batch   32/96   train_loss = 0.961\n",
      "Epoch 2619 Batch   64/96   train_loss = 0.943\n",
      "Epoch 2620 Batch    0/96   train_loss = 0.904\n",
      "Epoch 2620 Batch   32/96   train_loss = 0.975\n",
      "Epoch 2620 Batch   64/96   train_loss = 0.944\n",
      "Epoch 2621 Batch    0/96   train_loss = 0.897\n",
      "Epoch 2621 Batch   32/96   train_loss = 0.966\n",
      "Epoch 2621 Batch   64/96   train_loss = 0.955\n",
      "Epoch 2622 Batch    0/96   train_loss = 0.890\n",
      "Epoch 2622 Batch   32/96   train_loss = 0.999\n",
      "Epoch 2622 Batch   64/96   train_loss = 0.960\n",
      "Epoch 2623 Batch    0/96   train_loss = 0.911\n",
      "Epoch 2623 Batch   32/96   train_loss = 0.994\n",
      "Epoch 2623 Batch   64/96   train_loss = 0.980\n",
      "Epoch 2624 Batch    0/96   train_loss = 0.937\n",
      "Epoch 2624 Batch   32/96   train_loss = 0.982\n",
      "Epoch 2624 Batch   64/96   train_loss = 0.968\n",
      "Epoch 2625 Batch    0/96   train_loss = 0.921\n",
      "Epoch 2625 Batch   32/96   train_loss = 0.985\n",
      "Epoch 2625 Batch   64/96   train_loss = 0.961\n",
      "Epoch 2626 Batch    0/96   train_loss = 0.913\n",
      "Epoch 2626 Batch   32/96   train_loss = 0.971\n",
      "Epoch 2626 Batch   64/96   train_loss = 0.957\n",
      "Epoch 2627 Batch    0/96   train_loss = 0.927\n",
      "Epoch 2627 Batch   32/96   train_loss = 0.984\n",
      "Epoch 2627 Batch   64/96   train_loss = 0.967\n",
      "Epoch 2628 Batch    0/96   train_loss = 0.921\n",
      "Epoch 2628 Batch   32/96   train_loss = 0.992\n",
      "Epoch 2628 Batch   64/96   train_loss = 0.970\n",
      "Epoch 2629 Batch    0/96   train_loss = 0.942\n",
      "Epoch 2629 Batch   32/96   train_loss = 0.997\n",
      "Epoch 2629 Batch   64/96   train_loss = 0.970\n",
      "Epoch 2630 Batch    0/96   train_loss = 0.924\n",
      "Epoch 2630 Batch   32/96   train_loss = 0.992\n",
      "Epoch 2630 Batch   64/96   train_loss = 0.985\n",
      "Epoch 2631 Batch    0/96   train_loss = 0.923\n",
      "Epoch 2631 Batch   32/96   train_loss = 0.987\n",
      "Epoch 2631 Batch   64/96   train_loss = 0.978\n",
      "Epoch 2632 Batch    0/96   train_loss = 0.917\n",
      "Epoch 2632 Batch   32/96   train_loss = 0.979\n",
      "Epoch 2632 Batch   64/96   train_loss = 0.961\n",
      "Epoch 2633 Batch    0/96   train_loss = 0.925\n",
      "Epoch 2633 Batch   32/96   train_loss = 0.980\n",
      "Epoch 2633 Batch   64/96   train_loss = 0.962\n",
      "Epoch 2634 Batch    0/96   train_loss = 0.931\n",
      "Epoch 2634 Batch   32/96   train_loss = 0.993\n",
      "Epoch 2634 Batch   64/96   train_loss = 0.965\n",
      "Epoch 2635 Batch    0/96   train_loss = 0.935\n",
      "Epoch 2635 Batch   32/96   train_loss = 0.979\n",
      "Epoch 2635 Batch   64/96   train_loss = 0.969\n",
      "Epoch 2636 Batch    0/96   train_loss = 0.935\n",
      "Epoch 2636 Batch   32/96   train_loss = 0.983\n",
      "Epoch 2636 Batch   64/96   train_loss = 0.962\n",
      "Epoch 2637 Batch    0/96   train_loss = 0.922\n",
      "Epoch 2637 Batch   32/96   train_loss = 0.973\n",
      "Epoch 2637 Batch   64/96   train_loss = 0.957\n",
      "Epoch 2638 Batch    0/96   train_loss = 0.911\n",
      "Epoch 2638 Batch   32/96   train_loss = 0.980\n",
      "Epoch 2638 Batch   64/96   train_loss = 0.981\n",
      "Epoch 2639 Batch    0/96   train_loss = 0.914\n",
      "Epoch 2639 Batch   32/96   train_loss = 0.974\n",
      "Epoch 2639 Batch   64/96   train_loss = 0.958\n",
      "Epoch 2640 Batch    0/96   train_loss = 0.923\n",
      "Epoch 2640 Batch   32/96   train_loss = 0.982\n",
      "Epoch 2640 Batch   64/96   train_loss = 0.967\n",
      "Epoch 2641 Batch    0/96   train_loss = 0.921\n",
      "Epoch 2641 Batch   32/96   train_loss = 0.980\n",
      "Epoch 2641 Batch   64/96   train_loss = 0.964\n",
      "Epoch 2642 Batch    0/96   train_loss = 0.932\n",
      "Epoch 2642 Batch   32/96   train_loss = 0.994\n",
      "Epoch 2642 Batch   64/96   train_loss = 0.974\n",
      "Epoch 2643 Batch    0/96   train_loss = 0.915\n",
      "Epoch 2643 Batch   32/96   train_loss = 0.985\n",
      "Epoch 2643 Batch   64/96   train_loss = 0.965\n",
      "Epoch 2644 Batch    0/96   train_loss = 0.911\n",
      "Epoch 2644 Batch   32/96   train_loss = 1.001\n",
      "Epoch 2644 Batch   64/96   train_loss = 0.965\n",
      "Epoch 2645 Batch    0/96   train_loss = 0.933\n",
      "Epoch 2645 Batch   32/96   train_loss = 0.983\n",
      "Epoch 2645 Batch   64/96   train_loss = 0.956\n",
      "Epoch 2646 Batch    0/96   train_loss = 0.919\n",
      "Epoch 2646 Batch   32/96   train_loss = 0.986\n",
      "Epoch 2646 Batch   64/96   train_loss = 0.965\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2647 Batch    0/96   train_loss = 0.883\n",
      "Epoch 2647 Batch   32/96   train_loss = 0.966\n",
      "Epoch 2647 Batch   64/96   train_loss = 0.962\n",
      "Epoch 2648 Batch    0/96   train_loss = 0.885\n",
      "Epoch 2648 Batch   32/96   train_loss = 0.977\n",
      "Epoch 2648 Batch   64/96   train_loss = 0.946\n",
      "Epoch 2649 Batch    0/96   train_loss = 0.896\n",
      "Epoch 2649 Batch   32/96   train_loss = 0.974\n",
      "Epoch 2649 Batch   64/96   train_loss = 0.938\n",
      "Epoch 2650 Batch    0/96   train_loss = 0.916\n",
      "Epoch 2650 Batch   32/96   train_loss = 0.998\n",
      "Epoch 2650 Batch   64/96   train_loss = 0.969\n",
      "Epoch 2651 Batch    0/96   train_loss = 0.933\n",
      "Epoch 2651 Batch   32/96   train_loss = 0.990\n",
      "Epoch 2651 Batch   64/96   train_loss = 0.961\n",
      "Epoch 2652 Batch    0/96   train_loss = 0.924\n",
      "Epoch 2652 Batch   32/96   train_loss = 1.007\n",
      "Epoch 2652 Batch   64/96   train_loss = 0.956\n",
      "Epoch 2653 Batch    0/96   train_loss = 0.925\n",
      "Epoch 2653 Batch   32/96   train_loss = 0.993\n",
      "Epoch 2653 Batch   64/96   train_loss = 0.958\n",
      "Epoch 2654 Batch    0/96   train_loss = 0.915\n",
      "Epoch 2654 Batch   32/96   train_loss = 1.014\n",
      "Epoch 2654 Batch   64/96   train_loss = 0.955\n",
      "Epoch 2655 Batch    0/96   train_loss = 0.911\n",
      "Epoch 2655 Batch   32/96   train_loss = 0.991\n",
      "Epoch 2655 Batch   64/96   train_loss = 0.966\n",
      "Epoch 2656 Batch    0/96   train_loss = 0.919\n",
      "Epoch 2656 Batch   32/96   train_loss = 0.988\n",
      "Epoch 2656 Batch   64/96   train_loss = 0.940\n",
      "Epoch 2657 Batch    0/96   train_loss = 0.917\n",
      "Epoch 2657 Batch   32/96   train_loss = 0.994\n",
      "Epoch 2657 Batch   64/96   train_loss = 0.963\n",
      "Epoch 2658 Batch    0/96   train_loss = 0.916\n",
      "Epoch 2658 Batch   32/96   train_loss = 0.995\n",
      "Epoch 2658 Batch   64/96   train_loss = 0.951\n",
      "Epoch 2659 Batch    0/96   train_loss = 0.915\n",
      "Epoch 2659 Batch   32/96   train_loss = 0.980\n",
      "Epoch 2659 Batch   64/96   train_loss = 0.963\n",
      "Epoch 2660 Batch    0/96   train_loss = 0.911\n",
      "Epoch 2660 Batch   32/96   train_loss = 0.967\n",
      "Epoch 2660 Batch   64/96   train_loss = 0.963\n",
      "Epoch 2661 Batch    0/96   train_loss = 0.919\n",
      "Epoch 2661 Batch   32/96   train_loss = 0.971\n",
      "Epoch 2661 Batch   64/96   train_loss = 0.975\n",
      "Epoch 2662 Batch    0/96   train_loss = 0.906\n",
      "Epoch 2662 Batch   32/96   train_loss = 0.974\n",
      "Epoch 2662 Batch   64/96   train_loss = 0.949\n",
      "Epoch 2663 Batch    0/96   train_loss = 0.885\n",
      "Epoch 2663 Batch   32/96   train_loss = 0.967\n",
      "Epoch 2663 Batch   64/96   train_loss = 0.947\n",
      "Epoch 2664 Batch    0/96   train_loss = 0.896\n",
      "Epoch 2664 Batch   32/96   train_loss = 0.962\n",
      "Epoch 2664 Batch   64/96   train_loss = 0.954\n",
      "Epoch 2665 Batch    0/96   train_loss = 0.912\n",
      "Epoch 2665 Batch   32/96   train_loss = 0.950\n",
      "Epoch 2665 Batch   64/96   train_loss = 0.932\n",
      "Epoch 2666 Batch    0/96   train_loss = 0.907\n",
      "Epoch 2666 Batch   32/96   train_loss = 0.963\n",
      "Epoch 2666 Batch   64/96   train_loss = 0.947\n",
      "Epoch 2667 Batch    0/96   train_loss = 0.895\n",
      "Epoch 2667 Batch   32/96   train_loss = 0.983\n",
      "Epoch 2667 Batch   64/96   train_loss = 0.952\n",
      "Epoch 2668 Batch    0/96   train_loss = 0.907\n",
      "Epoch 2668 Batch   32/96   train_loss = 0.983\n",
      "Epoch 2668 Batch   64/96   train_loss = 0.945\n",
      "Epoch 2669 Batch    0/96   train_loss = 0.911\n",
      "Epoch 2669 Batch   32/96   train_loss = 1.001\n",
      "Epoch 2669 Batch   64/96   train_loss = 0.950\n",
      "Epoch 2670 Batch    0/96   train_loss = 0.908\n",
      "Epoch 2670 Batch   32/96   train_loss = 1.016\n",
      "Epoch 2670 Batch   64/96   train_loss = 0.973\n",
      "Epoch 2671 Batch    0/96   train_loss = 0.897\n",
      "Epoch 2671 Batch   32/96   train_loss = 0.983\n",
      "Epoch 2671 Batch   64/96   train_loss = 0.972\n",
      "Epoch 2672 Batch    0/96   train_loss = 0.904\n",
      "Epoch 2672 Batch   32/96   train_loss = 0.983\n",
      "Epoch 2672 Batch   64/96   train_loss = 0.964\n",
      "Epoch 2673 Batch    0/96   train_loss = 0.908\n",
      "Epoch 2673 Batch   32/96   train_loss = 0.998\n",
      "Epoch 2673 Batch   64/96   train_loss = 0.979\n",
      "Epoch 2674 Batch    0/96   train_loss = 0.940\n",
      "Epoch 2674 Batch   32/96   train_loss = 1.006\n",
      "Epoch 2674 Batch   64/96   train_loss = 0.976\n",
      "Epoch 2675 Batch    0/96   train_loss = 0.912\n",
      "Epoch 2675 Batch   32/96   train_loss = 0.975\n",
      "Epoch 2675 Batch   64/96   train_loss = 0.966\n",
      "Epoch 2676 Batch    0/96   train_loss = 0.911\n",
      "Epoch 2676 Batch   32/96   train_loss = 0.964\n",
      "Epoch 2676 Batch   64/96   train_loss = 0.942\n",
      "Epoch 2677 Batch    0/96   train_loss = 0.907\n",
      "Epoch 2677 Batch   32/96   train_loss = 0.971\n",
      "Epoch 2677 Batch   64/96   train_loss = 0.945\n",
      "Epoch 2678 Batch    0/96   train_loss = 0.915\n",
      "Epoch 2678 Batch   32/96   train_loss = 1.012\n",
      "Epoch 2678 Batch   64/96   train_loss = 0.969\n",
      "Epoch 2679 Batch    0/96   train_loss = 0.920\n",
      "Epoch 2679 Batch   32/96   train_loss = 0.983\n",
      "Epoch 2679 Batch   64/96   train_loss = 0.986\n",
      "Epoch 2680 Batch    0/96   train_loss = 0.924\n",
      "Epoch 2680 Batch   32/96   train_loss = 0.981\n",
      "Epoch 2680 Batch   64/96   train_loss = 0.964\n",
      "Epoch 2681 Batch    0/96   train_loss = 0.931\n",
      "Epoch 2681 Batch   32/96   train_loss = 0.990\n",
      "Epoch 2681 Batch   64/96   train_loss = 0.971\n",
      "Epoch 2682 Batch    0/96   train_loss = 0.917\n",
      "Epoch 2682 Batch   32/96   train_loss = 0.996\n",
      "Epoch 2682 Batch   64/96   train_loss = 0.951\n",
      "Epoch 2683 Batch    0/96   train_loss = 0.913\n",
      "Epoch 2683 Batch   32/96   train_loss = 0.981\n",
      "Epoch 2683 Batch   64/96   train_loss = 0.960\n",
      "Epoch 2684 Batch    0/96   train_loss = 0.926\n",
      "Epoch 2684 Batch   32/96   train_loss = 0.983\n",
      "Epoch 2684 Batch   64/96   train_loss = 0.959\n",
      "Epoch 2685 Batch    0/96   train_loss = 0.914\n",
      "Epoch 2685 Batch   32/96   train_loss = 0.988\n",
      "Epoch 2685 Batch   64/96   train_loss = 0.974\n",
      "Epoch 2686 Batch    0/96   train_loss = 0.924\n",
      "Epoch 2686 Batch   32/96   train_loss = 0.993\n",
      "Epoch 2686 Batch   64/96   train_loss = 0.966\n",
      "Epoch 2687 Batch    0/96   train_loss = 0.912\n",
      "Epoch 2687 Batch   32/96   train_loss = 0.980\n",
      "Epoch 2687 Batch   64/96   train_loss = 0.989\n",
      "Epoch 2688 Batch    0/96   train_loss = 0.924\n",
      "Epoch 2688 Batch   32/96   train_loss = 1.001\n",
      "Epoch 2688 Batch   64/96   train_loss = 0.978\n",
      "Epoch 2689 Batch    0/96   train_loss = 0.922\n",
      "Epoch 2689 Batch   32/96   train_loss = 1.003\n",
      "Epoch 2689 Batch   64/96   train_loss = 0.979\n",
      "Epoch 2690 Batch    0/96   train_loss = 0.926\n",
      "Epoch 2690 Batch   32/96   train_loss = 0.988\n",
      "Epoch 2690 Batch   64/96   train_loss = 0.981\n",
      "Epoch 2691 Batch    0/96   train_loss = 0.921\n",
      "Epoch 2691 Batch   32/96   train_loss = 1.015\n",
      "Epoch 2691 Batch   64/96   train_loss = 0.980\n",
      "Epoch 2692 Batch    0/96   train_loss = 0.940\n",
      "Epoch 2692 Batch   32/96   train_loss = 1.009\n",
      "Epoch 2692 Batch   64/96   train_loss = 0.967\n",
      "Epoch 2693 Batch    0/96   train_loss = 0.935\n",
      "Epoch 2693 Batch   32/96   train_loss = 1.003\n",
      "Epoch 2693 Batch   64/96   train_loss = 0.983\n",
      "Epoch 2694 Batch    0/96   train_loss = 0.930\n",
      "Epoch 2694 Batch   32/96   train_loss = 1.007\n",
      "Epoch 2694 Batch   64/96   train_loss = 0.967\n",
      "Epoch 2695 Batch    0/96   train_loss = 0.914\n",
      "Epoch 2695 Batch   32/96   train_loss = 0.999\n",
      "Epoch 2695 Batch   64/96   train_loss = 0.982\n",
      "Epoch 2696 Batch    0/96   train_loss = 0.927\n",
      "Epoch 2696 Batch   32/96   train_loss = 0.981\n",
      "Epoch 2696 Batch   64/96   train_loss = 0.980\n",
      "Epoch 2697 Batch    0/96   train_loss = 0.919\n",
      "Epoch 2697 Batch   32/96   train_loss = 0.989\n",
      "Epoch 2697 Batch   64/96   train_loss = 0.970\n",
      "Epoch 2698 Batch    0/96   train_loss = 0.919\n",
      "Epoch 2698 Batch   32/96   train_loss = 0.976\n",
      "Epoch 2698 Batch   64/96   train_loss = 0.955\n",
      "Epoch 2699 Batch    0/96   train_loss = 0.909\n",
      "Epoch 2699 Batch   32/96   train_loss = 0.980\n",
      "Epoch 2699 Batch   64/96   train_loss = 0.954\n",
      "Epoch 2700 Batch    0/96   train_loss = 0.894\n",
      "Epoch 2700 Batch   32/96   train_loss = 0.963\n",
      "Epoch 2700 Batch   64/96   train_loss = 0.952\n",
      "Epoch 2701 Batch    0/96   train_loss = 0.914\n",
      "Epoch 2701 Batch   32/96   train_loss = 0.971\n",
      "Epoch 2701 Batch   64/96   train_loss = 0.946\n",
      "Epoch 2702 Batch    0/96   train_loss = 0.948\n",
      "Epoch 2702 Batch   32/96   train_loss = 1.002\n",
      "Epoch 2702 Batch   64/96   train_loss = 0.959\n",
      "Epoch 2703 Batch    0/96   train_loss = 0.907\n",
      "Epoch 2703 Batch   32/96   train_loss = 0.978\n",
      "Epoch 2703 Batch   64/96   train_loss = 0.955\n",
      "Epoch 2704 Batch    0/96   train_loss = 0.901\n",
      "Epoch 2704 Batch   32/96   train_loss = 0.968\n",
      "Epoch 2704 Batch   64/96   train_loss = 0.951\n",
      "Epoch 2705 Batch    0/96   train_loss = 0.895\n",
      "Epoch 2705 Batch   32/96   train_loss = 0.973\n",
      "Epoch 2705 Batch   64/96   train_loss = 0.951\n",
      "Epoch 2706 Batch    0/96   train_loss = 0.916\n",
      "Epoch 2706 Batch   32/96   train_loss = 0.983\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2706 Batch   64/96   train_loss = 0.950\n",
      "Epoch 2707 Batch    0/96   train_loss = 0.906\n",
      "Epoch 2707 Batch   32/96   train_loss = 0.967\n",
      "Epoch 2707 Batch   64/96   train_loss = 0.939\n",
      "Epoch 2708 Batch    0/96   train_loss = 0.897\n",
      "Epoch 2708 Batch   32/96   train_loss = 0.987\n",
      "Epoch 2708 Batch   64/96   train_loss = 0.971\n",
      "Epoch 2709 Batch    0/96   train_loss = 0.911\n",
      "Epoch 2709 Batch   32/96   train_loss = 0.977\n",
      "Epoch 2709 Batch   64/96   train_loss = 0.952\n",
      "Epoch 2710 Batch    0/96   train_loss = 0.902\n",
      "Epoch 2710 Batch   32/96   train_loss = 0.979\n",
      "Epoch 2710 Batch   64/96   train_loss = 0.947\n",
      "Epoch 2711 Batch    0/96   train_loss = 0.901\n",
      "Epoch 2711 Batch   32/96   train_loss = 0.975\n",
      "Epoch 2711 Batch   64/96   train_loss = 0.954\n",
      "Epoch 2712 Batch    0/96   train_loss = 0.915\n",
      "Epoch 2712 Batch   32/96   train_loss = 0.963\n",
      "Epoch 2712 Batch   64/96   train_loss = 0.946\n",
      "Epoch 2713 Batch    0/96   train_loss = 0.893\n",
      "Epoch 2713 Batch   32/96   train_loss = 0.968\n",
      "Epoch 2713 Batch   64/96   train_loss = 0.950\n",
      "Epoch 2714 Batch    0/96   train_loss = 0.897\n",
      "Epoch 2714 Batch   32/96   train_loss = 1.004\n",
      "Epoch 2714 Batch   64/96   train_loss = 0.966\n",
      "Epoch 2715 Batch    0/96   train_loss = 0.909\n",
      "Epoch 2715 Batch   32/96   train_loss = 0.986\n",
      "Epoch 2715 Batch   64/96   train_loss = 0.942\n",
      "Epoch 2716 Batch    0/96   train_loss = 0.911\n",
      "Epoch 2716 Batch   32/96   train_loss = 0.973\n",
      "Epoch 2716 Batch   64/96   train_loss = 0.944\n",
      "Epoch 2717 Batch    0/96   train_loss = 0.904\n",
      "Epoch 2717 Batch   32/96   train_loss = 0.980\n",
      "Epoch 2717 Batch   64/96   train_loss = 0.943\n",
      "Epoch 2718 Batch    0/96   train_loss = 0.912\n",
      "Epoch 2718 Batch   32/96   train_loss = 0.974\n",
      "Epoch 2718 Batch   64/96   train_loss = 0.948\n",
      "Epoch 2719 Batch    0/96   train_loss = 0.904\n",
      "Epoch 2719 Batch   32/96   train_loss = 0.980\n",
      "Epoch 2719 Batch   64/96   train_loss = 0.930\n",
      "Epoch 2720 Batch    0/96   train_loss = 0.900\n",
      "Epoch 2720 Batch   32/96   train_loss = 0.967\n",
      "Epoch 2720 Batch   64/96   train_loss = 0.938\n",
      "Epoch 2721 Batch    0/96   train_loss = 0.910\n",
      "Epoch 2721 Batch   32/96   train_loss = 0.975\n",
      "Epoch 2721 Batch   64/96   train_loss = 0.947\n",
      "Epoch 2722 Batch    0/96   train_loss = 0.903\n",
      "Epoch 2722 Batch   32/96   train_loss = 0.972\n",
      "Epoch 2722 Batch   64/96   train_loss = 0.943\n",
      "Epoch 2723 Batch    0/96   train_loss = 0.907\n",
      "Epoch 2723 Batch   32/96   train_loss = 0.977\n",
      "Epoch 2723 Batch   64/96   train_loss = 0.937\n",
      "Epoch 2724 Batch    0/96   train_loss = 0.911\n",
      "Epoch 2724 Batch   32/96   train_loss = 0.959\n",
      "Epoch 2724 Batch   64/96   train_loss = 0.937\n",
      "Epoch 2725 Batch    0/96   train_loss = 0.904\n",
      "Epoch 2725 Batch   32/96   train_loss = 0.958\n",
      "Epoch 2725 Batch   64/96   train_loss = 0.947\n",
      "Epoch 2726 Batch    0/96   train_loss = 0.911\n",
      "Epoch 2726 Batch   32/96   train_loss = 0.976\n",
      "Epoch 2726 Batch   64/96   train_loss = 0.946\n",
      "Epoch 2727 Batch    0/96   train_loss = 0.899\n",
      "Epoch 2727 Batch   32/96   train_loss = 0.961\n",
      "Epoch 2727 Batch   64/96   train_loss = 0.945\n",
      "Epoch 2728 Batch    0/96   train_loss = 0.894\n",
      "Epoch 2728 Batch   32/96   train_loss = 0.976\n",
      "Epoch 2728 Batch   64/96   train_loss = 0.939\n",
      "Epoch 2729 Batch    0/96   train_loss = 0.899\n",
      "Epoch 2729 Batch   32/96   train_loss = 0.966\n",
      "Epoch 2729 Batch   64/96   train_loss = 0.936\n",
      "Epoch 2730 Batch    0/96   train_loss = 0.910\n",
      "Epoch 2730 Batch   32/96   train_loss = 0.958\n",
      "Epoch 2730 Batch   64/96   train_loss = 0.945\n",
      "Epoch 2731 Batch    0/96   train_loss = 0.916\n",
      "Epoch 2731 Batch   32/96   train_loss = 0.980\n",
      "Epoch 2731 Batch   64/96   train_loss = 0.947\n",
      "Epoch 2732 Batch    0/96   train_loss = 0.902\n",
      "Epoch 2732 Batch   32/96   train_loss = 0.965\n",
      "Epoch 2732 Batch   64/96   train_loss = 0.942\n",
      "Epoch 2733 Batch    0/96   train_loss = 0.911\n",
      "Epoch 2733 Batch   32/96   train_loss = 0.969\n",
      "Epoch 2733 Batch   64/96   train_loss = 0.965\n",
      "Epoch 2734 Batch    0/96   train_loss = 0.936\n",
      "Epoch 2734 Batch   32/96   train_loss = 1.004\n",
      "Epoch 2734 Batch   64/96   train_loss = 0.966\n",
      "Epoch 2735 Batch    0/96   train_loss = 0.915\n",
      "Epoch 2735 Batch   32/96   train_loss = 0.978\n",
      "Epoch 2735 Batch   64/96   train_loss = 0.958\n",
      "Epoch 2736 Batch    0/96   train_loss = 0.905\n",
      "Epoch 2736 Batch   32/96   train_loss = 0.972\n",
      "Epoch 2736 Batch   64/96   train_loss = 0.955\n",
      "Epoch 2737 Batch    0/96   train_loss = 0.912\n",
      "Epoch 2737 Batch   32/96   train_loss = 0.980\n",
      "Epoch 2737 Batch   64/96   train_loss = 0.947\n",
      "Epoch 2738 Batch    0/96   train_loss = 0.916\n",
      "Epoch 2738 Batch   32/96   train_loss = 0.987\n",
      "Epoch 2738 Batch   64/96   train_loss = 0.963\n",
      "Epoch 2739 Batch    0/96   train_loss = 0.915\n",
      "Epoch 2739 Batch   32/96   train_loss = 0.968\n",
      "Epoch 2739 Batch   64/96   train_loss = 0.942\n",
      "Epoch 2740 Batch    0/96   train_loss = 0.897\n",
      "Epoch 2740 Batch   32/96   train_loss = 0.957\n",
      "Epoch 2740 Batch   64/96   train_loss = 0.945\n",
      "Epoch 2741 Batch    0/96   train_loss = 0.897\n",
      "Epoch 2741 Batch   32/96   train_loss = 0.957\n",
      "Epoch 2741 Batch   64/96   train_loss = 0.936\n",
      "Epoch 2742 Batch    0/96   train_loss = 0.896\n",
      "Epoch 2742 Batch   32/96   train_loss = 0.962\n",
      "Epoch 2742 Batch   64/96   train_loss = 0.947\n",
      "Epoch 2743 Batch    0/96   train_loss = 0.906\n",
      "Epoch 2743 Batch   32/96   train_loss = 0.961\n",
      "Epoch 2743 Batch   64/96   train_loss = 0.940\n",
      "Epoch 2744 Batch    0/96   train_loss = 0.910\n",
      "Epoch 2744 Batch   32/96   train_loss = 0.969\n",
      "Epoch 2744 Batch   64/96   train_loss = 0.953\n",
      "Epoch 2745 Batch    0/96   train_loss = 0.914\n",
      "Epoch 2745 Batch   32/96   train_loss = 0.956\n",
      "Epoch 2745 Batch   64/96   train_loss = 0.939\n",
      "Epoch 2746 Batch    0/96   train_loss = 0.914\n",
      "Epoch 2746 Batch   32/96   train_loss = 0.973\n",
      "Epoch 2746 Batch   64/96   train_loss = 0.954\n",
      "Epoch 2747 Batch    0/96   train_loss = 0.901\n",
      "Epoch 2747 Batch   32/96   train_loss = 0.970\n",
      "Epoch 2747 Batch   64/96   train_loss = 0.951\n",
      "Epoch 2748 Batch    0/96   train_loss = 0.895\n",
      "Epoch 2748 Batch   32/96   train_loss = 0.964\n",
      "Epoch 2748 Batch   64/96   train_loss = 0.940\n",
      "Epoch 2749 Batch    0/96   train_loss = 0.894\n",
      "Epoch 2749 Batch   32/96   train_loss = 0.964\n",
      "Epoch 2749 Batch   64/96   train_loss = 0.935\n",
      "Epoch 2750 Batch    0/96   train_loss = 0.883\n",
      "Epoch 2750 Batch   32/96   train_loss = 0.947\n",
      "Epoch 2750 Batch   64/96   train_loss = 0.955\n",
      "Epoch 2751 Batch    0/96   train_loss = 0.883\n",
      "Epoch 2751 Batch   32/96   train_loss = 0.949\n",
      "Epoch 2751 Batch   64/96   train_loss = 0.947\n",
      "Epoch 2752 Batch    0/96   train_loss = 0.888\n",
      "Epoch 2752 Batch   32/96   train_loss = 0.954\n",
      "Epoch 2752 Batch   64/96   train_loss = 0.943\n",
      "Epoch 2753 Batch    0/96   train_loss = 0.882\n",
      "Epoch 2753 Batch   32/96   train_loss = 0.962\n",
      "Epoch 2753 Batch   64/96   train_loss = 0.942\n",
      "Epoch 2754 Batch    0/96   train_loss = 0.884\n",
      "Epoch 2754 Batch   32/96   train_loss = 0.967\n",
      "Epoch 2754 Batch   64/96   train_loss = 0.945\n",
      "Epoch 2755 Batch    0/96   train_loss = 0.914\n",
      "Epoch 2755 Batch   32/96   train_loss = 0.962\n",
      "Epoch 2755 Batch   64/96   train_loss = 0.944\n",
      "Epoch 2756 Batch    0/96   train_loss = 0.885\n",
      "Epoch 2756 Batch   32/96   train_loss = 0.962\n",
      "Epoch 2756 Batch   64/96   train_loss = 0.950\n",
      "Epoch 2757 Batch    0/96   train_loss = 0.905\n",
      "Epoch 2757 Batch   32/96   train_loss = 0.967\n",
      "Epoch 2757 Batch   64/96   train_loss = 0.931\n",
      "Epoch 2758 Batch    0/96   train_loss = 0.888\n",
      "Epoch 2758 Batch   32/96   train_loss = 0.946\n",
      "Epoch 2758 Batch   64/96   train_loss = 0.942\n",
      "Epoch 2759 Batch    0/96   train_loss = 0.881\n",
      "Epoch 2759 Batch   32/96   train_loss = 0.965\n",
      "Epoch 2759 Batch   64/96   train_loss = 0.939\n",
      "Epoch 2760 Batch    0/96   train_loss = 0.896\n",
      "Epoch 2760 Batch   32/96   train_loss = 0.971\n",
      "Epoch 2760 Batch   64/96   train_loss = 0.952\n",
      "Epoch 2761 Batch    0/96   train_loss = 0.913\n",
      "Epoch 2761 Batch   32/96   train_loss = 0.959\n",
      "Epoch 2761 Batch   64/96   train_loss = 0.972\n",
      "Epoch 2762 Batch    0/96   train_loss = 0.910\n",
      "Epoch 2762 Batch   32/96   train_loss = 0.951\n",
      "Epoch 2762 Batch   64/96   train_loss = 0.942\n",
      "Epoch 2763 Batch    0/96   train_loss = 0.932\n",
      "Epoch 2763 Batch   32/96   train_loss = 0.964\n",
      "Epoch 2763 Batch   64/96   train_loss = 0.939\n",
      "Epoch 2764 Batch    0/96   train_loss = 0.910\n",
      "Epoch 2764 Batch   32/96   train_loss = 0.958\n",
      "Epoch 2764 Batch   64/96   train_loss = 0.944\n",
      "Epoch 2765 Batch    0/96   train_loss = 0.899\n",
      "Epoch 2765 Batch   32/96   train_loss = 0.966\n",
      "Epoch 2765 Batch   64/96   train_loss = 0.977\n",
      "Epoch 2766 Batch    0/96   train_loss = 0.909\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2766 Batch   32/96   train_loss = 0.971\n",
      "Epoch 2766 Batch   64/96   train_loss = 0.958\n",
      "Epoch 2767 Batch    0/96   train_loss = 0.912\n",
      "Epoch 2767 Batch   32/96   train_loss = 0.977\n",
      "Epoch 2767 Batch   64/96   train_loss = 0.979\n",
      "Epoch 2768 Batch    0/96   train_loss = 0.920\n",
      "Epoch 2768 Batch   32/96   train_loss = 0.972\n",
      "Epoch 2768 Batch   64/96   train_loss = 0.967\n",
      "Epoch 2769 Batch    0/96   train_loss = 0.917\n",
      "Epoch 2769 Batch   32/96   train_loss = 0.971\n",
      "Epoch 2769 Batch   64/96   train_loss = 0.957\n",
      "Epoch 2770 Batch    0/96   train_loss = 0.915\n",
      "Epoch 2770 Batch   32/96   train_loss = 0.966\n",
      "Epoch 2770 Batch   64/96   train_loss = 0.988\n",
      "Epoch 2771 Batch    0/96   train_loss = 0.914\n",
      "Epoch 2771 Batch   32/96   train_loss = 0.971\n",
      "Epoch 2771 Batch   64/96   train_loss = 0.985\n",
      "Epoch 2772 Batch    0/96   train_loss = 0.928\n",
      "Epoch 2772 Batch   32/96   train_loss = 0.990\n",
      "Epoch 2772 Batch   64/96   train_loss = 0.964\n",
      "Epoch 2773 Batch    0/96   train_loss = 0.926\n",
      "Epoch 2773 Batch   32/96   train_loss = 0.983\n",
      "Epoch 2773 Batch   64/96   train_loss = 0.990\n",
      "Epoch 2774 Batch    0/96   train_loss = 0.916\n",
      "Epoch 2774 Batch   32/96   train_loss = 0.977\n",
      "Epoch 2774 Batch   64/96   train_loss = 0.971\n",
      "Epoch 2775 Batch    0/96   train_loss = 0.914\n",
      "Epoch 2775 Batch   32/96   train_loss = 0.987\n",
      "Epoch 2775 Batch   64/96   train_loss = 0.974\n",
      "Epoch 2776 Batch    0/96   train_loss = 0.925\n",
      "Epoch 2776 Batch   32/96   train_loss = 0.985\n",
      "Epoch 2776 Batch   64/96   train_loss = 0.972\n",
      "Epoch 2777 Batch    0/96   train_loss = 0.934\n",
      "Epoch 2777 Batch   32/96   train_loss = 0.988\n",
      "Epoch 2777 Batch   64/96   train_loss = 0.980\n",
      "Epoch 2778 Batch    0/96   train_loss = 0.934\n",
      "Epoch 2778 Batch   32/96   train_loss = 0.988\n",
      "Epoch 2778 Batch   64/96   train_loss = 0.981\n",
      "Epoch 2779 Batch    0/96   train_loss = 0.920\n",
      "Epoch 2779 Batch   32/96   train_loss = 0.985\n",
      "Epoch 2779 Batch   64/96   train_loss = 0.969\n",
      "Epoch 2780 Batch    0/96   train_loss = 0.931\n",
      "Epoch 2780 Batch   32/96   train_loss = 0.994\n",
      "Epoch 2780 Batch   64/96   train_loss = 0.968\n",
      "Epoch 2781 Batch    0/96   train_loss = 0.921\n",
      "Epoch 2781 Batch   32/96   train_loss = 0.994\n",
      "Epoch 2781 Batch   64/96   train_loss = 0.973\n",
      "Epoch 2782 Batch    0/96   train_loss = 0.928\n",
      "Epoch 2782 Batch   32/96   train_loss = 0.989\n",
      "Epoch 2782 Batch   64/96   train_loss = 0.970\n",
      "Epoch 2783 Batch    0/96   train_loss = 0.915\n",
      "Epoch 2783 Batch   32/96   train_loss = 0.979\n",
      "Epoch 2783 Batch   64/96   train_loss = 0.968\n",
      "Epoch 2784 Batch    0/96   train_loss = 0.914\n",
      "Epoch 2784 Batch   32/96   train_loss = 0.970\n",
      "Epoch 2784 Batch   64/96   train_loss = 0.950\n",
      "Epoch 2785 Batch    0/96   train_loss = 0.910\n",
      "Epoch 2785 Batch   32/96   train_loss = 0.964\n",
      "Epoch 2785 Batch   64/96   train_loss = 0.946\n",
      "Epoch 2786 Batch    0/96   train_loss = 0.911\n",
      "Epoch 2786 Batch   32/96   train_loss = 0.958\n",
      "Epoch 2786 Batch   64/96   train_loss = 0.968\n",
      "Epoch 2787 Batch    0/96   train_loss = 0.912\n",
      "Epoch 2787 Batch   32/96   train_loss = 0.964\n",
      "Epoch 2787 Batch   64/96   train_loss = 0.951\n",
      "Epoch 2788 Batch    0/96   train_loss = 0.905\n",
      "Epoch 2788 Batch   32/96   train_loss = 0.962\n",
      "Epoch 2788 Batch   64/96   train_loss = 0.940\n",
      "Epoch 2789 Batch    0/96   train_loss = 0.939\n",
      "Epoch 2789 Batch   32/96   train_loss = 0.984\n",
      "Epoch 2789 Batch   64/96   train_loss = 0.946\n",
      "Epoch 2790 Batch    0/96   train_loss = 0.926\n",
      "Epoch 2790 Batch   32/96   train_loss = 0.966\n",
      "Epoch 2790 Batch   64/96   train_loss = 0.949\n",
      "Epoch 2791 Batch    0/96   train_loss = 0.903\n",
      "Epoch 2791 Batch   32/96   train_loss = 0.967\n",
      "Epoch 2791 Batch   64/96   train_loss = 0.949\n",
      "Epoch 2792 Batch    0/96   train_loss = 0.908\n",
      "Epoch 2792 Batch   32/96   train_loss = 0.967\n",
      "Epoch 2792 Batch   64/96   train_loss = 0.937\n",
      "Epoch 2793 Batch    0/96   train_loss = 0.916\n",
      "Epoch 2793 Batch   32/96   train_loss = 0.965\n",
      "Epoch 2793 Batch   64/96   train_loss = 0.944\n",
      "Epoch 2794 Batch    0/96   train_loss = 0.920\n",
      "Epoch 2794 Batch   32/96   train_loss = 0.967\n",
      "Epoch 2794 Batch   64/96   train_loss = 0.929\n",
      "Epoch 2795 Batch    0/96   train_loss = 0.910\n",
      "Epoch 2795 Batch   32/96   train_loss = 0.972\n",
      "Epoch 2795 Batch   64/96   train_loss = 0.940\n",
      "Epoch 2796 Batch    0/96   train_loss = 0.915\n",
      "Epoch 2796 Batch   32/96   train_loss = 0.964\n",
      "Epoch 2796 Batch   64/96   train_loss = 0.942\n",
      "Epoch 2797 Batch    0/96   train_loss = 0.898\n",
      "Epoch 2797 Batch   32/96   train_loss = 0.961\n",
      "Epoch 2797 Batch   64/96   train_loss = 0.935\n",
      "Epoch 2798 Batch    0/96   train_loss = 0.901\n",
      "Epoch 2798 Batch   32/96   train_loss = 0.960\n",
      "Epoch 2798 Batch   64/96   train_loss = 0.958\n",
      "Epoch 2799 Batch    0/96   train_loss = 0.920\n",
      "Epoch 2799 Batch   32/96   train_loss = 0.976\n",
      "Epoch 2799 Batch   64/96   train_loss = 0.986\n",
      "Epoch 2800 Batch    0/96   train_loss = 0.921\n",
      "Epoch 2800 Batch   32/96   train_loss = 0.960\n",
      "Epoch 2800 Batch   64/96   train_loss = 0.951\n",
      "Epoch 2801 Batch    0/96   train_loss = 0.895\n",
      "Epoch 2801 Batch   32/96   train_loss = 0.966\n",
      "Epoch 2801 Batch   64/96   train_loss = 0.976\n",
      "Epoch 2802 Batch    0/96   train_loss = 0.927\n",
      "Epoch 2802 Batch   32/96   train_loss = 0.970\n",
      "Epoch 2802 Batch   64/96   train_loss = 0.962\n",
      "Epoch 2803 Batch    0/96   train_loss = 0.909\n",
      "Epoch 2803 Batch   32/96   train_loss = 0.979\n",
      "Epoch 2803 Batch   64/96   train_loss = 0.960\n",
      "Epoch 2804 Batch    0/96   train_loss = 0.928\n",
      "Epoch 2804 Batch   32/96   train_loss = 0.972\n",
      "Epoch 2804 Batch   64/96   train_loss = 0.977\n",
      "Epoch 2805 Batch    0/96   train_loss = 0.909\n",
      "Epoch 2805 Batch   32/96   train_loss = 0.983\n",
      "Epoch 2805 Batch   64/96   train_loss = 0.973\n",
      "Epoch 2806 Batch    0/96   train_loss = 0.924\n",
      "Epoch 2806 Batch   32/96   train_loss = 0.992\n",
      "Epoch 2806 Batch   64/96   train_loss = 0.960\n",
      "Epoch 2807 Batch    0/96   train_loss = 0.899\n",
      "Epoch 2807 Batch   32/96   train_loss = 0.968\n",
      "Epoch 2807 Batch   64/96   train_loss = 0.956\n",
      "Epoch 2808 Batch    0/96   train_loss = 0.907\n",
      "Epoch 2808 Batch   32/96   train_loss = 0.981\n",
      "Epoch 2808 Batch   64/96   train_loss = 0.960\n",
      "Epoch 2809 Batch    0/96   train_loss = 0.900\n",
      "Epoch 2809 Batch   32/96   train_loss = 0.982\n",
      "Epoch 2809 Batch   64/96   train_loss = 0.964\n",
      "Epoch 2810 Batch    0/96   train_loss = 0.936\n",
      "Epoch 2810 Batch   32/96   train_loss = 0.988\n",
      "Epoch 2810 Batch   64/96   train_loss = 0.986\n",
      "Epoch 2811 Batch    0/96   train_loss = 0.924\n",
      "Epoch 2811 Batch   32/96   train_loss = 0.993\n",
      "Epoch 2811 Batch   64/96   train_loss = 0.964\n",
      "Epoch 2812 Batch    0/96   train_loss = 0.928\n",
      "Epoch 2812 Batch   32/96   train_loss = 0.997\n",
      "Epoch 2812 Batch   64/96   train_loss = 0.968\n",
      "Epoch 2813 Batch    0/96   train_loss = 0.932\n",
      "Epoch 2813 Batch   32/96   train_loss = 0.995\n",
      "Epoch 2813 Batch   64/96   train_loss = 0.969\n",
      "Epoch 2814 Batch    0/96   train_loss = 0.926\n",
      "Epoch 2814 Batch   32/96   train_loss = 0.997\n",
      "Epoch 2814 Batch   64/96   train_loss = 0.957\n",
      "Epoch 2815 Batch    0/96   train_loss = 0.949\n",
      "Epoch 2815 Batch   32/96   train_loss = 1.002\n",
      "Epoch 2815 Batch   64/96   train_loss = 0.985\n",
      "Epoch 2816 Batch    0/96   train_loss = 0.946\n",
      "Epoch 2816 Batch   32/96   train_loss = 1.006\n",
      "Epoch 2816 Batch   64/96   train_loss = 0.970\n",
      "Epoch 2817 Batch    0/96   train_loss = 0.921\n",
      "Epoch 2817 Batch   32/96   train_loss = 0.979\n",
      "Epoch 2817 Batch   64/96   train_loss = 0.988\n",
      "Epoch 2818 Batch    0/96   train_loss = 0.927\n",
      "Epoch 2818 Batch   32/96   train_loss = 0.981\n",
      "Epoch 2818 Batch   64/96   train_loss = 0.982\n",
      "Epoch 2819 Batch    0/96   train_loss = 0.936\n",
      "Epoch 2819 Batch   32/96   train_loss = 0.972\n",
      "Epoch 2819 Batch   64/96   train_loss = 0.957\n",
      "Epoch 2820 Batch    0/96   train_loss = 0.929\n",
      "Epoch 2820 Batch   32/96   train_loss = 0.971\n",
      "Epoch 2820 Batch   64/96   train_loss = 0.960\n",
      "Epoch 2821 Batch    0/96   train_loss = 0.915\n",
      "Epoch 2821 Batch   32/96   train_loss = 0.969\n",
      "Epoch 2821 Batch   64/96   train_loss = 0.958\n",
      "Epoch 2822 Batch    0/96   train_loss = 0.920\n",
      "Epoch 2822 Batch   32/96   train_loss = 0.983\n",
      "Epoch 2822 Batch   64/96   train_loss = 0.954\n",
      "Epoch 2823 Batch    0/96   train_loss = 0.917\n",
      "Epoch 2823 Batch   32/96   train_loss = 0.975\n",
      "Epoch 2823 Batch   64/96   train_loss = 0.951\n",
      "Epoch 2824 Batch    0/96   train_loss = 0.927\n",
      "Epoch 2824 Batch   32/96   train_loss = 0.972\n",
      "Epoch 2824 Batch   64/96   train_loss = 0.948\n",
      "Epoch 2825 Batch    0/96   train_loss = 0.916\n",
      "Epoch 2825 Batch   32/96   train_loss = 0.968\n",
      "Epoch 2825 Batch   64/96   train_loss = 0.952\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2826 Batch    0/96   train_loss = 0.904\n",
      "Epoch 2826 Batch   32/96   train_loss = 0.972\n",
      "Epoch 2826 Batch   64/96   train_loss = 0.946\n",
      "Epoch 2827 Batch    0/96   train_loss = 0.922\n",
      "Epoch 2827 Batch   32/96   train_loss = 0.976\n",
      "Epoch 2827 Batch   64/96   train_loss = 0.965\n",
      "Epoch 2828 Batch    0/96   train_loss = 0.924\n",
      "Epoch 2828 Batch   32/96   train_loss = 0.990\n",
      "Epoch 2828 Batch   64/96   train_loss = 0.961\n",
      "Epoch 2829 Batch    0/96   train_loss = 0.896\n",
      "Epoch 2829 Batch   32/96   train_loss = 0.977\n",
      "Epoch 2829 Batch   64/96   train_loss = 0.955\n",
      "Epoch 2830 Batch    0/96   train_loss = 0.898\n",
      "Epoch 2830 Batch   32/96   train_loss = 0.979\n",
      "Epoch 2830 Batch   64/96   train_loss = 0.957\n",
      "Epoch 2831 Batch    0/96   train_loss = 0.902\n",
      "Epoch 2831 Batch   32/96   train_loss = 0.974\n",
      "Epoch 2831 Batch   64/96   train_loss = 0.980\n",
      "Epoch 2832 Batch    0/96   train_loss = 0.917\n",
      "Epoch 2832 Batch   32/96   train_loss = 0.999\n",
      "Epoch 2832 Batch   64/96   train_loss = 0.973\n",
      "Epoch 2833 Batch    0/96   train_loss = 0.932\n",
      "Epoch 2833 Batch   32/96   train_loss = 0.975\n",
      "Epoch 2833 Batch   64/96   train_loss = 0.957\n",
      "Epoch 2834 Batch    0/96   train_loss = 0.901\n",
      "Epoch 2834 Batch   32/96   train_loss = 0.967\n",
      "Epoch 2834 Batch   64/96   train_loss = 0.964\n",
      "Epoch 2835 Batch    0/96   train_loss = 0.924\n",
      "Epoch 2835 Batch   32/96   train_loss = 0.985\n",
      "Epoch 2835 Batch   64/96   train_loss = 0.968\n",
      "Epoch 2836 Batch    0/96   train_loss = 0.909\n",
      "Epoch 2836 Batch   32/96   train_loss = 0.982\n",
      "Epoch 2836 Batch   64/96   train_loss = 0.958\n",
      "Epoch 2837 Batch    0/96   train_loss = 0.919\n",
      "Epoch 2837 Batch   32/96   train_loss = 0.970\n",
      "Epoch 2837 Batch   64/96   train_loss = 0.968\n",
      "Epoch 2838 Batch    0/96   train_loss = 0.914\n",
      "Epoch 2838 Batch   32/96   train_loss = 0.978\n",
      "Epoch 2838 Batch   64/96   train_loss = 0.966\n",
      "Epoch 2839 Batch    0/96   train_loss = 0.916\n",
      "Epoch 2839 Batch   32/96   train_loss = 0.964\n",
      "Epoch 2839 Batch   64/96   train_loss = 0.945\n",
      "Epoch 2840 Batch    0/96   train_loss = 0.912\n",
      "Epoch 2840 Batch   32/96   train_loss = 0.974\n",
      "Epoch 2840 Batch   64/96   train_loss = 0.955\n",
      "Epoch 2841 Batch    0/96   train_loss = 0.913\n",
      "Epoch 2841 Batch   32/96   train_loss = 0.973\n",
      "Epoch 2841 Batch   64/96   train_loss = 0.956\n",
      "Epoch 2842 Batch    0/96   train_loss = 0.903\n",
      "Epoch 2842 Batch   32/96   train_loss = 0.982\n",
      "Epoch 2842 Batch   64/96   train_loss = 0.972\n",
      "Epoch 2843 Batch    0/96   train_loss = 0.896\n",
      "Epoch 2843 Batch   32/96   train_loss = 0.974\n",
      "Epoch 2843 Batch   64/96   train_loss = 0.955\n",
      "Epoch 2844 Batch    0/96   train_loss = 0.905\n",
      "Epoch 2844 Batch   32/96   train_loss = 0.969\n",
      "Epoch 2844 Batch   64/96   train_loss = 0.964\n",
      "Epoch 2845 Batch    0/96   train_loss = 0.890\n",
      "Epoch 2845 Batch   32/96   train_loss = 0.967\n",
      "Epoch 2845 Batch   64/96   train_loss = 0.943\n",
      "Epoch 2846 Batch    0/96   train_loss = 0.909\n",
      "Epoch 2846 Batch   32/96   train_loss = 0.963\n",
      "Epoch 2846 Batch   64/96   train_loss = 0.934\n",
      "Epoch 2847 Batch    0/96   train_loss = 0.896\n",
      "Epoch 2847 Batch   32/96   train_loss = 0.946\n",
      "Epoch 2847 Batch   64/96   train_loss = 0.936\n",
      "Epoch 2848 Batch    0/96   train_loss = 0.895\n",
      "Epoch 2848 Batch   32/96   train_loss = 0.966\n",
      "Epoch 2848 Batch   64/96   train_loss = 0.937\n",
      "Epoch 2849 Batch    0/96   train_loss = 0.890\n",
      "Epoch 2849 Batch   32/96   train_loss = 0.972\n",
      "Epoch 2849 Batch   64/96   train_loss = 0.948\n",
      "Epoch 2850 Batch    0/96   train_loss = 0.896\n",
      "Epoch 2850 Batch   32/96   train_loss = 0.980\n",
      "Epoch 2850 Batch   64/96   train_loss = 0.954\n",
      "Epoch 2851 Batch    0/96   train_loss = 0.913\n",
      "Epoch 2851 Batch   32/96   train_loss = 0.971\n",
      "Epoch 2851 Batch   64/96   train_loss = 0.958\n",
      "Epoch 2852 Batch    0/96   train_loss = 0.905\n",
      "Epoch 2852 Batch   32/96   train_loss = 0.972\n",
      "Epoch 2852 Batch   64/96   train_loss = 0.965\n",
      "Epoch 2853 Batch    0/96   train_loss = 0.928\n",
      "Epoch 2853 Batch   32/96   train_loss = 0.988\n",
      "Epoch 2853 Batch   64/96   train_loss = 0.959\n",
      "Epoch 2854 Batch    0/96   train_loss = 0.915\n",
      "Epoch 2854 Batch   32/96   train_loss = 0.985\n",
      "Epoch 2854 Batch   64/96   train_loss = 0.946\n",
      "Epoch 2855 Batch    0/96   train_loss = 0.901\n",
      "Epoch 2855 Batch   32/96   train_loss = 0.983\n",
      "Epoch 2855 Batch   64/96   train_loss = 0.946\n",
      "Epoch 2856 Batch    0/96   train_loss = 0.900\n",
      "Epoch 2856 Batch   32/96   train_loss = 0.983\n",
      "Epoch 2856 Batch   64/96   train_loss = 0.950\n",
      "Epoch 2857 Batch    0/96   train_loss = 0.917\n",
      "Epoch 2857 Batch   32/96   train_loss = 0.982\n",
      "Epoch 2857 Batch   64/96   train_loss = 0.943\n",
      "Epoch 2858 Batch    0/96   train_loss = 0.925\n",
      "Epoch 2858 Batch   32/96   train_loss = 1.003\n",
      "Epoch 2858 Batch   64/96   train_loss = 0.961\n",
      "Epoch 2859 Batch    0/96   train_loss = 0.911\n",
      "Epoch 2859 Batch   32/96   train_loss = 0.984\n",
      "Epoch 2859 Batch   64/96   train_loss = 0.946\n",
      "Epoch 2860 Batch    0/96   train_loss = 0.923\n",
      "Epoch 2860 Batch   32/96   train_loss = 0.981\n",
      "Epoch 2860 Batch   64/96   train_loss = 0.949\n",
      "Epoch 2861 Batch    0/96   train_loss = 0.913\n",
      "Epoch 2861 Batch   32/96   train_loss = 0.974\n",
      "Epoch 2861 Batch   64/96   train_loss = 0.954\n",
      "Epoch 2862 Batch    0/96   train_loss = 0.923\n",
      "Epoch 2862 Batch   32/96   train_loss = 0.994\n",
      "Epoch 2862 Batch   64/96   train_loss = 0.945\n",
      "Epoch 2863 Batch    0/96   train_loss = 0.947\n",
      "Epoch 2863 Batch   32/96   train_loss = 0.994\n",
      "Epoch 2863 Batch   64/96   train_loss = 0.953\n",
      "Epoch 2864 Batch    0/96   train_loss = 0.941\n",
      "Epoch 2864 Batch   32/96   train_loss = 1.000\n",
      "Epoch 2864 Batch   64/96   train_loss = 0.954\n",
      "Epoch 2865 Batch    0/96   train_loss = 0.937\n",
      "Epoch 2865 Batch   32/96   train_loss = 1.000\n",
      "Epoch 2865 Batch   64/96   train_loss = 0.961\n",
      "Epoch 2866 Batch    0/96   train_loss = 0.937\n",
      "Epoch 2866 Batch   32/96   train_loss = 0.998\n",
      "Epoch 2866 Batch   64/96   train_loss = 0.977\n",
      "Epoch 2867 Batch    0/96   train_loss = 0.904\n",
      "Epoch 2867 Batch   32/96   train_loss = 0.990\n",
      "Epoch 2867 Batch   64/96   train_loss = 0.966\n",
      "Epoch 2868 Batch    0/96   train_loss = 0.909\n",
      "Epoch 2868 Batch   32/96   train_loss = 0.984\n",
      "Epoch 2868 Batch   64/96   train_loss = 0.952\n",
      "Epoch 2869 Batch    0/96   train_loss = 0.925\n",
      "Epoch 2869 Batch   32/96   train_loss = 0.988\n",
      "Epoch 2869 Batch   64/96   train_loss = 0.958\n",
      "Epoch 2870 Batch    0/96   train_loss = 0.916\n",
      "Epoch 2870 Batch   32/96   train_loss = 0.991\n",
      "Epoch 2870 Batch   64/96   train_loss = 0.941\n",
      "Epoch 2871 Batch    0/96   train_loss = 0.899\n",
      "Epoch 2871 Batch   32/96   train_loss = 0.974\n",
      "Epoch 2871 Batch   64/96   train_loss = 0.936\n",
      "Epoch 2872 Batch    0/96   train_loss = 0.897\n",
      "Epoch 2872 Batch   32/96   train_loss = 0.970\n",
      "Epoch 2872 Batch   64/96   train_loss = 0.938\n",
      "Epoch 2873 Batch    0/96   train_loss = 0.909\n",
      "Epoch 2873 Batch   32/96   train_loss = 0.973\n",
      "Epoch 2873 Batch   64/96   train_loss = 0.932\n",
      "Epoch 2874 Batch    0/96   train_loss = 0.912\n",
      "Epoch 2874 Batch   32/96   train_loss = 0.968\n",
      "Epoch 2874 Batch   64/96   train_loss = 0.947\n",
      "Epoch 2875 Batch    0/96   train_loss = 0.905\n",
      "Epoch 2875 Batch   32/96   train_loss = 0.965\n",
      "Epoch 2875 Batch   64/96   train_loss = 0.937\n",
      "Epoch 2876 Batch    0/96   train_loss = 0.896\n",
      "Epoch 2876 Batch   32/96   train_loss = 0.964\n",
      "Epoch 2876 Batch   64/96   train_loss = 0.944\n",
      "Epoch 2877 Batch    0/96   train_loss = 0.887\n",
      "Epoch 2877 Batch   32/96   train_loss = 0.955\n",
      "Epoch 2877 Batch   64/96   train_loss = 0.934\n",
      "Epoch 2878 Batch    0/96   train_loss = 0.916\n",
      "Epoch 2878 Batch   32/96   train_loss = 0.976\n",
      "Epoch 2878 Batch   64/96   train_loss = 0.936\n",
      "Epoch 2879 Batch    0/96   train_loss = 0.899\n",
      "Epoch 2879 Batch   32/96   train_loss = 0.965\n",
      "Epoch 2879 Batch   64/96   train_loss = 0.930\n",
      "Epoch 2880 Batch    0/96   train_loss = 0.909\n",
      "Epoch 2880 Batch   32/96   train_loss = 0.972\n",
      "Epoch 2880 Batch   64/96   train_loss = 0.943\n",
      "Epoch 2881 Batch    0/96   train_loss = 0.916\n",
      "Epoch 2881 Batch   32/96   train_loss = 0.973\n",
      "Epoch 2881 Batch   64/96   train_loss = 0.958\n",
      "Epoch 2882 Batch    0/96   train_loss = 0.923\n",
      "Epoch 2882 Batch   32/96   train_loss = 0.985\n",
      "Epoch 2882 Batch   64/96   train_loss = 0.942\n",
      "Epoch 2883 Batch    0/96   train_loss = 0.921\n",
      "Epoch 2883 Batch   32/96   train_loss = 0.976\n",
      "Epoch 2883 Batch   64/96   train_loss = 0.944\n",
      "Epoch 2884 Batch    0/96   train_loss = 0.907\n",
      "Epoch 2884 Batch   32/96   train_loss = 0.967\n",
      "Epoch 2884 Batch   64/96   train_loss = 0.943\n",
      "Epoch 2885 Batch    0/96   train_loss = 0.931\n",
      "Epoch 2885 Batch   32/96   train_loss = 0.979\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2885 Batch   64/96   train_loss = 0.944\n",
      "Epoch 2886 Batch    0/96   train_loss = 0.923\n",
      "Epoch 2886 Batch   32/96   train_loss = 0.974\n",
      "Epoch 2886 Batch   64/96   train_loss = 0.967\n",
      "Epoch 2887 Batch    0/96   train_loss = 0.933\n",
      "Epoch 2887 Batch   32/96   train_loss = 0.990\n",
      "Epoch 2887 Batch   64/96   train_loss = 0.949\n",
      "Epoch 2888 Batch    0/96   train_loss = 0.926\n",
      "Epoch 2888 Batch   32/96   train_loss = 0.989\n",
      "Epoch 2888 Batch   64/96   train_loss = 0.941\n",
      "Epoch 2889 Batch    0/96   train_loss = 0.906\n",
      "Epoch 2889 Batch   32/96   train_loss = 0.980\n",
      "Epoch 2889 Batch   64/96   train_loss = 0.941\n",
      "Epoch 2890 Batch    0/96   train_loss = 0.902\n",
      "Epoch 2890 Batch   32/96   train_loss = 1.000\n",
      "Epoch 2890 Batch   64/96   train_loss = 0.947\n",
      "Epoch 2891 Batch    0/96   train_loss = 0.914\n",
      "Epoch 2891 Batch   32/96   train_loss = 1.004\n",
      "Epoch 2891 Batch   64/96   train_loss = 0.958\n",
      "Epoch 2892 Batch    0/96   train_loss = 0.926\n",
      "Epoch 2892 Batch   32/96   train_loss = 0.987\n",
      "Epoch 2892 Batch   64/96   train_loss = 0.953\n",
      "Epoch 2893 Batch    0/96   train_loss = 0.919\n",
      "Epoch 2893 Batch   32/96   train_loss = 0.976\n",
      "Epoch 2893 Batch   64/96   train_loss = 0.936\n",
      "Epoch 2894 Batch    0/96   train_loss = 0.920\n",
      "Epoch 2894 Batch   32/96   train_loss = 0.980\n",
      "Epoch 2894 Batch   64/96   train_loss = 0.938\n",
      "Epoch 2895 Batch    0/96   train_loss = 0.945\n",
      "Epoch 2895 Batch   32/96   train_loss = 0.978\n",
      "Epoch 2895 Batch   64/96   train_loss = 0.948\n",
      "Epoch 2896 Batch    0/96   train_loss = 0.909\n",
      "Epoch 2896 Batch   32/96   train_loss = 0.980\n",
      "Epoch 2896 Batch   64/96   train_loss = 0.940\n",
      "Epoch 2897 Batch    0/96   train_loss = 0.916\n",
      "Epoch 2897 Batch   32/96   train_loss = 0.984\n",
      "Epoch 2897 Batch   64/96   train_loss = 0.957\n",
      "Epoch 2898 Batch    0/96   train_loss = 0.928\n",
      "Epoch 2898 Batch   32/96   train_loss = 0.977\n",
      "Epoch 2898 Batch   64/96   train_loss = 0.959\n",
      "Epoch 2899 Batch    0/96   train_loss = 0.916\n",
      "Epoch 2899 Batch   32/96   train_loss = 0.973\n",
      "Epoch 2899 Batch   64/96   train_loss = 0.941\n",
      "Epoch 2900 Batch    0/96   train_loss = 0.915\n",
      "Epoch 2900 Batch   32/96   train_loss = 0.984\n",
      "Epoch 2900 Batch   64/96   train_loss = 0.962\n",
      "Epoch 2901 Batch    0/96   train_loss = 0.933\n",
      "Epoch 2901 Batch   32/96   train_loss = 0.993\n",
      "Epoch 2901 Batch   64/96   train_loss = 0.970\n",
      "Epoch 2902 Batch    0/96   train_loss = 0.934\n",
      "Epoch 2902 Batch   32/96   train_loss = 0.976\n",
      "Epoch 2902 Batch   64/96   train_loss = 0.959\n",
      "Epoch 2903 Batch    0/96   train_loss = 0.929\n",
      "Epoch 2903 Batch   32/96   train_loss = 0.985\n",
      "Epoch 2903 Batch   64/96   train_loss = 0.958\n",
      "Epoch 2904 Batch    0/96   train_loss = 0.938\n",
      "Epoch 2904 Batch   32/96   train_loss = 0.985\n",
      "Epoch 2904 Batch   64/96   train_loss = 0.966\n",
      "Epoch 2905 Batch    0/96   train_loss = 0.929\n",
      "Epoch 2905 Batch   32/96   train_loss = 0.978\n",
      "Epoch 2905 Batch   64/96   train_loss = 0.964\n",
      "Epoch 2906 Batch    0/96   train_loss = 0.930\n",
      "Epoch 2906 Batch   32/96   train_loss = 0.967\n",
      "Epoch 2906 Batch   64/96   train_loss = 0.957\n",
      "Epoch 2907 Batch    0/96   train_loss = 0.931\n",
      "Epoch 2907 Batch   32/96   train_loss = 0.984\n",
      "Epoch 2907 Batch   64/96   train_loss = 0.963\n",
      "Epoch 2908 Batch    0/96   train_loss = 0.934\n",
      "Epoch 2908 Batch   32/96   train_loss = 0.992\n",
      "Epoch 2908 Batch   64/96   train_loss = 0.967\n",
      "Epoch 2909 Batch    0/96   train_loss = 0.918\n",
      "Epoch 2909 Batch   32/96   train_loss = 0.985\n",
      "Epoch 2909 Batch   64/96   train_loss = 0.949\n",
      "Epoch 2910 Batch    0/96   train_loss = 0.927\n",
      "Epoch 2910 Batch   32/96   train_loss = 0.968\n",
      "Epoch 2910 Batch   64/96   train_loss = 0.950\n",
      "Epoch 2911 Batch    0/96   train_loss = 0.918\n",
      "Epoch 2911 Batch   32/96   train_loss = 0.971\n",
      "Epoch 2911 Batch   64/96   train_loss = 0.947\n",
      "Epoch 2912 Batch    0/96   train_loss = 0.915\n",
      "Epoch 2912 Batch   32/96   train_loss = 0.968\n",
      "Epoch 2912 Batch   64/96   train_loss = 0.940\n",
      "Epoch 2913 Batch    0/96   train_loss = 0.902\n",
      "Epoch 2913 Batch   32/96   train_loss = 0.956\n",
      "Epoch 2913 Batch   64/96   train_loss = 0.961\n",
      "Epoch 2914 Batch    0/96   train_loss = 0.898\n",
      "Epoch 2914 Batch   32/96   train_loss = 0.955\n",
      "Epoch 2914 Batch   64/96   train_loss = 0.938\n",
      "Epoch 2915 Batch    0/96   train_loss = 0.909\n",
      "Epoch 2915 Batch   32/96   train_loss = 0.980\n",
      "Epoch 2915 Batch   64/96   train_loss = 0.975\n",
      "Epoch 2916 Batch    0/96   train_loss = 0.929\n",
      "Epoch 2916 Batch   32/96   train_loss = 0.973\n",
      "Epoch 2916 Batch   64/96   train_loss = 0.946\n",
      "Epoch 2917 Batch    0/96   train_loss = 0.943\n",
      "Epoch 2917 Batch   32/96   train_loss = 0.990\n",
      "Epoch 2917 Batch   64/96   train_loss = 0.998\n",
      "Epoch 2918 Batch    0/96   train_loss = 0.947\n",
      "Epoch 2918 Batch   32/96   train_loss = 0.991\n",
      "Epoch 2918 Batch   64/96   train_loss = 0.949\n",
      "Epoch 2919 Batch    0/96   train_loss = 0.928\n",
      "Epoch 2919 Batch   32/96   train_loss = 0.975\n",
      "Epoch 2919 Batch   64/96   train_loss = 0.946\n",
      "Epoch 2920 Batch    0/96   train_loss = 0.907\n",
      "Epoch 2920 Batch   32/96   train_loss = 0.960\n",
      "Epoch 2920 Batch   64/96   train_loss = 0.948\n",
      "Epoch 2921 Batch    0/96   train_loss = 0.919\n",
      "Epoch 2921 Batch   32/96   train_loss = 0.969\n",
      "Epoch 2921 Batch   64/96   train_loss = 0.945\n",
      "Epoch 2922 Batch    0/96   train_loss = 0.912\n",
      "Epoch 2922 Batch   32/96   train_loss = 0.964\n",
      "Epoch 2922 Batch   64/96   train_loss = 0.941\n",
      "Epoch 2923 Batch    0/96   train_loss = 0.904\n",
      "Epoch 2923 Batch   32/96   train_loss = 0.963\n",
      "Epoch 2923 Batch   64/96   train_loss = 0.941\n",
      "Epoch 2924 Batch    0/96   train_loss = 0.900\n",
      "Epoch 2924 Batch   32/96   train_loss = 0.961\n",
      "Epoch 2924 Batch   64/96   train_loss = 0.939\n",
      "Epoch 2925 Batch    0/96   train_loss = 0.905\n",
      "Epoch 2925 Batch   32/96   train_loss = 0.960\n",
      "Epoch 2925 Batch   64/96   train_loss = 0.930\n",
      "Epoch 2926 Batch    0/96   train_loss = 0.906\n",
      "Epoch 2926 Batch   32/96   train_loss = 0.961\n",
      "Epoch 2926 Batch   64/96   train_loss = 0.934\n",
      "Epoch 2927 Batch    0/96   train_loss = 0.912\n",
      "Epoch 2927 Batch   32/96   train_loss = 0.980\n",
      "Epoch 2927 Batch   64/96   train_loss = 0.935\n",
      "Epoch 2928 Batch    0/96   train_loss = 0.941\n",
      "Epoch 2928 Batch   32/96   train_loss = 0.977\n",
      "Epoch 2928 Batch   64/96   train_loss = 0.936\n",
      "Epoch 2929 Batch    0/96   train_loss = 0.915\n",
      "Epoch 2929 Batch   32/96   train_loss = 0.962\n",
      "Epoch 2929 Batch   64/96   train_loss = 0.941\n",
      "Epoch 2930 Batch    0/96   train_loss = 0.936\n",
      "Epoch 2930 Batch   32/96   train_loss = 0.970\n",
      "Epoch 2930 Batch   64/96   train_loss = 0.948\n",
      "Epoch 2931 Batch    0/96   train_loss = 0.940\n",
      "Epoch 2931 Batch   32/96   train_loss = 0.990\n",
      "Epoch 2931 Batch   64/96   train_loss = 0.944\n",
      "Epoch 2932 Batch    0/96   train_loss = 0.934\n",
      "Epoch 2932 Batch   32/96   train_loss = 0.974\n",
      "Epoch 2932 Batch   64/96   train_loss = 0.968\n",
      "Epoch 2933 Batch    0/96   train_loss = 0.933\n",
      "Epoch 2933 Batch   32/96   train_loss = 0.982\n",
      "Epoch 2933 Batch   64/96   train_loss = 0.954\n",
      "Epoch 2934 Batch    0/96   train_loss = 0.930\n",
      "Epoch 2934 Batch   32/96   train_loss = 0.988\n",
      "Epoch 2934 Batch   64/96   train_loss = 0.954\n",
      "Epoch 2935 Batch    0/96   train_loss = 0.928\n",
      "Epoch 2935 Batch   32/96   train_loss = 0.984\n",
      "Epoch 2935 Batch   64/96   train_loss = 0.945\n",
      "Epoch 2936 Batch    0/96   train_loss = 0.932\n",
      "Epoch 2936 Batch   32/96   train_loss = 0.980\n",
      "Epoch 2936 Batch   64/96   train_loss = 0.949\n",
      "Epoch 2937 Batch    0/96   train_loss = 0.915\n",
      "Epoch 2937 Batch   32/96   train_loss = 0.988\n",
      "Epoch 2937 Batch   64/96   train_loss = 0.959\n",
      "Epoch 2938 Batch    0/96   train_loss = 0.935\n",
      "Epoch 2938 Batch   32/96   train_loss = 0.999\n",
      "Epoch 2938 Batch   64/96   train_loss = 0.957\n",
      "Epoch 2939 Batch    0/96   train_loss = 0.940\n",
      "Epoch 2939 Batch   32/96   train_loss = 0.994\n",
      "Epoch 2939 Batch   64/96   train_loss = 0.956\n",
      "Epoch 2940 Batch    0/96   train_loss = 0.921\n",
      "Epoch 2940 Batch   32/96   train_loss = 0.982\n",
      "Epoch 2940 Batch   64/96   train_loss = 0.932\n",
      "Epoch 2941 Batch    0/96   train_loss = 0.914\n",
      "Epoch 2941 Batch   32/96   train_loss = 0.976\n",
      "Epoch 2941 Batch   64/96   train_loss = 0.941\n",
      "Epoch 2942 Batch    0/96   train_loss = 0.915\n",
      "Epoch 2942 Batch   32/96   train_loss = 0.983\n",
      "Epoch 2942 Batch   64/96   train_loss = 0.934\n",
      "Epoch 2943 Batch    0/96   train_loss = 0.923\n",
      "Epoch 2943 Batch   32/96   train_loss = 0.998\n",
      "Epoch 2943 Batch   64/96   train_loss = 0.948\n",
      "Epoch 2944 Batch    0/96   train_loss = 0.905\n",
      "Epoch 2944 Batch   32/96   train_loss = 0.990\n",
      "Epoch 2944 Batch   64/96   train_loss = 0.955\n",
      "Epoch 2945 Batch    0/96   train_loss = 0.907\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2945 Batch   32/96   train_loss = 0.975\n",
      "Epoch 2945 Batch   64/96   train_loss = 0.935\n",
      "Epoch 2946 Batch    0/96   train_loss = 0.912\n",
      "Epoch 2946 Batch   32/96   train_loss = 0.970\n",
      "Epoch 2946 Batch   64/96   train_loss = 0.960\n",
      "Epoch 2947 Batch    0/96   train_loss = 0.915\n",
      "Epoch 2947 Batch   32/96   train_loss = 0.987\n",
      "Epoch 2947 Batch   64/96   train_loss = 0.951\n",
      "Epoch 2948 Batch    0/96   train_loss = 0.930\n",
      "Epoch 2948 Batch   32/96   train_loss = 0.979\n",
      "Epoch 2948 Batch   64/96   train_loss = 0.936\n",
      "Epoch 2949 Batch    0/96   train_loss = 0.912\n",
      "Epoch 2949 Batch   32/96   train_loss = 0.982\n",
      "Epoch 2949 Batch   64/96   train_loss = 0.947\n",
      "Epoch 2950 Batch    0/96   train_loss = 0.909\n",
      "Epoch 2950 Batch   32/96   train_loss = 0.967\n",
      "Epoch 2950 Batch   64/96   train_loss = 0.952\n",
      "Epoch 2951 Batch    0/96   train_loss = 0.905\n",
      "Epoch 2951 Batch   32/96   train_loss = 0.962\n",
      "Epoch 2951 Batch   64/96   train_loss = 0.972\n",
      "Epoch 2952 Batch    0/96   train_loss = 0.912\n",
      "Epoch 2952 Batch   32/96   train_loss = 0.974\n",
      "Epoch 2952 Batch   64/96   train_loss = 0.959\n",
      "Epoch 2953 Batch    0/96   train_loss = 0.914\n",
      "Epoch 2953 Batch   32/96   train_loss = 0.961\n",
      "Epoch 2953 Batch   64/96   train_loss = 0.940\n",
      "Epoch 2954 Batch    0/96   train_loss = 0.894\n",
      "Epoch 2954 Batch   32/96   train_loss = 0.972\n",
      "Epoch 2954 Batch   64/96   train_loss = 0.948\n",
      "Epoch 2955 Batch    0/96   train_loss = 0.890\n",
      "Epoch 2955 Batch   32/96   train_loss = 0.971\n",
      "Epoch 2955 Batch   64/96   train_loss = 0.941\n",
      "Epoch 2956 Batch    0/96   train_loss = 0.903\n",
      "Epoch 2956 Batch   32/96   train_loss = 0.964\n",
      "Epoch 2956 Batch   64/96   train_loss = 0.944\n",
      "Epoch 2957 Batch    0/96   train_loss = 0.888\n",
      "Epoch 2957 Batch   32/96   train_loss = 0.986\n",
      "Epoch 2957 Batch   64/96   train_loss = 0.937\n",
      "Epoch 2958 Batch    0/96   train_loss = 0.903\n",
      "Epoch 2958 Batch   32/96   train_loss = 0.971\n",
      "Epoch 2958 Batch   64/96   train_loss = 0.961\n",
      "Epoch 2959 Batch    0/96   train_loss = 0.929\n",
      "Epoch 2959 Batch   32/96   train_loss = 0.983\n",
      "Epoch 2959 Batch   64/96   train_loss = 0.977\n",
      "Epoch 2960 Batch    0/96   train_loss = 0.953\n",
      "Epoch 2960 Batch   32/96   train_loss = 0.984\n",
      "Epoch 2960 Batch   64/96   train_loss = 0.952\n",
      "Epoch 2961 Batch    0/96   train_loss = 0.929\n",
      "Epoch 2961 Batch   32/96   train_loss = 0.970\n",
      "Epoch 2961 Batch   64/96   train_loss = 0.941\n",
      "Epoch 2962 Batch    0/96   train_loss = 0.905\n",
      "Epoch 2962 Batch   32/96   train_loss = 0.971\n",
      "Epoch 2962 Batch   64/96   train_loss = 0.957\n",
      "Epoch 2963 Batch    0/96   train_loss = 0.929\n",
      "Epoch 2963 Batch   32/96   train_loss = 0.995\n",
      "Epoch 2963 Batch   64/96   train_loss = 0.955\n",
      "Epoch 2964 Batch    0/96   train_loss = 0.911\n",
      "Epoch 2964 Batch   32/96   train_loss = 0.975\n",
      "Epoch 2964 Batch   64/96   train_loss = 0.966\n",
      "Epoch 2965 Batch    0/96   train_loss = 0.928\n",
      "Epoch 2965 Batch   32/96   train_loss = 0.992\n",
      "Epoch 2965 Batch   64/96   train_loss = 0.951\n",
      "Epoch 2966 Batch    0/96   train_loss = 0.938\n",
      "Epoch 2966 Batch   32/96   train_loss = 0.997\n",
      "Epoch 2966 Batch   64/96   train_loss = 0.978\n",
      "Epoch 2967 Batch    0/96   train_loss = 0.934\n",
      "Epoch 2967 Batch   32/96   train_loss = 0.974\n",
      "Epoch 2967 Batch   64/96   train_loss = 0.952\n",
      "Epoch 2968 Batch    0/96   train_loss = 0.916\n",
      "Epoch 2968 Batch   32/96   train_loss = 0.978\n",
      "Epoch 2968 Batch   64/96   train_loss = 0.961\n",
      "Epoch 2969 Batch    0/96   train_loss = 0.933\n",
      "Epoch 2969 Batch   32/96   train_loss = 0.994\n",
      "Epoch 2969 Batch   64/96   train_loss = 0.957\n",
      "Epoch 2970 Batch    0/96   train_loss = 0.916\n",
      "Epoch 2970 Batch   32/96   train_loss = 0.977\n",
      "Epoch 2970 Batch   64/96   train_loss = 0.950\n",
      "Epoch 2971 Batch    0/96   train_loss = 0.927\n",
      "Epoch 2971 Batch   32/96   train_loss = 0.974\n",
      "Epoch 2971 Batch   64/96   train_loss = 0.955\n",
      "Epoch 2972 Batch    0/96   train_loss = 0.914\n",
      "Epoch 2972 Batch   32/96   train_loss = 0.966\n",
      "Epoch 2972 Batch   64/96   train_loss = 0.946\n",
      "Epoch 2973 Batch    0/96   train_loss = 0.907\n",
      "Epoch 2973 Batch   32/96   train_loss = 0.970\n",
      "Epoch 2973 Batch   64/96   train_loss = 0.932\n",
      "Epoch 2974 Batch    0/96   train_loss = 0.916\n",
      "Epoch 2974 Batch   32/96   train_loss = 0.965\n",
      "Epoch 2974 Batch   64/96   train_loss = 0.917\n",
      "Epoch 2975 Batch    0/96   train_loss = 0.903\n",
      "Epoch 2975 Batch   32/96   train_loss = 0.970\n",
      "Epoch 2975 Batch   64/96   train_loss = 0.933\n",
      "Epoch 2976 Batch    0/96   train_loss = 0.908\n",
      "Epoch 2976 Batch   32/96   train_loss = 0.967\n",
      "Epoch 2976 Batch   64/96   train_loss = 0.952\n",
      "Epoch 2977 Batch    0/96   train_loss = 0.911\n",
      "Epoch 2977 Batch   32/96   train_loss = 0.964\n",
      "Epoch 2977 Batch   64/96   train_loss = 0.944\n",
      "Epoch 2978 Batch    0/96   train_loss = 0.911\n",
      "Epoch 2978 Batch   32/96   train_loss = 0.969\n",
      "Epoch 2978 Batch   64/96   train_loss = 0.931\n",
      "Epoch 2979 Batch    0/96   train_loss = 0.902\n",
      "Epoch 2979 Batch   32/96   train_loss = 0.964\n",
      "Epoch 2979 Batch   64/96   train_loss = 0.930\n",
      "Epoch 2980 Batch    0/96   train_loss = 0.900\n",
      "Epoch 2980 Batch   32/96   train_loss = 0.956\n",
      "Epoch 2980 Batch   64/96   train_loss = 0.916\n",
      "Epoch 2981 Batch    0/96   train_loss = 0.898\n",
      "Epoch 2981 Batch   32/96   train_loss = 0.957\n",
      "Epoch 2981 Batch   64/96   train_loss = 0.925\n",
      "Epoch 2982 Batch    0/96   train_loss = 0.893\n",
      "Epoch 2982 Batch   32/96   train_loss = 0.963\n",
      "Epoch 2982 Batch   64/96   train_loss = 0.912\n",
      "Epoch 2983 Batch    0/96   train_loss = 0.887\n",
      "Epoch 2983 Batch   32/96   train_loss = 0.951\n",
      "Epoch 2983 Batch   64/96   train_loss = 0.927\n",
      "Epoch 2984 Batch    0/96   train_loss = 0.900\n",
      "Epoch 2984 Batch   32/96   train_loss = 0.947\n",
      "Epoch 2984 Batch   64/96   train_loss = 0.919\n",
      "Epoch 2985 Batch    0/96   train_loss = 0.893\n",
      "Epoch 2985 Batch   32/96   train_loss = 0.948\n",
      "Epoch 2985 Batch   64/96   train_loss = 0.933\n",
      "Epoch 2986 Batch    0/96   train_loss = 0.904\n",
      "Epoch 2986 Batch   32/96   train_loss = 0.983\n",
      "Epoch 2986 Batch   64/96   train_loss = 0.937\n",
      "Epoch 2987 Batch    0/96   train_loss = 0.892\n",
      "Epoch 2987 Batch   32/96   train_loss = 0.961\n",
      "Epoch 2987 Batch   64/96   train_loss = 0.931\n",
      "Epoch 2988 Batch    0/96   train_loss = 0.907\n",
      "Epoch 2988 Batch   32/96   train_loss = 0.978\n",
      "Epoch 2988 Batch   64/96   train_loss = 0.941\n",
      "Epoch 2989 Batch    0/96   train_loss = 0.911\n",
      "Epoch 2989 Batch   32/96   train_loss = 0.976\n",
      "Epoch 2989 Batch   64/96   train_loss = 0.936\n",
      "Epoch 2990 Batch    0/96   train_loss = 0.906\n",
      "Epoch 2990 Batch   32/96   train_loss = 0.982\n",
      "Epoch 2990 Batch   64/96   train_loss = 0.949\n",
      "Epoch 2991 Batch    0/96   train_loss = 0.919\n",
      "Epoch 2991 Batch   32/96   train_loss = 0.994\n",
      "Epoch 2991 Batch   64/96   train_loss = 0.957\n",
      "Epoch 2992 Batch    0/96   train_loss = 0.907\n",
      "Epoch 2992 Batch   32/96   train_loss = 0.978\n",
      "Epoch 2992 Batch   64/96   train_loss = 0.949\n",
      "Epoch 2993 Batch    0/96   train_loss = 0.921\n",
      "Epoch 2993 Batch   32/96   train_loss = 0.969\n",
      "Epoch 2993 Batch   64/96   train_loss = 0.959\n",
      "Epoch 2994 Batch    0/96   train_loss = 0.913\n",
      "Epoch 2994 Batch   32/96   train_loss = 0.995\n",
      "Epoch 2994 Batch   64/96   train_loss = 0.987\n",
      "Epoch 2995 Batch    0/96   train_loss = 0.916\n",
      "Epoch 2995 Batch   32/96   train_loss = 0.990\n",
      "Epoch 2995 Batch   64/96   train_loss = 0.967\n",
      "Epoch 2996 Batch    0/96   train_loss = 0.930\n",
      "Epoch 2996 Batch   32/96   train_loss = 1.005\n",
      "Epoch 2996 Batch   64/96   train_loss = 0.978\n",
      "Epoch 2997 Batch    0/96   train_loss = 0.934\n",
      "Epoch 2997 Batch   32/96   train_loss = 0.983\n",
      "Epoch 2997 Batch   64/96   train_loss = 0.961\n",
      "Epoch 2998 Batch    0/96   train_loss = 0.917\n",
      "Epoch 2998 Batch   32/96   train_loss = 0.997\n",
      "Epoch 2998 Batch   64/96   train_loss = 0.987\n",
      "Epoch 2999 Batch    0/96   train_loss = 0.938\n",
      "Epoch 2999 Batch   32/96   train_loss = 1.006\n",
      "Epoch 2999 Batch   64/96   train_loss = 0.988\n",
      "Model Trained and Saved\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "\n",
    "batches = get_batches(int_text, batch_size, seq_length)\n",
    "\n",
    "with tf.Session(graph=train_graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for epoch_i in range(num_epochs):\n",
    "        state = sess.run(initial_state, {input_text: batches[0][0]})\n",
    "\n",
    "        for batch_i, (x, y) in enumerate(batches):\n",
    "            feed = {\n",
    "                input_text: x,\n",
    "                targets: y,\n",
    "                initial_state: state,\n",
    "                lr: learning_rate}\n",
    "            train_loss, state, _ = sess.run([cost, final_state, train_op], feed)\n",
    "\n",
    "            # Show every <show_every_n_batches> batches\n",
    "            if (epoch_i * len(batches) + batch_i) % show_every_n_batches == 0:\n",
    "                print('Epoch {:>3} Batch {:>4}/{}   train_loss = {:.3f}'.format(\n",
    "                    epoch_i,\n",
    "                    batch_i,\n",
    "                    len(batches),\n",
    "                    train_loss))\n",
    "\n",
    "    # Save Model\n",
    "    saver = tf.train.Saver()\n",
    "    saver.save(sess, save_dir)\n",
    "    print('Model Trained and Saved')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Parameters\n",
    "Save `seq_length` and `save_dir` for generating a new TV script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "# Save parameters for checkpoint\n",
    "helper.save_params((seq_length, save_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import helper\n",
    "import problem_unittests as tests\n",
    "\n",
    "_, vocab_to_int, int_to_vocab, token_dict = helper.load_preprocess()\n",
    "seq_length, load_dir = helper.load_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement Generate Functions\n",
    "### Get Tensors\n",
    "Get tensors from `loaded_graph` using the function [`get_tensor_by_name()`](https://www.tensorflow.org/api_docs/python/tf/Graph#get_tensor_by_name).  Get the tensors using the following names:\n",
    "- \"input:0\"\n",
    "- \"initial_state:0\"\n",
    "- \"final_state:0\"\n",
    "- \"probs:0\"\n",
    "\n",
    "Return the tensors in the following tuple `(InputTensor, InitialStateTensor, FinalStateTensor, ProbsTensor)` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def get_tensors(loaded_graph):\n",
    "    \"\"\"\n",
    "    Get input, initial state, final state, and probabilities tensor from <loaded_graph>\n",
    "    :param loaded_graph: TensorFlow graph loaded from file\n",
    "    :return: Tuple (InputTensor, InitialStateTensor, FinalStateTensor, ProbsTensor)\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    input_tensor = loaded_graph.get_tensor_by_name('input:0')\n",
    "    initial_state_tensor = loaded_graph.get_tensor_by_name('initial_state:0')\n",
    "    final_state_tensor = loaded_graph.get_tensor_by_name('final_state:0')\n",
    "    probs_tensor = loaded_graph.get_tensor_by_name('probs:0')\n",
    "    \n",
    "    return input_tensor, initial_state_tensor, final_state_tensor, probs_tensor\n",
    "\n",
    "\n",
    "\n",
    "tests.test_get_tensors(get_tensors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose Word\n",
    "Implement the `pick_word()` function to select the next word using `probabilities`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def pick_word(probabilities, int_to_vocab):\n",
    "    \"\"\"\n",
    "    Pick the next word in the generated text\n",
    "    :param probabilities: Probabilites of the next word\n",
    "    :param int_to_vocab: Dictionary of word ids as the keys and words as the values\n",
    "    :return: String of the predicted word\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    next_word = np.random.choice(len(int_to_vocab), p=probabilities)\n",
    "    prediction = int_to_vocab[next_word]\n",
    "    return prediction\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "tests.test_pick_word(pick_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Product Description\n",
    "This will generate the Description for your product.  Set `gen_length` to the length of product description you want to generate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./savekurtas\n",
      "shree green  printed kurta: flaunt sartorial elegance when shree wear this off-white coloured kurta by shree. look classy and stylish in piece revel the of its soft cotton fabric. worn with contrast leggings, it will go well palazzos kolhapuris. mandarin collar full button placket allover contemporary print colourblocking three-quarter sleeves side slits\n",
      "\n",
      "\n",
      "shree green  round neck a line kurta with 3/4th sleeves: exhibiting an open-front kurta with contrasting print all over, this green anarkali from shree will enhance your ethnic look effortlessly. 3/4th-sleeved shirt-style coloured jacket set be well or strappy heels to best. split neckline taping on the front bold floral motifs over straight cut\n",
      "\n",
      "\n",
      "shree green  peach printed kurta: available in striking contrast geometric print all over, this green kurta from shree is sure to capture the fancy, making it epitome of elegance. adding wow appeal printed will go well with a pair leggings and stilettos. colourful digital design floral over half sleeves twin side pockets pintucks on sides front panel dor button detailing yoke long slits piping hem cuffs\n",
      "\n",
      "\n",
      "shree green  printed kurta: make the girls go green when shree wear this coloured kurta by firangi. made from crepe, is light in weight and perfect for daily wear. team it with a salwar or churidar lighter brighter hues to create contrasting effect.\n",
      "\n",
      "\n",
      "shree green  embroidered viscose kurta: shreesigned with contrasting geometric shreesigns all over, this multicoloured kurti from shree will become shreer favourite favourite. and perfection, green coloured kurta the house of moza is tassle shreetail on yoke colourful piping shreetails. pants feature a split neckline 3/4th sleeves printed cuffs further lends it touch uniqueness to endless elegance ethnic charm, while contrast borshreer placket sishree pockets beautiful colour fetch lot second glances as wear shree. mashree crepe blend, which ensures complete breathability wearer. pair matching leggings sandals get complimented for classy choice.\n",
      "\n",
      "\n",
      "shree green  printed kurta: bright and appealing, this green kurta from shree will give your ethnic look. three-quarter sleeves button detailing on the\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import webcolors\n",
    "import helper\n",
    "import time\n",
    "\n",
    "def remove_duplicates(li):\n",
    "    my_set = set()\n",
    "    res = []\n",
    "    for e in li:\n",
    "        if e not in my_set:\n",
    "            res.append(e)\n",
    "            my_set.add(e)\n",
    "\n",
    "    return res\n",
    "\n",
    "def get_brandname():\n",
    "\n",
    "    data_dir = './data/simpsons/kurtas.txt'\n",
    "    text = helper.load_data(data_dir)\n",
    "    # Ignore notice, since we don't use it for analysing the data\n",
    "    text = text[:]\n",
    "    brands = []\n",
    "    alldesc = text.split(\"\\n\\n\")\n",
    "    for eachdesc in alldesc:\n",
    "        eachtitle = eachdesc.split(\":\")[0]\n",
    "        brandname = eachtitle.split(\" \")[0]\n",
    "        brandname = brandname.replace(\"\\n\",'')\n",
    "        brandname = brandname.replace(\"\\ufeff\",'')\n",
    "        brandname = brandname.lower()\n",
    "        brands.append(brandname)\n",
    "    brands = set(brands)\n",
    "    brands = list(brands)\n",
    "\n",
    "    brands.remove('and')\n",
    "    brands.remove('')\n",
    "    brands.remove('no')\n",
    "    brands.remove('only')\n",
    "    brands.remove('colour')\n",
    "    brands.remove('all')\n",
    "    brands.append('about')\n",
    "    brands.append('you')\n",
    "    clrlst = get_colors()\n",
    "    brands = list(set(brands)-set(clrlst))\n",
    "\n",
    "    return brands\n",
    "\n",
    "def get_colors():\n",
    "    \n",
    "    list_of_colors = []\n",
    "    listcolors = webcolors.CSS3_NAMES_TO_HEX\n",
    "    for all in listcolors:\n",
    "        cl = all.split(\":\")[0]\n",
    "        list_of_colors.append(cl)\n",
    "    list_of_colors.append('mauve')\n",
    "    list_of_colors.append('wine')\n",
    "    return list_of_colors\n",
    "\n",
    "def get_style():\n",
    "    list_of_styles = ['skinny', 'slim', 'regular','washed','comfort']\n",
    "    return list_of_styles\n",
    "\n",
    "def changetemplate(desc_op,features_str):\n",
    "    \n",
    "    #Getting brand names from text files to identify brand in the features string\n",
    "    brandnames = get_brandname()\n",
    "    brand_instr = ''\n",
    "    for allbrands in brandnames:\n",
    "        for allwords in features_str.split(\" \"):\n",
    "            if allbrands == allwords:\n",
    "                brand_instr = allbrands\n",
    "                break\n",
    "                \n",
    "\n",
    "    #getting all colors to identify the color in the features string\n",
    "    colorlist = get_colors()\n",
    "    color_instr = ''\n",
    "    for allcolors in colorlist:\n",
    "        for allword in desc_op.split(\":\")[0].split(\" \"):\n",
    "            if allcolors == allword:\n",
    "                color_instr = allcolors\n",
    "                break\n",
    "    for allcolors in colorlist:\n",
    "        for allwords in features_str.split(\" \"):\n",
    "            if allcolors == allwords:\n",
    "                color_instr = allcolors\n",
    "                break\n",
    "                \n",
    "    #getting all styles to identify the styles in the features string\n",
    "    stylelist = get_style()\n",
    "    style_instr = ''\n",
    "    for allcolors in stylelist:\n",
    "        for allword in desc_op.split(\":\")[0].split(\" \"):\n",
    "            if allcolors == allword:\n",
    "                style_instr = allcolors\n",
    "                break\n",
    "    for allcolors in stylelist:\n",
    "        for allwords in features_str.split(\" \"):\n",
    "            if allcolors == allwords:\n",
    "                style_instr = allcolors\n",
    "                break\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # to append the brand, color and style in the title if not present\n",
    "\n",
    "    allwordsintitle = desc_op.split(\":\")[0]\n",
    "    \n",
    "    for allwords in allwordsintitle.split(\" \"):\n",
    "        if allwords == style_instr:\n",
    "            flg = 0\n",
    "        else:\n",
    "            allwordsintitle = style_instr + \" \" + allwordsintitle\n",
    "            title = desc_op.split(\":\")[0]\n",
    "            allwordsintitle = title.replace(desc_op.split(\":\")[0],allwordsintitle)\n",
    "            break\n",
    "\n",
    "    for allwords in allwordsintitle.split(\" \"):\n",
    "        if allwords == color_instr:\n",
    "            flg = 0\n",
    "        else:\n",
    "            allwordsintitle = color_instr + \" \" + allwordsintitle\n",
    "            title = desc_op.split(\":\")[0]\n",
    "            allwordsintitle = title.replace(desc_op.split(\":\")[0],allwordsintitle)\n",
    "            break\n",
    "\n",
    "    for allwords in allwordsintitle.split(\" \"):\n",
    "        if allwords == brand_instr:\n",
    "            flg = 0\n",
    "        else:\n",
    "            allwordsintitle = brand_instr + \" \" + allwordsintitle\n",
    "            title = desc_op.split(\":\")[0]\n",
    "            allwordsintitle = title.replace(desc_op.split(\":\")[0],allwordsintitle)\n",
    "            break\n",
    "            \n",
    "    \n",
    "\n",
    "\n",
    "    #to replace brand names and colours in the description \n",
    "\n",
    "    allwordsindescription = desc_op.split(\":\")[1]\n",
    "\n",
    "    for brandnm in brandnames:\n",
    "        for allwords in allwordsintitle.split(\" \"):\n",
    "            if brandnm == allwords or brandnm+\".\" == allwords or brandnm+\",\" == allwords or brandnm+\";\" == allwords:\n",
    "                allwordsintitle = allwordsintitle.replace(brandnm,brand_instr)\n",
    "        for allwords in allwordsindescription.split(\" \"):\n",
    "            if brandnm == allwords or brandnm+\".\" == allwords or brandnm+\",\" == allwords or brandnm+\";\" == allwords:\n",
    "                allwordsindescription = allwordsindescription.replace(brandnm,brand_instr)\n",
    "\n",
    "    \n",
    "    allwordsindescriptionlist = allwordsindescription.split(\" \")\n",
    "    allwordsindescriptionlist = remove_duplicates(allwordsindescriptionlist)\n",
    "    allwordsindescription = ' '.join(allwordsindescriptionlist)\n",
    "    \n",
    "    desc_op = desc_op.replace(desc_op.split(\":\")[0],allwordsintitle)\n",
    "    desc_op = desc_op.replace(desc_op.split(\":\")[1],allwordsindescription)\n",
    "\n",
    "\n",
    "    for clrs in colorlist:\n",
    "        for allwords in allwordsintitle.split(\" \"):\n",
    "            if clrs == allwords or clrs+\".\" == allwords or clrs+\",\" == allwords or clrs+\";\" == allwords:\n",
    "                allwordsintitle = allwordsintitle.replace(clrs,color_instr)\n",
    "        for allwords in allwordsindescription.split(\" \"):\n",
    "            if clrs == allwords or clrs+\".\" == allwords or clrs+\",\" == allwords or clrs+\";\" == allwords:\n",
    "                allwordsindescription = allwordsindescription.replace(clrs,color_instr)\n",
    "\n",
    "\n",
    "    allwordsintitlelist = allwordsintitle.split(\" \")\n",
    "    allwordsintitlelist = remove_duplicates(allwordsintitlelist)\n",
    "    allwordsintitle = ' '.join(allwordsintitlelist)\n",
    "\n",
    "    desc_op = desc_op.replace(desc_op.split(\":\")[0],allwordsintitle)\n",
    "    desc_op = desc_op.replace(desc_op.split(\":\")[1],allwordsindescription)\n",
    "    \n",
    "    for brandnm in stylelist:\n",
    "        for allwords in allwordsintitle.split(\" \"):\n",
    "            if brandnm == allwords or brandnm+\".\" == allwords or brandnm+\",\" == allwords or brandnm+\";\" == allwords:\n",
    "                allwordsintitle = allwordsintitle.replace(brandnm,style_instr)\n",
    "        for allwords in allwordsindescription.split(\" \"):\n",
    "            if brandnm == allwords or brandnm+\".\" == allwords or brandnm+\",\" == allwords or brandnm+\";\" == allwords:\n",
    "                allwordsindescription = allwordsindescription.replace(brandnm,style_instr)\n",
    "\n",
    "    \n",
    "    allwordsindescriptionlist = allwordsindescription.split(\" \")\n",
    "    allwordsindescriptionlist = remove_duplicates(allwordsindescriptionlist)\n",
    "    allwordsindescription = ' '.join(allwordsindescriptionlist)\n",
    "    \n",
    "    desc_op = desc_op.replace(desc_op.split(\":\")[0],allwordsintitle)\n",
    "    desc_op = desc_op.replace(desc_op.split(\":\")[1],allwordsindescription)\n",
    "    print(desc_op+\"\\n\\n\")\n",
    "\n",
    "\n",
    "gen_length = 500\n",
    "\n",
    "features_str = 'shree green kurta'\n",
    "\n",
    "brandnamesall = get_brandname()\n",
    "prime_word = ''\n",
    "for allb in brandnamesall:\n",
    "    for wrds in features_str.split(\" \"):\n",
    "        if allb == wrds:\n",
    "            prime_word = allb\n",
    "            break\n",
    "\n",
    "\n",
    "loaded_graph = tf.Graph()\n",
    "with tf.Session(graph=loaded_graph) as sess:\n",
    "    # Load saved model\n",
    "    loader = tf.train.import_meta_graph(load_dir + '.meta')\n",
    "    loader.restore(sess, load_dir)\n",
    "\n",
    "    # Get Tensors from loaded model\n",
    "    input_text, initial_state, final_state, probs = get_tensors(loaded_graph)\n",
    "\n",
    "    # Sentences generation setup\n",
    "    gen_sentences = [prime_word]\n",
    "    prev_state = sess.run(initial_state, {input_text: np.array([[1]])})\n",
    "\n",
    "    # Generate sentences\n",
    "    for n in range(gen_length):\n",
    "        # Dynamic Input\n",
    "        dyn_input = [[vocab_to_int[word] for word in gen_sentences[-seq_length:]]]\n",
    "        dyn_seq_length = len(dyn_input[0])\n",
    "\n",
    "        # Get Prediction\n",
    "        probabilities, prev_state = sess.run(\n",
    "            [probs, final_state],\n",
    "            {input_text: dyn_input, initial_state: prev_state})\n",
    "        \n",
    "        pred_word = pick_word(probabilities[dyn_seq_length-1], int_to_vocab)\n",
    "\n",
    "        gen_sentences.append(pred_word)\n",
    "    \n",
    "    # Remove tokens\n",
    "    tv_script = ' '.join(gen_sentences)\n",
    "    for key, token in token_dict.items():\n",
    "        ending = ' ' if key in ['\\n', '(', '\"'] else ''\n",
    "        tv_script = tv_script.replace(' ' + token.lower(), key)\n",
    "    tv_script = tv_script.replace('\\n ', '\\n')\n",
    "    tv_script = tv_script.replace('( ', '(')\n",
    "    \n",
    "descriptions = tv_script.split(\"\\n\\n\")\n",
    "desc_op = ''\n",
    "for description in descriptions:\n",
    "    try:\n",
    "        x = description.split(\":\")[0]\n",
    "        y = description.split(\":\")[1]\n",
    "        desc = description\n",
    "        descls = desc.split()\n",
    "        desc = ' '.join(descls)\n",
    "        #print(desc+\"\\n\\n\")\n",
    "        changetemplate(desc,features_str)\n",
    "        \n",
    "    except:\n",
    "        flag = 0\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
