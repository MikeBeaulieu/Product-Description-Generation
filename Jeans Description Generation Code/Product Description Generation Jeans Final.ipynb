{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Product Description Generation\n",
    "In this project, you can generate descriptions of various apparel items like shirts, jeans, kurtas using RNNs. The descriptions of 10400 products for each apparel item has been used fro jabong.com to train the neural network. The Neural Network built will generate a product description for a product from the given sets of apparels i.e. shirts, jeans, shoes, watches and kurtas.\n",
    "## Get the Data\n",
    "The data consists of 10400 descriptions of different shirts obtained from jabong.com with distinguishing features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "import helper\n",
    "\n",
    "data_dir = './data/simpsons/jeans.txt'\n",
    "text = helper.load_data(data_dir)\n",
    "# Ignore notice, since we don't use it for analysing the data\n",
    "text = text[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Stats\n",
      "Roughly the number of unique words: 5416\n",
      "Number of scenes: 10491\n",
      "Average number of sentences in each scene: 0.999904680202078\n",
      "Number of lines: 20981\n",
      "Average number of words in each line: 28.161002812068062\n",
      "\n",
      "The sentences 0 to 10:\n",
      "ï»¿Moda Rapido Navy Blue Slim Tapered Fit Mid-Rise Mildly Distressed Stretchable Jeans: Navy blue dark wash 5-pocket mid-rise jeans, mildly distressed with light fade, has a button and zip closure, a waistband with belt loops\n",
      "\n",
      "\n",
      "Mufti Black Solid Slim Fit Jeans: Spread your spark as your walk the streets wearing these jeans by Mufti. Showcasing a signature retro style, these jeans features solid style and slim fit. Jump off a roof or slide down a hilltop to taste the thrill first-hand in these jeans.\n",
      "\n",
      "\n",
      "Mufti Light Blue Washed Comfort Fit Jeans: Hit the streets in style wearing these jeans by Mufti. Regular fit and soft material of these jeans will keep you at ease all day long. Jump off a roof or slide down a hilltop to taste the thrill first-hand in these jeans.\n",
      "\n",
      "\n",
      "Mast & Harbour Khaki Skinny Fit Mid-Rise Clean Look Stretchable Jeans: Khaki coloured wash 5-pocket mid-rise jeans, clean look with no fade, has a button and zip closure, waistband with belt loops\n"
     ]
    }
   ],
   "source": [
    "view_sentence_range = (0, 10)\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "\n",
    "print('Dataset Stats')\n",
    "print('Roughly the number of unique words: {}'.format(len({word: None for word in text.split()})))\n",
    "scenes = text.split('\\n\\n')\n",
    "print('Number of scenes: {}'.format(len(scenes)))\n",
    "sentence_count_scene = [scene.count('\\n') for scene in scenes]\n",
    "print('Average number of sentences in each scene: {}'.format(np.average(sentence_count_scene)))\n",
    "\n",
    "sentences = [sentence for scene in scenes for sentence in scene.split('\\n')]\n",
    "print('Number of lines: {}'.format(len(sentences)))\n",
    "word_count_sentence = [len(sentence.split()) for sentence in sentences]\n",
    "print('Average number of words in each line: {}'.format(np.average(word_count_sentence)))\n",
    "\n",
    "print()\n",
    "print('The sentences {} to {}:'.format(*view_sentence_range))\n",
    "print('\\n'.join(text.split('\\n')[view_sentence_range[0]:view_sentence_range[1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement Preprocessing Functions\n",
    "The first thing to do to any dataset is preprocessing.  Implement the following preprocessing functions below:\n",
    "- Lookup Table\n",
    "- Tokenize Punctuation\n",
    "\n",
    "### Lookup Table\n",
    "To create a word embedding, you first need to transform the words to ids.  In this function, create two dictionaries:\n",
    "- Dictionary to go from the words to an id, we'll call `vocab_to_int`\n",
    "- Dictionary to go from the id to word, we'll call `int_to_vocab`\n",
    "\n",
    "Return these dictionaries in the following tuple `(vocab_to_int, int_to_vocab)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/test/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import problem_unittests as tests\n",
    "\n",
    "def create_lookup_tables(text):\n",
    "    \"\"\"\n",
    "    Create lookup tables for vocabulary\n",
    "    :param text: The text of tv scripts split into words\n",
    "    :return: A tuple of dicts (vocab_to_int, int_to_vocab)\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    text = list(set(text))   \n",
    "    index = range(len(text))    \n",
    "    int_to_vocab = dict(zip(index, text))   \n",
    "    vocab_to_int = dict(zip(text, index))       \n",
    "    return vocab_to_int, int_to_vocab\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_create_lookup_tables(create_lookup_tables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize Punctuation\n",
    "We'll be splitting the script into a word array using spaces as delimiters.  However, punctuations like periods and exclamation marks make it hard for the neural network to distinguish between the word \"bye\" and \"bye!\".\n",
    "\n",
    "Implement the function `token_lookup` to return a dict that will be used to tokenize symbols like \"!\" into \"||Exclamation_Mark||\".  Create a dictionary for the following symbols where the symbol is the key and value is the token:\n",
    "- Period ( . )\n",
    "- Comma ( , )\n",
    "- Quotation Mark ( \" )\n",
    "- Semicolon ( ; )\n",
    "- Exclamation mark ( ! )\n",
    "- Question mark ( ? )\n",
    "- Left Parentheses ( ( )\n",
    "- Right Parentheses ( ) )\n",
    "- Dash ( -- )\n",
    "- Return ( \\n )\n",
    "\n",
    "This dictionary will be used to token the symbols and add the delimiter (space) around it.  This separates the symbols as it's own word, making it easier for the neural network to predict on the next word. Make sure you don't use a token that could be confused as a word. Instead of using the token \"dash\", try using something like \"||dash||\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def token_lookup():\n",
    "    \"\"\"\n",
    "    Generate a dict to turn punctuation into a token.\n",
    "    :return: Tokenize dictionary where the key is the punctuation and the value is the token\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    token = {}\n",
    "    token['.']=\"||period||\"\n",
    "    token[',']=\"||comma||\"\n",
    "    token['\"']=\"||quotation_mark||\"\n",
    "    token[';']=\"||semicolon||\"\n",
    "    token['!']=\"||exclamation_mark||\"\n",
    "    token['?']=\"||question_mark||\"\n",
    "    token['(']=\"||left_parantheses||\"\n",
    "    token[')']=\"||right_parantheses||\"\n",
    "    token['--'] = \"||dash||\"\n",
    "    token['\\n'] = \"||return||\"\n",
    "    \n",
    "    return token\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_tokenize(token_lookup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess all the data and save it\n",
    "Running the code cell below will preprocess all the data and save it to file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "# Preprocess Training, Validation, and Testing Data\n",
    "helper.preprocess_and_save_data(data_dir, token_lookup, create_lookup_tables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Point\n",
    "This is your first checkpoint. If you ever decide to come back to this notebook or have to restart the notebook, you can start from here. The preprocessed data has been saved to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "import helper\n",
    "import numpy as np\n",
    "import problem_unittests as tests\n",
    "\n",
    "int_text, vocab_to_int, int_to_vocab, token_dict = helper.load_preprocess()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the Neural Network\n",
    "You'll build the components necessary to build a RNN by implementing the following functions below:\n",
    "- get_inputs\n",
    "- get_init_cell\n",
    "- get_embed\n",
    "- build_rnn\n",
    "- build_nn\n",
    "- get_batches\n",
    "\n",
    "### Check the Version of TensorFlow and Access to GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 1.1.0\n",
      "Default GPU Device: /gpu:0\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "from distutils.version import LooseVersion\n",
    "import warnings\n",
    "import tensorflow as tf\n",
    "\n",
    "# Check TensorFlow Version\n",
    "assert LooseVersion(tf.__version__) >= LooseVersion('1.0'), 'Please use TensorFlow version 1.0 or newer'\n",
    "print('TensorFlow Version: {}'.format(tf.__version__))\n",
    "\n",
    "# Check for a GPU\n",
    "if not tf.test.gpu_device_name():\n",
    "    warnings.warn('No GPU found. Please use a GPU to train your neural network.')\n",
    "else:\n",
    "    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input\n",
    "Implement the `get_inputs()` function to create TF Placeholders for the Neural Network.  It should create the following placeholders:\n",
    "- Input text placeholder named \"input\" using the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder) `name` parameter.\n",
    "- Targets placeholder\n",
    "- Learning Rate placeholder\n",
    "\n",
    "Return the placeholders in the following tuple `(Input, Targets, LearningRate)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def get_inputs():\n",
    "    \"\"\"\n",
    "    Create TF Placeholders for input, targets, and learning rate.\n",
    "    :return: Tuple (input, targets, learning rate)\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    INPUT = tf.placeholder(tf.int32, shape=(None,None), name=\"input\")\n",
    "    TARGETS = tf.placeholder(tf.int32, shape=(None,None), name='targets')\n",
    "    LEARNING_RATE = tf.placeholder(tf.float32, shape=None, name='learning_rate')\n",
    "    return INPUT, TARGETS, LEARNING_RATE\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_get_inputs(get_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build RNN Cell and Initialize\n",
    "Stack one or more [`BasicLSTMCells`](https://www.tensorflow.org/api_docs/python/tf/contrib/rnn/BasicLSTMCell) in a [`MultiRNNCell`](https://www.tensorflow.org/api_docs/python/tf/contrib/rnn/MultiRNNCell).\n",
    "- The Rnn size should be set using `rnn_size`\n",
    "- Initalize Cell State using the MultiRNNCell's [`zero_state()`](https://www.tensorflow.org/api_docs/python/tf/contrib/rnn/MultiRNNCell#zero_state) function\n",
    "    - Apply the name \"initial_state\" to the initial state using [`tf.identity()`](https://www.tensorflow.org/api_docs/python/tf/identity)\n",
    "\n",
    "Return the cell and initial state in the following tuple `(Cell, InitialState)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def get_init_cell(batch_size, rnn_size):\n",
    "    \"\"\"\n",
    "    Create an RNN Cell and initialize it.\n",
    "    :param batch_size: Size of batches\n",
    "    :param rnn_size: Size of RNNs\n",
    "    :return: Tuple (cell, initialize state)\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    lstm1 = tf.contrib.rnn.BasicLSTMCell(rnn_size)\n",
    "    lstm2 = tf.contrib.rnn.BasicLSTMCell(rnn_size)\n",
    "    lstm3 = tf.contrib.rnn.BasicLSTMCell(rnn_size)\n",
    "    lstm4 = tf.contrib.rnn.BasicLSTMCell(rnn_size)\n",
    "    cell = tf.contrib.rnn.MultiRNNCell([lstm1,lstm2,lstm3,lstm4])\n",
    "    initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "    initial_state = tf.identity(initial_state, name='initial_state')\n",
    "   \n",
    "     \n",
    "    return cell, initial_state\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_get_init_cell(get_init_cell)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Embedding\n",
    "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def get_embed(input_data, vocab_size, embed_dim):\n",
    "    \"\"\"\n",
    "    Create embedding for <input_data>.\n",
    "    :param input_data: TF placeholder for text input.\n",
    "    :param vocab_size: Number of words in vocabulary.\n",
    "    :param embed_dim: Number of embedding dimensionstea\n",
    "    :return: Embedded input.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    embedding = tf.Variable(tf.random_uniform((vocab_size, embed_dim), -1, 1))\n",
    "    embed = tf.nn.embedding_lookup(embedding, input_data)\n",
    "    return embed\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_get_embed(get_embed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build RNN\n",
    "You created a RNN Cell in the `get_init_cell()` function.  Time to use the cell to create a RNN.\n",
    "- Build the RNN using the [`tf.nn.dynamic_rnn()`](https://www.tensorflow.org/api_docs/python/tf/nn/dynamic_rnn)\n",
    " - Apply the name \"final_state\" to the final state using [`tf.identity()`](https://www.tensorflow.org/api_docs/python/tf/identity)\n",
    "\n",
    "Return the outputs and final_state state in the following tuple `(Outputs, FinalState)` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def build_rnn(cell, inputs):\n",
    "    \"\"\"\n",
    "    Create a RNN using a RNN Cell\n",
    "    :param cell: RNN Cell\n",
    "    :param inputs: Input text data\n",
    "    :return: Tuple (Outputs, Final State)\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    Outputs, States = tf.nn.dynamic_rnn(cell, inputs,dtype='float32')\n",
    "    FinalState = tf.identity(States,name=\"final_state\")\n",
    "    return Outputs, FinalState\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_build_rnn(build_rnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the Neural Network\n",
    "Apply the functions you implemented above to:\n",
    "- Apply embedding to `input_data` using your `get_embed(input_data, vocab_size, embed_dim)` function.\n",
    "- Build RNN using `cell` and your `build_rnn(cell, inputs)` function.\n",
    "- Apply a fully connected layer with a linear activation and `vocab_size` as the number of outputs.\n",
    "\n",
    "Return the logits and final state in the following tuple (Logits, FinalState) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def build_nn(cell, rnn_size, input_data, vocab_size, embed_dim):\n",
    "    \"\"\"\n",
    "    Build part of the neural network\n",
    "    :param cell: RNN cell\n",
    "    :param rnn_size: Size of rnns\n",
    "    :param input_data: Input data\n",
    "    :param vocab_size: Vocabulary size\n",
    "    :param embed_dim: Number of embedding dimensions\n",
    "    :return: Tuple (Logits, FinalState)\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    embed = get_embed(input_data, vocab_size, embed_dim)\n",
    "    outputs, final_state = build_rnn(cell, embed)\n",
    "    logits = tf.contrib.layers.fully_connected(outputs, vocab_size, activation_fn=None,\n",
    "                                               weights_initializer=tf.truncated_normal_initializer(stddev=0.1),\n",
    "                                               biases_initializer=tf.zeros_initializer())\n",
    "    return logits, final_state\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_build_nn(build_nn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batches\n",
    "Implement `get_batches` to create batches of input and targets using `int_text`.  The batches should be a Numpy array with the shape `(number of batches, 2, batch size, sequence length)`. Each batch contains two elements:\n",
    "- The first element is a single batch of **input** with the shape `[batch size, sequence length]`\n",
    "- The second element is a single batch of **targets** with the shape `[batch size, sequence length]`\n",
    "\n",
    "If you can't fill the last batch with enough data, drop the last batch.\n",
    "\n",
    "For example, `get_batches([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20], 3, 2)` would return a Numpy array of the following:\n",
    "```\n",
    "[\n",
    "  # First Batch\n",
    "  [\n",
    "    # Batch of Input\n",
    "    [[ 1  2], [ 7  8], [13 14]]\n",
    "    # Batch of targets\n",
    "    [[ 2  3], [ 8  9], [14 15]]\n",
    "  ]\n",
    "\n",
    "  # Second Batch\n",
    "  [\n",
    "    # Batch of Input\n",
    "    [[ 3  4], [ 9 10], [15 16]]\n",
    "    # Batch of targets\n",
    "    [[ 4  5], [10 11], [16 17]]\n",
    "  ]\n",
    "\n",
    "  # Third Batch\n",
    "  [\n",
    "    # Batch of Input\n",
    "    [[ 5  6], [11 12], [17 18]]\n",
    "    # Batch of targets\n",
    "    [[ 6  7], [12 13], [18  1]]\n",
    "  ]\n",
    "]\n",
    "```\n",
    "\n",
    "Notice that the last target value in the last batch is the first input value of the first batch. In this case, `1`. This is a common technique used when creating sequence batches, although it is rather unintuitive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def get_batches(int_text, batch_size, seq_length):\n",
    "    \"\"\"\n",
    "    Return batches of input and target\n",
    "    :param int_text: Text with the words replaced by their ids\n",
    "    :param batch_size: The size of batch\n",
    "    :param seq_length: The length of sequence\n",
    "    :return: Batches as a Numpy array\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    num_batches = int(len(int_text) / (batch_size * seq_length))\n",
    "    in_data = np.array(int_text[: num_batches * batch_size * seq_length])\n",
    "    out_data = np.array(int_text[1: num_batches * batch_size * seq_length + 1])\n",
    "    out_data[-1] = in_data[0]\n",
    "\n",
    "    in_batches = np.split(in_data.reshape(batch_size, -1), num_batches, 1)\n",
    "    out_batches = np.split(out_data.reshape(batch_size, -1), num_batches, 1)\n",
    "    return np.array(list(zip(in_batches, out_batches)))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_get_batches(get_batches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Training\n",
    "### Hyperparameters\n",
    "Tune the following parameters:\n",
    "\n",
    "- Set `num_epochs` to the number of epochs.\n",
    "- Set `batch_size` to the batch size.\n",
    "- Set `rnn_size` to the size of the RNNs.\n",
    "- Set `embed_dim` to the size of the embedding.\n",
    "- Set `seq_length` to the length of sequence.\n",
    "- Set `learning_rate` to the learning rate.\n",
    "- Set `show_every_n_batches` to the number of batches the neural network should print progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Number of Epochs\n",
    "num_epochs = 3000\n",
    "# Batch Size\n",
    "batch_size = 512\n",
    "# RNN Size\n",
    "rnn_size = 256\n",
    "# Embedding Dimension Size\n",
    "embed_dim = 256\n",
    "# Sequence Length\n",
    "seq_length = 16\n",
    "# Learning Rate\n",
    "learning_rate = 0.01\n",
    "# Show stats for every n number of batches\n",
    "show_every_n_batches = 32\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "save_dir = './savejeans'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the Graph\n",
    "Build the graph using the neural network you implemented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "from tensorflow.contrib import seq2seq\n",
    "\n",
    "train_graph = tf.Graph()\n",
    "with train_graph.as_default():\n",
    "    vocab_size = len(int_to_vocab)\n",
    "    input_text, targets, lr = get_inputs()\n",
    "    input_data_shape = tf.shape(input_text)\n",
    "    cell, initial_state = get_init_cell(input_data_shape[0], rnn_size)\n",
    "    logits, final_state = build_nn(cell, rnn_size, input_text, vocab_size, embed_dim)\n",
    "\n",
    "    # Probabilities for generating words\n",
    "    probs = tf.nn.softmax(logits, name='probs')\n",
    "\n",
    "    # Loss function\n",
    "    cost = seq2seq.sequence_loss(\n",
    "        logits,\n",
    "        targets,\n",
    "        tf.ones([input_data_shape[0], input_data_shape[1]]))\n",
    "\n",
    "    # Optimizer\n",
    "    optimizer = tf.train.AdamOptimizer(lr)\n",
    "\n",
    "    # Gradient Clipping\n",
    "    gradients = optimizer.compute_gradients(cost)\n",
    "    capped_gradients = [(tf.clip_by_value(grad, -1., 1.), var) for grad, var in gradients if grad is not None]\n",
    "    train_op = optimizer.apply_gradients(capped_gradients)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train\n",
    "Train the neural network on the preprocessed data.  If you have a hard time getting a good loss, check the [forums](https://discussions.udacity.com/) to see if anyone is having the same problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   0 Batch    0/81   train_loss = 8.146\n",
      "Epoch   0 Batch   32/81   train_loss = 5.453\n",
      "Epoch   0 Batch   64/81   train_loss = 5.157\n",
      "Epoch   1 Batch   15/81   train_loss = 4.859\n",
      "Epoch   1 Batch   47/81   train_loss = 4.424\n",
      "Epoch   1 Batch   79/81   train_loss = 4.208\n",
      "Epoch   2 Batch   30/81   train_loss = 3.802\n",
      "Epoch   2 Batch   62/81   train_loss = 3.435\n",
      "Epoch   3 Batch   13/81   train_loss = 3.259\n",
      "Epoch   3 Batch   45/81   train_loss = 3.036\n",
      "Epoch   3 Batch   77/81   train_loss = 2.758\n",
      "Epoch   4 Batch   28/81   train_loss = 2.484\n",
      "Epoch   4 Batch   60/81   train_loss = 2.308\n",
      "Epoch   5 Batch   11/81   train_loss = 2.199\n",
      "Epoch   5 Batch   43/81   train_loss = 1.964\n",
      "Epoch   5 Batch   75/81   train_loss = 1.906\n",
      "Epoch   6 Batch   26/81   train_loss = 1.815\n",
      "Epoch   6 Batch   58/81   train_loss = 1.709\n",
      "Epoch   7 Batch    9/81   train_loss = 1.760\n",
      "Epoch   7 Batch   41/81   train_loss = 1.591\n",
      "Epoch   7 Batch   73/81   train_loss = 1.599\n",
      "Epoch   8 Batch   24/81   train_loss = 1.526\n",
      "Epoch   8 Batch   56/81   train_loss = 1.521\n",
      "Epoch   9 Batch    7/81   train_loss = 1.457\n",
      "Epoch   9 Batch   39/81   train_loss = 1.362\n",
      "Epoch   9 Batch   71/81   train_loss = 1.379\n",
      "Epoch  10 Batch   22/81   train_loss = 1.432\n",
      "Epoch  10 Batch   54/81   train_loss = 1.349\n",
      "Epoch  11 Batch    5/81   train_loss = 1.303\n",
      "Epoch  11 Batch   37/81   train_loss = 1.333\n",
      "Epoch  11 Batch   69/81   train_loss = 1.312\n",
      "Epoch  12 Batch   20/81   train_loss = 1.279\n",
      "Epoch  12 Batch   52/81   train_loss = 1.324\n",
      "Epoch  13 Batch    3/81   train_loss = 1.223\n",
      "Epoch  13 Batch   35/81   train_loss = 1.198\n",
      "Epoch  13 Batch   67/81   train_loss = 1.257\n",
      "Epoch  14 Batch   18/81   train_loss = 1.244\n",
      "Epoch  14 Batch   50/81   train_loss = 1.201\n",
      "Epoch  15 Batch    1/81   train_loss = 1.217\n",
      "Epoch  15 Batch   33/81   train_loss = 1.133\n",
      "Epoch  15 Batch   65/81   train_loss = 1.196\n",
      "Epoch  16 Batch   16/81   train_loss = 1.196\n",
      "Epoch  16 Batch   48/81   train_loss = 1.163\n",
      "Epoch  16 Batch   80/81   train_loss = 1.190\n",
      "Epoch  17 Batch   31/81   train_loss = 1.084\n",
      "Epoch  17 Batch   63/81   train_loss = 1.144\n",
      "Epoch  18 Batch   14/81   train_loss = 1.138\n",
      "Epoch  18 Batch   46/81   train_loss = 1.104\n",
      "Epoch  18 Batch   78/81   train_loss = 1.139\n",
      "Epoch  19 Batch   29/81   train_loss = 1.088\n",
      "Epoch  19 Batch   61/81   train_loss = 1.093\n",
      "Epoch  20 Batch   12/81   train_loss = 1.096\n",
      "Epoch  20 Batch   44/81   train_loss = 1.070\n",
      "Epoch  20 Batch   76/81   train_loss = 1.092\n",
      "Epoch  21 Batch   27/81   train_loss = 1.017\n",
      "Epoch  21 Batch   59/81   train_loss = 1.042\n",
      "Epoch  22 Batch   10/81   train_loss = 1.106\n",
      "Epoch  22 Batch   42/81   train_loss = 1.008\n",
      "Epoch  22 Batch   74/81   train_loss = 1.022\n",
      "Epoch  23 Batch   25/81   train_loss = 1.048\n",
      "Epoch  23 Batch   57/81   train_loss = 1.007\n",
      "Epoch  24 Batch    8/81   train_loss = 1.105\n",
      "Epoch  24 Batch   40/81   train_loss = 1.014\n",
      "Epoch  24 Batch   72/81   train_loss = 1.053\n",
      "Epoch  25 Batch   23/81   train_loss = 1.014\n",
      "Epoch  25 Batch   55/81   train_loss = 1.025\n",
      "Epoch  26 Batch    6/81   train_loss = 1.032\n",
      "Epoch  26 Batch   38/81   train_loss = 1.029\n",
      "Epoch  26 Batch   70/81   train_loss = 0.993\n",
      "Epoch  27 Batch   21/81   train_loss = 1.075\n",
      "Epoch  27 Batch   53/81   train_loss = 0.999\n",
      "Epoch  28 Batch    4/81   train_loss = 1.010\n",
      "Epoch  28 Batch   36/81   train_loss = 1.027\n",
      "Epoch  28 Batch   68/81   train_loss = 1.031\n",
      "Epoch  29 Batch   19/81   train_loss = 0.948\n",
      "Epoch  29 Batch   51/81   train_loss = 1.022\n",
      "Epoch  30 Batch    2/81   train_loss = 1.001\n",
      "Epoch  30 Batch   34/81   train_loss = 0.983\n",
      "Epoch  30 Batch   66/81   train_loss = 0.994\n",
      "Epoch  31 Batch   17/81   train_loss = 0.975\n",
      "Epoch  31 Batch   49/81   train_loss = 0.945\n",
      "Epoch  32 Batch    0/81   train_loss = 0.976\n",
      "Epoch  32 Batch   32/81   train_loss = 0.927\n",
      "Epoch  32 Batch   64/81   train_loss = 0.978\n",
      "Epoch  33 Batch   15/81   train_loss = 0.964\n",
      "Epoch  33 Batch   47/81   train_loss = 0.926\n",
      "Epoch  33 Batch   79/81   train_loss = 0.954\n",
      "Epoch  34 Batch   30/81   train_loss = 0.949\n",
      "Epoch  34 Batch   62/81   train_loss = 0.961\n",
      "Epoch  35 Batch   13/81   train_loss = 0.975\n",
      "Epoch  35 Batch   45/81   train_loss = 0.925\n",
      "Epoch  35 Batch   77/81   train_loss = 0.957\n",
      "Epoch  36 Batch   28/81   train_loss = 0.927\n",
      "Epoch  36 Batch   60/81   train_loss = 0.947\n",
      "Epoch  37 Batch   11/81   train_loss = 0.950\n",
      "Epoch  37 Batch   43/81   train_loss = 0.915\n",
      "Epoch  37 Batch   75/81   train_loss = 0.938\n",
      "Epoch  38 Batch   26/81   train_loss = 0.926\n",
      "Epoch  38 Batch   58/81   train_loss = 0.897\n",
      "Epoch  39 Batch    9/81   train_loss = 0.964\n",
      "Epoch  39 Batch   41/81   train_loss = 0.904\n",
      "Epoch  39 Batch   73/81   train_loss = 0.925\n",
      "Epoch  40 Batch   24/81   train_loss = 0.924\n",
      "Epoch  40 Batch   56/81   train_loss = 0.930\n",
      "Epoch  41 Batch    7/81   train_loss = 0.928\n",
      "Epoch  41 Batch   39/81   train_loss = 0.879\n",
      "Epoch  41 Batch   71/81   train_loss = 0.877\n",
      "Epoch  42 Batch   22/81   train_loss = 0.959\n",
      "Epoch  42 Batch   54/81   train_loss = 0.924\n",
      "Epoch  43 Batch    5/81   train_loss = 0.900\n",
      "Epoch  43 Batch   37/81   train_loss = 0.941\n",
      "Epoch  43 Batch   69/81   train_loss = 0.921\n",
      "Epoch  44 Batch   20/81   train_loss = 0.904\n",
      "Epoch  44 Batch   52/81   train_loss = 0.915\n",
      "Epoch  45 Batch    3/81   train_loss = 0.898\n",
      "Epoch  45 Batch   35/81   train_loss = 0.884\n",
      "Epoch  45 Batch   67/81   train_loss = 0.925\n",
      "Epoch  46 Batch   18/81   train_loss = 0.916\n",
      "Epoch  46 Batch   50/81   train_loss = 0.868\n",
      "Epoch  47 Batch    1/81   train_loss = 0.926\n",
      "Epoch  47 Batch   33/81   train_loss = 0.852\n",
      "Epoch  47 Batch   65/81   train_loss = 0.893\n",
      "Epoch  48 Batch   16/81   train_loss = 0.956\n",
      "Epoch  48 Batch   48/81   train_loss = 0.917\n",
      "Epoch  48 Batch   80/81   train_loss = 0.925\n",
      "Epoch  49 Batch   31/81   train_loss = 0.885\n",
      "Epoch  49 Batch   63/81   train_loss = 0.920\n",
      "Epoch  50 Batch   14/81   train_loss = 0.945\n",
      "Epoch  50 Batch   46/81   train_loss = 0.903\n",
      "Epoch  50 Batch   78/81   train_loss = 0.931\n",
      "Epoch  51 Batch   29/81   train_loss = 0.887\n",
      "Epoch  51 Batch   61/81   train_loss = 0.914\n",
      "Epoch  52 Batch   12/81   train_loss = 0.949\n",
      "Epoch  52 Batch   44/81   train_loss = 0.865\n",
      "Epoch  52 Batch   76/81   train_loss = 0.890\n",
      "Epoch  53 Batch   27/81   train_loss = 0.855\n",
      "Epoch  53 Batch   59/81   train_loss = 0.882\n",
      "Epoch  54 Batch   10/81   train_loss = 0.926\n",
      "Epoch  54 Batch   42/81   train_loss = 0.840\n",
      "Epoch  54 Batch   74/81   train_loss = 0.846\n",
      "Epoch  55 Batch   25/81   train_loss = 0.885\n",
      "Epoch  55 Batch   57/81   train_loss = 0.853\n",
      "Epoch  56 Batch    8/81   train_loss = 0.870\n",
      "Epoch  56 Batch   40/81   train_loss = 0.866\n",
      "Epoch  56 Batch   72/81   train_loss = 0.865\n",
      "Epoch  57 Batch   23/81   train_loss = 0.850\n",
      "Epoch  57 Batch   55/81   train_loss = 0.882\n",
      "Epoch  58 Batch    6/81   train_loss = 0.841\n",
      "Epoch  58 Batch   38/81   train_loss = 0.894\n",
      "Epoch  58 Batch   70/81   train_loss = 0.866\n",
      "Epoch  59 Batch   21/81   train_loss = 0.896\n",
      "Epoch  59 Batch   53/81   train_loss = 0.863\n",
      "Epoch  60 Batch    4/81   train_loss = 0.859\n",
      "Epoch  60 Batch   36/81   train_loss = 0.872\n",
      "Epoch  60 Batch   68/81   train_loss = 0.875\n",
      "Epoch  61 Batch   19/81   train_loss = 0.825\n",
      "Epoch  61 Batch   51/81   train_loss = 0.890\n",
      "Epoch  62 Batch    2/81   train_loss = 0.867\n",
      "Epoch  62 Batch   34/81   train_loss = 0.882\n",
      "Epoch  62 Batch   66/81   train_loss = 0.874\n",
      "Epoch  63 Batch   17/81   train_loss = 0.865\n",
      "Epoch  63 Batch   49/81   train_loss = 0.860\n",
      "Epoch  64 Batch    0/81   train_loss = 0.888\n",
      "Epoch  64 Batch   32/81   train_loss = 0.874\n",
      "Epoch  64 Batch   64/81   train_loss = 0.904\n",
      "Epoch  65 Batch   15/81   train_loss = 0.877\n",
      "Epoch  65 Batch   47/81   train_loss = 0.880\n",
      "Epoch  65 Batch   79/81   train_loss = 0.891\n",
      "Epoch  66 Batch   30/81   train_loss = 0.895\n",
      "Epoch  66 Batch   62/81   train_loss = 0.866\n",
      "Epoch  67 Batch   13/81   train_loss = 0.907\n",
      "Epoch  67 Batch   45/81   train_loss = 0.857\n",
      "Epoch  67 Batch   77/81   train_loss = 0.886\n",
      "Epoch  68 Batch   28/81   train_loss = 0.843\n",
      "Epoch  68 Batch   60/81   train_loss = 0.868\n",
      "Epoch  69 Batch   11/81   train_loss = 0.847\n",
      "Epoch  69 Batch   43/81   train_loss = 0.823\n",
      "Epoch  69 Batch   75/81   train_loss = 0.857\n",
      "Epoch  70 Batch   26/81   train_loss = 0.853\n",
      "Epoch  70 Batch   58/81   train_loss = 0.829\n",
      "Epoch  71 Batch    9/81   train_loss = 0.859\n",
      "Epoch  71 Batch   41/81   train_loss = 0.811\n",
      "Epoch  71 Batch   73/81   train_loss = 0.836\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  72 Batch   24/81   train_loss = 0.850\n",
      "Epoch  72 Batch   56/81   train_loss = 0.845\n",
      "Epoch  73 Batch    7/81   train_loss = 0.844\n",
      "Epoch  73 Batch   39/81   train_loss = 0.802\n",
      "Epoch  73 Batch   71/81   train_loss = 0.809\n",
      "Epoch  74 Batch   22/81   train_loss = 0.847\n",
      "Epoch  74 Batch   54/81   train_loss = 0.841\n",
      "Epoch  75 Batch    5/81   train_loss = 0.830\n",
      "Epoch  75 Batch   37/81   train_loss = 0.841\n",
      "Epoch  75 Batch   69/81   train_loss = 0.830\n",
      "Epoch  76 Batch   20/81   train_loss = 0.815\n",
      "Epoch  76 Batch   52/81   train_loss = 0.843\n",
      "Epoch  77 Batch    3/81   train_loss = 0.824\n",
      "Epoch  77 Batch   35/81   train_loss = 0.802\n",
      "Epoch  77 Batch   67/81   train_loss = 0.854\n",
      "Epoch  78 Batch   18/81   train_loss = 0.836\n",
      "Epoch  78 Batch   50/81   train_loss = 0.804\n",
      "Epoch  79 Batch    1/81   train_loss = 0.847\n",
      "Epoch  79 Batch   33/81   train_loss = 0.809\n",
      "Epoch  79 Batch   65/81   train_loss = 0.835\n",
      "Epoch  80 Batch   16/81   train_loss = 0.868\n",
      "Epoch  80 Batch   48/81   train_loss = 0.841\n",
      "Epoch  80 Batch   80/81   train_loss = 0.835\n",
      "Epoch  81 Batch   31/81   train_loss = 0.802\n",
      "Epoch  81 Batch   63/81   train_loss = 0.833\n",
      "Epoch  82 Batch   14/81   train_loss = 0.829\n",
      "Epoch  82 Batch   46/81   train_loss = 0.824\n",
      "Epoch  82 Batch   78/81   train_loss = 0.830\n",
      "Epoch  83 Batch   29/81   train_loss = 0.810\n",
      "Epoch  83 Batch   61/81   train_loss = 0.848\n",
      "Epoch  84 Batch   12/81   train_loss = 0.840\n",
      "Epoch  84 Batch   44/81   train_loss = 0.814\n",
      "Epoch  84 Batch   76/81   train_loss = 0.833\n",
      "Epoch  85 Batch   27/81   train_loss = 0.784\n",
      "Epoch  85 Batch   59/81   train_loss = 0.821\n",
      "Epoch  86 Batch   10/81   train_loss = 0.856\n",
      "Epoch  86 Batch   42/81   train_loss = 0.814\n",
      "Epoch  86 Batch   74/81   train_loss = 0.862\n",
      "Epoch  87 Batch   25/81   train_loss = 0.843\n",
      "Epoch  87 Batch   57/81   train_loss = 0.826\n",
      "Epoch  88 Batch    8/81   train_loss = 0.869\n",
      "Epoch  88 Batch   40/81   train_loss = 0.829\n",
      "Epoch  88 Batch   72/81   train_loss = 0.849\n",
      "Epoch  89 Batch   23/81   train_loss = 0.827\n",
      "Epoch  89 Batch   55/81   train_loss = 0.840\n",
      "Epoch  90 Batch    6/81   train_loss = 0.817\n",
      "Epoch  90 Batch   38/81   train_loss = 0.848\n",
      "Epoch  90 Batch   70/81   train_loss = 0.829\n",
      "Epoch  91 Batch   21/81   train_loss = 0.865\n",
      "Epoch  91 Batch   53/81   train_loss = 0.824\n",
      "Epoch  92 Batch    4/81   train_loss = 0.847\n",
      "Epoch  92 Batch   36/81   train_loss = 0.839\n",
      "Epoch  92 Batch   68/81   train_loss = 0.855\n",
      "Epoch  93 Batch   19/81   train_loss = 0.800\n",
      "Epoch  93 Batch   51/81   train_loss = 0.853\n",
      "Epoch  94 Batch    2/81   train_loss = 0.832\n",
      "Epoch  94 Batch   34/81   train_loss = 0.853\n",
      "Epoch  94 Batch   66/81   train_loss = 0.827\n",
      "Epoch  95 Batch   17/81   train_loss = 0.807\n",
      "Epoch  95 Batch   49/81   train_loss = 0.817\n",
      "Epoch  96 Batch    0/81   train_loss = 0.840\n",
      "Epoch  96 Batch   32/81   train_loss = 0.801\n",
      "Epoch  96 Batch   64/81   train_loss = 0.823\n",
      "Epoch  97 Batch   15/81   train_loss = 0.816\n",
      "Epoch  97 Batch   47/81   train_loss = 0.809\n",
      "Epoch  97 Batch   79/81   train_loss = 0.807\n",
      "Epoch  98 Batch   30/81   train_loss = 0.804\n",
      "Epoch  98 Batch   62/81   train_loss = 0.798\n",
      "Epoch  99 Batch   13/81   train_loss = 0.842\n",
      "Epoch  99 Batch   45/81   train_loss = 0.803\n",
      "Epoch  99 Batch   77/81   train_loss = 0.837\n",
      "Epoch 100 Batch   28/81   train_loss = 0.793\n",
      "Epoch 100 Batch   60/81   train_loss = 0.814\n",
      "Epoch 101 Batch   11/81   train_loss = 0.823\n",
      "Epoch 101 Batch   43/81   train_loss = 0.793\n",
      "Epoch 101 Batch   75/81   train_loss = 0.811\n",
      "Epoch 102 Batch   26/81   train_loss = 0.819\n",
      "Epoch 102 Batch   58/81   train_loss = 0.851\n",
      "Epoch 103 Batch    9/81   train_loss = 0.837\n",
      "Epoch 103 Batch   41/81   train_loss = 0.805\n",
      "Epoch 103 Batch   73/81   train_loss = 0.854\n",
      "Epoch 104 Batch   24/81   train_loss = 0.818\n",
      "Epoch 104 Batch   56/81   train_loss = 0.838\n",
      "Epoch 105 Batch    7/81   train_loss = 0.853\n",
      "Epoch 105 Batch   39/81   train_loss = 0.792\n",
      "Epoch 105 Batch   71/81   train_loss = 0.800\n",
      "Epoch 106 Batch   22/81   train_loss = 0.855\n",
      "Epoch 106 Batch   54/81   train_loss = 0.845\n",
      "Epoch 107 Batch    5/81   train_loss = 0.819\n",
      "Epoch 107 Batch   37/81   train_loss = 0.847\n",
      "Epoch 107 Batch   69/81   train_loss = 0.852\n",
      "Epoch 108 Batch   20/81   train_loss = 0.830\n",
      "Epoch 108 Batch   52/81   train_loss = 0.863\n",
      "Epoch 109 Batch    3/81   train_loss = 0.840\n",
      "Epoch 109 Batch   35/81   train_loss = 0.831\n",
      "Epoch 109 Batch   67/81   train_loss = 0.879\n",
      "Epoch 110 Batch   18/81   train_loss = 0.867\n",
      "Epoch 110 Batch   50/81   train_loss = 0.817\n",
      "Epoch 111 Batch    1/81   train_loss = 0.870\n",
      "Epoch 111 Batch   33/81   train_loss = 0.799\n",
      "Epoch 111 Batch   65/81   train_loss = 0.850\n",
      "Epoch 112 Batch   16/81   train_loss = 0.852\n",
      "Epoch 112 Batch   48/81   train_loss = 0.800\n",
      "Epoch 112 Batch   80/81   train_loss = 0.825\n",
      "Epoch 113 Batch   31/81   train_loss = 0.791\n",
      "Epoch 113 Batch   63/81   train_loss = 0.800\n",
      "Epoch 114 Batch   14/81   train_loss = 0.806\n",
      "Epoch 114 Batch   46/81   train_loss = 0.798\n",
      "Epoch 114 Batch   78/81   train_loss = 0.803\n",
      "Epoch 115 Batch   29/81   train_loss = 0.775\n",
      "Epoch 115 Batch   61/81   train_loss = 0.788\n",
      "Epoch 116 Batch   12/81   train_loss = 0.805\n",
      "Epoch 116 Batch   44/81   train_loss = 0.768\n",
      "Epoch 116 Batch   76/81   train_loss = 0.793\n",
      "Epoch 117 Batch   27/81   train_loss = 0.762\n",
      "Epoch 117 Batch   59/81   train_loss = 0.773\n",
      "Epoch 118 Batch   10/81   train_loss = 0.815\n",
      "Epoch 118 Batch   42/81   train_loss = 0.764\n",
      "Epoch 118 Batch   74/81   train_loss = 0.769\n",
      "Epoch 119 Batch   25/81   train_loss = 0.787\n",
      "Epoch 119 Batch   57/81   train_loss = 0.784\n",
      "Epoch 120 Batch    8/81   train_loss = 0.783\n",
      "Epoch 120 Batch   40/81   train_loss = 0.754\n",
      "Epoch 120 Batch   72/81   train_loss = 0.767\n",
      "Epoch 121 Batch   23/81   train_loss = 0.782\n",
      "Epoch 121 Batch   55/81   train_loss = 0.778\n",
      "Epoch 122 Batch    6/81   train_loss = 0.778\n",
      "Epoch 122 Batch   38/81   train_loss = 0.794\n",
      "Epoch 122 Batch   70/81   train_loss = 0.769\n",
      "Epoch 123 Batch   21/81   train_loss = 0.821\n",
      "Epoch 123 Batch   53/81   train_loss = 0.778\n",
      "Epoch 124 Batch    4/81   train_loss = 0.796\n",
      "Epoch 124 Batch   36/81   train_loss = 0.792\n",
      "Epoch 124 Batch   68/81   train_loss = 0.795\n",
      "Epoch 125 Batch   19/81   train_loss = 0.765\n",
      "Epoch 125 Batch   51/81   train_loss = 0.818\n",
      "Epoch 126 Batch    2/81   train_loss = 0.792\n",
      "Epoch 126 Batch   34/81   train_loss = 0.799\n",
      "Epoch 126 Batch   66/81   train_loss = 0.791\n",
      "Epoch 127 Batch   17/81   train_loss = 0.807\n",
      "Epoch 127 Batch   49/81   train_loss = 0.781\n",
      "Epoch 128 Batch    0/81   train_loss = 0.791\n",
      "Epoch 128 Batch   32/81   train_loss = 0.772\n",
      "Epoch 128 Batch   64/81   train_loss = 0.809\n",
      "Epoch 129 Batch   15/81   train_loss = 0.801\n",
      "Epoch 129 Batch   47/81   train_loss = 0.795\n",
      "Epoch 129 Batch   79/81   train_loss = 0.784\n",
      "Epoch 130 Batch   30/81   train_loss = 0.790\n",
      "Epoch 130 Batch   62/81   train_loss = 0.783\n",
      "Epoch 131 Batch   13/81   train_loss = 0.795\n",
      "Epoch 131 Batch   45/81   train_loss = 0.786\n",
      "Epoch 131 Batch   77/81   train_loss = 0.806\n",
      "Epoch 132 Batch   28/81   train_loss = 0.766\n",
      "Epoch 132 Batch   60/81   train_loss = 0.797\n",
      "Epoch 133 Batch   11/81   train_loss = 0.786\n",
      "Epoch 133 Batch   43/81   train_loss = 0.758\n",
      "Epoch 133 Batch   75/81   train_loss = 0.802\n",
      "Epoch 134 Batch   26/81   train_loss = 0.789\n",
      "Epoch 134 Batch   58/81   train_loss = 0.764\n",
      "Epoch 135 Batch    9/81   train_loss = 0.817\n",
      "Epoch 135 Batch   41/81   train_loss = 0.775\n",
      "Epoch 135 Batch   73/81   train_loss = 0.807\n",
      "Epoch 136 Batch   24/81   train_loss = 0.815\n",
      "Epoch 136 Batch   56/81   train_loss = 0.806\n",
      "Epoch 137 Batch    7/81   train_loss = 0.792\n",
      "Epoch 137 Batch   39/81   train_loss = 0.769\n",
      "Epoch 137 Batch   71/81   train_loss = 0.792\n",
      "Epoch 138 Batch   22/81   train_loss = 0.811\n",
      "Epoch 138 Batch   54/81   train_loss = 0.781\n",
      "Epoch 139 Batch    5/81   train_loss = 0.792\n",
      "Epoch 139 Batch   37/81   train_loss = 0.807\n",
      "Epoch 139 Batch   69/81   train_loss = 0.817\n",
      "Epoch 140 Batch   20/81   train_loss = 0.793\n",
      "Epoch 140 Batch   52/81   train_loss = 0.811\n",
      "Epoch 141 Batch    3/81   train_loss = 0.781\n",
      "Epoch 141 Batch   35/81   train_loss = 0.775\n",
      "Epoch 141 Batch   67/81   train_loss = 0.838\n",
      "Epoch 142 Batch   18/81   train_loss = 0.813\n",
      "Epoch 142 Batch   50/81   train_loss = 0.771\n",
      "Epoch 143 Batch    1/81   train_loss = 0.836\n",
      "Epoch 143 Batch   33/81   train_loss = 0.779\n",
      "Epoch 143 Batch   65/81   train_loss = 0.809\n",
      "Epoch 144 Batch   16/81   train_loss = 0.850\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 144 Batch   48/81   train_loss = 0.787\n",
      "Epoch 144 Batch   80/81   train_loss = 0.813\n",
      "Epoch 145 Batch   31/81   train_loss = 0.778\n",
      "Epoch 145 Batch   63/81   train_loss = 0.795\n",
      "Epoch 146 Batch   14/81   train_loss = 0.833\n",
      "Epoch 146 Batch   46/81   train_loss = 0.833\n",
      "Epoch 146 Batch   78/81   train_loss = 0.818\n",
      "Epoch 147 Batch   29/81   train_loss = 0.796\n",
      "Epoch 147 Batch   61/81   train_loss = 0.805\n",
      "Epoch 148 Batch   12/81   train_loss = 0.839\n",
      "Epoch 148 Batch   44/81   train_loss = 0.787\n",
      "Epoch 148 Batch   76/81   train_loss = 0.811\n",
      "Epoch 149 Batch   27/81   train_loss = 0.773\n",
      "Epoch 149 Batch   59/81   train_loss = 0.804\n",
      "Epoch 150 Batch   10/81   train_loss = 0.817\n",
      "Epoch 150 Batch   42/81   train_loss = 0.791\n",
      "Epoch 150 Batch   74/81   train_loss = 0.804\n",
      "Epoch 151 Batch   25/81   train_loss = 0.812\n",
      "Epoch 151 Batch   57/81   train_loss = 0.816\n",
      "Epoch 152 Batch    8/81   train_loss = 0.827\n",
      "Epoch 152 Batch   40/81   train_loss = 0.776\n",
      "Epoch 152 Batch   72/81   train_loss = 0.800\n",
      "Epoch 153 Batch   23/81   train_loss = 0.783\n",
      "Epoch 153 Batch   55/81   train_loss = 0.804\n",
      "Epoch 154 Batch    6/81   train_loss = 0.807\n",
      "Epoch 154 Batch   38/81   train_loss = 0.826\n",
      "Epoch 154 Batch   70/81   train_loss = 0.803\n",
      "Epoch 155 Batch   21/81   train_loss = 0.845\n",
      "Epoch 155 Batch   53/81   train_loss = 0.792\n",
      "Epoch 156 Batch    4/81   train_loss = 0.819\n",
      "Epoch 156 Batch   36/81   train_loss = 0.814\n",
      "Epoch 156 Batch   68/81   train_loss = 0.822\n",
      "Epoch 157 Batch   19/81   train_loss = 0.782\n",
      "Epoch 157 Batch   51/81   train_loss = 0.831\n",
      "Epoch 158 Batch    2/81   train_loss = 0.810\n",
      "Epoch 158 Batch   34/81   train_loss = 0.815\n",
      "Epoch 158 Batch   66/81   train_loss = 0.815\n",
      "Epoch 159 Batch   17/81   train_loss = 0.800\n",
      "Epoch 159 Batch   49/81   train_loss = 0.783\n",
      "Epoch 160 Batch    0/81   train_loss = 0.820\n",
      "Epoch 160 Batch   32/81   train_loss = 0.778\n",
      "Epoch 160 Batch   64/81   train_loss = 0.815\n",
      "Epoch 161 Batch   15/81   train_loss = 0.806\n",
      "Epoch 161 Batch   47/81   train_loss = 0.793\n",
      "Epoch 161 Batch   79/81   train_loss = 0.785\n",
      "Epoch 162 Batch   30/81   train_loss = 0.798\n",
      "Epoch 162 Batch   62/81   train_loss = 0.792\n",
      "Epoch 163 Batch   13/81   train_loss = 0.805\n",
      "Epoch 163 Batch   45/81   train_loss = 0.765\n",
      "Epoch 163 Batch   77/81   train_loss = 0.809\n",
      "Epoch 164 Batch   28/81   train_loss = 0.783\n",
      "Epoch 164 Batch   60/81   train_loss = 0.780\n",
      "Epoch 165 Batch   11/81   train_loss = 0.806\n",
      "Epoch 165 Batch   43/81   train_loss = 0.761\n",
      "Epoch 165 Batch   75/81   train_loss = 0.795\n",
      "Epoch 166 Batch   26/81   train_loss = 0.794\n",
      "Epoch 166 Batch   58/81   train_loss = 0.758\n",
      "Epoch 167 Batch    9/81   train_loss = 0.820\n",
      "Epoch 167 Batch   41/81   train_loss = 0.749\n",
      "Epoch 167 Batch   73/81   train_loss = 0.798\n",
      "Epoch 168 Batch   24/81   train_loss = 0.809\n",
      "Epoch 168 Batch   56/81   train_loss = 0.782\n",
      "Epoch 169 Batch    7/81   train_loss = 0.786\n",
      "Epoch 169 Batch   39/81   train_loss = 0.762\n",
      "Epoch 169 Batch   71/81   train_loss = 0.765\n",
      "Epoch 170 Batch   22/81   train_loss = 0.801\n",
      "Epoch 170 Batch   54/81   train_loss = 0.770\n",
      "Epoch 171 Batch    5/81   train_loss = 0.799\n",
      "Epoch 171 Batch   37/81   train_loss = 0.794\n",
      "Epoch 171 Batch   69/81   train_loss = 0.803\n",
      "Epoch 172 Batch   20/81   train_loss = 0.792\n",
      "Epoch 172 Batch   52/81   train_loss = 0.801\n",
      "Epoch 173 Batch    3/81   train_loss = 0.790\n",
      "Epoch 173 Batch   35/81   train_loss = 0.789\n",
      "Epoch 173 Batch   67/81   train_loss = 0.812\n",
      "Epoch 174 Batch   18/81   train_loss = 0.817\n",
      "Epoch 174 Batch   50/81   train_loss = 0.766\n",
      "Epoch 175 Batch    1/81   train_loss = 0.840\n",
      "Epoch 175 Batch   33/81   train_loss = 0.782\n",
      "Epoch 175 Batch   65/81   train_loss = 0.836\n",
      "Epoch 176 Batch   16/81   train_loss = 0.863\n",
      "Epoch 176 Batch   48/81   train_loss = 0.812\n",
      "Epoch 176 Batch   80/81   train_loss = 0.822\n",
      "Epoch 177 Batch   31/81   train_loss = 0.815\n",
      "Epoch 177 Batch   63/81   train_loss = 0.810\n",
      "Epoch 178 Batch   14/81   train_loss = 0.818\n",
      "Epoch 178 Batch   46/81   train_loss = 0.823\n",
      "Epoch 178 Batch   78/81   train_loss = 0.809\n",
      "Epoch 179 Batch   29/81   train_loss = 0.792\n",
      "Epoch 179 Batch   61/81   train_loss = 0.819\n",
      "Epoch 180 Batch   12/81   train_loss = 0.801\n",
      "Epoch 180 Batch   44/81   train_loss = 0.793\n",
      "Epoch 180 Batch   76/81   train_loss = 0.829\n",
      "Epoch 181 Batch   27/81   train_loss = 0.784\n",
      "Epoch 181 Batch   59/81   train_loss = 0.807\n",
      "Epoch 182 Batch   10/81   train_loss = 0.833\n",
      "Epoch 182 Batch   42/81   train_loss = 0.799\n",
      "Epoch 182 Batch   74/81   train_loss = 0.809\n",
      "Epoch 183 Batch   25/81   train_loss = 0.801\n",
      "Epoch 183 Batch   57/81   train_loss = 0.808\n",
      "Epoch 184 Batch    8/81   train_loss = 0.823\n",
      "Epoch 184 Batch   40/81   train_loss = 0.801\n",
      "Epoch 184 Batch   72/81   train_loss = 0.791\n",
      "Epoch 185 Batch   23/81   train_loss = 0.820\n",
      "Epoch 185 Batch   55/81   train_loss = 0.826\n",
      "Epoch 186 Batch    6/81   train_loss = 0.797\n",
      "Epoch 186 Batch   38/81   train_loss = 0.844\n",
      "Epoch 186 Batch   70/81   train_loss = 0.803\n",
      "Epoch 187 Batch   21/81   train_loss = 0.835\n",
      "Epoch 187 Batch   53/81   train_loss = 0.817\n",
      "Epoch 188 Batch    4/81   train_loss = 0.812\n",
      "Epoch 188 Batch   36/81   train_loss = 0.818\n",
      "Epoch 188 Batch   68/81   train_loss = 0.817\n",
      "Epoch 189 Batch   19/81   train_loss = 0.780\n",
      "Epoch 189 Batch   51/81   train_loss = 0.841\n",
      "Epoch 190 Batch    2/81   train_loss = 0.810\n",
      "Epoch 190 Batch   34/81   train_loss = 0.822\n",
      "Epoch 190 Batch   66/81   train_loss = 0.789\n",
      "Epoch 191 Batch   17/81   train_loss = 0.803\n",
      "Epoch 191 Batch   49/81   train_loss = 0.799\n",
      "Epoch 192 Batch    0/81   train_loss = 0.806\n",
      "Epoch 192 Batch   32/81   train_loss = 0.764\n",
      "Epoch 192 Batch   64/81   train_loss = 0.798\n",
      "Epoch 193 Batch   15/81   train_loss = 0.792\n",
      "Epoch 193 Batch   47/81   train_loss = 0.783\n",
      "Epoch 193 Batch   79/81   train_loss = 0.771\n",
      "Epoch 194 Batch   30/81   train_loss = 0.790\n",
      "Epoch 194 Batch   62/81   train_loss = 0.770\n",
      "Epoch 195 Batch   13/81   train_loss = 0.785\n",
      "Epoch 195 Batch   45/81   train_loss = 0.785\n",
      "Epoch 195 Batch   77/81   train_loss = 0.796\n",
      "Epoch 196 Batch   28/81   train_loss = 0.761\n",
      "Epoch 196 Batch   60/81   train_loss = 0.790\n",
      "Epoch 197 Batch   11/81   train_loss = 0.779\n",
      "Epoch 197 Batch   43/81   train_loss = 0.758\n",
      "Epoch 197 Batch   75/81   train_loss = 0.774\n",
      "Epoch 198 Batch   26/81   train_loss = 0.779\n",
      "Epoch 198 Batch   58/81   train_loss = 0.755\n",
      "Epoch 199 Batch    9/81   train_loss = 0.803\n",
      "Epoch 199 Batch   41/81   train_loss = 0.740\n",
      "Epoch 199 Batch   73/81   train_loss = 0.793\n",
      "Epoch 200 Batch   24/81   train_loss = 0.777\n",
      "Epoch 200 Batch   56/81   train_loss = 0.792\n",
      "Epoch 201 Batch    7/81   train_loss = 0.775\n",
      "Epoch 201 Batch   39/81   train_loss = 0.771\n",
      "Epoch 201 Batch   71/81   train_loss = 0.757\n",
      "Epoch 202 Batch   22/81   train_loss = 0.797\n",
      "Epoch 202 Batch   54/81   train_loss = 0.773\n",
      "Epoch 203 Batch    5/81   train_loss = 0.779\n",
      "Epoch 203 Batch   37/81   train_loss = 0.774\n",
      "Epoch 203 Batch   69/81   train_loss = 0.786\n",
      "Epoch 204 Batch   20/81   train_loss = 0.761\n",
      "Epoch 204 Batch   52/81   train_loss = 0.816\n",
      "Epoch 205 Batch    3/81   train_loss = 0.770\n",
      "Epoch 205 Batch   35/81   train_loss = 0.766\n",
      "Epoch 205 Batch   67/81   train_loss = 0.799\n",
      "Epoch 206 Batch   18/81   train_loss = 0.805\n",
      "Epoch 206 Batch   50/81   train_loss = 0.763\n",
      "Epoch 207 Batch    1/81   train_loss = 0.804\n",
      "Epoch 207 Batch   33/81   train_loss = 0.767\n",
      "Epoch 207 Batch   65/81   train_loss = 0.810\n",
      "Epoch 208 Batch   16/81   train_loss = 0.844\n",
      "Epoch 208 Batch   48/81   train_loss = 0.783\n",
      "Epoch 208 Batch   80/81   train_loss = 0.804\n",
      "Epoch 209 Batch   31/81   train_loss = 0.782\n",
      "Epoch 209 Batch   63/81   train_loss = 0.789\n",
      "Epoch 210 Batch   14/81   train_loss = 0.795\n",
      "Epoch 210 Batch   46/81   train_loss = 0.792\n",
      "Epoch 210 Batch   78/81   train_loss = 0.783\n",
      "Epoch 211 Batch   29/81   train_loss = 0.762\n",
      "Epoch 211 Batch   61/81   train_loss = 0.786\n",
      "Epoch 212 Batch   12/81   train_loss = 0.810\n",
      "Epoch 212 Batch   44/81   train_loss = 0.769\n",
      "Epoch 212 Batch   76/81   train_loss = 0.817\n",
      "Epoch 213 Batch   27/81   train_loss = 0.770\n",
      "Epoch 213 Batch   59/81   train_loss = 0.785\n",
      "Epoch 214 Batch   10/81   train_loss = 0.813\n",
      "Epoch 214 Batch   42/81   train_loss = 0.771\n",
      "Epoch 214 Batch   74/81   train_loss = 0.785\n",
      "Epoch 215 Batch   25/81   train_loss = 0.789\n",
      "Epoch 215 Batch   57/81   train_loss = 0.778\n",
      "Epoch 216 Batch    8/81   train_loss = 0.798\n",
      "Epoch 216 Batch   40/81   train_loss = 0.760\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 216 Batch   72/81   train_loss = 0.774\n",
      "Epoch 217 Batch   23/81   train_loss = 0.786\n",
      "Epoch 217 Batch   55/81   train_loss = 0.803\n",
      "Epoch 218 Batch    6/81   train_loss = 0.777\n",
      "Epoch 218 Batch   38/81   train_loss = 0.798\n",
      "Epoch 218 Batch   70/81   train_loss = 0.778\n",
      "Epoch 219 Batch   21/81   train_loss = 0.800\n",
      "Epoch 219 Batch   53/81   train_loss = 0.774\n",
      "Epoch 220 Batch    4/81   train_loss = 0.784\n",
      "Epoch 220 Batch   36/81   train_loss = 0.793\n",
      "Epoch 220 Batch   68/81   train_loss = 0.787\n",
      "Epoch 221 Batch   19/81   train_loss = 0.744\n",
      "Epoch 221 Batch   51/81   train_loss = 0.810\n",
      "Epoch 222 Batch    2/81   train_loss = 0.794\n",
      "Epoch 222 Batch   34/81   train_loss = 0.804\n",
      "Epoch 222 Batch   66/81   train_loss = 0.792\n",
      "Epoch 223 Batch   17/81   train_loss = 0.804\n",
      "Epoch 223 Batch   49/81   train_loss = 0.788\n",
      "Epoch 224 Batch    0/81   train_loss = 0.793\n",
      "Epoch 224 Batch   32/81   train_loss = 0.770\n",
      "Epoch 224 Batch   64/81   train_loss = 0.790\n",
      "Epoch 225 Batch   15/81   train_loss = 0.792\n",
      "Epoch 225 Batch   47/81   train_loss = 0.775\n",
      "Epoch 225 Batch   79/81   train_loss = 0.786\n",
      "Epoch 226 Batch   30/81   train_loss = 0.786\n",
      "Epoch 226 Batch   62/81   train_loss = 0.791\n",
      "Epoch 227 Batch   13/81   train_loss = 0.797\n",
      "Epoch 227 Batch   45/81   train_loss = 0.756\n",
      "Epoch 227 Batch   77/81   train_loss = 0.799\n",
      "Epoch 228 Batch   28/81   train_loss = 0.771\n",
      "Epoch 228 Batch   60/81   train_loss = 0.794\n",
      "Epoch 229 Batch   11/81   train_loss = 0.791\n",
      "Epoch 229 Batch   43/81   train_loss = 0.769\n",
      "Epoch 229 Batch   75/81   train_loss = 0.786\n",
      "Epoch 230 Batch   26/81   train_loss = 0.804\n",
      "Epoch 230 Batch   58/81   train_loss = 0.755\n",
      "Epoch 231 Batch    9/81   train_loss = 0.809\n",
      "Epoch 231 Batch   41/81   train_loss = 0.754\n",
      "Epoch 231 Batch   73/81   train_loss = 0.798\n",
      "Epoch 232 Batch   24/81   train_loss = 0.794\n",
      "Epoch 232 Batch   56/81   train_loss = 0.807\n",
      "Epoch 233 Batch    7/81   train_loss = 0.793\n",
      "Epoch 233 Batch   39/81   train_loss = 0.773\n",
      "Epoch 233 Batch   71/81   train_loss = 0.780\n",
      "Epoch 234 Batch   22/81   train_loss = 0.812\n",
      "Epoch 234 Batch   54/81   train_loss = 0.796\n",
      "Epoch 235 Batch    5/81   train_loss = 0.796\n",
      "Epoch 235 Batch   37/81   train_loss = 0.790\n",
      "Epoch 235 Batch   69/81   train_loss = 0.806\n",
      "Epoch 236 Batch   20/81   train_loss = 0.806\n",
      "Epoch 236 Batch   52/81   train_loss = 0.832\n",
      "Epoch 237 Batch    3/81   train_loss = 0.787\n",
      "Epoch 237 Batch   35/81   train_loss = 0.779\n",
      "Epoch 237 Batch   67/81   train_loss = 0.827\n",
      "Epoch 238 Batch   18/81   train_loss = 0.821\n",
      "Epoch 238 Batch   50/81   train_loss = 0.763\n",
      "Epoch 239 Batch    1/81   train_loss = 0.817\n",
      "Epoch 239 Batch   33/81   train_loss = 0.790\n",
      "Epoch 239 Batch   65/81   train_loss = 0.829\n",
      "Epoch 240 Batch   16/81   train_loss = 0.842\n",
      "Epoch 240 Batch   48/81   train_loss = 0.809\n",
      "Epoch 240 Batch   80/81   train_loss = 0.805\n",
      "Epoch 241 Batch   31/81   train_loss = 0.772\n",
      "Epoch 241 Batch   63/81   train_loss = 0.793\n",
      "Epoch 242 Batch   14/81   train_loss = 0.822\n",
      "Epoch 242 Batch   46/81   train_loss = 0.799\n",
      "Epoch 242 Batch   78/81   train_loss = 0.786\n",
      "Epoch 243 Batch   29/81   train_loss = 0.765\n",
      "Epoch 243 Batch   61/81   train_loss = 0.790\n",
      "Epoch 244 Batch   12/81   train_loss = 0.801\n",
      "Epoch 244 Batch   44/81   train_loss = 0.789\n",
      "Epoch 244 Batch   76/81   train_loss = 0.808\n",
      "Epoch 245 Batch   27/81   train_loss = 0.762\n",
      "Epoch 245 Batch   59/81   train_loss = 0.786\n",
      "Epoch 246 Batch   10/81   train_loss = 0.791\n",
      "Epoch 246 Batch   42/81   train_loss = 0.790\n",
      "Epoch 246 Batch   74/81   train_loss = 0.806\n",
      "Epoch 247 Batch   25/81   train_loss = 0.800\n",
      "Epoch 247 Batch   57/81   train_loss = 0.804\n",
      "Epoch 248 Batch    8/81   train_loss = 0.822\n",
      "Epoch 248 Batch   40/81   train_loss = 0.778\n",
      "Epoch 248 Batch   72/81   train_loss = 0.789\n",
      "Epoch 249 Batch   23/81   train_loss = 0.778\n",
      "Epoch 249 Batch   55/81   train_loss = 0.807\n",
      "Epoch 250 Batch    6/81   train_loss = 0.799\n",
      "Epoch 250 Batch   38/81   train_loss = 0.827\n",
      "Epoch 250 Batch   70/81   train_loss = 0.805\n",
      "Epoch 251 Batch   21/81   train_loss = 0.839\n",
      "Epoch 251 Batch   53/81   train_loss = 0.786\n",
      "Epoch 252 Batch    4/81   train_loss = 0.804\n",
      "Epoch 252 Batch   36/81   train_loss = 0.800\n",
      "Epoch 252 Batch   68/81   train_loss = 0.812\n",
      "Epoch 253 Batch   19/81   train_loss = 0.760\n",
      "Epoch 253 Batch   51/81   train_loss = 0.858\n",
      "Epoch 254 Batch    2/81   train_loss = 0.806\n",
      "Epoch 254 Batch   34/81   train_loss = 0.815\n",
      "Epoch 254 Batch   66/81   train_loss = 0.801\n",
      "Epoch 255 Batch   17/81   train_loss = 0.807\n",
      "Epoch 255 Batch   49/81   train_loss = 0.800\n",
      "Epoch 256 Batch    0/81   train_loss = 0.823\n",
      "Epoch 256 Batch   32/81   train_loss = 0.794\n",
      "Epoch 256 Batch   64/81   train_loss = 0.828\n",
      "Epoch 257 Batch   15/81   train_loss = 0.804\n",
      "Epoch 257 Batch   47/81   train_loss = 0.801\n",
      "Epoch 257 Batch   79/81   train_loss = 0.792\n",
      "Epoch 258 Batch   30/81   train_loss = 0.792\n",
      "Epoch 258 Batch   62/81   train_loss = 0.817\n",
      "Epoch 259 Batch   13/81   train_loss = 0.812\n",
      "Epoch 259 Batch   45/81   train_loss = 0.802\n",
      "Epoch 259 Batch   77/81   train_loss = 0.819\n",
      "Epoch 260 Batch   28/81   train_loss = 0.797\n",
      "Epoch 260 Batch   60/81   train_loss = 0.801\n",
      "Epoch 261 Batch   11/81   train_loss = 0.811\n",
      "Epoch 261 Batch   43/81   train_loss = 0.781\n",
      "Epoch 261 Batch   75/81   train_loss = 0.815\n",
      "Epoch 262 Batch   26/81   train_loss = 0.832\n",
      "Epoch 262 Batch   58/81   train_loss = 0.783\n",
      "Epoch 263 Batch    9/81   train_loss = 0.831\n",
      "Epoch 263 Batch   41/81   train_loss = 0.751\n",
      "Epoch 263 Batch   73/81   train_loss = 0.800\n",
      "Epoch 264 Batch   24/81   train_loss = 0.814\n",
      "Epoch 264 Batch   56/81   train_loss = 0.817\n",
      "Epoch 265 Batch    7/81   train_loss = 0.806\n",
      "Epoch 265 Batch   39/81   train_loss = 0.769\n",
      "Epoch 265 Batch   71/81   train_loss = 0.798\n",
      "Epoch 266 Batch   22/81   train_loss = 0.819\n",
      "Epoch 266 Batch   54/81   train_loss = 0.806\n",
      "Epoch 267 Batch    5/81   train_loss = 0.808\n",
      "Epoch 267 Batch   37/81   train_loss = 0.814\n",
      "Epoch 267 Batch   69/81   train_loss = 0.822\n",
      "Epoch 268 Batch   20/81   train_loss = 0.802\n",
      "Epoch 268 Batch   52/81   train_loss = 0.818\n",
      "Epoch 269 Batch    3/81   train_loss = 0.787\n",
      "Epoch 269 Batch   35/81   train_loss = 0.768\n",
      "Epoch 269 Batch   67/81   train_loss = 0.819\n",
      "Epoch 270 Batch   18/81   train_loss = 0.809\n",
      "Epoch 270 Batch   50/81   train_loss = 0.772\n",
      "Epoch 271 Batch    1/81   train_loss = 0.812\n",
      "Epoch 271 Batch   33/81   train_loss = 0.767\n",
      "Epoch 271 Batch   65/81   train_loss = 0.815\n",
      "Epoch 272 Batch   16/81   train_loss = 0.837\n",
      "Epoch 272 Batch   48/81   train_loss = 0.778\n",
      "Epoch 272 Batch   80/81   train_loss = 0.787\n",
      "Epoch 273 Batch   31/81   train_loss = 0.779\n",
      "Epoch 273 Batch   63/81   train_loss = 0.786\n",
      "Epoch 274 Batch   14/81   train_loss = 0.795\n",
      "Epoch 274 Batch   46/81   train_loss = 0.797\n",
      "Epoch 274 Batch   78/81   train_loss = 0.781\n",
      "Epoch 275 Batch   29/81   train_loss = 0.763\n",
      "Epoch 275 Batch   61/81   train_loss = 0.787\n",
      "Epoch 276 Batch   12/81   train_loss = 0.811\n",
      "Epoch 276 Batch   44/81   train_loss = 0.772\n",
      "Epoch 276 Batch   76/81   train_loss = 0.824\n",
      "Epoch 277 Batch   27/81   train_loss = 0.763\n",
      "Epoch 277 Batch   59/81   train_loss = 0.769\n",
      "Epoch 278 Batch   10/81   train_loss = 0.812\n",
      "Epoch 278 Batch   42/81   train_loss = 0.766\n",
      "Epoch 278 Batch   74/81   train_loss = 0.781\n",
      "Epoch 279 Batch   25/81   train_loss = 0.781\n",
      "Epoch 279 Batch   57/81   train_loss = 0.784\n",
      "Epoch 280 Batch    8/81   train_loss = 0.808\n",
      "Epoch 280 Batch   40/81   train_loss = 0.769\n",
      "Epoch 280 Batch   72/81   train_loss = 0.784\n",
      "Epoch 281 Batch   23/81   train_loss = 0.774\n",
      "Epoch 281 Batch   55/81   train_loss = 0.791\n",
      "Epoch 282 Batch    6/81   train_loss = 0.777\n",
      "Epoch 282 Batch   38/81   train_loss = 0.816\n",
      "Epoch 282 Batch   70/81   train_loss = 0.775\n",
      "Epoch 283 Batch   21/81   train_loss = 0.824\n",
      "Epoch 283 Batch   53/81   train_loss = 0.767\n",
      "Epoch 284 Batch    4/81   train_loss = 0.781\n",
      "Epoch 284 Batch   36/81   train_loss = 0.793\n",
      "Epoch 284 Batch   68/81   train_loss = 0.794\n",
      "Epoch 285 Batch   19/81   train_loss = 0.762\n",
      "Epoch 285 Batch   51/81   train_loss = 0.818\n",
      "Epoch 286 Batch    2/81   train_loss = 0.819\n",
      "Epoch 286 Batch   34/81   train_loss = 0.808\n",
      "Epoch 286 Batch   66/81   train_loss = 0.793\n",
      "Epoch 287 Batch   17/81   train_loss = 0.795\n",
      "Epoch 287 Batch   49/81   train_loss = 0.782\n",
      "Epoch 288 Batch    0/81   train_loss = 0.806\n",
      "Epoch 288 Batch   32/81   train_loss = 0.778\n",
      "Epoch 288 Batch   64/81   train_loss = 0.789\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 289 Batch   15/81   train_loss = 0.785\n",
      "Epoch 289 Batch   47/81   train_loss = 0.772\n",
      "Epoch 289 Batch   79/81   train_loss = 0.778\n",
      "Epoch 290 Batch   30/81   train_loss = 0.781\n",
      "Epoch 290 Batch   62/81   train_loss = 0.776\n",
      "Epoch 291 Batch   13/81   train_loss = 0.802\n",
      "Epoch 291 Batch   45/81   train_loss = 0.773\n",
      "Epoch 291 Batch   77/81   train_loss = 0.786\n",
      "Epoch 292 Batch   28/81   train_loss = 0.765\n",
      "Epoch 292 Batch   60/81   train_loss = 0.782\n",
      "Epoch 293 Batch   11/81   train_loss = 0.777\n",
      "Epoch 293 Batch   43/81   train_loss = 0.756\n",
      "Epoch 293 Batch   75/81   train_loss = 0.769\n",
      "Epoch 294 Batch   26/81   train_loss = 0.785\n",
      "Epoch 294 Batch   58/81   train_loss = 0.762\n",
      "Epoch 295 Batch    9/81   train_loss = 0.816\n",
      "Epoch 295 Batch   41/81   train_loss = 0.758\n",
      "Epoch 295 Batch   73/81   train_loss = 0.783\n",
      "Epoch 296 Batch   24/81   train_loss = 0.812\n",
      "Epoch 296 Batch   56/81   train_loss = 0.808\n",
      "Epoch 297 Batch    7/81   train_loss = 0.795\n",
      "Epoch 297 Batch   39/81   train_loss = 0.761\n",
      "Epoch 297 Batch   71/81   train_loss = 0.764\n",
      "Epoch 298 Batch   22/81   train_loss = 0.801\n",
      "Epoch 298 Batch   54/81   train_loss = 0.785\n",
      "Epoch 299 Batch    5/81   train_loss = 0.793\n",
      "Epoch 299 Batch   37/81   train_loss = 0.789\n",
      "Epoch 299 Batch   69/81   train_loss = 0.806\n",
      "Epoch 300 Batch   20/81   train_loss = 0.798\n",
      "Epoch 300 Batch   52/81   train_loss = 0.811\n",
      "Epoch 301 Batch    3/81   train_loss = 0.789\n",
      "Epoch 301 Batch   35/81   train_loss = 0.780\n",
      "Epoch 301 Batch   67/81   train_loss = 0.816\n",
      "Epoch 302 Batch   18/81   train_loss = 0.817\n",
      "Epoch 302 Batch   50/81   train_loss = 0.775\n",
      "Epoch 303 Batch    1/81   train_loss = 0.825\n",
      "Epoch 303 Batch   33/81   train_loss = 0.781\n",
      "Epoch 303 Batch   65/81   train_loss = 0.819\n",
      "Epoch 304 Batch   16/81   train_loss = 0.838\n",
      "Epoch 304 Batch   48/81   train_loss = 0.813\n",
      "Epoch 304 Batch   80/81   train_loss = 0.799\n",
      "Epoch 305 Batch   31/81   train_loss = 0.794\n",
      "Epoch 305 Batch   63/81   train_loss = 0.809\n",
      "Epoch 306 Batch   14/81   train_loss = 0.805\n",
      "Epoch 306 Batch   46/81   train_loss = 0.785\n",
      "Epoch 306 Batch   78/81   train_loss = 0.794\n",
      "Epoch 307 Batch   29/81   train_loss = 0.772\n",
      "Epoch 307 Batch   61/81   train_loss = 0.783\n",
      "Epoch 308 Batch   12/81   train_loss = 0.823\n",
      "Epoch 308 Batch   44/81   train_loss = 0.793\n",
      "Epoch 308 Batch   76/81   train_loss = 0.817\n",
      "Epoch 309 Batch   27/81   train_loss = 0.775\n",
      "Epoch 309 Batch   59/81   train_loss = 0.805\n",
      "Epoch 310 Batch   10/81   train_loss = 0.818\n",
      "Epoch 310 Batch   42/81   train_loss = 0.794\n",
      "Epoch 310 Batch   74/81   train_loss = 0.805\n",
      "Epoch 311 Batch   25/81   train_loss = 0.818\n",
      "Epoch 311 Batch   57/81   train_loss = 0.801\n",
      "Epoch 312 Batch    8/81   train_loss = 0.826\n",
      "Epoch 312 Batch   40/81   train_loss = 0.791\n",
      "Epoch 312 Batch   72/81   train_loss = 0.784\n",
      "Epoch 313 Batch   23/81   train_loss = 0.797\n",
      "Epoch 313 Batch   55/81   train_loss = 0.805\n",
      "Epoch 314 Batch    6/81   train_loss = 0.805\n",
      "Epoch 314 Batch   38/81   train_loss = 0.835\n",
      "Epoch 314 Batch   70/81   train_loss = 0.817\n",
      "Epoch 315 Batch   21/81   train_loss = 0.850\n",
      "Epoch 315 Batch   53/81   train_loss = 0.797\n",
      "Epoch 316 Batch    4/81   train_loss = 0.811\n",
      "Epoch 316 Batch   36/81   train_loss = 0.816\n",
      "Epoch 316 Batch   68/81   train_loss = 0.828\n",
      "Epoch 317 Batch   19/81   train_loss = 0.801\n",
      "Epoch 317 Batch   51/81   train_loss = 0.851\n",
      "Epoch 318 Batch    2/81   train_loss = 0.813\n",
      "Epoch 318 Batch   34/81   train_loss = 0.814\n",
      "Epoch 318 Batch   66/81   train_loss = 0.797\n",
      "Epoch 319 Batch   17/81   train_loss = 0.824\n",
      "Epoch 319 Batch   49/81   train_loss = 0.796\n",
      "Epoch 320 Batch    0/81   train_loss = 0.804\n",
      "Epoch 320 Batch   32/81   train_loss = 0.803\n",
      "Epoch 320 Batch   64/81   train_loss = 0.831\n",
      "Epoch 321 Batch   15/81   train_loss = 0.801\n",
      "Epoch 321 Batch   47/81   train_loss = 0.799\n",
      "Epoch 321 Batch   79/81   train_loss = 0.790\n",
      "Epoch 322 Batch   30/81   train_loss = 0.794\n",
      "Epoch 322 Batch   62/81   train_loss = 0.789\n",
      "Epoch 323 Batch   13/81   train_loss = 0.795\n",
      "Epoch 323 Batch   45/81   train_loss = 0.779\n",
      "Epoch 323 Batch   77/81   train_loss = 0.809\n",
      "Epoch 324 Batch   28/81   train_loss = 0.785\n",
      "Epoch 324 Batch   60/81   train_loss = 0.803\n",
      "Epoch 325 Batch   11/81   train_loss = 0.789\n",
      "Epoch 325 Batch   43/81   train_loss = 0.776\n",
      "Epoch 325 Batch   75/81   train_loss = 0.791\n",
      "Epoch 326 Batch   26/81   train_loss = 0.790\n",
      "Epoch 326 Batch   58/81   train_loss = 0.789\n",
      "Epoch 327 Batch    9/81   train_loss = 0.823\n",
      "Epoch 327 Batch   41/81   train_loss = 0.770\n",
      "Epoch 327 Batch   73/81   train_loss = 0.798\n",
      "Epoch 328 Batch   24/81   train_loss = 0.795\n",
      "Epoch 328 Batch   56/81   train_loss = 0.817\n",
      "Epoch 329 Batch    7/81   train_loss = 0.802\n",
      "Epoch 329 Batch   39/81   train_loss = 0.789\n",
      "Epoch 329 Batch   71/81   train_loss = 0.786\n",
      "Epoch 330 Batch   22/81   train_loss = 0.817\n",
      "Epoch 330 Batch   54/81   train_loss = 0.787\n",
      "Epoch 331 Batch    5/81   train_loss = 0.804\n",
      "Epoch 331 Batch   37/81   train_loss = 0.803\n",
      "Epoch 331 Batch   69/81   train_loss = 0.801\n",
      "Epoch 332 Batch   20/81   train_loss = 0.802\n",
      "Epoch 332 Batch   52/81   train_loss = 0.837\n",
      "Epoch 333 Batch    3/81   train_loss = 0.786\n",
      "Epoch 333 Batch   35/81   train_loss = 0.781\n",
      "Epoch 333 Batch   67/81   train_loss = 0.839\n",
      "Epoch 334 Batch   18/81   train_loss = 0.826\n",
      "Epoch 334 Batch   50/81   train_loss = 0.786\n",
      "Epoch 335 Batch    1/81   train_loss = 0.818\n",
      "Epoch 335 Batch   33/81   train_loss = 0.774\n",
      "Epoch 335 Batch   65/81   train_loss = 0.815\n",
      "Epoch 336 Batch   16/81   train_loss = 0.831\n",
      "Epoch 336 Batch   48/81   train_loss = 0.800\n",
      "Epoch 336 Batch   80/81   train_loss = 0.809\n",
      "Epoch 337 Batch   31/81   train_loss = 0.783\n",
      "Epoch 337 Batch   63/81   train_loss = 0.804\n",
      "Epoch 338 Batch   14/81   train_loss = 0.809\n",
      "Epoch 338 Batch   46/81   train_loss = 0.799\n",
      "Epoch 338 Batch   78/81   train_loss = 0.805\n",
      "Epoch 339 Batch   29/81   train_loss = 0.767\n",
      "Epoch 339 Batch   61/81   train_loss = 0.811\n",
      "Epoch 340 Batch   12/81   train_loss = 0.822\n",
      "Epoch 340 Batch   44/81   train_loss = 0.785\n",
      "Epoch 340 Batch   76/81   train_loss = 0.810\n",
      "Epoch 341 Batch   27/81   train_loss = 0.776\n",
      "Epoch 341 Batch   59/81   train_loss = 0.790\n",
      "Epoch 342 Batch   10/81   train_loss = 0.808\n",
      "Epoch 342 Batch   42/81   train_loss = 0.767\n",
      "Epoch 342 Batch   74/81   train_loss = 0.798\n",
      "Epoch 343 Batch   25/81   train_loss = 0.778\n",
      "Epoch 343 Batch   57/81   train_loss = 0.791\n",
      "Epoch 344 Batch    8/81   train_loss = 0.812\n",
      "Epoch 344 Batch   40/81   train_loss = 0.774\n",
      "Epoch 344 Batch   72/81   train_loss = 0.786\n",
      "Epoch 345 Batch   23/81   train_loss = 0.793\n",
      "Epoch 345 Batch   55/81   train_loss = 0.790\n",
      "Epoch 346 Batch    6/81   train_loss = 0.789\n",
      "Epoch 346 Batch   38/81   train_loss = 0.809\n",
      "Epoch 346 Batch   70/81   train_loss = 0.778\n",
      "Epoch 347 Batch   21/81   train_loss = 0.827\n",
      "Epoch 347 Batch   53/81   train_loss = 0.771\n",
      "Epoch 348 Batch    4/81   train_loss = 0.798\n",
      "Epoch 348 Batch   36/81   train_loss = 0.801\n",
      "Epoch 348 Batch   68/81   train_loss = 0.803\n",
      "Epoch 349 Batch   19/81   train_loss = 0.763\n",
      "Epoch 349 Batch   51/81   train_loss = 0.819\n",
      "Epoch 350 Batch    2/81   train_loss = 0.813\n",
      "Epoch 350 Batch   34/81   train_loss = 0.793\n",
      "Epoch 350 Batch   66/81   train_loss = 0.789\n",
      "Epoch 351 Batch   17/81   train_loss = 0.804\n",
      "Epoch 351 Batch   49/81   train_loss = 0.808\n",
      "Epoch 352 Batch    0/81   train_loss = 0.800\n",
      "Epoch 352 Batch   32/81   train_loss = 0.771\n",
      "Epoch 352 Batch   64/81   train_loss = 0.794\n",
      "Epoch 353 Batch   15/81   train_loss = 0.795\n",
      "Epoch 353 Batch   47/81   train_loss = 0.789\n",
      "Epoch 353 Batch   79/81   train_loss = 0.788\n",
      "Epoch 354 Batch   30/81   train_loss = 0.783\n",
      "Epoch 354 Batch   62/81   train_loss = 0.786\n",
      "Epoch 355 Batch   13/81   train_loss = 0.790\n",
      "Epoch 355 Batch   45/81   train_loss = 0.780\n",
      "Epoch 355 Batch   77/81   train_loss = 0.795\n",
      "Epoch 356 Batch   28/81   train_loss = 0.770\n",
      "Epoch 356 Batch   60/81   train_loss = 0.794\n",
      "Epoch 357 Batch   11/81   train_loss = 0.776\n",
      "Epoch 357 Batch   43/81   train_loss = 0.768\n",
      "Epoch 357 Batch   75/81   train_loss = 0.781\n",
      "Epoch 358 Batch   26/81   train_loss = 0.798\n",
      "Epoch 358 Batch   58/81   train_loss = 0.774\n",
      "Epoch 359 Batch    9/81   train_loss = 0.812\n",
      "Epoch 359 Batch   41/81   train_loss = 0.766\n",
      "Epoch 359 Batch   73/81   train_loss = 0.784\n",
      "Epoch 360 Batch   24/81   train_loss = 0.789\n",
      "Epoch 360 Batch   56/81   train_loss = 0.798\n",
      "Epoch 361 Batch    7/81   train_loss = 0.789\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 361 Batch   39/81   train_loss = 0.769\n",
      "Epoch 361 Batch   71/81   train_loss = 0.764\n",
      "Epoch 362 Batch   22/81   train_loss = 0.808\n",
      "Epoch 362 Batch   54/81   train_loss = 0.799\n",
      "Epoch 363 Batch    5/81   train_loss = 0.791\n",
      "Epoch 363 Batch   37/81   train_loss = 0.786\n",
      "Epoch 363 Batch   69/81   train_loss = 0.806\n",
      "Epoch 364 Batch   20/81   train_loss = 0.795\n",
      "Epoch 364 Batch   52/81   train_loss = 0.802\n",
      "Epoch 365 Batch    3/81   train_loss = 0.773\n",
      "Epoch 365 Batch   35/81   train_loss = 0.785\n",
      "Epoch 365 Batch   67/81   train_loss = 0.827\n",
      "Epoch 366 Batch   18/81   train_loss = 0.811\n",
      "Epoch 366 Batch   50/81   train_loss = 0.775\n",
      "Epoch 367 Batch    1/81   train_loss = 0.801\n",
      "Epoch 367 Batch   33/81   train_loss = 0.793\n",
      "Epoch 367 Batch   65/81   train_loss = 0.822\n",
      "Epoch 368 Batch   16/81   train_loss = 0.843\n",
      "Epoch 368 Batch   48/81   train_loss = 0.790\n",
      "Epoch 368 Batch   80/81   train_loss = 0.811\n",
      "Epoch 369 Batch   31/81   train_loss = 0.782\n",
      "Epoch 369 Batch   63/81   train_loss = 0.792\n",
      "Epoch 370 Batch   14/81   train_loss = 0.802\n",
      "Epoch 370 Batch   46/81   train_loss = 0.797\n",
      "Epoch 370 Batch   78/81   train_loss = 0.795\n",
      "Epoch 371 Batch   29/81   train_loss = 0.767\n",
      "Epoch 371 Batch   61/81   train_loss = 0.802\n",
      "Epoch 372 Batch   12/81   train_loss = 0.819\n",
      "Epoch 372 Batch   44/81   train_loss = 0.787\n",
      "Epoch 372 Batch   76/81   train_loss = 0.799\n",
      "Epoch 373 Batch   27/81   train_loss = 0.769\n",
      "Epoch 373 Batch   59/81   train_loss = 0.795\n",
      "Epoch 374 Batch   10/81   train_loss = 0.815\n",
      "Epoch 374 Batch   42/81   train_loss = 0.793\n",
      "Epoch 374 Batch   74/81   train_loss = 0.800\n",
      "Epoch 375 Batch   25/81   train_loss = 0.797\n",
      "Epoch 375 Batch   57/81   train_loss = 0.821\n",
      "Epoch 376 Batch    8/81   train_loss = 0.819\n",
      "Epoch 376 Batch   40/81   train_loss = 0.810\n",
      "Epoch 376 Batch   72/81   train_loss = 0.771\n",
      "Epoch 377 Batch   23/81   train_loss = 0.776\n",
      "Epoch 377 Batch   55/81   train_loss = 0.798\n",
      "Epoch 378 Batch    6/81   train_loss = 0.802\n",
      "Epoch 378 Batch   38/81   train_loss = 0.839\n",
      "Epoch 378 Batch   70/81   train_loss = 0.790\n",
      "Epoch 379 Batch   21/81   train_loss = 0.844\n",
      "Epoch 379 Batch   53/81   train_loss = 0.785\n",
      "Epoch 380 Batch    4/81   train_loss = 0.800\n",
      "Epoch 380 Batch   36/81   train_loss = 0.815\n",
      "Epoch 380 Batch   68/81   train_loss = 0.820\n",
      "Epoch 381 Batch   19/81   train_loss = 0.772\n",
      "Epoch 381 Batch   51/81   train_loss = 0.816\n",
      "Epoch 382 Batch    2/81   train_loss = 0.814\n",
      "Epoch 382 Batch   34/81   train_loss = 0.811\n",
      "Epoch 382 Batch   66/81   train_loss = 0.803\n",
      "Epoch 383 Batch   17/81   train_loss = 0.808\n",
      "Epoch 383 Batch   49/81   train_loss = 0.798\n",
      "Epoch 384 Batch    0/81   train_loss = 0.805\n",
      "Epoch 384 Batch   32/81   train_loss = 0.777\n",
      "Epoch 384 Batch   64/81   train_loss = 0.816\n",
      "Epoch 385 Batch   15/81   train_loss = 0.796\n",
      "Epoch 385 Batch   47/81   train_loss = 0.788\n",
      "Epoch 385 Batch   79/81   train_loss = 0.802\n",
      "Epoch 386 Batch   30/81   train_loss = 0.798\n",
      "Epoch 386 Batch   62/81   train_loss = 0.783\n",
      "Epoch 387 Batch   13/81   train_loss = 0.791\n",
      "Epoch 387 Batch   45/81   train_loss = 0.787\n",
      "Epoch 387 Batch   77/81   train_loss = 0.813\n",
      "Epoch 388 Batch   28/81   train_loss = 0.775\n",
      "Epoch 388 Batch   60/81   train_loss = 0.798\n",
      "Epoch 389 Batch   11/81   train_loss = 0.805\n",
      "Epoch 389 Batch   43/81   train_loss = 0.754\n",
      "Epoch 389 Batch   75/81   train_loss = 0.778\n",
      "Epoch 390 Batch   26/81   train_loss = 0.788\n",
      "Epoch 390 Batch   58/81   train_loss = 0.780\n",
      "Epoch 391 Batch    9/81   train_loss = 0.797\n",
      "Epoch 391 Batch   41/81   train_loss = 0.755\n",
      "Epoch 391 Batch   73/81   train_loss = 0.803\n",
      "Epoch 392 Batch   24/81   train_loss = 0.778\n",
      "Epoch 392 Batch   56/81   train_loss = 0.813\n",
      "Epoch 393 Batch    7/81   train_loss = 0.803\n",
      "Epoch 393 Batch   39/81   train_loss = 0.767\n",
      "Epoch 393 Batch   71/81   train_loss = 0.770\n",
      "Epoch 394 Batch   22/81   train_loss = 0.818\n",
      "Epoch 394 Batch   54/81   train_loss = 0.781\n",
      "Epoch 395 Batch    5/81   train_loss = 0.784\n",
      "Epoch 395 Batch   37/81   train_loss = 0.796\n",
      "Epoch 395 Batch   69/81   train_loss = 0.805\n",
      "Epoch 396 Batch   20/81   train_loss = 0.794\n",
      "Epoch 396 Batch   52/81   train_loss = 0.815\n",
      "Epoch 397 Batch    3/81   train_loss = 0.774\n",
      "Epoch 397 Batch   35/81   train_loss = 0.768\n",
      "Epoch 397 Batch   67/81   train_loss = 0.827\n",
      "Epoch 398 Batch   18/81   train_loss = 0.805\n",
      "Epoch 398 Batch   50/81   train_loss = 0.778\n",
      "Epoch 399 Batch    1/81   train_loss = 0.803\n",
      "Epoch 399 Batch   33/81   train_loss = 0.771\n",
      "Epoch 399 Batch   65/81   train_loss = 0.827\n",
      "Epoch 400 Batch   16/81   train_loss = 0.822\n",
      "Epoch 400 Batch   48/81   train_loss = 0.779\n",
      "Epoch 400 Batch   80/81   train_loss = 0.791\n",
      "Epoch 401 Batch   31/81   train_loss = 0.757\n",
      "Epoch 401 Batch   63/81   train_loss = 0.779\n",
      "Epoch 402 Batch   14/81   train_loss = 0.794\n",
      "Epoch 402 Batch   46/81   train_loss = 0.799\n",
      "Epoch 402 Batch   78/81   train_loss = 0.785\n",
      "Epoch 403 Batch   29/81   train_loss = 0.770\n",
      "Epoch 403 Batch   61/81   train_loss = 0.791\n",
      "Epoch 404 Batch   12/81   train_loss = 0.806\n",
      "Epoch 404 Batch   44/81   train_loss = 0.775\n",
      "Epoch 404 Batch   76/81   train_loss = 0.793\n",
      "Epoch 405 Batch   27/81   train_loss = 0.750\n",
      "Epoch 405 Batch   59/81   train_loss = 0.778\n",
      "Epoch 406 Batch   10/81   train_loss = 0.792\n",
      "Epoch 406 Batch   42/81   train_loss = 0.784\n",
      "Epoch 406 Batch   74/81   train_loss = 0.791\n",
      "Epoch 407 Batch   25/81   train_loss = 0.793\n",
      "Epoch 407 Batch   57/81   train_loss = 0.793\n",
      "Epoch 408 Batch    8/81   train_loss = 0.810\n",
      "Epoch 408 Batch   40/81   train_loss = 0.781\n",
      "Epoch 408 Batch   72/81   train_loss = 0.776\n",
      "Epoch 409 Batch   23/81   train_loss = 0.779\n",
      "Epoch 409 Batch   55/81   train_loss = 0.789\n",
      "Epoch 410 Batch    6/81   train_loss = 0.777\n",
      "Epoch 410 Batch   38/81   train_loss = 0.814\n",
      "Epoch 410 Batch   70/81   train_loss = 0.784\n",
      "Epoch 411 Batch   21/81   train_loss = 0.835\n",
      "Epoch 411 Batch   53/81   train_loss = 0.775\n",
      "Epoch 412 Batch    4/81   train_loss = 0.807\n",
      "Epoch 412 Batch   36/81   train_loss = 0.806\n",
      "Epoch 412 Batch   68/81   train_loss = 0.803\n",
      "Epoch 413 Batch   19/81   train_loss = 0.764\n",
      "Epoch 413 Batch   51/81   train_loss = 0.833\n",
      "Epoch 414 Batch    2/81   train_loss = 0.803\n",
      "Epoch 414 Batch   34/81   train_loss = 0.817\n",
      "Epoch 414 Batch   66/81   train_loss = 0.802\n",
      "Epoch 415 Batch   17/81   train_loss = 0.806\n",
      "Epoch 415 Batch   49/81   train_loss = 0.793\n",
      "Epoch 416 Batch    0/81   train_loss = 0.811\n",
      "Epoch 416 Batch   32/81   train_loss = 0.776\n",
      "Epoch 416 Batch   64/81   train_loss = 0.814\n",
      "Epoch 417 Batch   15/81   train_loss = 0.794\n",
      "Epoch 417 Batch   47/81   train_loss = 0.804\n",
      "Epoch 417 Batch   79/81   train_loss = 0.790\n",
      "Epoch 418 Batch   30/81   train_loss = 0.796\n",
      "Epoch 418 Batch   62/81   train_loss = 0.782\n",
      "Epoch 419 Batch   13/81   train_loss = 0.808\n",
      "Epoch 419 Batch   45/81   train_loss = 0.786\n",
      "Epoch 419 Batch   77/81   train_loss = 0.814\n",
      "Epoch 420 Batch   28/81   train_loss = 0.756\n",
      "Epoch 420 Batch   60/81   train_loss = 0.783\n",
      "Epoch 421 Batch   11/81   train_loss = 0.800\n",
      "Epoch 421 Batch   43/81   train_loss = 0.765\n",
      "Epoch 421 Batch   75/81   train_loss = 0.768\n",
      "Epoch 422 Batch   26/81   train_loss = 0.791\n",
      "Epoch 422 Batch   58/81   train_loss = 0.778\n",
      "Epoch 423 Batch    9/81   train_loss = 0.797\n",
      "Epoch 423 Batch   41/81   train_loss = 0.757\n",
      "Epoch 423 Batch   73/81   train_loss = 0.799\n",
      "Epoch 424 Batch   24/81   train_loss = 0.774\n",
      "Epoch 424 Batch   56/81   train_loss = 0.808\n",
      "Epoch 425 Batch    7/81   train_loss = 0.799\n",
      "Epoch 425 Batch   39/81   train_loss = 0.760\n",
      "Epoch 425 Batch   71/81   train_loss = 0.771\n",
      "Epoch 426 Batch   22/81   train_loss = 0.807\n",
      "Epoch 426 Batch   54/81   train_loss = 0.774\n",
      "Epoch 427 Batch    5/81   train_loss = 0.809\n",
      "Epoch 427 Batch   37/81   train_loss = 0.784\n",
      "Epoch 427 Batch   69/81   train_loss = 0.793\n",
      "Epoch 428 Batch   20/81   train_loss = 0.791\n",
      "Epoch 428 Batch   52/81   train_loss = 0.796\n",
      "Epoch 429 Batch    3/81   train_loss = 0.767\n",
      "Epoch 429 Batch   35/81   train_loss = 0.770\n",
      "Epoch 429 Batch   67/81   train_loss = 0.826\n",
      "Epoch 430 Batch   18/81   train_loss = 0.817\n",
      "Epoch 430 Batch   50/81   train_loss = 0.772\n",
      "Epoch 431 Batch    1/81   train_loss = 0.799\n",
      "Epoch 431 Batch   33/81   train_loss = 0.777\n",
      "Epoch 431 Batch   65/81   train_loss = 0.803\n",
      "Epoch 432 Batch   16/81   train_loss = 0.826\n",
      "Epoch 432 Batch   48/81   train_loss = 0.785\n",
      "Epoch 432 Batch   80/81   train_loss = 0.805\n",
      "Epoch 433 Batch   31/81   train_loss = 0.764\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 433 Batch   63/81   train_loss = 0.788\n",
      "Epoch 434 Batch   14/81   train_loss = 0.801\n",
      "Epoch 434 Batch   46/81   train_loss = 0.802\n",
      "Epoch 434 Batch   78/81   train_loss = 0.787\n",
      "Epoch 435 Batch   29/81   train_loss = 0.760\n",
      "Epoch 435 Batch   61/81   train_loss = 0.773\n",
      "Epoch 436 Batch   12/81   train_loss = 0.796\n",
      "Epoch 436 Batch   44/81   train_loss = 0.786\n",
      "Epoch 436 Batch   76/81   train_loss = 0.792\n",
      "Epoch 437 Batch   27/81   train_loss = 0.758\n",
      "Epoch 437 Batch   59/81   train_loss = 0.780\n",
      "Epoch 438 Batch   10/81   train_loss = 0.802\n",
      "Epoch 438 Batch   42/81   train_loss = 0.774\n",
      "Epoch 438 Batch   74/81   train_loss = 0.784\n",
      "Epoch 439 Batch   25/81   train_loss = 0.790\n",
      "Epoch 439 Batch   57/81   train_loss = 0.803\n",
      "Epoch 440 Batch    8/81   train_loss = 0.792\n",
      "Epoch 440 Batch   40/81   train_loss = 0.783\n",
      "Epoch 440 Batch   72/81   train_loss = 0.782\n",
      "Epoch 441 Batch   23/81   train_loss = 0.786\n",
      "Epoch 441 Batch   55/81   train_loss = 0.796\n",
      "Epoch 442 Batch    6/81   train_loss = 0.780\n",
      "Epoch 442 Batch   38/81   train_loss = 0.815\n",
      "Epoch 442 Batch   70/81   train_loss = 0.794\n",
      "Epoch 443 Batch   21/81   train_loss = 0.826\n",
      "Epoch 443 Batch   53/81   train_loss = 0.779\n",
      "Epoch 444 Batch    4/81   train_loss = 0.801\n",
      "Epoch 444 Batch   36/81   train_loss = 0.800\n",
      "Epoch 444 Batch   68/81   train_loss = 0.815\n",
      "Epoch 445 Batch   19/81   train_loss = 0.754\n",
      "Epoch 445 Batch   51/81   train_loss = 0.827\n",
      "Epoch 446 Batch    2/81   train_loss = 0.805\n",
      "Epoch 446 Batch   34/81   train_loss = 0.800\n",
      "Epoch 446 Batch   66/81   train_loss = 0.796\n",
      "Epoch 447 Batch   17/81   train_loss = 0.805\n",
      "Epoch 447 Batch   49/81   train_loss = 0.794\n",
      "Epoch 448 Batch    0/81   train_loss = 0.803\n",
      "Epoch 448 Batch   32/81   train_loss = 0.766\n",
      "Epoch 448 Batch   64/81   train_loss = 0.821\n",
      "Epoch 449 Batch   15/81   train_loss = 0.797\n",
      "Epoch 449 Batch   47/81   train_loss = 0.795\n",
      "Epoch 449 Batch   79/81   train_loss = 0.783\n",
      "Epoch 450 Batch   30/81   train_loss = 0.790\n",
      "Epoch 450 Batch   62/81   train_loss = 0.782\n",
      "Epoch 451 Batch   13/81   train_loss = 0.807\n",
      "Epoch 451 Batch   45/81   train_loss = 0.775\n",
      "Epoch 451 Batch   77/81   train_loss = 0.810\n",
      "Epoch 452 Batch   28/81   train_loss = 0.757\n",
      "Epoch 452 Batch   60/81   train_loss = 0.782\n",
      "Epoch 453 Batch   11/81   train_loss = 0.804\n",
      "Epoch 453 Batch   43/81   train_loss = 0.763\n",
      "Epoch 453 Batch   75/81   train_loss = 0.780\n",
      "Epoch 454 Batch   26/81   train_loss = 0.775\n",
      "Epoch 454 Batch   58/81   train_loss = 0.771\n",
      "Epoch 455 Batch    9/81   train_loss = 0.821\n",
      "Epoch 455 Batch   41/81   train_loss = 0.769\n",
      "Epoch 455 Batch   73/81   train_loss = 0.802\n",
      "Epoch 456 Batch   24/81   train_loss = 0.795\n",
      "Epoch 456 Batch   56/81   train_loss = 0.805\n",
      "Epoch 457 Batch    7/81   train_loss = 0.810\n",
      "Epoch 457 Batch   39/81   train_loss = 0.745\n",
      "Epoch 457 Batch   71/81   train_loss = 0.772\n",
      "Epoch 458 Batch   22/81   train_loss = 0.818\n",
      "Epoch 458 Batch   54/81   train_loss = 0.778\n",
      "Epoch 459 Batch    5/81   train_loss = 0.791\n",
      "Epoch 459 Batch   37/81   train_loss = 0.794\n",
      "Epoch 459 Batch   69/81   train_loss = 0.798\n",
      "Epoch 460 Batch   20/81   train_loss = 0.789\n",
      "Epoch 460 Batch   52/81   train_loss = 0.806\n",
      "Epoch 461 Batch    3/81   train_loss = 0.787\n",
      "Epoch 461 Batch   35/81   train_loss = 0.777\n",
      "Epoch 461 Batch   67/81   train_loss = 0.819\n",
      "Epoch 462 Batch   18/81   train_loss = 0.814\n",
      "Epoch 462 Batch   50/81   train_loss = 0.772\n",
      "Epoch 463 Batch    1/81   train_loss = 0.805\n",
      "Epoch 463 Batch   33/81   train_loss = 0.779\n",
      "Epoch 463 Batch   65/81   train_loss = 0.821\n",
      "Epoch 464 Batch   16/81   train_loss = 0.841\n",
      "Epoch 464 Batch   48/81   train_loss = 0.804\n",
      "Epoch 464 Batch   80/81   train_loss = 0.807\n",
      "Epoch 465 Batch   31/81   train_loss = 0.769\n",
      "Epoch 465 Batch   63/81   train_loss = 0.812\n",
      "Epoch 466 Batch   14/81   train_loss = 0.802\n",
      "Epoch 466 Batch   46/81   train_loss = 0.803\n",
      "Epoch 466 Batch   78/81   train_loss = 0.803\n",
      "Epoch 467 Batch   29/81   train_loss = 0.786\n",
      "Epoch 467 Batch   61/81   train_loss = 0.785\n",
      "Epoch 468 Batch   12/81   train_loss = 0.814\n",
      "Epoch 468 Batch   44/81   train_loss = 0.805\n",
      "Epoch 468 Batch   76/81   train_loss = 0.791\n",
      "Epoch 469 Batch   27/81   train_loss = 0.766\n",
      "Epoch 469 Batch   59/81   train_loss = 0.789\n",
      "Epoch 470 Batch   10/81   train_loss = 0.809\n",
      "Epoch 470 Batch   42/81   train_loss = 0.771\n",
      "Epoch 470 Batch   74/81   train_loss = 0.799\n",
      "Epoch 471 Batch   25/81   train_loss = 0.794\n",
      "Epoch 471 Batch   57/81   train_loss = 0.795\n",
      "Epoch 472 Batch    8/81   train_loss = 0.800\n",
      "Epoch 472 Batch   40/81   train_loss = 0.782\n",
      "Epoch 472 Batch   72/81   train_loss = 0.777\n",
      "Epoch 473 Batch   23/81   train_loss = 0.780\n",
      "Epoch 473 Batch   55/81   train_loss = 0.813\n",
      "Epoch 474 Batch    6/81   train_loss = 0.780\n",
      "Epoch 474 Batch   38/81   train_loss = 0.812\n",
      "Epoch 474 Batch   70/81   train_loss = 0.802\n",
      "Epoch 475 Batch   21/81   train_loss = 0.832\n",
      "Epoch 475 Batch   53/81   train_loss = 0.782\n",
      "Epoch 476 Batch    4/81   train_loss = 0.817\n",
      "Epoch 476 Batch   36/81   train_loss = 0.821\n",
      "Epoch 476 Batch   68/81   train_loss = 0.816\n",
      "Epoch 477 Batch   19/81   train_loss = 0.781\n",
      "Epoch 477 Batch   51/81   train_loss = 0.852\n",
      "Epoch 478 Batch    2/81   train_loss = 0.829\n",
      "Epoch 478 Batch   34/81   train_loss = 0.817\n",
      "Epoch 478 Batch   66/81   train_loss = 0.844\n",
      "Epoch 479 Batch   17/81   train_loss = 0.825\n",
      "Epoch 479 Batch   49/81   train_loss = 0.832\n",
      "Epoch 480 Batch    0/81   train_loss = 0.807\n",
      "Epoch 480 Batch   32/81   train_loss = 0.786\n",
      "Epoch 480 Batch   64/81   train_loss = 0.808\n",
      "Epoch 481 Batch   15/81   train_loss = 0.801\n",
      "Epoch 481 Batch   47/81   train_loss = 0.791\n",
      "Epoch 481 Batch   79/81   train_loss = 0.818\n",
      "Epoch 482 Batch   30/81   train_loss = 0.799\n",
      "Epoch 482 Batch   62/81   train_loss = 0.794\n",
      "Epoch 483 Batch   13/81   train_loss = 0.804\n",
      "Epoch 483 Batch   45/81   train_loss = 0.809\n",
      "Epoch 483 Batch   77/81   train_loss = 0.809\n",
      "Epoch 484 Batch   28/81   train_loss = 0.776\n",
      "Epoch 484 Batch   60/81   train_loss = 0.784\n",
      "Epoch 485 Batch   11/81   train_loss = 0.809\n",
      "Epoch 485 Batch   43/81   train_loss = 0.761\n",
      "Epoch 485 Batch   75/81   train_loss = 0.782\n",
      "Epoch 486 Batch   26/81   train_loss = 0.797\n",
      "Epoch 486 Batch   58/81   train_loss = 0.770\n",
      "Epoch 487 Batch    9/81   train_loss = 0.836\n",
      "Epoch 487 Batch   41/81   train_loss = 0.755\n",
      "Epoch 487 Batch   73/81   train_loss = 0.795\n",
      "Epoch 488 Batch   24/81   train_loss = 0.795\n",
      "Epoch 488 Batch   56/81   train_loss = 0.811\n",
      "Epoch 489 Batch    7/81   train_loss = 0.795\n",
      "Epoch 489 Batch   39/81   train_loss = 0.755\n",
      "Epoch 489 Batch   71/81   train_loss = 0.769\n",
      "Epoch 490 Batch   22/81   train_loss = 0.810\n",
      "Epoch 490 Batch   54/81   train_loss = 0.785\n",
      "Epoch 491 Batch    5/81   train_loss = 0.795\n",
      "Epoch 491 Batch   37/81   train_loss = 0.795\n",
      "Epoch 491 Batch   69/81   train_loss = 0.793\n",
      "Epoch 492 Batch   20/81   train_loss = 0.796\n",
      "Epoch 492 Batch   52/81   train_loss = 0.796\n",
      "Epoch 493 Batch    3/81   train_loss = 0.797\n",
      "Epoch 493 Batch   35/81   train_loss = 0.770\n",
      "Epoch 493 Batch   67/81   train_loss = 0.817\n",
      "Epoch 494 Batch   18/81   train_loss = 0.815\n",
      "Epoch 494 Batch   50/81   train_loss = 0.763\n",
      "Epoch 495 Batch    1/81   train_loss = 0.805\n",
      "Epoch 495 Batch   33/81   train_loss = 0.782\n",
      "Epoch 495 Batch   65/81   train_loss = 0.821\n",
      "Epoch 496 Batch   16/81   train_loss = 0.830\n",
      "Epoch 496 Batch   48/81   train_loss = 0.786\n",
      "Epoch 496 Batch   80/81   train_loss = 0.809\n",
      "Epoch 497 Batch   31/81   train_loss = 0.764\n",
      "Epoch 497 Batch   63/81   train_loss = 0.774\n",
      "Epoch 498 Batch   14/81   train_loss = 0.805\n",
      "Epoch 498 Batch   46/81   train_loss = 0.785\n",
      "Epoch 498 Batch   78/81   train_loss = 0.785\n",
      "Epoch 499 Batch   29/81   train_loss = 0.746\n",
      "Epoch 499 Batch   61/81   train_loss = 0.791\n",
      "Epoch 500 Batch   12/81   train_loss = 0.785\n",
      "Epoch 500 Batch   44/81   train_loss = 0.782\n",
      "Epoch 500 Batch   76/81   train_loss = 0.802\n",
      "Epoch 501 Batch   27/81   train_loss = 0.767\n",
      "Epoch 501 Batch   59/81   train_loss = 0.770\n",
      "Epoch 502 Batch   10/81   train_loss = 0.805\n",
      "Epoch 502 Batch   42/81   train_loss = 0.769\n",
      "Epoch 502 Batch   74/81   train_loss = 0.776\n",
      "Epoch 503 Batch   25/81   train_loss = 0.806\n",
      "Epoch 503 Batch   57/81   train_loss = 0.792\n",
      "Epoch 504 Batch    8/81   train_loss = 0.785\n",
      "Epoch 504 Batch   40/81   train_loss = 0.771\n",
      "Epoch 504 Batch   72/81   train_loss = 0.775\n",
      "Epoch 505 Batch   23/81   train_loss = 0.767\n",
      "Epoch 505 Batch   55/81   train_loss = 0.808\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 506 Batch    6/81   train_loss = 0.784\n",
      "Epoch 506 Batch   38/81   train_loss = 0.822\n",
      "Epoch 506 Batch   70/81   train_loss = 0.788\n",
      "Epoch 507 Batch   21/81   train_loss = 0.823\n",
      "Epoch 507 Batch   53/81   train_loss = 0.779\n",
      "Epoch 508 Batch    4/81   train_loss = 0.809\n",
      "Epoch 508 Batch   36/81   train_loss = 0.800\n",
      "Epoch 508 Batch   68/81   train_loss = 0.815\n",
      "Epoch 509 Batch   19/81   train_loss = 0.763\n",
      "Epoch 509 Batch   51/81   train_loss = 0.837\n",
      "Epoch 510 Batch    2/81   train_loss = 0.797\n",
      "Epoch 510 Batch   34/81   train_loss = 0.806\n",
      "Epoch 510 Batch   66/81   train_loss = 0.817\n",
      "Epoch 511 Batch   17/81   train_loss = 0.803\n",
      "Epoch 511 Batch   49/81   train_loss = 0.812\n",
      "Epoch 512 Batch    0/81   train_loss = 0.803\n",
      "Epoch 512 Batch   32/81   train_loss = 0.774\n",
      "Epoch 512 Batch   64/81   train_loss = 0.811\n",
      "Epoch 513 Batch   15/81   train_loss = 0.797\n",
      "Epoch 513 Batch   47/81   train_loss = 0.789\n",
      "Epoch 513 Batch   79/81   train_loss = 0.777\n",
      "Epoch 514 Batch   30/81   train_loss = 0.779\n",
      "Epoch 514 Batch   62/81   train_loss = 0.781\n",
      "Epoch 515 Batch   13/81   train_loss = 0.799\n",
      "Epoch 515 Batch   45/81   train_loss = 0.780\n",
      "Epoch 515 Batch   77/81   train_loss = 0.817\n",
      "Epoch 516 Batch   28/81   train_loss = 0.758\n",
      "Epoch 516 Batch   60/81   train_loss = 0.790\n",
      "Epoch 517 Batch   11/81   train_loss = 0.801\n",
      "Epoch 517 Batch   43/81   train_loss = 0.769\n",
      "Epoch 517 Batch   75/81   train_loss = 0.781\n",
      "Epoch 518 Batch   26/81   train_loss = 0.786\n",
      "Epoch 518 Batch   58/81   train_loss = 0.773\n",
      "Epoch 519 Batch    9/81   train_loss = 0.817\n",
      "Epoch 519 Batch   41/81   train_loss = 0.756\n",
      "Epoch 519 Batch   73/81   train_loss = 0.809\n",
      "Epoch 520 Batch   24/81   train_loss = 0.785\n",
      "Epoch 520 Batch   56/81   train_loss = 0.806\n",
      "Epoch 521 Batch    7/81   train_loss = 0.796\n",
      "Epoch 521 Batch   39/81   train_loss = 0.756\n",
      "Epoch 521 Batch   71/81   train_loss = 0.784\n",
      "Epoch 522 Batch   22/81   train_loss = 0.823\n",
      "Epoch 522 Batch   54/81   train_loss = 0.776\n",
      "Epoch 523 Batch    5/81   train_loss = 0.825\n",
      "Epoch 523 Batch   37/81   train_loss = 0.812\n",
      "Epoch 523 Batch   69/81   train_loss = 0.796\n",
      "Epoch 524 Batch   20/81   train_loss = 0.798\n",
      "Epoch 524 Batch   52/81   train_loss = 0.829\n",
      "Epoch 525 Batch    3/81   train_loss = 0.783\n",
      "Epoch 525 Batch   35/81   train_loss = 0.784\n",
      "Epoch 525 Batch   67/81   train_loss = 0.834\n",
      "Epoch 526 Batch   18/81   train_loss = 0.819\n",
      "Epoch 526 Batch   50/81   train_loss = 0.782\n",
      "Epoch 527 Batch    1/81   train_loss = 0.812\n",
      "Epoch 527 Batch   33/81   train_loss = 0.792\n",
      "Epoch 527 Batch   65/81   train_loss = 0.818\n",
      "Epoch 528 Batch   16/81   train_loss = 0.854\n",
      "Epoch 528 Batch   48/81   train_loss = 0.810\n",
      "Epoch 528 Batch   80/81   train_loss = 0.818\n",
      "Epoch 529 Batch   31/81   train_loss = 0.770\n",
      "Epoch 529 Batch   63/81   train_loss = 0.817\n",
      "Epoch 530 Batch   14/81   train_loss = 0.814\n",
      "Epoch 530 Batch   46/81   train_loss = 0.793\n",
      "Epoch 530 Batch   78/81   train_loss = 0.798\n",
      "Epoch 531 Batch   29/81   train_loss = 0.814\n",
      "Epoch 531 Batch   61/81   train_loss = 0.808\n",
      "Epoch 532 Batch   12/81   train_loss = 0.813\n",
      "Epoch 532 Batch   44/81   train_loss = 0.826\n",
      "Epoch 532 Batch   76/81   train_loss = 0.844\n",
      "Epoch 533 Batch   27/81   train_loss = 0.779\n",
      "Epoch 533 Batch   59/81   train_loss = 0.802\n",
      "Epoch 534 Batch   10/81   train_loss = 0.837\n",
      "Epoch 534 Batch   42/81   train_loss = 0.793\n",
      "Epoch 534 Batch   74/81   train_loss = 0.802\n",
      "Epoch 535 Batch   25/81   train_loss = 0.815\n",
      "Epoch 535 Batch   57/81   train_loss = 0.846\n",
      "Epoch 536 Batch    8/81   train_loss = 0.803\n",
      "Epoch 536 Batch   40/81   train_loss = 0.787\n",
      "Epoch 536 Batch   72/81   train_loss = 0.777\n",
      "Epoch 537 Batch   23/81   train_loss = 0.795\n",
      "Epoch 537 Batch   55/81   train_loss = 0.802\n",
      "Epoch 538 Batch    6/81   train_loss = 0.792\n",
      "Epoch 538 Batch   38/81   train_loss = 0.813\n",
      "Epoch 538 Batch   70/81   train_loss = 0.805\n",
      "Epoch 539 Batch   21/81   train_loss = 0.831\n",
      "Epoch 539 Batch   53/81   train_loss = 0.785\n",
      "Epoch 540 Batch    4/81   train_loss = 0.796\n",
      "Epoch 540 Batch   36/81   train_loss = 0.815\n",
      "Epoch 540 Batch   68/81   train_loss = 0.802\n",
      "Epoch 541 Batch   19/81   train_loss = 0.782\n",
      "Epoch 541 Batch   51/81   train_loss = 0.838\n",
      "Epoch 542 Batch    2/81   train_loss = 0.814\n",
      "Epoch 542 Batch   34/81   train_loss = 0.806\n",
      "Epoch 542 Batch   66/81   train_loss = 0.797\n",
      "Epoch 543 Batch   17/81   train_loss = 0.810\n",
      "Epoch 543 Batch   49/81   train_loss = 0.796\n",
      "Epoch 544 Batch    0/81   train_loss = 0.809\n",
      "Epoch 544 Batch   32/81   train_loss = 0.762\n",
      "Epoch 544 Batch   64/81   train_loss = 0.801\n",
      "Epoch 545 Batch   15/81   train_loss = 0.794\n",
      "Epoch 545 Batch   47/81   train_loss = 0.781\n",
      "Epoch 545 Batch   79/81   train_loss = 0.792\n",
      "Epoch 546 Batch   30/81   train_loss = 0.783\n",
      "Epoch 546 Batch   62/81   train_loss = 0.776\n",
      "Epoch 547 Batch   13/81   train_loss = 0.811\n",
      "Epoch 547 Batch   45/81   train_loss = 0.777\n",
      "Epoch 547 Batch   77/81   train_loss = 0.805\n",
      "Epoch 548 Batch   28/81   train_loss = 0.757\n",
      "Epoch 548 Batch   60/81   train_loss = 0.788\n",
      "Epoch 549 Batch   11/81   train_loss = 0.794\n",
      "Epoch 549 Batch   43/81   train_loss = 0.774\n",
      "Epoch 549 Batch   75/81   train_loss = 0.776\n",
      "Epoch 550 Batch   26/81   train_loss = 0.792\n",
      "Epoch 550 Batch   58/81   train_loss = 0.754\n",
      "Epoch 551 Batch    9/81   train_loss = 0.837\n",
      "Epoch 551 Batch   41/81   train_loss = 0.755\n",
      "Epoch 551 Batch   73/81   train_loss = 0.791\n",
      "Epoch 552 Batch   24/81   train_loss = 0.781\n",
      "Epoch 552 Batch   56/81   train_loss = 0.787\n",
      "Epoch 553 Batch    7/81   train_loss = 0.790\n",
      "Epoch 553 Batch   39/81   train_loss = 0.751\n",
      "Epoch 553 Batch   71/81   train_loss = 0.788\n",
      "Epoch 554 Batch   22/81   train_loss = 0.816\n",
      "Epoch 554 Batch   54/81   train_loss = 0.782\n",
      "Epoch 555 Batch    5/81   train_loss = 0.794\n",
      "Epoch 555 Batch   37/81   train_loss = 0.771\n",
      "Epoch 555 Batch   69/81   train_loss = 0.790\n",
      "Epoch 556 Batch   20/81   train_loss = 0.780\n",
      "Epoch 556 Batch   52/81   train_loss = 0.802\n",
      "Epoch 557 Batch    3/81   train_loss = 0.756\n",
      "Epoch 557 Batch   35/81   train_loss = 0.769\n",
      "Epoch 557 Batch   67/81   train_loss = 0.805\n",
      "Epoch 558 Batch   18/81   train_loss = 0.796\n",
      "Epoch 558 Batch   50/81   train_loss = 0.781\n",
      "Epoch 559 Batch    1/81   train_loss = 0.818\n",
      "Epoch 559 Batch   33/81   train_loss = 0.784\n",
      "Epoch 559 Batch   65/81   train_loss = 0.814\n",
      "Epoch 560 Batch   16/81   train_loss = 0.830\n",
      "Epoch 560 Batch   48/81   train_loss = 0.787\n",
      "Epoch 560 Batch   80/81   train_loss = 0.787\n",
      "Epoch 561 Batch   31/81   train_loss = 0.771\n",
      "Epoch 561 Batch   63/81   train_loss = 0.793\n",
      "Epoch 562 Batch   14/81   train_loss = 0.800\n",
      "Epoch 562 Batch   46/81   train_loss = 0.785\n",
      "Epoch 562 Batch   78/81   train_loss = 0.791\n",
      "Epoch 563 Batch   29/81   train_loss = 0.776\n",
      "Epoch 563 Batch   61/81   train_loss = 0.786\n",
      "Epoch 564 Batch   12/81   train_loss = 0.801\n",
      "Epoch 564 Batch   44/81   train_loss = 0.775\n",
      "Epoch 564 Batch   76/81   train_loss = 0.782\n",
      "Epoch 565 Batch   27/81   train_loss = 0.756\n",
      "Epoch 565 Batch   59/81   train_loss = 0.788\n",
      "Epoch 566 Batch   10/81   train_loss = 0.809\n",
      "Epoch 566 Batch   42/81   train_loss = 0.754\n",
      "Epoch 566 Batch   74/81   train_loss = 0.781\n",
      "Epoch 567 Batch   25/81   train_loss = 0.772\n",
      "Epoch 567 Batch   57/81   train_loss = 0.783\n",
      "Epoch 568 Batch    8/81   train_loss = 0.803\n",
      "Epoch 568 Batch   40/81   train_loss = 0.760\n",
      "Epoch 568 Batch   72/81   train_loss = 0.766\n",
      "Epoch 569 Batch   23/81   train_loss = 0.769\n",
      "Epoch 569 Batch   55/81   train_loss = 0.783\n",
      "Epoch 570 Batch    6/81   train_loss = 0.786\n",
      "Epoch 570 Batch   38/81   train_loss = 0.798\n",
      "Epoch 570 Batch   70/81   train_loss = 0.762\n",
      "Epoch 571 Batch   21/81   train_loss = 0.823\n",
      "Epoch 571 Batch   53/81   train_loss = 0.772\n",
      "Epoch 572 Batch    4/81   train_loss = 0.788\n",
      "Epoch 572 Batch   36/81   train_loss = 0.796\n",
      "Epoch 572 Batch   68/81   train_loss = 0.804\n",
      "Epoch 573 Batch   19/81   train_loss = 0.770\n",
      "Epoch 573 Batch   51/81   train_loss = 0.822\n",
      "Epoch 574 Batch    2/81   train_loss = 0.807\n",
      "Epoch 574 Batch   34/81   train_loss = 0.795\n",
      "Epoch 574 Batch   66/81   train_loss = 0.799\n",
      "Epoch 575 Batch   17/81   train_loss = 0.812\n",
      "Epoch 575 Batch   49/81   train_loss = 0.786\n",
      "Epoch 576 Batch    0/81   train_loss = 0.787\n",
      "Epoch 576 Batch   32/81   train_loss = 0.763\n",
      "Epoch 576 Batch   64/81   train_loss = 0.791\n",
      "Epoch 577 Batch   15/81   train_loss = 0.787\n",
      "Epoch 577 Batch   47/81   train_loss = 0.780\n",
      "Epoch 577 Batch   79/81   train_loss = 0.783\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 578 Batch   30/81   train_loss = 0.770\n",
      "Epoch 578 Batch   62/81   train_loss = 0.785\n",
      "Epoch 579 Batch   13/81   train_loss = 0.801\n",
      "Epoch 579 Batch   45/81   train_loss = 0.761\n",
      "Epoch 579 Batch   77/81   train_loss = 0.793\n",
      "Epoch 580 Batch   28/81   train_loss = 0.764\n",
      "Epoch 580 Batch   60/81   train_loss = 0.795\n",
      "Epoch 581 Batch   11/81   train_loss = 0.783\n",
      "Epoch 581 Batch   43/81   train_loss = 0.757\n",
      "Epoch 581 Batch   75/81   train_loss = 0.778\n",
      "Epoch 582 Batch   26/81   train_loss = 0.776\n",
      "Epoch 582 Batch   58/81   train_loss = 0.755\n",
      "Epoch 583 Batch    9/81   train_loss = 0.821\n",
      "Epoch 583 Batch   41/81   train_loss = 0.735\n",
      "Epoch 583 Batch   73/81   train_loss = 0.799\n",
      "Epoch 584 Batch   24/81   train_loss = 0.778\n",
      "Epoch 584 Batch   56/81   train_loss = 0.787\n",
      "Epoch 585 Batch    7/81   train_loss = 0.775\n",
      "Epoch 585 Batch   39/81   train_loss = 0.771\n",
      "Epoch 585 Batch   71/81   train_loss = 0.779\n",
      "Epoch 586 Batch   22/81   train_loss = 0.806\n",
      "Epoch 586 Batch   54/81   train_loss = 0.779\n",
      "Epoch 587 Batch    5/81   train_loss = 0.796\n",
      "Epoch 587 Batch   37/81   train_loss = 0.778\n",
      "Epoch 587 Batch   69/81   train_loss = 0.800\n",
      "Epoch 588 Batch   20/81   train_loss = 0.772\n",
      "Epoch 588 Batch   52/81   train_loss = 0.791\n",
      "Epoch 589 Batch    3/81   train_loss = 0.787\n",
      "Epoch 589 Batch   35/81   train_loss = 0.758\n",
      "Epoch 589 Batch   67/81   train_loss = 0.811\n",
      "Epoch 590 Batch   18/81   train_loss = 0.802\n",
      "Epoch 590 Batch   50/81   train_loss = 0.766\n",
      "Epoch 591 Batch    1/81   train_loss = 0.807\n",
      "Epoch 591 Batch   33/81   train_loss = 0.780\n",
      "Epoch 591 Batch   65/81   train_loss = 0.812\n",
      "Epoch 592 Batch   16/81   train_loss = 0.834\n",
      "Epoch 592 Batch   48/81   train_loss = 0.780\n",
      "Epoch 592 Batch   80/81   train_loss = 0.804\n",
      "Epoch 593 Batch   31/81   train_loss = 0.773\n",
      "Epoch 593 Batch   63/81   train_loss = 0.786\n",
      "Epoch 594 Batch   14/81   train_loss = 0.800\n",
      "Epoch 594 Batch   46/81   train_loss = 0.784\n",
      "Epoch 594 Batch   78/81   train_loss = 0.775\n",
      "Epoch 595 Batch   29/81   train_loss = 0.766\n",
      "Epoch 595 Batch   61/81   train_loss = 0.786\n",
      "Epoch 596 Batch   12/81   train_loss = 0.789\n",
      "Epoch 596 Batch   44/81   train_loss = 0.769\n",
      "Epoch 596 Batch   76/81   train_loss = 0.786\n",
      "Epoch 597 Batch   27/81   train_loss = 0.758\n",
      "Epoch 597 Batch   59/81   train_loss = 0.769\n",
      "Epoch 598 Batch   10/81   train_loss = 0.787\n",
      "Epoch 598 Batch   42/81   train_loss = 0.766\n",
      "Epoch 598 Batch   74/81   train_loss = 0.796\n",
      "Epoch 599 Batch   25/81   train_loss = 0.780\n",
      "Epoch 599 Batch   57/81   train_loss = 0.793\n",
      "Epoch 600 Batch    8/81   train_loss = 0.786\n",
      "Epoch 600 Batch   40/81   train_loss = 0.756\n",
      "Epoch 600 Batch   72/81   train_loss = 0.764\n",
      "Epoch 601 Batch   23/81   train_loss = 0.759\n",
      "Epoch 601 Batch   55/81   train_loss = 0.778\n",
      "Epoch 602 Batch    6/81   train_loss = 0.773\n",
      "Epoch 602 Batch   38/81   train_loss = 0.791\n",
      "Epoch 602 Batch   70/81   train_loss = 0.776\n",
      "Epoch 603 Batch   21/81   train_loss = 0.819\n",
      "Epoch 603 Batch   53/81   train_loss = 0.777\n",
      "Epoch 604 Batch    4/81   train_loss = 0.783\n",
      "Epoch 604 Batch   36/81   train_loss = 0.817\n",
      "Epoch 604 Batch   68/81   train_loss = 0.797\n",
      "Epoch 605 Batch   19/81   train_loss = 0.763\n",
      "Epoch 605 Batch   51/81   train_loss = 0.813\n",
      "Epoch 606 Batch    2/81   train_loss = 0.814\n",
      "Epoch 606 Batch   34/81   train_loss = 0.802\n",
      "Epoch 606 Batch   66/81   train_loss = 0.796\n",
      "Epoch 607 Batch   17/81   train_loss = 0.796\n",
      "Epoch 607 Batch   49/81   train_loss = 0.797\n",
      "Epoch 608 Batch    0/81   train_loss = 0.793\n",
      "Epoch 608 Batch   32/81   train_loss = 0.764\n",
      "Epoch 608 Batch   64/81   train_loss = 0.795\n",
      "Epoch 609 Batch   15/81   train_loss = 0.768\n",
      "Epoch 609 Batch   47/81   train_loss = 0.792\n",
      "Epoch 609 Batch   79/81   train_loss = 0.783\n",
      "Epoch 610 Batch   30/81   train_loss = 0.774\n",
      "Epoch 610 Batch   62/81   train_loss = 0.785\n",
      "Epoch 611 Batch   13/81   train_loss = 0.784\n",
      "Epoch 611 Batch   45/81   train_loss = 0.757\n",
      "Epoch 611 Batch   77/81   train_loss = 0.790\n",
      "Epoch 612 Batch   28/81   train_loss = 0.750\n",
      "Epoch 612 Batch   60/81   train_loss = 0.796\n",
      "Epoch 613 Batch   11/81   train_loss = 0.782\n",
      "Epoch 613 Batch   43/81   train_loss = 0.752\n",
      "Epoch 613 Batch   75/81   train_loss = 0.777\n",
      "Epoch 614 Batch   26/81   train_loss = 0.783\n",
      "Epoch 614 Batch   58/81   train_loss = 0.766\n",
      "Epoch 615 Batch    9/81   train_loss = 0.804\n",
      "Epoch 615 Batch   41/81   train_loss = 0.734\n",
      "Epoch 615 Batch   73/81   train_loss = 0.768\n",
      "Epoch 616 Batch   24/81   train_loss = 0.794\n",
      "Epoch 616 Batch   56/81   train_loss = 0.806\n",
      "Epoch 617 Batch    7/81   train_loss = 0.799\n",
      "Epoch 617 Batch   39/81   train_loss = 0.760\n",
      "Epoch 617 Batch   71/81   train_loss = 0.770\n",
      "Epoch 618 Batch   22/81   train_loss = 0.804\n",
      "Epoch 618 Batch   54/81   train_loss = 0.774\n",
      "Epoch 619 Batch    5/81   train_loss = 0.805\n",
      "Epoch 619 Batch   37/81   train_loss = 0.790\n",
      "Epoch 619 Batch   69/81   train_loss = 0.787\n",
      "Epoch 620 Batch   20/81   train_loss = 0.774\n",
      "Epoch 620 Batch   52/81   train_loss = 0.800\n",
      "Epoch 621 Batch    3/81   train_loss = 0.772\n",
      "Epoch 621 Batch   35/81   train_loss = 0.772\n",
      "Epoch 621 Batch   67/81   train_loss = 0.821\n",
      "Epoch 622 Batch   18/81   train_loss = 0.809\n",
      "Epoch 622 Batch   50/81   train_loss = 0.780\n",
      "Epoch 623 Batch    1/81   train_loss = 0.811\n",
      "Epoch 623 Batch   33/81   train_loss = 0.777\n",
      "Epoch 623 Batch   65/81   train_loss = 0.819\n",
      "Epoch 624 Batch   16/81   train_loss = 0.837\n",
      "Epoch 624 Batch   48/81   train_loss = 0.785\n",
      "Epoch 624 Batch   80/81   train_loss = 0.817\n",
      "Epoch 625 Batch   31/81   train_loss = 0.772\n",
      "Epoch 625 Batch   63/81   train_loss = 0.791\n",
      "Epoch 626 Batch   14/81   train_loss = 0.790\n",
      "Epoch 626 Batch   46/81   train_loss = 0.782\n",
      "Epoch 626 Batch   78/81   train_loss = 0.778\n",
      "Epoch 627 Batch   29/81   train_loss = 0.769\n",
      "Epoch 627 Batch   61/81   train_loss = 0.799\n",
      "Epoch 628 Batch   12/81   train_loss = 0.794\n",
      "Epoch 628 Batch   44/81   train_loss = 0.777\n",
      "Epoch 628 Batch   76/81   train_loss = 0.794\n",
      "Epoch 629 Batch   27/81   train_loss = 0.776\n",
      "Epoch 629 Batch   59/81   train_loss = 0.783\n",
      "Epoch 630 Batch   10/81   train_loss = 0.803\n",
      "Epoch 630 Batch   42/81   train_loss = 0.773\n",
      "Epoch 630 Batch   74/81   train_loss = 0.796\n",
      "Epoch 631 Batch   25/81   train_loss = 0.782\n",
      "Epoch 631 Batch   57/81   train_loss = 0.781\n",
      "Epoch 632 Batch    8/81   train_loss = 0.789\n",
      "Epoch 632 Batch   40/81   train_loss = 0.756\n",
      "Epoch 632 Batch   72/81   train_loss = 0.758\n",
      "Epoch 633 Batch   23/81   train_loss = 0.771\n",
      "Epoch 633 Batch   55/81   train_loss = 0.791\n",
      "Epoch 634 Batch    6/81   train_loss = 0.776\n",
      "Epoch 634 Batch   38/81   train_loss = 0.797\n",
      "Epoch 634 Batch   70/81   train_loss = 0.780\n",
      "Epoch 635 Batch   21/81   train_loss = 0.817\n",
      "Epoch 635 Batch   53/81   train_loss = 0.769\n",
      "Epoch 636 Batch    4/81   train_loss = 0.784\n",
      "Epoch 636 Batch   36/81   train_loss = 0.795\n",
      "Epoch 636 Batch   68/81   train_loss = 0.790\n",
      "Epoch 637 Batch   19/81   train_loss = 0.749\n",
      "Epoch 637 Batch   51/81   train_loss = 0.802\n",
      "Epoch 638 Batch    2/81   train_loss = 0.790\n",
      "Epoch 638 Batch   34/81   train_loss = 0.783\n",
      "Epoch 638 Batch   66/81   train_loss = 0.784\n",
      "Epoch 639 Batch   17/81   train_loss = 0.799\n",
      "Epoch 639 Batch   49/81   train_loss = 0.797\n",
      "Epoch 640 Batch    0/81   train_loss = 0.792\n",
      "Epoch 640 Batch   32/81   train_loss = 0.744\n",
      "Epoch 640 Batch   64/81   train_loss = 0.796\n",
      "Epoch 641 Batch   15/81   train_loss = 0.770\n",
      "Epoch 641 Batch   47/81   train_loss = 0.768\n",
      "Epoch 641 Batch   79/81   train_loss = 0.783\n",
      "Epoch 642 Batch   30/81   train_loss = 0.776\n",
      "Epoch 642 Batch   62/81   train_loss = 0.774\n",
      "Epoch 643 Batch   13/81   train_loss = 0.804\n",
      "Epoch 643 Batch   45/81   train_loss = 0.781\n",
      "Epoch 643 Batch   77/81   train_loss = 0.784\n",
      "Epoch 644 Batch   28/81   train_loss = 0.773\n",
      "Epoch 644 Batch   60/81   train_loss = 0.783\n",
      "Epoch 645 Batch   11/81   train_loss = 0.793\n",
      "Epoch 645 Batch   43/81   train_loss = 0.760\n",
      "Epoch 645 Batch   75/81   train_loss = 0.773\n",
      "Epoch 646 Batch   26/81   train_loss = 0.783\n",
      "Epoch 646 Batch   58/81   train_loss = 0.763\n",
      "Epoch 647 Batch    9/81   train_loss = 0.816\n",
      "Epoch 647 Batch   41/81   train_loss = 0.754\n",
      "Epoch 647 Batch   73/81   train_loss = 0.778\n",
      "Epoch 648 Batch   24/81   train_loss = 0.784\n",
      "Epoch 648 Batch   56/81   train_loss = 0.782\n",
      "Epoch 649 Batch    7/81   train_loss = 0.782\n",
      "Epoch 649 Batch   39/81   train_loss = 0.751\n",
      "Epoch 649 Batch   71/81   train_loss = 0.776\n",
      "Epoch 650 Batch   22/81   train_loss = 0.793\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 650 Batch   54/81   train_loss = 0.781\n",
      "Epoch 651 Batch    5/81   train_loss = 0.802\n",
      "Epoch 651 Batch   37/81   train_loss = 0.786\n",
      "Epoch 651 Batch   69/81   train_loss = 0.789\n",
      "Epoch 652 Batch   20/81   train_loss = 0.784\n",
      "Epoch 652 Batch   52/81   train_loss = 0.817\n",
      "Epoch 653 Batch    3/81   train_loss = 0.770\n",
      "Epoch 653 Batch   35/81   train_loss = 0.778\n",
      "Epoch 653 Batch   67/81   train_loss = 0.803\n",
      "Epoch 654 Batch   18/81   train_loss = 0.806\n",
      "Epoch 654 Batch   50/81   train_loss = 0.776\n",
      "Epoch 655 Batch    1/81   train_loss = 0.804\n",
      "Epoch 655 Batch   33/81   train_loss = 0.759\n",
      "Epoch 655 Batch   65/81   train_loss = 0.817\n",
      "Epoch 656 Batch   16/81   train_loss = 0.799\n",
      "Epoch 656 Batch   48/81   train_loss = 0.787\n",
      "Epoch 656 Batch   80/81   train_loss = 0.785\n",
      "Epoch 657 Batch   31/81   train_loss = 0.763\n",
      "Epoch 657 Batch   63/81   train_loss = 0.794\n",
      "Epoch 658 Batch   14/81   train_loss = 0.795\n",
      "Epoch 658 Batch   46/81   train_loss = 0.788\n",
      "Epoch 658 Batch   78/81   train_loss = 0.778\n",
      "Epoch 659 Batch   29/81   train_loss = 0.778\n",
      "Epoch 659 Batch   61/81   train_loss = 0.777\n",
      "Epoch 660 Batch   12/81   train_loss = 0.789\n",
      "Epoch 660 Batch   44/81   train_loss = 0.771\n",
      "Epoch 660 Batch   76/81   train_loss = 0.797\n",
      "Epoch 661 Batch   27/81   train_loss = 0.771\n",
      "Epoch 661 Batch   59/81   train_loss = 0.770\n",
      "Epoch 662 Batch   10/81   train_loss = 0.795\n",
      "Epoch 662 Batch   42/81   train_loss = 0.766\n",
      "Epoch 662 Batch   74/81   train_loss = 0.786\n",
      "Epoch 663 Batch   25/81   train_loss = 0.779\n",
      "Epoch 663 Batch   57/81   train_loss = 0.770\n",
      "Epoch 664 Batch    8/81   train_loss = 0.801\n",
      "Epoch 664 Batch   40/81   train_loss = 0.759\n",
      "Epoch 664 Batch   72/81   train_loss = 0.756\n",
      "Epoch 665 Batch   23/81   train_loss = 0.759\n",
      "Epoch 665 Batch   55/81   train_loss = 0.793\n",
      "Epoch 666 Batch    6/81   train_loss = 0.772\n",
      "Epoch 666 Batch   38/81   train_loss = 0.801\n",
      "Epoch 666 Batch   70/81   train_loss = 0.777\n",
      "Epoch 667 Batch   21/81   train_loss = 0.825\n",
      "Epoch 667 Batch   53/81   train_loss = 0.773\n",
      "Epoch 668 Batch    4/81   train_loss = 0.778\n",
      "Epoch 668 Batch   36/81   train_loss = 0.807\n",
      "Epoch 668 Batch   68/81   train_loss = 0.801\n",
      "Epoch 669 Batch   19/81   train_loss = 0.761\n",
      "Epoch 669 Batch   51/81   train_loss = 0.815\n",
      "Epoch 670 Batch    2/81   train_loss = 0.823\n",
      "Epoch 670 Batch   34/81   train_loss = 0.793\n",
      "Epoch 670 Batch   66/81   train_loss = 0.789\n",
      "Epoch 671 Batch   17/81   train_loss = 0.808\n",
      "Epoch 671 Batch   49/81   train_loss = 0.802\n",
      "Epoch 672 Batch    0/81   train_loss = 0.781\n",
      "Epoch 672 Batch   32/81   train_loss = 0.751\n",
      "Epoch 672 Batch   64/81   train_loss = 0.810\n",
      "Epoch 673 Batch   15/81   train_loss = 0.787\n",
      "Epoch 673 Batch   47/81   train_loss = 0.787\n",
      "Epoch 673 Batch   79/81   train_loss = 0.805\n",
      "Epoch 674 Batch   30/81   train_loss = 0.791\n",
      "Epoch 674 Batch   62/81   train_loss = 0.775\n",
      "Epoch 675 Batch   13/81   train_loss = 0.822\n",
      "Epoch 675 Batch   45/81   train_loss = 0.784\n",
      "Epoch 675 Batch   77/81   train_loss = 0.808\n",
      "Epoch 676 Batch   28/81   train_loss = 0.753\n",
      "Epoch 676 Batch   60/81   train_loss = 0.798\n",
      "Epoch 677 Batch   11/81   train_loss = 0.783\n",
      "Epoch 677 Batch   43/81   train_loss = 0.755\n",
      "Epoch 677 Batch   75/81   train_loss = 0.772\n",
      "Epoch 678 Batch   26/81   train_loss = 0.787\n",
      "Epoch 678 Batch   58/81   train_loss = 0.754\n",
      "Epoch 679 Batch    9/81   train_loss = 0.827\n",
      "Epoch 679 Batch   41/81   train_loss = 0.751\n",
      "Epoch 679 Batch   73/81   train_loss = 0.785\n",
      "Epoch 680 Batch   24/81   train_loss = 0.793\n",
      "Epoch 680 Batch   56/81   train_loss = 0.817\n",
      "Epoch 681 Batch    7/81   train_loss = 0.788\n",
      "Epoch 681 Batch   39/81   train_loss = 0.760\n",
      "Epoch 681 Batch   71/81   train_loss = 0.764\n",
      "Epoch 682 Batch   22/81   train_loss = 0.809\n",
      "Epoch 682 Batch   54/81   train_loss = 0.761\n",
      "Epoch 683 Batch    5/81   train_loss = 0.807\n",
      "Epoch 683 Batch   37/81   train_loss = 0.796\n",
      "Epoch 683 Batch   69/81   train_loss = 0.795\n",
      "Epoch 684 Batch   20/81   train_loss = 0.784\n",
      "Epoch 684 Batch   52/81   train_loss = 0.798\n",
      "Epoch 685 Batch    3/81   train_loss = 0.763\n",
      "Epoch 685 Batch   35/81   train_loss = 0.784\n",
      "Epoch 685 Batch   67/81   train_loss = 0.817\n",
      "Epoch 686 Batch   18/81   train_loss = 0.804\n",
      "Epoch 686 Batch   50/81   train_loss = 0.767\n",
      "Epoch 687 Batch    1/81   train_loss = 0.815\n",
      "Epoch 687 Batch   33/81   train_loss = 0.791\n",
      "Epoch 687 Batch   65/81   train_loss = 0.817\n",
      "Epoch 688 Batch   16/81   train_loss = 0.841\n",
      "Epoch 688 Batch   48/81   train_loss = 0.786\n",
      "Epoch 688 Batch   80/81   train_loss = 0.803\n",
      "Epoch 689 Batch   31/81   train_loss = 0.769\n",
      "Epoch 689 Batch   63/81   train_loss = 0.791\n",
      "Epoch 690 Batch   14/81   train_loss = 0.796\n",
      "Epoch 690 Batch   46/81   train_loss = 0.787\n",
      "Epoch 690 Batch   78/81   train_loss = 0.763\n",
      "Epoch 691 Batch   29/81   train_loss = 0.781\n",
      "Epoch 691 Batch   61/81   train_loss = 0.786\n",
      "Epoch 692 Batch   12/81   train_loss = 0.784\n",
      "Epoch 692 Batch   44/81   train_loss = 0.762\n",
      "Epoch 692 Batch   76/81   train_loss = 0.784\n",
      "Epoch 693 Batch   27/81   train_loss = 0.763\n",
      "Epoch 693 Batch   59/81   train_loss = 0.778\n",
      "Epoch 694 Batch   10/81   train_loss = 0.794\n",
      "Epoch 694 Batch   42/81   train_loss = 0.763\n",
      "Epoch 694 Batch   74/81   train_loss = 0.775\n",
      "Epoch 695 Batch   25/81   train_loss = 0.787\n",
      "Epoch 695 Batch   57/81   train_loss = 0.796\n",
      "Epoch 696 Batch    8/81   train_loss = 0.789\n",
      "Epoch 696 Batch   40/81   train_loss = 0.761\n",
      "Epoch 696 Batch   72/81   train_loss = 0.761\n",
      "Epoch 697 Batch   23/81   train_loss = 0.755\n",
      "Epoch 697 Batch   55/81   train_loss = 0.804\n",
      "Epoch 698 Batch    6/81   train_loss = 0.790\n",
      "Epoch 698 Batch   38/81   train_loss = 0.800\n",
      "Epoch 698 Batch   70/81   train_loss = 0.753\n",
      "Epoch 699 Batch   21/81   train_loss = 0.830\n",
      "Epoch 699 Batch   53/81   train_loss = 0.773\n",
      "Epoch 700 Batch    4/81   train_loss = 0.777\n",
      "Epoch 700 Batch   36/81   train_loss = 0.821\n",
      "Epoch 700 Batch   68/81   train_loss = 0.812\n",
      "Epoch 701 Batch   19/81   train_loss = 0.762\n",
      "Epoch 701 Batch   51/81   train_loss = 0.820\n",
      "Epoch 702 Batch    2/81   train_loss = 0.802\n",
      "Epoch 702 Batch   34/81   train_loss = 0.802\n",
      "Epoch 702 Batch   66/81   train_loss = 0.795\n",
      "Epoch 703 Batch   17/81   train_loss = 0.804\n",
      "Epoch 703 Batch   49/81   train_loss = 0.795\n",
      "Epoch 704 Batch    0/81   train_loss = 0.789\n",
      "Epoch 704 Batch   32/81   train_loss = 0.770\n",
      "Epoch 704 Batch   64/81   train_loss = 0.780\n",
      "Epoch 705 Batch   15/81   train_loss = 0.772\n",
      "Epoch 705 Batch   47/81   train_loss = 0.780\n",
      "Epoch 705 Batch   79/81   train_loss = 0.772\n",
      "Epoch 706 Batch   30/81   train_loss = 0.785\n",
      "Epoch 706 Batch   62/81   train_loss = 0.789\n",
      "Epoch 707 Batch   13/81   train_loss = 0.790\n",
      "Epoch 707 Batch   45/81   train_loss = 0.783\n",
      "Epoch 707 Batch   77/81   train_loss = 0.791\n",
      "Epoch 708 Batch   28/81   train_loss = 0.762\n",
      "Epoch 708 Batch   60/81   train_loss = 0.796\n",
      "Epoch 709 Batch   11/81   train_loss = 0.796\n",
      "Epoch 709 Batch   43/81   train_loss = 0.758\n",
      "Epoch 709 Batch   75/81   train_loss = 0.780\n",
      "Epoch 710 Batch   26/81   train_loss = 0.787\n",
      "Epoch 710 Batch   58/81   train_loss = 0.756\n",
      "Epoch 711 Batch    9/81   train_loss = 0.803\n",
      "Epoch 711 Batch   41/81   train_loss = 0.744\n",
      "Epoch 711 Batch   73/81   train_loss = 0.777\n",
      "Epoch 712 Batch   24/81   train_loss = 0.784\n",
      "Epoch 712 Batch   56/81   train_loss = 0.803\n",
      "Epoch 713 Batch    7/81   train_loss = 0.791\n",
      "Epoch 713 Batch   39/81   train_loss = 0.755\n",
      "Epoch 713 Batch   71/81   train_loss = 0.772\n",
      "Epoch 714 Batch   22/81   train_loss = 0.813\n",
      "Epoch 714 Batch   54/81   train_loss = 0.776\n",
      "Epoch 715 Batch    5/81   train_loss = 0.796\n",
      "Epoch 715 Batch   37/81   train_loss = 0.789\n",
      "Epoch 715 Batch   69/81   train_loss = 0.789\n",
      "Epoch 716 Batch   20/81   train_loss = 0.786\n",
      "Epoch 716 Batch   52/81   train_loss = 0.802\n",
      "Epoch 717 Batch    3/81   train_loss = 0.759\n",
      "Epoch 717 Batch   35/81   train_loss = 0.741\n",
      "Epoch 717 Batch   67/81   train_loss = 0.815\n",
      "Epoch 718 Batch   18/81   train_loss = 0.795\n",
      "Epoch 718 Batch   50/81   train_loss = 0.776\n",
      "Epoch 719 Batch    1/81   train_loss = 0.807\n",
      "Epoch 719 Batch   33/81   train_loss = 0.779\n",
      "Epoch 719 Batch   65/81   train_loss = 0.804\n",
      "Epoch 720 Batch   16/81   train_loss = 0.823\n",
      "Epoch 720 Batch   48/81   train_loss = 0.780\n",
      "Epoch 720 Batch   80/81   train_loss = 0.797\n",
      "Epoch 721 Batch   31/81   train_loss = 0.765\n",
      "Epoch 721 Batch   63/81   train_loss = 0.800\n",
      "Epoch 722 Batch   14/81   train_loss = 0.808\n",
      "Epoch 722 Batch   46/81   train_loss = 0.787\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 722 Batch   78/81   train_loss = 0.796\n",
      "Epoch 723 Batch   29/81   train_loss = 0.773\n",
      "Epoch 723 Batch   61/81   train_loss = 0.797\n",
      "Epoch 724 Batch   12/81   train_loss = 0.800\n",
      "Epoch 724 Batch   44/81   train_loss = 0.788\n",
      "Epoch 724 Batch   76/81   train_loss = 0.808\n",
      "Epoch 725 Batch   27/81   train_loss = 0.780\n",
      "Epoch 725 Batch   59/81   train_loss = 0.780\n",
      "Epoch 726 Batch   10/81   train_loss = 0.806\n",
      "Epoch 726 Batch   42/81   train_loss = 0.769\n",
      "Epoch 726 Batch   74/81   train_loss = 0.792\n",
      "Epoch 727 Batch   25/81   train_loss = 0.774\n",
      "Epoch 727 Batch   57/81   train_loss = 0.784\n",
      "Epoch 728 Batch    8/81   train_loss = 0.800\n",
      "Epoch 728 Batch   40/81   train_loss = 0.788\n",
      "Epoch 728 Batch   72/81   train_loss = 0.780\n",
      "Epoch 729 Batch   23/81   train_loss = 0.780\n",
      "Epoch 729 Batch   55/81   train_loss = 0.811\n",
      "Epoch 730 Batch    6/81   train_loss = 0.790\n",
      "Epoch 730 Batch   38/81   train_loss = 0.821\n",
      "Epoch 730 Batch   70/81   train_loss = 0.770\n",
      "Epoch 731 Batch   21/81   train_loss = 0.845\n",
      "Epoch 731 Batch   53/81   train_loss = 0.781\n",
      "Epoch 732 Batch    4/81   train_loss = 0.786\n",
      "Epoch 732 Batch   36/81   train_loss = 0.809\n",
      "Epoch 732 Batch   68/81   train_loss = 0.805\n",
      "Epoch 733 Batch   19/81   train_loss = 0.756\n",
      "Epoch 733 Batch   51/81   train_loss = 0.830\n",
      "Epoch 734 Batch    2/81   train_loss = 0.794\n",
      "Epoch 734 Batch   34/81   train_loss = 0.801\n",
      "Epoch 734 Batch   66/81   train_loss = 0.780\n",
      "Epoch 735 Batch   17/81   train_loss = 0.790\n",
      "Epoch 735 Batch   49/81   train_loss = 0.799\n",
      "Epoch 736 Batch    0/81   train_loss = 0.786\n",
      "Epoch 736 Batch   32/81   train_loss = 0.791\n",
      "Epoch 736 Batch   64/81   train_loss = 0.787\n",
      "Epoch 737 Batch   15/81   train_loss = 0.767\n",
      "Epoch 737 Batch   47/81   train_loss = 0.779\n",
      "Epoch 737 Batch   79/81   train_loss = 0.789\n",
      "Epoch 738 Batch   30/81   train_loss = 0.786\n",
      "Epoch 738 Batch   62/81   train_loss = 0.795\n",
      "Epoch 739 Batch   13/81   train_loss = 0.789\n",
      "Epoch 739 Batch   45/81   train_loss = 0.781\n",
      "Epoch 739 Batch   77/81   train_loss = 0.811\n",
      "Epoch 740 Batch   28/81   train_loss = 0.776\n",
      "Epoch 740 Batch   60/81   train_loss = 0.817\n",
      "Epoch 741 Batch   11/81   train_loss = 0.782\n",
      "Epoch 741 Batch   43/81   train_loss = 0.776\n",
      "Epoch 741 Batch   75/81   train_loss = 0.787\n",
      "Epoch 742 Batch   26/81   train_loss = 0.788\n",
      "Epoch 742 Batch   58/81   train_loss = 0.778\n",
      "Epoch 743 Batch    9/81   train_loss = 0.825\n",
      "Epoch 743 Batch   41/81   train_loss = 0.759\n",
      "Epoch 743 Batch   73/81   train_loss = 0.799\n",
      "Epoch 744 Batch   24/81   train_loss = 0.799\n",
      "Epoch 744 Batch   56/81   train_loss = 0.821\n",
      "Epoch 745 Batch    7/81   train_loss = 0.783\n",
      "Epoch 745 Batch   39/81   train_loss = 0.768\n",
      "Epoch 745 Batch   71/81   train_loss = 0.774\n",
      "Epoch 746 Batch   22/81   train_loss = 0.812\n",
      "Epoch 746 Batch   54/81   train_loss = 0.790\n",
      "Epoch 747 Batch    5/81   train_loss = 0.827\n",
      "Epoch 747 Batch   37/81   train_loss = 0.811\n",
      "Epoch 747 Batch   69/81   train_loss = 0.817\n",
      "Epoch 748 Batch   20/81   train_loss = 0.826\n",
      "Epoch 748 Batch   52/81   train_loss = 0.840\n",
      "Epoch 749 Batch    3/81   train_loss = 0.792\n",
      "Epoch 749 Batch   35/81   train_loss = 0.789\n",
      "Epoch 749 Batch   67/81   train_loss = 0.822\n",
      "Epoch 750 Batch   18/81   train_loss = 0.813\n",
      "Epoch 750 Batch   50/81   train_loss = 0.798\n",
      "Epoch 751 Batch    1/81   train_loss = 0.824\n",
      "Epoch 751 Batch   33/81   train_loss = 0.792\n",
      "Epoch 751 Batch   65/81   train_loss = 0.828\n",
      "Epoch 752 Batch   16/81   train_loss = 0.855\n",
      "Epoch 752 Batch   48/81   train_loss = 0.794\n",
      "Epoch 752 Batch   80/81   train_loss = 0.813\n",
      "Epoch 753 Batch   31/81   train_loss = 0.790\n",
      "Epoch 753 Batch   63/81   train_loss = 0.814\n",
      "Epoch 754 Batch   14/81   train_loss = 0.819\n",
      "Epoch 754 Batch   46/81   train_loss = 0.829\n",
      "Epoch 754 Batch   78/81   train_loss = 0.796\n",
      "Epoch 755 Batch   29/81   train_loss = 0.782\n",
      "Epoch 755 Batch   61/81   train_loss = 0.809\n",
      "Epoch 756 Batch   12/81   train_loss = 0.814\n",
      "Epoch 756 Batch   44/81   train_loss = 0.792\n",
      "Epoch 756 Batch   76/81   train_loss = 0.826\n",
      "Epoch 757 Batch   27/81   train_loss = 0.771\n",
      "Epoch 757 Batch   59/81   train_loss = 0.799\n",
      "Epoch 758 Batch   10/81   train_loss = 0.815\n",
      "Epoch 758 Batch   42/81   train_loss = 0.779\n",
      "Epoch 758 Batch   74/81   train_loss = 0.808\n",
      "Epoch 759 Batch   25/81   train_loss = 0.801\n",
      "Epoch 759 Batch   57/81   train_loss = 0.821\n",
      "Epoch 760 Batch    8/81   train_loss = 0.819\n",
      "Epoch 760 Batch   40/81   train_loss = 0.789\n",
      "Epoch 760 Batch   72/81   train_loss = 0.779\n",
      "Epoch 761 Batch   23/81   train_loss = 0.786\n",
      "Epoch 761 Batch   55/81   train_loss = 0.798\n",
      "Epoch 762 Batch    6/81   train_loss = 0.800\n",
      "Epoch 762 Batch   38/81   train_loss = 0.809\n",
      "Epoch 762 Batch   70/81   train_loss = 0.783\n",
      "Epoch 763 Batch   21/81   train_loss = 0.846\n",
      "Epoch 763 Batch   53/81   train_loss = 0.810\n",
      "Epoch 764 Batch    4/81   train_loss = 0.802\n",
      "Epoch 764 Batch   36/81   train_loss = 0.813\n",
      "Epoch 764 Batch   68/81   train_loss = 0.825\n",
      "Epoch 765 Batch   19/81   train_loss = 0.755\n",
      "Epoch 765 Batch   51/81   train_loss = 0.826\n",
      "Epoch 766 Batch    2/81   train_loss = 0.834\n",
      "Epoch 766 Batch   34/81   train_loss = 0.809\n",
      "Epoch 766 Batch   66/81   train_loss = 0.823\n",
      "Epoch 767 Batch   17/81   train_loss = 0.817\n",
      "Epoch 767 Batch   49/81   train_loss = 0.805\n",
      "Epoch 768 Batch    0/81   train_loss = 0.800\n",
      "Epoch 768 Batch   32/81   train_loss = 0.792\n",
      "Epoch 768 Batch   64/81   train_loss = 0.818\n",
      "Epoch 769 Batch   15/81   train_loss = 0.801\n",
      "Epoch 769 Batch   47/81   train_loss = 0.808\n",
      "Epoch 769 Batch   79/81   train_loss = 0.828\n",
      "Epoch 770 Batch   30/81   train_loss = 0.796\n",
      "Epoch 770 Batch   62/81   train_loss = 0.794\n",
      "Epoch 771 Batch   13/81   train_loss = 0.804\n",
      "Epoch 771 Batch   45/81   train_loss = 0.788\n",
      "Epoch 771 Batch   77/81   train_loss = 0.809\n",
      "Epoch 772 Batch   28/81   train_loss = 0.775\n",
      "Epoch 772 Batch   60/81   train_loss = 0.818\n",
      "Epoch 773 Batch   11/81   train_loss = 0.796\n",
      "Epoch 773 Batch   43/81   train_loss = 0.763\n",
      "Epoch 773 Batch   75/81   train_loss = 0.784\n",
      "Epoch 774 Batch   26/81   train_loss = 0.789\n",
      "Epoch 774 Batch   58/81   train_loss = 0.766\n",
      "Epoch 775 Batch    9/81   train_loss = 0.835\n",
      "Epoch 775 Batch   41/81   train_loss = 0.757\n",
      "Epoch 775 Batch   73/81   train_loss = 0.789\n",
      "Epoch 776 Batch   24/81   train_loss = 0.792\n",
      "Epoch 776 Batch   56/81   train_loss = 0.812\n",
      "Epoch 777 Batch    7/81   train_loss = 0.794\n",
      "Epoch 777 Batch   39/81   train_loss = 0.767\n",
      "Epoch 777 Batch   71/81   train_loss = 0.776\n",
      "Epoch 778 Batch   22/81   train_loss = 0.815\n",
      "Epoch 778 Batch   54/81   train_loss = 0.792\n",
      "Epoch 779 Batch    5/81   train_loss = 0.809\n",
      "Epoch 779 Batch   37/81   train_loss = 0.792\n",
      "Epoch 779 Batch   69/81   train_loss = 0.801\n",
      "Epoch 780 Batch   20/81   train_loss = 0.793\n",
      "Epoch 780 Batch   52/81   train_loss = 0.812\n",
      "Epoch 781 Batch    3/81   train_loss = 0.779\n",
      "Epoch 781 Batch   35/81   train_loss = 0.771\n",
      "Epoch 781 Batch   67/81   train_loss = 0.817\n",
      "Epoch 782 Batch   18/81   train_loss = 0.821\n",
      "Epoch 782 Batch   50/81   train_loss = 0.769\n",
      "Epoch 783 Batch    1/81   train_loss = 0.793\n",
      "Epoch 783 Batch   33/81   train_loss = 0.780\n",
      "Epoch 783 Batch   65/81   train_loss = 0.817\n",
      "Epoch 784 Batch   16/81   train_loss = 0.839\n",
      "Epoch 784 Batch   48/81   train_loss = 0.781\n",
      "Epoch 784 Batch   80/81   train_loss = 0.814\n",
      "Epoch 785 Batch   31/81   train_loss = 0.772\n",
      "Epoch 785 Batch   63/81   train_loss = 0.787\n",
      "Epoch 786 Batch   14/81   train_loss = 0.797\n",
      "Epoch 786 Batch   46/81   train_loss = 0.779\n",
      "Epoch 786 Batch   78/81   train_loss = 0.779\n",
      "Epoch 787 Batch   29/81   train_loss = 0.786\n",
      "Epoch 787 Batch   61/81   train_loss = 0.778\n",
      "Epoch 788 Batch   12/81   train_loss = 0.791\n",
      "Epoch 788 Batch   44/81   train_loss = 0.777\n",
      "Epoch 788 Batch   76/81   train_loss = 0.806\n",
      "Epoch 789 Batch   27/81   train_loss = 0.758\n",
      "Epoch 789 Batch   59/81   train_loss = 0.769\n",
      "Epoch 790 Batch   10/81   train_loss = 0.777\n",
      "Epoch 790 Batch   42/81   train_loss = 0.760\n",
      "Epoch 790 Batch   74/81   train_loss = 0.773\n",
      "Epoch 791 Batch   25/81   train_loss = 0.774\n",
      "Epoch 791 Batch   57/81   train_loss = 0.794\n",
      "Epoch 792 Batch    8/81   train_loss = 0.792\n",
      "Epoch 792 Batch   40/81   train_loss = 0.761\n",
      "Epoch 792 Batch   72/81   train_loss = 0.771\n",
      "Epoch 793 Batch   23/81   train_loss = 0.751\n",
      "Epoch 793 Batch   55/81   train_loss = 0.793\n",
      "Epoch 794 Batch    6/81   train_loss = 0.790\n",
      "Epoch 794 Batch   38/81   train_loss = 0.800\n",
      "Epoch 794 Batch   70/81   train_loss = 0.772\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 795 Batch   21/81   train_loss = 0.818\n",
      "Epoch 795 Batch   53/81   train_loss = 0.777\n",
      "Epoch 796 Batch    4/81   train_loss = 0.786\n",
      "Epoch 796 Batch   36/81   train_loss = 0.786\n",
      "Epoch 796 Batch   68/81   train_loss = 0.807\n",
      "Epoch 797 Batch   19/81   train_loss = 0.759\n",
      "Epoch 797 Batch   51/81   train_loss = 0.822\n",
      "Epoch 798 Batch    2/81   train_loss = 0.806\n",
      "Epoch 798 Batch   34/81   train_loss = 0.790\n",
      "Epoch 798 Batch   66/81   train_loss = 0.785\n",
      "Epoch 799 Batch   17/81   train_loss = 0.795\n",
      "Epoch 799 Batch   49/81   train_loss = 0.785\n",
      "Epoch 800 Batch    0/81   train_loss = 0.786\n",
      "Epoch 800 Batch   32/81   train_loss = 0.752\n",
      "Epoch 800 Batch   64/81   train_loss = 0.777\n",
      "Epoch 801 Batch   15/81   train_loss = 0.766\n",
      "Epoch 801 Batch   47/81   train_loss = 0.774\n",
      "Epoch 801 Batch   79/81   train_loss = 0.787\n",
      "Epoch 802 Batch   30/81   train_loss = 0.762\n",
      "Epoch 802 Batch   62/81   train_loss = 0.773\n",
      "Epoch 803 Batch   13/81   train_loss = 0.794\n",
      "Epoch 803 Batch   45/81   train_loss = 0.768\n",
      "Epoch 803 Batch   77/81   train_loss = 0.784\n",
      "Epoch 804 Batch   28/81   train_loss = 0.752\n",
      "Epoch 804 Batch   60/81   train_loss = 0.784\n",
      "Epoch 805 Batch   11/81   train_loss = 0.772\n",
      "Epoch 805 Batch   43/81   train_loss = 0.751\n",
      "Epoch 805 Batch   75/81   train_loss = 0.779\n",
      "Epoch 806 Batch   26/81   train_loss = 0.776\n",
      "Epoch 806 Batch   58/81   train_loss = 0.763\n",
      "Epoch 807 Batch    9/81   train_loss = 0.807\n",
      "Epoch 807 Batch   41/81   train_loss = 0.735\n",
      "Epoch 807 Batch   73/81   train_loss = 0.781\n",
      "Epoch 808 Batch   24/81   train_loss = 0.785\n",
      "Epoch 808 Batch   56/81   train_loss = 0.799\n",
      "Epoch 809 Batch    7/81   train_loss = 0.782\n",
      "Epoch 809 Batch   39/81   train_loss = 0.760\n",
      "Epoch 809 Batch   71/81   train_loss = 0.771\n",
      "Epoch 810 Batch   22/81   train_loss = 0.800\n",
      "Epoch 810 Batch   54/81   train_loss = 0.773\n",
      "Epoch 811 Batch    5/81   train_loss = 0.792\n",
      "Epoch 811 Batch   37/81   train_loss = 0.782\n",
      "Epoch 811 Batch   69/81   train_loss = 0.811\n",
      "Epoch 812 Batch   20/81   train_loss = 0.781\n",
      "Epoch 812 Batch   52/81   train_loss = 0.824\n",
      "Epoch 813 Batch    3/81   train_loss = 0.789\n",
      "Epoch 813 Batch   35/81   train_loss = 0.772\n",
      "Epoch 813 Batch   67/81   train_loss = 0.819\n",
      "Epoch 814 Batch   18/81   train_loss = 0.817\n",
      "Epoch 814 Batch   50/81   train_loss = 0.775\n",
      "Epoch 815 Batch    1/81   train_loss = 0.806\n",
      "Epoch 815 Batch   33/81   train_loss = 0.771\n",
      "Epoch 815 Batch   65/81   train_loss = 0.827\n",
      "Epoch 816 Batch   16/81   train_loss = 0.835\n",
      "Epoch 816 Batch   48/81   train_loss = 0.783\n",
      "Epoch 816 Batch   80/81   train_loss = 0.816\n",
      "Epoch 817 Batch   31/81   train_loss = 0.772\n",
      "Epoch 817 Batch   63/81   train_loss = 0.802\n",
      "Epoch 818 Batch   14/81   train_loss = 0.794\n",
      "Epoch 818 Batch   46/81   train_loss = 0.786\n",
      "Epoch 818 Batch   78/81   train_loss = 0.778\n",
      "Epoch 819 Batch   29/81   train_loss = 0.770\n",
      "Epoch 819 Batch   61/81   train_loss = 0.790\n",
      "Epoch 820 Batch   12/81   train_loss = 0.797\n",
      "Epoch 820 Batch   44/81   train_loss = 0.775\n",
      "Epoch 820 Batch   76/81   train_loss = 0.794\n",
      "Epoch 821 Batch   27/81   train_loss = 0.756\n",
      "Epoch 821 Batch   59/81   train_loss = 0.773\n",
      "Epoch 822 Batch   10/81   train_loss = 0.797\n",
      "Epoch 822 Batch   42/81   train_loss = 0.765\n",
      "Epoch 822 Batch   74/81   train_loss = 0.773\n",
      "Epoch 823 Batch   25/81   train_loss = 0.779\n",
      "Epoch 823 Batch   57/81   train_loss = 0.789\n",
      "Epoch 824 Batch    8/81   train_loss = 0.792\n",
      "Epoch 824 Batch   40/81   train_loss = 0.750\n",
      "Epoch 824 Batch   72/81   train_loss = 0.775\n",
      "Epoch 825 Batch   23/81   train_loss = 0.771\n",
      "Epoch 825 Batch   55/81   train_loss = 0.791\n",
      "Epoch 826 Batch    6/81   train_loss = 0.771\n",
      "Epoch 826 Batch   38/81   train_loss = 0.793\n",
      "Epoch 826 Batch   70/81   train_loss = 0.780\n",
      "Epoch 827 Batch   21/81   train_loss = 0.802\n",
      "Epoch 827 Batch   53/81   train_loss = 0.746\n",
      "Epoch 828 Batch    4/81   train_loss = 0.792\n",
      "Epoch 828 Batch   36/81   train_loss = 0.805\n",
      "Epoch 828 Batch   68/81   train_loss = 0.801\n",
      "Epoch 829 Batch   19/81   train_loss = 0.747\n",
      "Epoch 829 Batch   51/81   train_loss = 0.821\n",
      "Epoch 830 Batch    2/81   train_loss = 0.803\n",
      "Epoch 830 Batch   34/81   train_loss = 0.784\n",
      "Epoch 830 Batch   66/81   train_loss = 0.794\n",
      "Epoch 831 Batch   17/81   train_loss = 0.798\n",
      "Epoch 831 Batch   49/81   train_loss = 0.791\n",
      "Epoch 832 Batch    0/81   train_loss = 0.799\n",
      "Epoch 832 Batch   32/81   train_loss = 0.752\n",
      "Epoch 832 Batch   64/81   train_loss = 0.799\n",
      "Epoch 833 Batch   15/81   train_loss = 0.789\n",
      "Epoch 833 Batch   47/81   train_loss = 0.771\n",
      "Epoch 833 Batch   79/81   train_loss = 0.762\n",
      "Epoch 834 Batch   30/81   train_loss = 0.777\n",
      "Epoch 834 Batch   62/81   train_loss = 0.778\n",
      "Epoch 835 Batch   13/81   train_loss = 0.800\n",
      "Epoch 835 Batch   45/81   train_loss = 0.769\n",
      "Epoch 835 Batch   77/81   train_loss = 0.779\n",
      "Epoch 836 Batch   28/81   train_loss = 0.753\n",
      "Epoch 836 Batch   60/81   train_loss = 0.793\n",
      "Epoch 837 Batch   11/81   train_loss = 0.775\n",
      "Epoch 837 Batch   43/81   train_loss = 0.757\n",
      "Epoch 837 Batch   75/81   train_loss = 0.771\n",
      "Epoch 838 Batch   26/81   train_loss = 0.783\n",
      "Epoch 838 Batch   58/81   train_loss = 0.741\n",
      "Epoch 839 Batch    9/81   train_loss = 0.817\n",
      "Epoch 839 Batch   41/81   train_loss = 0.733\n",
      "Epoch 839 Batch   73/81   train_loss = 0.787\n",
      "Epoch 840 Batch   24/81   train_loss = 0.775\n",
      "Epoch 840 Batch   56/81   train_loss = 0.803\n",
      "Epoch 841 Batch    7/81   train_loss = 0.791\n",
      "Epoch 841 Batch   39/81   train_loss = 0.759\n",
      "Epoch 841 Batch   71/81   train_loss = 0.756\n",
      "Epoch 842 Batch   22/81   train_loss = 0.816\n",
      "Epoch 842 Batch   54/81   train_loss = 0.758\n",
      "Epoch 843 Batch    5/81   train_loss = 0.786\n",
      "Epoch 843 Batch   37/81   train_loss = 0.777\n",
      "Epoch 843 Batch   69/81   train_loss = 0.796\n",
      "Epoch 844 Batch   20/81   train_loss = 0.781\n",
      "Epoch 844 Batch   52/81   train_loss = 0.803\n",
      "Epoch 845 Batch    3/81   train_loss = 0.760\n",
      "Epoch 845 Batch   35/81   train_loss = 0.760\n",
      "Epoch 845 Batch   67/81   train_loss = 0.809\n",
      "Epoch 846 Batch   18/81   train_loss = 0.816\n",
      "Epoch 846 Batch   50/81   train_loss = 0.771\n",
      "Epoch 847 Batch    1/81   train_loss = 0.794\n",
      "Epoch 847 Batch   33/81   train_loss = 0.767\n",
      "Epoch 847 Batch   65/81   train_loss = 0.791\n",
      "Epoch 848 Batch   16/81   train_loss = 0.826\n",
      "Epoch 848 Batch   48/81   train_loss = 0.787\n",
      "Epoch 848 Batch   80/81   train_loss = 0.798\n",
      "Epoch 849 Batch   31/81   train_loss = 0.796\n",
      "Epoch 849 Batch   63/81   train_loss = 0.802\n",
      "Epoch 850 Batch   14/81   train_loss = 0.816\n",
      "Epoch 850 Batch   46/81   train_loss = 0.781\n",
      "Epoch 850 Batch   78/81   train_loss = 0.772\n",
      "Epoch 851 Batch   29/81   train_loss = 0.769\n",
      "Epoch 851 Batch   61/81   train_loss = 0.777\n",
      "Epoch 852 Batch   12/81   train_loss = 0.777\n",
      "Epoch 852 Batch   44/81   train_loss = 0.781\n",
      "Epoch 852 Batch   76/81   train_loss = 0.802\n",
      "Epoch 853 Batch   27/81   train_loss = 0.768\n",
      "Epoch 853 Batch   59/81   train_loss = 0.773\n",
      "Epoch 854 Batch   10/81   train_loss = 0.812\n",
      "Epoch 854 Batch   42/81   train_loss = 0.782\n",
      "Epoch 854 Batch   74/81   train_loss = 0.790\n",
      "Epoch 855 Batch   25/81   train_loss = 0.798\n",
      "Epoch 855 Batch   57/81   train_loss = 0.788\n",
      "Epoch 856 Batch    8/81   train_loss = 0.785\n",
      "Epoch 856 Batch   40/81   train_loss = 0.773\n",
      "Epoch 856 Batch   72/81   train_loss = 0.761\n",
      "Epoch 857 Batch   23/81   train_loss = 0.781\n",
      "Epoch 857 Batch   55/81   train_loss = 0.789\n",
      "Epoch 858 Batch    6/81   train_loss = 0.780\n",
      "Epoch 858 Batch   38/81   train_loss = 0.808\n",
      "Epoch 858 Batch   70/81   train_loss = 0.768\n",
      "Epoch 859 Batch   21/81   train_loss = 0.851\n",
      "Epoch 859 Batch   53/81   train_loss = 0.759\n",
      "Epoch 860 Batch    4/81   train_loss = 0.793\n",
      "Epoch 860 Batch   36/81   train_loss = 0.823\n",
      "Epoch 860 Batch   68/81   train_loss = 0.833\n",
      "Epoch 861 Batch   19/81   train_loss = 0.759\n",
      "Epoch 861 Batch   51/81   train_loss = 0.829\n",
      "Epoch 862 Batch    2/81   train_loss = 0.813\n",
      "Epoch 862 Batch   34/81   train_loss = 0.822\n",
      "Epoch 862 Batch   66/81   train_loss = 0.799\n",
      "Epoch 863 Batch   17/81   train_loss = 0.803\n",
      "Epoch 863 Batch   49/81   train_loss = 0.823\n",
      "Epoch 864 Batch    0/81   train_loss = 0.794\n",
      "Epoch 864 Batch   32/81   train_loss = 0.759\n",
      "Epoch 864 Batch   64/81   train_loss = 0.816\n",
      "Epoch 865 Batch   15/81   train_loss = 0.804\n",
      "Epoch 865 Batch   47/81   train_loss = 0.780\n",
      "Epoch 865 Batch   79/81   train_loss = 0.772\n",
      "Epoch 866 Batch   30/81   train_loss = 0.790\n",
      "Epoch 866 Batch   62/81   train_loss = 0.802\n",
      "Epoch 867 Batch   13/81   train_loss = 0.816\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 867 Batch   45/81   train_loss = 0.781\n",
      "Epoch 867 Batch   77/81   train_loss = 0.798\n",
      "Epoch 868 Batch   28/81   train_loss = 0.773\n",
      "Epoch 868 Batch   60/81   train_loss = 0.795\n",
      "Epoch 869 Batch   11/81   train_loss = 0.805\n",
      "Epoch 869 Batch   43/81   train_loss = 0.761\n",
      "Epoch 869 Batch   75/81   train_loss = 0.777\n",
      "Epoch 870 Batch   26/81   train_loss = 0.790\n",
      "Epoch 870 Batch   58/81   train_loss = 0.757\n",
      "Epoch 871 Batch    9/81   train_loss = 0.820\n",
      "Epoch 871 Batch   41/81   train_loss = 0.751\n",
      "Epoch 871 Batch   73/81   train_loss = 0.793\n",
      "Epoch 872 Batch   24/81   train_loss = 0.780\n",
      "Epoch 872 Batch   56/81   train_loss = 0.799\n",
      "Epoch 873 Batch    7/81   train_loss = 0.793\n",
      "Epoch 873 Batch   39/81   train_loss = 0.765\n",
      "Epoch 873 Batch   71/81   train_loss = 0.762\n",
      "Epoch 874 Batch   22/81   train_loss = 0.794\n",
      "Epoch 874 Batch   54/81   train_loss = 0.764\n",
      "Epoch 875 Batch    5/81   train_loss = 0.794\n",
      "Epoch 875 Batch   37/81   train_loss = 0.783\n",
      "Epoch 875 Batch   69/81   train_loss = 0.786\n",
      "Epoch 876 Batch   20/81   train_loss = 0.780\n",
      "Epoch 876 Batch   52/81   train_loss = 0.808\n",
      "Epoch 877 Batch    3/81   train_loss = 0.769\n",
      "Epoch 877 Batch   35/81   train_loss = 0.774\n",
      "Epoch 877 Batch   67/81   train_loss = 0.811\n",
      "Epoch 878 Batch   18/81   train_loss = 0.816\n",
      "Epoch 878 Batch   50/81   train_loss = 0.792\n",
      "Epoch 879 Batch    1/81   train_loss = 0.795\n",
      "Epoch 879 Batch   33/81   train_loss = 0.767\n",
      "Epoch 879 Batch   65/81   train_loss = 0.792\n",
      "Epoch 880 Batch   16/81   train_loss = 0.833\n",
      "Epoch 880 Batch   48/81   train_loss = 0.791\n",
      "Epoch 880 Batch   80/81   train_loss = 0.788\n",
      "Epoch 881 Batch   31/81   train_loss = 0.755\n",
      "Epoch 881 Batch   63/81   train_loss = 0.789\n",
      "Epoch 882 Batch   14/81   train_loss = 0.790\n",
      "Epoch 882 Batch   46/81   train_loss = 0.759\n",
      "Epoch 882 Batch   78/81   train_loss = 0.774\n",
      "Epoch 883 Batch   29/81   train_loss = 0.766\n",
      "Epoch 883 Batch   61/81   train_loss = 0.775\n",
      "Epoch 884 Batch   12/81   train_loss = 0.783\n",
      "Epoch 884 Batch   44/81   train_loss = 0.775\n",
      "Epoch 884 Batch   76/81   train_loss = 0.777\n",
      "Epoch 885 Batch   27/81   train_loss = 0.757\n",
      "Epoch 885 Batch   59/81   train_loss = 0.755\n",
      "Epoch 886 Batch   10/81   train_loss = 0.789\n",
      "Epoch 886 Batch   42/81   train_loss = 0.771\n",
      "Epoch 886 Batch   74/81   train_loss = 0.793\n",
      "Epoch 887 Batch   25/81   train_loss = 0.779\n",
      "Epoch 887 Batch   57/81   train_loss = 0.782\n",
      "Epoch 888 Batch    8/81   train_loss = 0.796\n",
      "Epoch 888 Batch   40/81   train_loss = 0.760\n",
      "Epoch 888 Batch   72/81   train_loss = 0.769\n",
      "Epoch 889 Batch   23/81   train_loss = 0.788\n",
      "Epoch 889 Batch   55/81   train_loss = 0.794\n",
      "Epoch 890 Batch    6/81   train_loss = 0.766\n",
      "Epoch 890 Batch   38/81   train_loss = 0.801\n",
      "Epoch 890 Batch   70/81   train_loss = 0.780\n",
      "Epoch 891 Batch   21/81   train_loss = 0.850\n",
      "Epoch 891 Batch   53/81   train_loss = 0.787\n",
      "Epoch 892 Batch    4/81   train_loss = 0.798\n",
      "Epoch 892 Batch   36/81   train_loss = 0.799\n",
      "Epoch 892 Batch   68/81   train_loss = 0.821\n",
      "Epoch 893 Batch   19/81   train_loss = 0.752\n",
      "Epoch 893 Batch   51/81   train_loss = 0.823\n",
      "Epoch 894 Batch    2/81   train_loss = 0.802\n",
      "Epoch 894 Batch   34/81   train_loss = 0.803\n",
      "Epoch 894 Batch   66/81   train_loss = 0.778\n",
      "Epoch 895 Batch   17/81   train_loss = 0.794\n",
      "Epoch 895 Batch   49/81   train_loss = 0.786\n",
      "Epoch 896 Batch    0/81   train_loss = 0.786\n",
      "Epoch 896 Batch   32/81   train_loss = 0.761\n",
      "Epoch 896 Batch   64/81   train_loss = 0.790\n",
      "Epoch 897 Batch   15/81   train_loss = 0.786\n",
      "Epoch 897 Batch   47/81   train_loss = 0.770\n",
      "Epoch 897 Batch   79/81   train_loss = 0.787\n",
      "Epoch 898 Batch   30/81   train_loss = 0.771\n",
      "Epoch 898 Batch   62/81   train_loss = 0.781\n",
      "Epoch 899 Batch   13/81   train_loss = 0.795\n",
      "Epoch 899 Batch   45/81   train_loss = 0.779\n",
      "Epoch 899 Batch   77/81   train_loss = 0.810\n",
      "Epoch 900 Batch   28/81   train_loss = 0.785\n",
      "Epoch 900 Batch   60/81   train_loss = 0.788\n",
      "Epoch 901 Batch   11/81   train_loss = 0.792\n",
      "Epoch 901 Batch   43/81   train_loss = 0.754\n",
      "Epoch 901 Batch   75/81   train_loss = 0.770\n",
      "Epoch 902 Batch   26/81   train_loss = 0.781\n",
      "Epoch 902 Batch   58/81   train_loss = 0.756\n",
      "Epoch 903 Batch    9/81   train_loss = 0.815\n",
      "Epoch 903 Batch   41/81   train_loss = 0.758\n",
      "Epoch 903 Batch   73/81   train_loss = 0.791\n",
      "Epoch 904 Batch   24/81   train_loss = 0.792\n",
      "Epoch 904 Batch   56/81   train_loss = 0.802\n",
      "Epoch 905 Batch    7/81   train_loss = 0.781\n",
      "Epoch 905 Batch   39/81   train_loss = 0.757\n",
      "Epoch 905 Batch   71/81   train_loss = 0.758\n",
      "Epoch 906 Batch   22/81   train_loss = 0.795\n",
      "Epoch 906 Batch   54/81   train_loss = 0.775\n",
      "Epoch 907 Batch    5/81   train_loss = 0.793\n",
      "Epoch 907 Batch   37/81   train_loss = 0.792\n",
      "Epoch 907 Batch   69/81   train_loss = 0.787\n",
      "Epoch 908 Batch   20/81   train_loss = 0.781\n",
      "Epoch 908 Batch   52/81   train_loss = 0.796\n",
      "Epoch 909 Batch    3/81   train_loss = 0.759\n",
      "Epoch 909 Batch   35/81   train_loss = 0.781\n",
      "Epoch 909 Batch   67/81   train_loss = 0.811\n",
      "Epoch 910 Batch   18/81   train_loss = 0.816\n",
      "Epoch 910 Batch   50/81   train_loss = 0.772\n",
      "Epoch 911 Batch    1/81   train_loss = 0.812\n",
      "Epoch 911 Batch   33/81   train_loss = 0.772\n",
      "Epoch 911 Batch   65/81   train_loss = 0.823\n",
      "Epoch 912 Batch   16/81   train_loss = 0.834\n",
      "Epoch 912 Batch   48/81   train_loss = 0.774\n",
      "Epoch 912 Batch   80/81   train_loss = 0.806\n",
      "Epoch 913 Batch   31/81   train_loss = 0.766\n",
      "Epoch 913 Batch   63/81   train_loss = 0.796\n",
      "Epoch 914 Batch   14/81   train_loss = 0.795\n",
      "Epoch 914 Batch   46/81   train_loss = 0.788\n",
      "Epoch 914 Batch   78/81   train_loss = 0.780\n",
      "Epoch 915 Batch   29/81   train_loss = 0.786\n",
      "Epoch 915 Batch   61/81   train_loss = 0.787\n",
      "Epoch 916 Batch   12/81   train_loss = 0.784\n",
      "Epoch 916 Batch   44/81   train_loss = 0.780\n",
      "Epoch 916 Batch   76/81   train_loss = 0.783\n",
      "Epoch 917 Batch   27/81   train_loss = 0.764\n",
      "Epoch 917 Batch   59/81   train_loss = 0.790\n",
      "Epoch 918 Batch   10/81   train_loss = 0.800\n",
      "Epoch 918 Batch   42/81   train_loss = 0.774\n",
      "Epoch 918 Batch   74/81   train_loss = 0.788\n",
      "Epoch 919 Batch   25/81   train_loss = 0.791\n",
      "Epoch 919 Batch   57/81   train_loss = 0.777\n",
      "Epoch 920 Batch    8/81   train_loss = 0.788\n",
      "Epoch 920 Batch   40/81   train_loss = 0.780\n",
      "Epoch 920 Batch   72/81   train_loss = 0.761\n",
      "Epoch 921 Batch   23/81   train_loss = 0.770\n",
      "Epoch 921 Batch   55/81   train_loss = 0.791\n",
      "Epoch 922 Batch    6/81   train_loss = 0.765\n",
      "Epoch 922 Batch   38/81   train_loss = 0.796\n",
      "Epoch 922 Batch   70/81   train_loss = 0.771\n",
      "Epoch 923 Batch   21/81   train_loss = 0.823\n",
      "Epoch 923 Batch   53/81   train_loss = 0.777\n",
      "Epoch 924 Batch    4/81   train_loss = 0.780\n",
      "Epoch 924 Batch   36/81   train_loss = 0.799\n",
      "Epoch 924 Batch   68/81   train_loss = 0.817\n",
      "Epoch 925 Batch   19/81   train_loss = 0.761\n",
      "Epoch 925 Batch   51/81   train_loss = 0.830\n",
      "Epoch 926 Batch    2/81   train_loss = 0.787\n",
      "Epoch 926 Batch   34/81   train_loss = 0.804\n",
      "Epoch 926 Batch   66/81   train_loss = 0.781\n",
      "Epoch 927 Batch   17/81   train_loss = 0.810\n",
      "Epoch 927 Batch   49/81   train_loss = 0.786\n",
      "Epoch 928 Batch    0/81   train_loss = 0.778\n",
      "Epoch 928 Batch   32/81   train_loss = 0.770\n",
      "Epoch 928 Batch   64/81   train_loss = 0.810\n",
      "Epoch 929 Batch   15/81   train_loss = 0.781\n",
      "Epoch 929 Batch   47/81   train_loss = 0.777\n",
      "Epoch 929 Batch   79/81   train_loss = 0.770\n",
      "Epoch 930 Batch   30/81   train_loss = 0.780\n",
      "Epoch 930 Batch   62/81   train_loss = 0.772\n",
      "Epoch 931 Batch   13/81   train_loss = 0.784\n",
      "Epoch 931 Batch   45/81   train_loss = 0.776\n",
      "Epoch 931 Batch   77/81   train_loss = 0.793\n",
      "Epoch 932 Batch   28/81   train_loss = 0.761\n",
      "Epoch 932 Batch   60/81   train_loss = 0.794\n",
      "Epoch 933 Batch   11/81   train_loss = 0.789\n",
      "Epoch 933 Batch   43/81   train_loss = 0.759\n",
      "Epoch 933 Batch   75/81   train_loss = 0.769\n",
      "Epoch 934 Batch   26/81   train_loss = 0.785\n",
      "Epoch 934 Batch   58/81   train_loss = 0.744\n",
      "Epoch 935 Batch    9/81   train_loss = 0.797\n",
      "Epoch 935 Batch   41/81   train_loss = 0.750\n",
      "Epoch 935 Batch   73/81   train_loss = 0.776\n",
      "Epoch 936 Batch   24/81   train_loss = 0.783\n",
      "Epoch 936 Batch   56/81   train_loss = 0.792\n",
      "Epoch 937 Batch    7/81   train_loss = 0.766\n",
      "Epoch 937 Batch   39/81   train_loss = 0.752\n",
      "Epoch 937 Batch   71/81   train_loss = 0.745\n",
      "Epoch 938 Batch   22/81   train_loss = 0.801\n",
      "Epoch 938 Batch   54/81   train_loss = 0.779\n",
      "Epoch 939 Batch    5/81   train_loss = 0.786\n",
      "Epoch 939 Batch   37/81   train_loss = 0.787\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 939 Batch   69/81   train_loss = 0.811\n",
      "Epoch 940 Batch   20/81   train_loss = 0.788\n",
      "Epoch 940 Batch   52/81   train_loss = 0.802\n",
      "Epoch 941 Batch    3/81   train_loss = 0.765\n",
      "Epoch 941 Batch   35/81   train_loss = 0.762\n",
      "Epoch 941 Batch   67/81   train_loss = 0.808\n",
      "Epoch 942 Batch   18/81   train_loss = 0.792\n",
      "Epoch 942 Batch   50/81   train_loss = 0.776\n",
      "Epoch 943 Batch    1/81   train_loss = 0.805\n",
      "Epoch 943 Batch   33/81   train_loss = 0.762\n",
      "Epoch 943 Batch   65/81   train_loss = 0.793\n",
      "Epoch 944 Batch   16/81   train_loss = 0.836\n",
      "Epoch 944 Batch   48/81   train_loss = 0.781\n",
      "Epoch 944 Batch   80/81   train_loss = 0.796\n",
      "Epoch 945 Batch   31/81   train_loss = 0.772\n",
      "Epoch 945 Batch   63/81   train_loss = 0.799\n",
      "Epoch 946 Batch   14/81   train_loss = 0.787\n",
      "Epoch 946 Batch   46/81   train_loss = 0.785\n",
      "Epoch 946 Batch   78/81   train_loss = 0.771\n",
      "Epoch 947 Batch   29/81   train_loss = 0.759\n",
      "Epoch 947 Batch   61/81   train_loss = 0.792\n",
      "Epoch 948 Batch   12/81   train_loss = 0.799\n",
      "Epoch 948 Batch   44/81   train_loss = 0.782\n",
      "Epoch 948 Batch   76/81   train_loss = 0.787\n",
      "Epoch 949 Batch   27/81   train_loss = 0.760\n",
      "Epoch 949 Batch   59/81   train_loss = 0.772\n",
      "Epoch 950 Batch   10/81   train_loss = 0.800\n",
      "Epoch 950 Batch   42/81   train_loss = 0.785\n",
      "Epoch 950 Batch   74/81   train_loss = 0.784\n",
      "Epoch 951 Batch   25/81   train_loss = 0.799\n",
      "Epoch 951 Batch   57/81   train_loss = 0.792\n",
      "Epoch 952 Batch    8/81   train_loss = 0.804\n",
      "Epoch 952 Batch   40/81   train_loss = 0.754\n",
      "Epoch 952 Batch   72/81   train_loss = 0.781\n",
      "Epoch 953 Batch   23/81   train_loss = 0.783\n",
      "Epoch 953 Batch   55/81   train_loss = 0.793\n",
      "Epoch 954 Batch    6/81   train_loss = 0.772\n",
      "Epoch 954 Batch   38/81   train_loss = 0.801\n",
      "Epoch 954 Batch   70/81   train_loss = 0.786\n",
      "Epoch 955 Batch   21/81   train_loss = 0.829\n",
      "Epoch 955 Batch   53/81   train_loss = 0.769\n",
      "Epoch 956 Batch    4/81   train_loss = 0.804\n",
      "Epoch 956 Batch   36/81   train_loss = 0.801\n",
      "Epoch 956 Batch   68/81   train_loss = 0.812\n",
      "Epoch 957 Batch   19/81   train_loss = 0.757\n",
      "Epoch 957 Batch   51/81   train_loss = 0.828\n",
      "Epoch 958 Batch    2/81   train_loss = 0.807\n",
      "Epoch 958 Batch   34/81   train_loss = 0.821\n",
      "Epoch 958 Batch   66/81   train_loss = 0.781\n",
      "Epoch 959 Batch   17/81   train_loss = 0.805\n",
      "Epoch 959 Batch   49/81   train_loss = 0.805\n",
      "Epoch 960 Batch    0/81   train_loss = 0.803\n",
      "Epoch 960 Batch   32/81   train_loss = 0.776\n",
      "Epoch 960 Batch   64/81   train_loss = 0.806\n",
      "Epoch 961 Batch   15/81   train_loss = 0.779\n",
      "Epoch 961 Batch   47/81   train_loss = 0.783\n",
      "Epoch 961 Batch   79/81   train_loss = 0.793\n",
      "Epoch 962 Batch   30/81   train_loss = 0.791\n",
      "Epoch 962 Batch   62/81   train_loss = 0.790\n",
      "Epoch 963 Batch   13/81   train_loss = 0.810\n",
      "Epoch 963 Batch   45/81   train_loss = 0.782\n",
      "Epoch 963 Batch   77/81   train_loss = 0.801\n",
      "Epoch 964 Batch   28/81   train_loss = 0.781\n",
      "Epoch 964 Batch   60/81   train_loss = 0.794\n",
      "Epoch 965 Batch   11/81   train_loss = 0.795\n",
      "Epoch 965 Batch   43/81   train_loss = 0.758\n",
      "Epoch 965 Batch   75/81   train_loss = 0.776\n",
      "Epoch 966 Batch   26/81   train_loss = 0.787\n",
      "Epoch 966 Batch   58/81   train_loss = 0.759\n",
      "Epoch 967 Batch    9/81   train_loss = 0.804\n",
      "Epoch 967 Batch   41/81   train_loss = 0.752\n",
      "Epoch 967 Batch   73/81   train_loss = 0.796\n",
      "Epoch 968 Batch   24/81   train_loss = 0.761\n",
      "Epoch 968 Batch   56/81   train_loss = 0.790\n",
      "Epoch 969 Batch    7/81   train_loss = 0.790\n",
      "Epoch 969 Batch   39/81   train_loss = 0.752\n",
      "Epoch 969 Batch   71/81   train_loss = 0.765\n",
      "Epoch 970 Batch   22/81   train_loss = 0.790\n",
      "Epoch 970 Batch   54/81   train_loss = 0.754\n",
      "Epoch 971 Batch    5/81   train_loss = 0.775\n",
      "Epoch 971 Batch   37/81   train_loss = 0.772\n",
      "Epoch 971 Batch   69/81   train_loss = 0.801\n",
      "Epoch 972 Batch   20/81   train_loss = 0.778\n",
      "Epoch 972 Batch   52/81   train_loss = 0.794\n",
      "Epoch 973 Batch    3/81   train_loss = 0.773\n",
      "Epoch 973 Batch   35/81   train_loss = 0.757\n",
      "Epoch 973 Batch   67/81   train_loss = 0.818\n",
      "Epoch 974 Batch   18/81   train_loss = 0.800\n",
      "Epoch 974 Batch   50/81   train_loss = 0.777\n",
      "Epoch 975 Batch    1/81   train_loss = 0.800\n",
      "Epoch 975 Batch   33/81   train_loss = 0.755\n",
      "Epoch 975 Batch   65/81   train_loss = 0.793\n",
      "Epoch 976 Batch   16/81   train_loss = 0.819\n",
      "Epoch 976 Batch   48/81   train_loss = 0.758\n",
      "Epoch 976 Batch   80/81   train_loss = 0.785\n",
      "Epoch 977 Batch   31/81   train_loss = 0.760\n",
      "Epoch 977 Batch   63/81   train_loss = 0.788\n",
      "Epoch 978 Batch   14/81   train_loss = 0.786\n",
      "Epoch 978 Batch   46/81   train_loss = 0.762\n",
      "Epoch 978 Batch   78/81   train_loss = 0.763\n",
      "Epoch 979 Batch   29/81   train_loss = 0.759\n",
      "Epoch 979 Batch   61/81   train_loss = 0.768\n",
      "Epoch 980 Batch   12/81   train_loss = 0.782\n",
      "Epoch 980 Batch   44/81   train_loss = 0.767\n",
      "Epoch 980 Batch   76/81   train_loss = 0.786\n",
      "Epoch 981 Batch   27/81   train_loss = 0.749\n",
      "Epoch 981 Batch   59/81   train_loss = 0.769\n",
      "Epoch 982 Batch   10/81   train_loss = 0.802\n",
      "Epoch 982 Batch   42/81   train_loss = 0.785\n",
      "Epoch 982 Batch   74/81   train_loss = 0.788\n",
      "Epoch 983 Batch   25/81   train_loss = 0.796\n",
      "Epoch 983 Batch   57/81   train_loss = 0.782\n",
      "Epoch 984 Batch    8/81   train_loss = 0.796\n",
      "Epoch 984 Batch   40/81   train_loss = 0.751\n",
      "Epoch 984 Batch   72/81   train_loss = 0.783\n",
      "Epoch 985 Batch   23/81   train_loss = 0.788\n",
      "Epoch 985 Batch   55/81   train_loss = 0.793\n",
      "Epoch 986 Batch    6/81   train_loss = 0.770\n",
      "Epoch 986 Batch   38/81   train_loss = 0.806\n",
      "Epoch 986 Batch   70/81   train_loss = 0.771\n",
      "Epoch 987 Batch   21/81   train_loss = 0.804\n",
      "Epoch 987 Batch   53/81   train_loss = 0.765\n",
      "Epoch 988 Batch    4/81   train_loss = 0.774\n",
      "Epoch 988 Batch   36/81   train_loss = 0.793\n",
      "Epoch 988 Batch   68/81   train_loss = 0.800\n",
      "Epoch 989 Batch   19/81   train_loss = 0.764\n",
      "Epoch 989 Batch   51/81   train_loss = 0.816\n",
      "Epoch 990 Batch    2/81   train_loss = 0.790\n",
      "Epoch 990 Batch   34/81   train_loss = 0.801\n",
      "Epoch 990 Batch   66/81   train_loss = 0.786\n",
      "Epoch 991 Batch   17/81   train_loss = 0.806\n",
      "Epoch 991 Batch   49/81   train_loss = 0.777\n",
      "Epoch 992 Batch    0/81   train_loss = 0.788\n",
      "Epoch 992 Batch   32/81   train_loss = 0.761\n",
      "Epoch 992 Batch   64/81   train_loss = 0.806\n",
      "Epoch 993 Batch   15/81   train_loss = 0.777\n",
      "Epoch 993 Batch   47/81   train_loss = 0.758\n",
      "Epoch 993 Batch   79/81   train_loss = 0.764\n",
      "Epoch 994 Batch   30/81   train_loss = 0.777\n",
      "Epoch 994 Batch   62/81   train_loss = 0.785\n",
      "Epoch 995 Batch   13/81   train_loss = 0.791\n",
      "Epoch 995 Batch   45/81   train_loss = 0.781\n",
      "Epoch 995 Batch   77/81   train_loss = 0.799\n",
      "Epoch 996 Batch   28/81   train_loss = 0.767\n",
      "Epoch 996 Batch   60/81   train_loss = 0.795\n",
      "Epoch 997 Batch   11/81   train_loss = 0.785\n",
      "Epoch 997 Batch   43/81   train_loss = 0.764\n",
      "Epoch 997 Batch   75/81   train_loss = 0.773\n",
      "Epoch 998 Batch   26/81   train_loss = 0.791\n",
      "Epoch 998 Batch   58/81   train_loss = 0.754\n",
      "Epoch 999 Batch    9/81   train_loss = 0.809\n",
      "Epoch 999 Batch   41/81   train_loss = 0.732\n",
      "Epoch 999 Batch   73/81   train_loss = 0.793\n",
      "Epoch 1000 Batch   24/81   train_loss = 0.785\n",
      "Epoch 1000 Batch   56/81   train_loss = 0.789\n",
      "Epoch 1001 Batch    7/81   train_loss = 0.777\n",
      "Epoch 1001 Batch   39/81   train_loss = 0.767\n",
      "Epoch 1001 Batch   71/81   train_loss = 0.764\n",
      "Epoch 1002 Batch   22/81   train_loss = 0.778\n",
      "Epoch 1002 Batch   54/81   train_loss = 0.766\n",
      "Epoch 1003 Batch    5/81   train_loss = 0.789\n",
      "Epoch 1003 Batch   37/81   train_loss = 0.774\n",
      "Epoch 1003 Batch   69/81   train_loss = 0.786\n",
      "Epoch 1004 Batch   20/81   train_loss = 0.776\n",
      "Epoch 1004 Batch   52/81   train_loss = 0.802\n",
      "Epoch 1005 Batch    3/81   train_loss = 0.765\n",
      "Epoch 1005 Batch   35/81   train_loss = 0.762\n",
      "Epoch 1005 Batch   67/81   train_loss = 0.821\n",
      "Epoch 1006 Batch   18/81   train_loss = 0.798\n",
      "Epoch 1006 Batch   50/81   train_loss = 0.768\n",
      "Epoch 1007 Batch    1/81   train_loss = 0.814\n",
      "Epoch 1007 Batch   33/81   train_loss = 0.755\n",
      "Epoch 1007 Batch   65/81   train_loss = 0.794\n",
      "Epoch 1008 Batch   16/81   train_loss = 0.814\n",
      "Epoch 1008 Batch   48/81   train_loss = 0.771\n",
      "Epoch 1008 Batch   80/81   train_loss = 0.786\n",
      "Epoch 1009 Batch   31/81   train_loss = 0.755\n",
      "Epoch 1009 Batch   63/81   train_loss = 0.787\n",
      "Epoch 1010 Batch   14/81   train_loss = 0.787\n",
      "Epoch 1010 Batch   46/81   train_loss = 0.777\n",
      "Epoch 1010 Batch   78/81   train_loss = 0.775\n",
      "Epoch 1011 Batch   29/81   train_loss = 0.774\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1011 Batch   61/81   train_loss = 0.777\n",
      "Epoch 1012 Batch   12/81   train_loss = 0.783\n",
      "Epoch 1012 Batch   44/81   train_loss = 0.756\n",
      "Epoch 1012 Batch   76/81   train_loss = 0.791\n",
      "Epoch 1013 Batch   27/81   train_loss = 0.751\n",
      "Epoch 1013 Batch   59/81   train_loss = 0.760\n",
      "Epoch 1014 Batch   10/81   train_loss = 0.775\n",
      "Epoch 1014 Batch   42/81   train_loss = 0.774\n",
      "Epoch 1014 Batch   74/81   train_loss = 0.788\n",
      "Epoch 1015 Batch   25/81   train_loss = 0.764\n",
      "Epoch 1015 Batch   57/81   train_loss = 0.792\n",
      "Epoch 1016 Batch    8/81   train_loss = 0.808\n",
      "Epoch 1016 Batch   40/81   train_loss = 0.759\n",
      "Epoch 1016 Batch   72/81   train_loss = 0.772\n",
      "Epoch 1017 Batch   23/81   train_loss = 0.767\n",
      "Epoch 1017 Batch   55/81   train_loss = 0.789\n",
      "Epoch 1018 Batch    6/81   train_loss = 0.770\n",
      "Epoch 1018 Batch   38/81   train_loss = 0.776\n",
      "Epoch 1018 Batch   70/81   train_loss = 0.779\n",
      "Epoch 1019 Batch   21/81   train_loss = 0.833\n",
      "Epoch 1019 Batch   53/81   train_loss = 0.760\n",
      "Epoch 1020 Batch    4/81   train_loss = 0.803\n",
      "Epoch 1020 Batch   36/81   train_loss = 0.789\n",
      "Epoch 1020 Batch   68/81   train_loss = 0.820\n",
      "Epoch 1021 Batch   19/81   train_loss = 0.784\n",
      "Epoch 1021 Batch   51/81   train_loss = 0.815\n",
      "Epoch 1022 Batch    2/81   train_loss = 0.827\n",
      "Epoch 1022 Batch   34/81   train_loss = 0.813\n",
      "Epoch 1022 Batch   66/81   train_loss = 0.784\n",
      "Epoch 1023 Batch   17/81   train_loss = 0.791\n",
      "Epoch 1023 Batch   49/81   train_loss = 0.807\n",
      "Epoch 1024 Batch    0/81   train_loss = 0.801\n",
      "Epoch 1024 Batch   32/81   train_loss = 0.759\n",
      "Epoch 1024 Batch   64/81   train_loss = 0.815\n",
      "Epoch 1025 Batch   15/81   train_loss = 0.810\n",
      "Epoch 1025 Batch   47/81   train_loss = 0.776\n",
      "Epoch 1025 Batch   79/81   train_loss = 0.772\n",
      "Epoch 1026 Batch   30/81   train_loss = 0.783\n",
      "Epoch 1026 Batch   62/81   train_loss = 0.784\n",
      "Epoch 1027 Batch   13/81   train_loss = 0.811\n",
      "Epoch 1027 Batch   45/81   train_loss = 0.764\n",
      "Epoch 1027 Batch   77/81   train_loss = 0.819\n",
      "Epoch 1028 Batch   28/81   train_loss = 0.755\n",
      "Epoch 1028 Batch   60/81   train_loss = 0.798\n",
      "Epoch 1029 Batch   11/81   train_loss = 0.799\n",
      "Epoch 1029 Batch   43/81   train_loss = 0.765\n",
      "Epoch 1029 Batch   75/81   train_loss = 0.773\n",
      "Epoch 1030 Batch   26/81   train_loss = 0.786\n",
      "Epoch 1030 Batch   58/81   train_loss = 0.765\n",
      "Epoch 1031 Batch    9/81   train_loss = 0.808\n",
      "Epoch 1031 Batch   41/81   train_loss = 0.754\n",
      "Epoch 1031 Batch   73/81   train_loss = 0.796\n",
      "Epoch 1032 Batch   24/81   train_loss = 0.779\n",
      "Epoch 1032 Batch   56/81   train_loss = 0.806\n",
      "Epoch 1033 Batch    7/81   train_loss = 0.787\n",
      "Epoch 1033 Batch   39/81   train_loss = 0.752\n",
      "Epoch 1033 Batch   71/81   train_loss = 0.784\n",
      "Epoch 1034 Batch   22/81   train_loss = 0.805\n",
      "Epoch 1034 Batch   54/81   train_loss = 0.780\n",
      "Epoch 1035 Batch    5/81   train_loss = 0.815\n",
      "Epoch 1035 Batch   37/81   train_loss = 0.791\n",
      "Epoch 1035 Batch   69/81   train_loss = 0.794\n",
      "Epoch 1036 Batch   20/81   train_loss = 0.785\n",
      "Epoch 1036 Batch   52/81   train_loss = 0.808\n",
      "Epoch 1037 Batch    3/81   train_loss = 0.767\n",
      "Epoch 1037 Batch   35/81   train_loss = 0.769\n",
      "Epoch 1037 Batch   67/81   train_loss = 0.807\n",
      "Epoch 1038 Batch   18/81   train_loss = 0.811\n",
      "Epoch 1038 Batch   50/81   train_loss = 0.763\n",
      "Epoch 1039 Batch    1/81   train_loss = 0.811\n",
      "Epoch 1039 Batch   33/81   train_loss = 0.775\n",
      "Epoch 1039 Batch   65/81   train_loss = 0.790\n",
      "Epoch 1040 Batch   16/81   train_loss = 0.839\n",
      "Epoch 1040 Batch   48/81   train_loss = 0.775\n",
      "Epoch 1040 Batch   80/81   train_loss = 0.782\n",
      "Epoch 1041 Batch   31/81   train_loss = 0.765\n",
      "Epoch 1041 Batch   63/81   train_loss = 0.796\n",
      "Epoch 1042 Batch   14/81   train_loss = 0.792\n",
      "Epoch 1042 Batch   46/81   train_loss = 0.800\n",
      "Epoch 1042 Batch   78/81   train_loss = 0.757\n",
      "Epoch 1043 Batch   29/81   train_loss = 0.785\n",
      "Epoch 1043 Batch   61/81   train_loss = 0.771\n",
      "Epoch 1044 Batch   12/81   train_loss = 0.792\n",
      "Epoch 1044 Batch   44/81   train_loss = 0.768\n",
      "Epoch 1044 Batch   76/81   train_loss = 0.788\n",
      "Epoch 1045 Batch   27/81   train_loss = 0.753\n",
      "Epoch 1045 Batch   59/81   train_loss = 0.776\n",
      "Epoch 1046 Batch   10/81   train_loss = 0.810\n",
      "Epoch 1046 Batch   42/81   train_loss = 0.763\n",
      "Epoch 1046 Batch   74/81   train_loss = 0.790\n",
      "Epoch 1047 Batch   25/81   train_loss = 0.783\n",
      "Epoch 1047 Batch   57/81   train_loss = 0.790\n",
      "Epoch 1048 Batch    8/81   train_loss = 0.796\n",
      "Epoch 1048 Batch   40/81   train_loss = 0.762\n",
      "Epoch 1048 Batch   72/81   train_loss = 0.765\n",
      "Epoch 1049 Batch   23/81   train_loss = 0.757\n",
      "Epoch 1049 Batch   55/81   train_loss = 0.784\n",
      "Epoch 1050 Batch    6/81   train_loss = 0.757\n",
      "Epoch 1050 Batch   38/81   train_loss = 0.797\n",
      "Epoch 1050 Batch   70/81   train_loss = 0.762\n",
      "Epoch 1051 Batch   21/81   train_loss = 0.838\n",
      "Epoch 1051 Batch   53/81   train_loss = 0.760\n",
      "Epoch 1052 Batch    4/81   train_loss = 0.795\n",
      "Epoch 1052 Batch   36/81   train_loss = 0.779\n",
      "Epoch 1052 Batch   68/81   train_loss = 0.803\n",
      "Epoch 1053 Batch   19/81   train_loss = 0.767\n",
      "Epoch 1053 Batch   51/81   train_loss = 0.819\n",
      "Epoch 1054 Batch    2/81   train_loss = 0.781\n",
      "Epoch 1054 Batch   34/81   train_loss = 0.792\n",
      "Epoch 1054 Batch   66/81   train_loss = 0.783\n",
      "Epoch 1055 Batch   17/81   train_loss = 0.806\n",
      "Epoch 1055 Batch   49/81   train_loss = 0.786\n",
      "Epoch 1056 Batch    0/81   train_loss = 0.783\n",
      "Epoch 1056 Batch   32/81   train_loss = 0.755\n",
      "Epoch 1056 Batch   64/81   train_loss = 0.812\n",
      "Epoch 1057 Batch   15/81   train_loss = 0.771\n",
      "Epoch 1057 Batch   47/81   train_loss = 0.769\n",
      "Epoch 1057 Batch   79/81   train_loss = 0.760\n",
      "Epoch 1058 Batch   30/81   train_loss = 0.767\n",
      "Epoch 1058 Batch   62/81   train_loss = 0.780\n",
      "Epoch 1059 Batch   13/81   train_loss = 0.788\n",
      "Epoch 1059 Batch   45/81   train_loss = 0.764\n",
      "Epoch 1059 Batch   77/81   train_loss = 0.788\n",
      "Epoch 1060 Batch   28/81   train_loss = 0.757\n",
      "Epoch 1060 Batch   60/81   train_loss = 0.786\n",
      "Epoch 1061 Batch   11/81   train_loss = 0.779\n",
      "Epoch 1061 Batch   43/81   train_loss = 0.752\n",
      "Epoch 1061 Batch   75/81   train_loss = 0.758\n",
      "Epoch 1062 Batch   26/81   train_loss = 0.771\n",
      "Epoch 1062 Batch   58/81   train_loss = 0.749\n",
      "Epoch 1063 Batch    9/81   train_loss = 0.787\n",
      "Epoch 1063 Batch   41/81   train_loss = 0.744\n",
      "Epoch 1063 Batch   73/81   train_loss = 0.766\n",
      "Epoch 1064 Batch   24/81   train_loss = 0.781\n",
      "Epoch 1064 Batch   56/81   train_loss = 0.803\n",
      "Epoch 1065 Batch    7/81   train_loss = 0.772\n",
      "Epoch 1065 Batch   39/81   train_loss = 0.755\n",
      "Epoch 1065 Batch   71/81   train_loss = 0.756\n",
      "Epoch 1066 Batch   22/81   train_loss = 0.786\n",
      "Epoch 1066 Batch   54/81   train_loss = 0.752\n",
      "Epoch 1067 Batch    5/81   train_loss = 0.794\n",
      "Epoch 1067 Batch   37/81   train_loss = 0.767\n",
      "Epoch 1067 Batch   69/81   train_loss = 0.775\n",
      "Epoch 1068 Batch   20/81   train_loss = 0.762\n",
      "Epoch 1068 Batch   52/81   train_loss = 0.800\n",
      "Epoch 1069 Batch    3/81   train_loss = 0.759\n",
      "Epoch 1069 Batch   35/81   train_loss = 0.758\n",
      "Epoch 1069 Batch   67/81   train_loss = 0.804\n",
      "Epoch 1070 Batch   18/81   train_loss = 0.796\n",
      "Epoch 1070 Batch   50/81   train_loss = 0.759\n",
      "Epoch 1071 Batch    1/81   train_loss = 0.800\n",
      "Epoch 1071 Batch   33/81   train_loss = 0.744\n",
      "Epoch 1071 Batch   65/81   train_loss = 0.785\n",
      "Epoch 1072 Batch   16/81   train_loss = 0.804\n",
      "Epoch 1072 Batch   48/81   train_loss = 0.781\n",
      "Epoch 1072 Batch   80/81   train_loss = 0.788\n",
      "Epoch 1073 Batch   31/81   train_loss = 0.774\n",
      "Epoch 1073 Batch   63/81   train_loss = 0.796\n",
      "Epoch 1074 Batch   14/81   train_loss = 0.804\n",
      "Epoch 1074 Batch   46/81   train_loss = 0.778\n",
      "Epoch 1074 Batch   78/81   train_loss = 0.762\n",
      "Epoch 1075 Batch   29/81   train_loss = 0.768\n",
      "Epoch 1075 Batch   61/81   train_loss = 0.765\n",
      "Epoch 1076 Batch   12/81   train_loss = 0.784\n",
      "Epoch 1076 Batch   44/81   train_loss = 0.776\n",
      "Epoch 1076 Batch   76/81   train_loss = 0.797\n",
      "Epoch 1077 Batch   27/81   train_loss = 0.745\n",
      "Epoch 1077 Batch   59/81   train_loss = 0.764\n",
      "Epoch 1078 Batch   10/81   train_loss = 0.790\n",
      "Epoch 1078 Batch   42/81   train_loss = 0.762\n",
      "Epoch 1078 Batch   74/81   train_loss = 0.770\n",
      "Epoch 1079 Batch   25/81   train_loss = 0.782\n",
      "Epoch 1079 Batch   57/81   train_loss = 0.763\n",
      "Epoch 1080 Batch    8/81   train_loss = 0.780\n",
      "Epoch 1080 Batch   40/81   train_loss = 0.742\n",
      "Epoch 1080 Batch   72/81   train_loss = 0.768\n",
      "Epoch 1081 Batch   23/81   train_loss = 0.750\n",
      "Epoch 1081 Batch   55/81   train_loss = 0.769\n",
      "Epoch 1082 Batch    6/81   train_loss = 0.768\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1082 Batch   38/81   train_loss = 0.788\n",
      "Epoch 1082 Batch   70/81   train_loss = 0.777\n",
      "Epoch 1083 Batch   21/81   train_loss = 0.813\n",
      "Epoch 1083 Batch   53/81   train_loss = 0.774\n",
      "Epoch 1084 Batch    4/81   train_loss = 0.778\n",
      "Epoch 1084 Batch   36/81   train_loss = 0.801\n",
      "Epoch 1084 Batch   68/81   train_loss = 0.796\n",
      "Epoch 1085 Batch   19/81   train_loss = 0.757\n",
      "Epoch 1085 Batch   51/81   train_loss = 0.822\n",
      "Epoch 1086 Batch    2/81   train_loss = 0.802\n",
      "Epoch 1086 Batch   34/81   train_loss = 0.805\n",
      "Epoch 1086 Batch   66/81   train_loss = 0.779\n",
      "Epoch 1087 Batch   17/81   train_loss = 0.791\n",
      "Epoch 1087 Batch   49/81   train_loss = 0.786\n",
      "Epoch 1088 Batch    0/81   train_loss = 0.769\n",
      "Epoch 1088 Batch   32/81   train_loss = 0.755\n",
      "Epoch 1088 Batch   64/81   train_loss = 0.810\n",
      "Epoch 1089 Batch   15/81   train_loss = 0.771\n",
      "Epoch 1089 Batch   47/81   train_loss = 0.766\n",
      "Epoch 1089 Batch   79/81   train_loss = 0.766\n",
      "Epoch 1090 Batch   30/81   train_loss = 0.768\n",
      "Epoch 1090 Batch   62/81   train_loss = 0.766\n",
      "Epoch 1091 Batch   13/81   train_loss = 0.798\n",
      "Epoch 1091 Batch   45/81   train_loss = 0.757\n",
      "Epoch 1091 Batch   77/81   train_loss = 0.797\n",
      "Epoch 1092 Batch   28/81   train_loss = 0.768\n",
      "Epoch 1092 Batch   60/81   train_loss = 0.786\n",
      "Epoch 1093 Batch   11/81   train_loss = 0.787\n",
      "Epoch 1093 Batch   43/81   train_loss = 0.757\n",
      "Epoch 1093 Batch   75/81   train_loss = 0.752\n",
      "Epoch 1094 Batch   26/81   train_loss = 0.798\n",
      "Epoch 1094 Batch   58/81   train_loss = 0.760\n",
      "Epoch 1095 Batch    9/81   train_loss = 0.804\n",
      "Epoch 1095 Batch   41/81   train_loss = 0.752\n",
      "Epoch 1095 Batch   73/81   train_loss = 0.788\n",
      "Epoch 1096 Batch   24/81   train_loss = 0.780\n",
      "Epoch 1096 Batch   56/81   train_loss = 0.800\n",
      "Epoch 1097 Batch    7/81   train_loss = 0.776\n",
      "Epoch 1097 Batch   39/81   train_loss = 0.756\n",
      "Epoch 1097 Batch   71/81   train_loss = 0.757\n",
      "Epoch 1098 Batch   22/81   train_loss = 0.794\n",
      "Epoch 1098 Batch   54/81   train_loss = 0.772\n",
      "Epoch 1099 Batch    5/81   train_loss = 0.783\n",
      "Epoch 1099 Batch   37/81   train_loss = 0.781\n",
      "Epoch 1099 Batch   69/81   train_loss = 0.790\n",
      "Epoch 1100 Batch   20/81   train_loss = 0.773\n",
      "Epoch 1100 Batch   52/81   train_loss = 0.802\n",
      "Epoch 1101 Batch    3/81   train_loss = 0.765\n",
      "Epoch 1101 Batch   35/81   train_loss = 0.750\n",
      "Epoch 1101 Batch   67/81   train_loss = 0.796\n",
      "Epoch 1102 Batch   18/81   train_loss = 0.807\n",
      "Epoch 1102 Batch   50/81   train_loss = 0.771\n",
      "Epoch 1103 Batch    1/81   train_loss = 0.784\n",
      "Epoch 1103 Batch   33/81   train_loss = 0.765\n",
      "Epoch 1103 Batch   65/81   train_loss = 0.804\n",
      "Epoch 1104 Batch   16/81   train_loss = 0.823\n",
      "Epoch 1104 Batch   48/81   train_loss = 0.777\n",
      "Epoch 1104 Batch   80/81   train_loss = 0.786\n",
      "Epoch 1105 Batch   31/81   train_loss = 0.776\n",
      "Epoch 1105 Batch   63/81   train_loss = 0.797\n",
      "Epoch 1106 Batch   14/81   train_loss = 0.779\n",
      "Epoch 1106 Batch   46/81   train_loss = 0.784\n",
      "Epoch 1106 Batch   78/81   train_loss = 0.782\n",
      "Epoch 1107 Batch   29/81   train_loss = 0.773\n",
      "Epoch 1107 Batch   61/81   train_loss = 0.791\n",
      "Epoch 1108 Batch   12/81   train_loss = 0.800\n",
      "Epoch 1108 Batch   44/81   train_loss = 0.788\n",
      "Epoch 1108 Batch   76/81   train_loss = 0.782\n",
      "Epoch 1109 Batch   27/81   train_loss = 0.753\n",
      "Epoch 1109 Batch   59/81   train_loss = 0.772\n",
      "Epoch 1110 Batch   10/81   train_loss = 0.808\n",
      "Epoch 1110 Batch   42/81   train_loss = 0.778\n",
      "Epoch 1110 Batch   74/81   train_loss = 0.769\n",
      "Epoch 1111 Batch   25/81   train_loss = 0.785\n",
      "Epoch 1111 Batch   57/81   train_loss = 0.776\n",
      "Epoch 1112 Batch    8/81   train_loss = 0.799\n",
      "Epoch 1112 Batch   40/81   train_loss = 0.751\n",
      "Epoch 1112 Batch   72/81   train_loss = 0.753\n",
      "Epoch 1113 Batch   23/81   train_loss = 0.756\n",
      "Epoch 1113 Batch   55/81   train_loss = 0.777\n",
      "Epoch 1114 Batch    6/81   train_loss = 0.756\n",
      "Epoch 1114 Batch   38/81   train_loss = 0.790\n",
      "Epoch 1114 Batch   70/81   train_loss = 0.779\n",
      "Epoch 1115 Batch   21/81   train_loss = 0.804\n",
      "Epoch 1115 Batch   53/81   train_loss = 0.771\n",
      "Epoch 1116 Batch    4/81   train_loss = 0.773\n",
      "Epoch 1116 Batch   36/81   train_loss = 0.800\n",
      "Epoch 1116 Batch   68/81   train_loss = 0.802\n",
      "Epoch 1117 Batch   19/81   train_loss = 0.751\n",
      "Epoch 1117 Batch   51/81   train_loss = 0.824\n",
      "Epoch 1118 Batch    2/81   train_loss = 0.787\n",
      "Epoch 1118 Batch   34/81   train_loss = 0.789\n",
      "Epoch 1118 Batch   66/81   train_loss = 0.785\n",
      "Epoch 1119 Batch   17/81   train_loss = 0.811\n",
      "Epoch 1119 Batch   49/81   train_loss = 0.789\n",
      "Epoch 1120 Batch    0/81   train_loss = 0.795\n",
      "Epoch 1120 Batch   32/81   train_loss = 0.773\n",
      "Epoch 1120 Batch   64/81   train_loss = 0.804\n",
      "Epoch 1121 Batch   15/81   train_loss = 0.782\n",
      "Epoch 1121 Batch   47/81   train_loss = 0.773\n",
      "Epoch 1121 Batch   79/81   train_loss = 0.781\n",
      "Epoch 1122 Batch   30/81   train_loss = 0.757\n",
      "Epoch 1122 Batch   62/81   train_loss = 0.783\n",
      "Epoch 1123 Batch   13/81   train_loss = 0.788\n",
      "Epoch 1123 Batch   45/81   train_loss = 0.759\n",
      "Epoch 1123 Batch   77/81   train_loss = 0.798\n",
      "Epoch 1124 Batch   28/81   train_loss = 0.756\n",
      "Epoch 1124 Batch   60/81   train_loss = 0.771\n",
      "Epoch 1125 Batch   11/81   train_loss = 0.767\n",
      "Epoch 1125 Batch   43/81   train_loss = 0.742\n",
      "Epoch 1125 Batch   75/81   train_loss = 0.760\n",
      "Epoch 1126 Batch   26/81   train_loss = 0.789\n",
      "Epoch 1126 Batch   58/81   train_loss = 0.749\n",
      "Epoch 1127 Batch    9/81   train_loss = 0.788\n",
      "Epoch 1127 Batch   41/81   train_loss = 0.737\n",
      "Epoch 1127 Batch   73/81   train_loss = 0.792\n",
      "Epoch 1128 Batch   24/81   train_loss = 0.765\n",
      "Epoch 1128 Batch   56/81   train_loss = 0.791\n",
      "Epoch 1129 Batch    7/81   train_loss = 0.767\n",
      "Epoch 1129 Batch   39/81   train_loss = 0.752\n",
      "Epoch 1129 Batch   71/81   train_loss = 0.761\n",
      "Epoch 1130 Batch   22/81   train_loss = 0.774\n",
      "Epoch 1130 Batch   54/81   train_loss = 0.756\n",
      "Epoch 1131 Batch    5/81   train_loss = 0.794\n",
      "Epoch 1131 Batch   37/81   train_loss = 0.772\n",
      "Epoch 1131 Batch   69/81   train_loss = 0.772\n",
      "Epoch 1132 Batch   20/81   train_loss = 0.778\n",
      "Epoch 1132 Batch   52/81   train_loss = 0.812\n",
      "Epoch 1133 Batch    3/81   train_loss = 0.756\n",
      "Epoch 1133 Batch   35/81   train_loss = 0.761\n",
      "Epoch 1133 Batch   67/81   train_loss = 0.820\n",
      "Epoch 1134 Batch   18/81   train_loss = 0.803\n",
      "Epoch 1134 Batch   50/81   train_loss = 0.768\n",
      "Epoch 1135 Batch    1/81   train_loss = 0.787\n",
      "Epoch 1135 Batch   33/81   train_loss = 0.754\n",
      "Epoch 1135 Batch   65/81   train_loss = 0.801\n",
      "Epoch 1136 Batch   16/81   train_loss = 0.823\n",
      "Epoch 1136 Batch   48/81   train_loss = 0.764\n",
      "Epoch 1136 Batch   80/81   train_loss = 0.794\n",
      "Epoch 1137 Batch   31/81   train_loss = 0.764\n",
      "Epoch 1137 Batch   63/81   train_loss = 0.788\n",
      "Epoch 1138 Batch   14/81   train_loss = 0.796\n",
      "Epoch 1138 Batch   46/81   train_loss = 0.791\n",
      "Epoch 1138 Batch   78/81   train_loss = 0.763\n",
      "Epoch 1139 Batch   29/81   train_loss = 0.757\n",
      "Epoch 1139 Batch   61/81   train_loss = 0.788\n",
      "Epoch 1140 Batch   12/81   train_loss = 0.783\n",
      "Epoch 1140 Batch   44/81   train_loss = 0.770\n",
      "Epoch 1140 Batch   76/81   train_loss = 0.786\n",
      "Epoch 1141 Batch   27/81   train_loss = 0.771\n",
      "Epoch 1141 Batch   59/81   train_loss = 0.757\n",
      "Epoch 1142 Batch   10/81   train_loss = 0.798\n",
      "Epoch 1142 Batch   42/81   train_loss = 0.762\n",
      "Epoch 1142 Batch   74/81   train_loss = 0.787\n",
      "Epoch 1143 Batch   25/81   train_loss = 0.784\n",
      "Epoch 1143 Batch   57/81   train_loss = 0.784\n",
      "Epoch 1144 Batch    8/81   train_loss = 0.778\n",
      "Epoch 1144 Batch   40/81   train_loss = 0.755\n",
      "Epoch 1144 Batch   72/81   train_loss = 0.764\n",
      "Epoch 1145 Batch   23/81   train_loss = 0.761\n",
      "Epoch 1145 Batch   55/81   train_loss = 0.774\n",
      "Epoch 1146 Batch    6/81   train_loss = 0.770\n",
      "Epoch 1146 Batch   38/81   train_loss = 0.797\n",
      "Epoch 1146 Batch   70/81   train_loss = 0.767\n",
      "Epoch 1147 Batch   21/81   train_loss = 0.812\n",
      "Epoch 1147 Batch   53/81   train_loss = 0.773\n",
      "Epoch 1148 Batch    4/81   train_loss = 0.780\n",
      "Epoch 1148 Batch   36/81   train_loss = 0.796\n",
      "Epoch 1148 Batch   68/81   train_loss = 0.807\n",
      "Epoch 1149 Batch   19/81   train_loss = 0.749\n",
      "Epoch 1149 Batch   51/81   train_loss = 0.824\n",
      "Epoch 1150 Batch    2/81   train_loss = 0.790\n",
      "Epoch 1150 Batch   34/81   train_loss = 0.799\n",
      "Epoch 1150 Batch   66/81   train_loss = 0.792\n",
      "Epoch 1151 Batch   17/81   train_loss = 0.796\n",
      "Epoch 1151 Batch   49/81   train_loss = 0.788\n",
      "Epoch 1152 Batch    0/81   train_loss = 0.792\n",
      "Epoch 1152 Batch   32/81   train_loss = 0.757\n",
      "Epoch 1152 Batch   64/81   train_loss = 0.788\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1153 Batch   15/81   train_loss = 0.771\n",
      "Epoch 1153 Batch   47/81   train_loss = 0.772\n",
      "Epoch 1153 Batch   79/81   train_loss = 0.763\n",
      "Epoch 1154 Batch   30/81   train_loss = 0.760\n",
      "Epoch 1154 Batch   62/81   train_loss = 0.779\n",
      "Epoch 1155 Batch   13/81   train_loss = 0.809\n",
      "Epoch 1155 Batch   45/81   train_loss = 0.752\n",
      "Epoch 1155 Batch   77/81   train_loss = 0.806\n",
      "Epoch 1156 Batch   28/81   train_loss = 0.741\n",
      "Epoch 1156 Batch   60/81   train_loss = 0.792\n",
      "Epoch 1157 Batch   11/81   train_loss = 0.769\n",
      "Epoch 1157 Batch   43/81   train_loss = 0.742\n",
      "Epoch 1157 Batch   75/81   train_loss = 0.774\n",
      "Epoch 1158 Batch   26/81   train_loss = 0.776\n",
      "Epoch 1158 Batch   58/81   train_loss = 0.751\n",
      "Epoch 1159 Batch    9/81   train_loss = 0.807\n",
      "Epoch 1159 Batch   41/81   train_loss = 0.738\n",
      "Epoch 1159 Batch   73/81   train_loss = 0.790\n",
      "Epoch 1160 Batch   24/81   train_loss = 0.781\n",
      "Epoch 1160 Batch   56/81   train_loss = 0.796\n",
      "Epoch 1161 Batch    7/81   train_loss = 0.766\n",
      "Epoch 1161 Batch   39/81   train_loss = 0.756\n",
      "Epoch 1161 Batch   71/81   train_loss = 0.772\n",
      "Epoch 1162 Batch   22/81   train_loss = 0.796\n",
      "Epoch 1162 Batch   54/81   train_loss = 0.756\n",
      "Epoch 1163 Batch    5/81   train_loss = 0.784\n",
      "Epoch 1163 Batch   37/81   train_loss = 0.776\n",
      "Epoch 1163 Batch   69/81   train_loss = 0.790\n",
      "Epoch 1164 Batch   20/81   train_loss = 0.769\n",
      "Epoch 1164 Batch   52/81   train_loss = 0.803\n",
      "Epoch 1165 Batch    3/81   train_loss = 0.770\n",
      "Epoch 1165 Batch   35/81   train_loss = 0.772\n",
      "Epoch 1165 Batch   67/81   train_loss = 0.804\n",
      "Epoch 1166 Batch   18/81   train_loss = 0.785\n",
      "Epoch 1166 Batch   50/81   train_loss = 0.749\n",
      "Epoch 1167 Batch    1/81   train_loss = 0.804\n",
      "Epoch 1167 Batch   33/81   train_loss = 0.769\n",
      "Epoch 1167 Batch   65/81   train_loss = 0.796\n",
      "Epoch 1168 Batch   16/81   train_loss = 0.822\n",
      "Epoch 1168 Batch   48/81   train_loss = 0.768\n",
      "Epoch 1168 Batch   80/81   train_loss = 0.808\n",
      "Epoch 1169 Batch   31/81   train_loss = 0.752\n",
      "Epoch 1169 Batch   63/81   train_loss = 0.798\n",
      "Epoch 1170 Batch   14/81   train_loss = 0.804\n",
      "Epoch 1170 Batch   46/81   train_loss = 0.782\n",
      "Epoch 1170 Batch   78/81   train_loss = 0.782\n",
      "Epoch 1171 Batch   29/81   train_loss = 0.778\n",
      "Epoch 1171 Batch   61/81   train_loss = 0.786\n",
      "Epoch 1172 Batch   12/81   train_loss = 0.801\n",
      "Epoch 1172 Batch   44/81   train_loss = 0.788\n",
      "Epoch 1172 Batch   76/81   train_loss = 0.779\n",
      "Epoch 1173 Batch   27/81   train_loss = 0.762\n",
      "Epoch 1173 Batch   59/81   train_loss = 0.781\n",
      "Epoch 1174 Batch   10/81   train_loss = 0.808\n",
      "Epoch 1174 Batch   42/81   train_loss = 0.763\n",
      "Epoch 1174 Batch   74/81   train_loss = 0.771\n",
      "Epoch 1175 Batch   25/81   train_loss = 0.782\n",
      "Epoch 1175 Batch   57/81   train_loss = 0.783\n",
      "Epoch 1176 Batch    8/81   train_loss = 0.792\n",
      "Epoch 1176 Batch   40/81   train_loss = 0.767\n",
      "Epoch 1176 Batch   72/81   train_loss = 0.769\n",
      "Epoch 1177 Batch   23/81   train_loss = 0.767\n",
      "Epoch 1177 Batch   55/81   train_loss = 0.781\n",
      "Epoch 1178 Batch    6/81   train_loss = 0.769\n",
      "Epoch 1178 Batch   38/81   train_loss = 0.796\n",
      "Epoch 1178 Batch   70/81   train_loss = 0.769\n",
      "Epoch 1179 Batch   21/81   train_loss = 0.830\n",
      "Epoch 1179 Batch   53/81   train_loss = 0.751\n",
      "Epoch 1180 Batch    4/81   train_loss = 0.774\n",
      "Epoch 1180 Batch   36/81   train_loss = 0.803\n",
      "Epoch 1180 Batch   68/81   train_loss = 0.804\n",
      "Epoch 1181 Batch   19/81   train_loss = 0.768\n",
      "Epoch 1181 Batch   51/81   train_loss = 0.817\n",
      "Epoch 1182 Batch    2/81   train_loss = 0.800\n",
      "Epoch 1182 Batch   34/81   train_loss = 0.797\n",
      "Epoch 1182 Batch   66/81   train_loss = 0.774\n",
      "Epoch 1183 Batch   17/81   train_loss = 0.804\n",
      "Epoch 1183 Batch   49/81   train_loss = 0.786\n",
      "Epoch 1184 Batch    0/81   train_loss = 0.770\n",
      "Epoch 1184 Batch   32/81   train_loss = 0.776\n",
      "Epoch 1184 Batch   64/81   train_loss = 0.795\n",
      "Epoch 1185 Batch   15/81   train_loss = 0.800\n",
      "Epoch 1185 Batch   47/81   train_loss = 0.763\n",
      "Epoch 1185 Batch   79/81   train_loss = 0.778\n",
      "Epoch 1186 Batch   30/81   train_loss = 0.756\n",
      "Epoch 1186 Batch   62/81   train_loss = 0.785\n",
      "Epoch 1187 Batch   13/81   train_loss = 0.771\n",
      "Epoch 1187 Batch   45/81   train_loss = 0.768\n",
      "Epoch 1187 Batch   77/81   train_loss = 0.794\n",
      "Epoch 1188 Batch   28/81   train_loss = 0.770\n",
      "Epoch 1188 Batch   60/81   train_loss = 0.778\n",
      "Epoch 1189 Batch   11/81   train_loss = 0.777\n",
      "Epoch 1189 Batch   43/81   train_loss = 0.732\n",
      "Epoch 1189 Batch   75/81   train_loss = 0.763\n",
      "Epoch 1190 Batch   26/81   train_loss = 0.784\n",
      "Epoch 1190 Batch   58/81   train_loss = 0.756\n",
      "Epoch 1191 Batch    9/81   train_loss = 0.792\n",
      "Epoch 1191 Batch   41/81   train_loss = 0.731\n",
      "Epoch 1191 Batch   73/81   train_loss = 0.782\n",
      "Epoch 1192 Batch   24/81   train_loss = 0.774\n",
      "Epoch 1192 Batch   56/81   train_loss = 0.780\n",
      "Epoch 1193 Batch    7/81   train_loss = 0.772\n",
      "Epoch 1193 Batch   39/81   train_loss = 0.749\n",
      "Epoch 1193 Batch   71/81   train_loss = 0.766\n",
      "Epoch 1194 Batch   22/81   train_loss = 0.791\n",
      "Epoch 1194 Batch   54/81   train_loss = 0.767\n",
      "Epoch 1195 Batch    5/81   train_loss = 0.770\n",
      "Epoch 1195 Batch   37/81   train_loss = 0.765\n",
      "Epoch 1195 Batch   69/81   train_loss = 0.781\n",
      "Epoch 1196 Batch   20/81   train_loss = 0.757\n",
      "Epoch 1196 Batch   52/81   train_loss = 0.782\n",
      "Epoch 1197 Batch    3/81   train_loss = 0.759\n",
      "Epoch 1197 Batch   35/81   train_loss = 0.744\n",
      "Epoch 1197 Batch   67/81   train_loss = 0.805\n",
      "Epoch 1198 Batch   18/81   train_loss = 0.788\n",
      "Epoch 1198 Batch   50/81   train_loss = 0.757\n",
      "Epoch 1199 Batch    1/81   train_loss = 0.793\n",
      "Epoch 1199 Batch   33/81   train_loss = 0.746\n",
      "Epoch 1199 Batch   65/81   train_loss = 0.773\n",
      "Epoch 1200 Batch   16/81   train_loss = 0.824\n",
      "Epoch 1200 Batch   48/81   train_loss = 0.760\n",
      "Epoch 1200 Batch   80/81   train_loss = 0.785\n",
      "Epoch 1201 Batch   31/81   train_loss = 0.747\n",
      "Epoch 1201 Batch   63/81   train_loss = 0.766\n",
      "Epoch 1202 Batch   14/81   train_loss = 0.787\n",
      "Epoch 1202 Batch   46/81   train_loss = 0.775\n",
      "Epoch 1202 Batch   78/81   train_loss = 0.763\n",
      "Epoch 1203 Batch   29/81   train_loss = 0.761\n",
      "Epoch 1203 Batch   61/81   train_loss = 0.773\n",
      "Epoch 1204 Batch   12/81   train_loss = 0.774\n",
      "Epoch 1204 Batch   44/81   train_loss = 0.769\n",
      "Epoch 1204 Batch   76/81   train_loss = 0.786\n",
      "Epoch 1205 Batch   27/81   train_loss = 0.756\n",
      "Epoch 1205 Batch   59/81   train_loss = 0.767\n",
      "Epoch 1206 Batch   10/81   train_loss = 0.791\n",
      "Epoch 1206 Batch   42/81   train_loss = 0.760\n",
      "Epoch 1206 Batch   74/81   train_loss = 0.773\n",
      "Epoch 1207 Batch   25/81   train_loss = 0.775\n",
      "Epoch 1207 Batch   57/81   train_loss = 0.778\n",
      "Epoch 1208 Batch    8/81   train_loss = 0.787\n",
      "Epoch 1208 Batch   40/81   train_loss = 0.733\n",
      "Epoch 1208 Batch   72/81   train_loss = 0.762\n",
      "Epoch 1209 Batch   23/81   train_loss = 0.760\n",
      "Epoch 1209 Batch   55/81   train_loss = 0.768\n",
      "Epoch 1210 Batch    6/81   train_loss = 0.753\n",
      "Epoch 1210 Batch   38/81   train_loss = 0.777\n",
      "Epoch 1210 Batch   70/81   train_loss = 0.766\n",
      "Epoch 1211 Batch   21/81   train_loss = 0.807\n",
      "Epoch 1211 Batch   53/81   train_loss = 0.753\n",
      "Epoch 1212 Batch    4/81   train_loss = 0.795\n",
      "Epoch 1212 Batch   36/81   train_loss = 0.786\n",
      "Epoch 1212 Batch   68/81   train_loss = 0.813\n",
      "Epoch 1213 Batch   19/81   train_loss = 0.754\n",
      "Epoch 1213 Batch   51/81   train_loss = 0.803\n",
      "Epoch 1214 Batch    2/81   train_loss = 0.793\n",
      "Epoch 1214 Batch   34/81   train_loss = 0.804\n",
      "Epoch 1214 Batch   66/81   train_loss = 0.780\n",
      "Epoch 1215 Batch   17/81   train_loss = 0.796\n",
      "Epoch 1215 Batch   49/81   train_loss = 0.772\n",
      "Epoch 1216 Batch    0/81   train_loss = 0.773\n",
      "Epoch 1216 Batch   32/81   train_loss = 0.743\n",
      "Epoch 1216 Batch   64/81   train_loss = 0.787\n",
      "Epoch 1217 Batch   15/81   train_loss = 0.776\n",
      "Epoch 1217 Batch   47/81   train_loss = 0.758\n",
      "Epoch 1217 Batch   79/81   train_loss = 0.777\n",
      "Epoch 1218 Batch   30/81   train_loss = 0.751\n",
      "Epoch 1218 Batch   62/81   train_loss = 0.757\n",
      "Epoch 1219 Batch   13/81   train_loss = 0.779\n",
      "Epoch 1219 Batch   45/81   train_loss = 0.759\n",
      "Epoch 1219 Batch   77/81   train_loss = 0.766\n",
      "Epoch 1220 Batch   28/81   train_loss = 0.740\n",
      "Epoch 1220 Batch   60/81   train_loss = 0.785\n",
      "Epoch 1221 Batch   11/81   train_loss = 0.762\n",
      "Epoch 1221 Batch   43/81   train_loss = 0.739\n",
      "Epoch 1221 Batch   75/81   train_loss = 0.757\n",
      "Epoch 1222 Batch   26/81   train_loss = 0.769\n",
      "Epoch 1222 Batch   58/81   train_loss = 0.750\n",
      "Epoch 1223 Batch    9/81   train_loss = 0.817\n",
      "Epoch 1223 Batch   41/81   train_loss = 0.741\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1223 Batch   73/81   train_loss = 0.770\n",
      "Epoch 1224 Batch   24/81   train_loss = 0.779\n",
      "Epoch 1224 Batch   56/81   train_loss = 0.800\n",
      "Epoch 1225 Batch    7/81   train_loss = 0.769\n",
      "Epoch 1225 Batch   39/81   train_loss = 0.749\n",
      "Epoch 1225 Batch   71/81   train_loss = 0.752\n",
      "Epoch 1226 Batch   22/81   train_loss = 0.781\n",
      "Epoch 1226 Batch   54/81   train_loss = 0.770\n",
      "Epoch 1227 Batch    5/81   train_loss = 0.784\n",
      "Epoch 1227 Batch   37/81   train_loss = 0.772\n",
      "Epoch 1227 Batch   69/81   train_loss = 0.780\n",
      "Epoch 1228 Batch   20/81   train_loss = 0.776\n",
      "Epoch 1228 Batch   52/81   train_loss = 0.797\n",
      "Epoch 1229 Batch    3/81   train_loss = 0.764\n",
      "Epoch 1229 Batch   35/81   train_loss = 0.765\n",
      "Epoch 1229 Batch   67/81   train_loss = 0.802\n",
      "Epoch 1230 Batch   18/81   train_loss = 0.803\n",
      "Epoch 1230 Batch   50/81   train_loss = 0.757\n",
      "Epoch 1231 Batch    1/81   train_loss = 0.789\n",
      "Epoch 1231 Batch   33/81   train_loss = 0.758\n",
      "Epoch 1231 Batch   65/81   train_loss = 0.781\n",
      "Epoch 1232 Batch   16/81   train_loss = 0.839\n",
      "Epoch 1232 Batch   48/81   train_loss = 0.773\n",
      "Epoch 1232 Batch   80/81   train_loss = 0.798\n",
      "Epoch 1233 Batch   31/81   train_loss = 0.758\n",
      "Epoch 1233 Batch   63/81   train_loss = 0.788\n",
      "Epoch 1234 Batch   14/81   train_loss = 0.788\n",
      "Epoch 1234 Batch   46/81   train_loss = 0.785\n",
      "Epoch 1234 Batch   78/81   train_loss = 0.786\n",
      "Epoch 1235 Batch   29/81   train_loss = 0.780\n",
      "Epoch 1235 Batch   61/81   train_loss = 0.782\n",
      "Epoch 1236 Batch   12/81   train_loss = 0.802\n",
      "Epoch 1236 Batch   44/81   train_loss = 0.785\n",
      "Epoch 1236 Batch   76/81   train_loss = 0.785\n",
      "Epoch 1237 Batch   27/81   train_loss = 0.752\n",
      "Epoch 1237 Batch   59/81   train_loss = 0.764\n",
      "Epoch 1238 Batch   10/81   train_loss = 0.807\n",
      "Epoch 1238 Batch   42/81   train_loss = 0.777\n",
      "Epoch 1238 Batch   74/81   train_loss = 0.771\n",
      "Epoch 1239 Batch   25/81   train_loss = 0.784\n",
      "Epoch 1239 Batch   57/81   train_loss = 0.783\n",
      "Epoch 1240 Batch    8/81   train_loss = 0.800\n",
      "Epoch 1240 Batch   40/81   train_loss = 0.747\n",
      "Epoch 1240 Batch   72/81   train_loss = 0.763\n",
      "Epoch 1241 Batch   23/81   train_loss = 0.771\n",
      "Epoch 1241 Batch   55/81   train_loss = 0.774\n",
      "Epoch 1242 Batch    6/81   train_loss = 0.760\n",
      "Epoch 1242 Batch   38/81   train_loss = 0.802\n",
      "Epoch 1242 Batch   70/81   train_loss = 0.764\n",
      "Epoch 1243 Batch   21/81   train_loss = 0.815\n",
      "Epoch 1243 Batch   53/81   train_loss = 0.754\n",
      "Epoch 1244 Batch    4/81   train_loss = 0.789\n",
      "Epoch 1244 Batch   36/81   train_loss = 0.809\n",
      "Epoch 1244 Batch   68/81   train_loss = 0.791\n",
      "Epoch 1245 Batch   19/81   train_loss = 0.757\n",
      "Epoch 1245 Batch   51/81   train_loss = 0.817\n",
      "Epoch 1246 Batch    2/81   train_loss = 0.806\n",
      "Epoch 1246 Batch   34/81   train_loss = 0.808\n",
      "Epoch 1246 Batch   66/81   train_loss = 0.785\n",
      "Epoch 1247 Batch   17/81   train_loss = 0.812\n",
      "Epoch 1247 Batch   49/81   train_loss = 0.780\n",
      "Epoch 1248 Batch    0/81   train_loss = 0.794\n",
      "Epoch 1248 Batch   32/81   train_loss = 0.775\n",
      "Epoch 1248 Batch   64/81   train_loss = 0.796\n",
      "Epoch 1249 Batch   15/81   train_loss = 0.784\n",
      "Epoch 1249 Batch   47/81   train_loss = 0.768\n",
      "Epoch 1249 Batch   79/81   train_loss = 0.776\n",
      "Epoch 1250 Batch   30/81   train_loss = 0.779\n",
      "Epoch 1250 Batch   62/81   train_loss = 0.776\n",
      "Epoch 1251 Batch   13/81   train_loss = 0.800\n",
      "Epoch 1251 Batch   45/81   train_loss = 0.754\n",
      "Epoch 1251 Batch   77/81   train_loss = 0.781\n",
      "Epoch 1252 Batch   28/81   train_loss = 0.741\n",
      "Epoch 1252 Batch   60/81   train_loss = 0.780\n",
      "Epoch 1253 Batch   11/81   train_loss = 0.788\n",
      "Epoch 1253 Batch   43/81   train_loss = 0.742\n",
      "Epoch 1253 Batch   75/81   train_loss = 0.756\n",
      "Epoch 1254 Batch   26/81   train_loss = 0.784\n",
      "Epoch 1254 Batch   58/81   train_loss = 0.748\n",
      "Epoch 1255 Batch    9/81   train_loss = 0.815\n",
      "Epoch 1255 Batch   41/81   train_loss = 0.733\n",
      "Epoch 1255 Batch   73/81   train_loss = 0.767\n",
      "Epoch 1256 Batch   24/81   train_loss = 0.782\n",
      "Epoch 1256 Batch   56/81   train_loss = 0.780\n",
      "Epoch 1257 Batch    7/81   train_loss = 0.777\n",
      "Epoch 1257 Batch   39/81   train_loss = 0.736\n",
      "Epoch 1257 Batch   71/81   train_loss = 0.758\n",
      "Epoch 1258 Batch   22/81   train_loss = 0.807\n",
      "Epoch 1258 Batch   54/81   train_loss = 0.753\n",
      "Epoch 1259 Batch    5/81   train_loss = 0.785\n",
      "Epoch 1259 Batch   37/81   train_loss = 0.794\n",
      "Epoch 1259 Batch   69/81   train_loss = 0.792\n",
      "Epoch 1260 Batch   20/81   train_loss = 0.787\n",
      "Epoch 1260 Batch   52/81   train_loss = 0.790\n",
      "Epoch 1261 Batch    3/81   train_loss = 0.776\n",
      "Epoch 1261 Batch   35/81   train_loss = 0.762\n",
      "Epoch 1261 Batch   67/81   train_loss = 0.801\n",
      "Epoch 1262 Batch   18/81   train_loss = 0.807\n",
      "Epoch 1262 Batch   50/81   train_loss = 0.761\n",
      "Epoch 1263 Batch    1/81   train_loss = 0.801\n",
      "Epoch 1263 Batch   33/81   train_loss = 0.775\n",
      "Epoch 1263 Batch   65/81   train_loss = 0.793\n",
      "Epoch 1264 Batch   16/81   train_loss = 0.845\n",
      "Epoch 1264 Batch   48/81   train_loss = 0.775\n",
      "Epoch 1264 Batch   80/81   train_loss = 0.794\n",
      "Epoch 1265 Batch   31/81   train_loss = 0.759\n",
      "Epoch 1265 Batch   63/81   train_loss = 0.797\n",
      "Epoch 1266 Batch   14/81   train_loss = 0.786\n",
      "Epoch 1266 Batch   46/81   train_loss = 0.778\n",
      "Epoch 1266 Batch   78/81   train_loss = 0.774\n",
      "Epoch 1267 Batch   29/81   train_loss = 0.763\n",
      "Epoch 1267 Batch   61/81   train_loss = 0.766\n",
      "Epoch 1268 Batch   12/81   train_loss = 0.804\n",
      "Epoch 1268 Batch   44/81   train_loss = 0.783\n",
      "Epoch 1268 Batch   76/81   train_loss = 0.785\n",
      "Epoch 1269 Batch   27/81   train_loss = 0.772\n",
      "Epoch 1269 Batch   59/81   train_loss = 0.762\n",
      "Epoch 1270 Batch   10/81   train_loss = 0.826\n",
      "Epoch 1270 Batch   42/81   train_loss = 0.785\n",
      "Epoch 1270 Batch   74/81   train_loss = 0.768\n",
      "Epoch 1271 Batch   25/81   train_loss = 0.803\n",
      "Epoch 1271 Batch   57/81   train_loss = 0.790\n",
      "Epoch 1272 Batch    8/81   train_loss = 0.800\n",
      "Epoch 1272 Batch   40/81   train_loss = 0.762\n",
      "Epoch 1272 Batch   72/81   train_loss = 0.769\n",
      "Epoch 1273 Batch   23/81   train_loss = 0.782\n",
      "Epoch 1273 Batch   55/81   train_loss = 0.782\n",
      "Epoch 1274 Batch    6/81   train_loss = 0.769\n",
      "Epoch 1274 Batch   38/81   train_loss = 0.809\n",
      "Epoch 1274 Batch   70/81   train_loss = 0.758\n",
      "Epoch 1275 Batch   21/81   train_loss = 0.795\n",
      "Epoch 1275 Batch   53/81   train_loss = 0.764\n",
      "Epoch 1276 Batch    4/81   train_loss = 0.785\n",
      "Epoch 1276 Batch   36/81   train_loss = 0.804\n",
      "Epoch 1276 Batch   68/81   train_loss = 0.798\n",
      "Epoch 1277 Batch   19/81   train_loss = 0.759\n",
      "Epoch 1277 Batch   51/81   train_loss = 0.811\n",
      "Epoch 1278 Batch    2/81   train_loss = 0.776\n",
      "Epoch 1278 Batch   34/81   train_loss = 0.795\n",
      "Epoch 1278 Batch   66/81   train_loss = 0.766\n",
      "Epoch 1279 Batch   17/81   train_loss = 0.785\n",
      "Epoch 1279 Batch   49/81   train_loss = 0.792\n",
      "Epoch 1280 Batch    0/81   train_loss = 0.763\n",
      "Epoch 1280 Batch   32/81   train_loss = 0.760\n",
      "Epoch 1280 Batch   64/81   train_loss = 0.774\n",
      "Epoch 1281 Batch   15/81   train_loss = 0.768\n",
      "Epoch 1281 Batch   47/81   train_loss = 0.764\n",
      "Epoch 1281 Batch   79/81   train_loss = 0.775\n",
      "Epoch 1282 Batch   30/81   train_loss = 0.781\n",
      "Epoch 1282 Batch   62/81   train_loss = 0.767\n",
      "Epoch 1283 Batch   13/81   train_loss = 0.790\n",
      "Epoch 1283 Batch   45/81   train_loss = 0.772\n",
      "Epoch 1283 Batch   77/81   train_loss = 0.798\n",
      "Epoch 1284 Batch   28/81   train_loss = 0.769\n",
      "Epoch 1284 Batch   60/81   train_loss = 0.777\n",
      "Epoch 1285 Batch   11/81   train_loss = 0.779\n",
      "Epoch 1285 Batch   43/81   train_loss = 0.759\n",
      "Epoch 1285 Batch   75/81   train_loss = 0.760\n",
      "Epoch 1286 Batch   26/81   train_loss = 0.789\n",
      "Epoch 1286 Batch   58/81   train_loss = 0.758\n",
      "Epoch 1287 Batch    9/81   train_loss = 0.806\n",
      "Epoch 1287 Batch   41/81   train_loss = 0.736\n",
      "Epoch 1287 Batch   73/81   train_loss = 0.781\n",
      "Epoch 1288 Batch   24/81   train_loss = 0.793\n",
      "Epoch 1288 Batch   56/81   train_loss = 0.782\n",
      "Epoch 1289 Batch    7/81   train_loss = 0.777\n",
      "Epoch 1289 Batch   39/81   train_loss = 0.741\n",
      "Epoch 1289 Batch   71/81   train_loss = 0.768\n",
      "Epoch 1290 Batch   22/81   train_loss = 0.810\n",
      "Epoch 1290 Batch   54/81   train_loss = 0.752\n",
      "Epoch 1291 Batch    5/81   train_loss = 0.799\n",
      "Epoch 1291 Batch   37/81   train_loss = 0.791\n",
      "Epoch 1291 Batch   69/81   train_loss = 0.771\n",
      "Epoch 1292 Batch   20/81   train_loss = 0.788\n",
      "Epoch 1292 Batch   52/81   train_loss = 0.791\n",
      "Epoch 1293 Batch    3/81   train_loss = 0.771\n",
      "Epoch 1293 Batch   35/81   train_loss = 0.770\n",
      "Epoch 1293 Batch   67/81   train_loss = 0.822\n",
      "Epoch 1294 Batch   18/81   train_loss = 0.794\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1294 Batch   50/81   train_loss = 0.784\n",
      "Epoch 1295 Batch    1/81   train_loss = 0.801\n",
      "Epoch 1295 Batch   33/81   train_loss = 0.766\n",
      "Epoch 1295 Batch   65/81   train_loss = 0.788\n",
      "Epoch 1296 Batch   16/81   train_loss = 0.819\n",
      "Epoch 1296 Batch   48/81   train_loss = 0.790\n",
      "Epoch 1296 Batch   80/81   train_loss = 0.811\n",
      "Epoch 1297 Batch   31/81   train_loss = 0.767\n",
      "Epoch 1297 Batch   63/81   train_loss = 0.780\n",
      "Epoch 1298 Batch   14/81   train_loss = 0.796\n",
      "Epoch 1298 Batch   46/81   train_loss = 0.786\n",
      "Epoch 1298 Batch   78/81   train_loss = 0.777\n",
      "Epoch 1299 Batch   29/81   train_loss = 0.776\n",
      "Epoch 1299 Batch   61/81   train_loss = 0.786\n",
      "Epoch 1300 Batch   12/81   train_loss = 0.789\n",
      "Epoch 1300 Batch   44/81   train_loss = 0.789\n",
      "Epoch 1300 Batch   76/81   train_loss = 0.790\n",
      "Epoch 1301 Batch   27/81   train_loss = 0.757\n",
      "Epoch 1301 Batch   59/81   train_loss = 0.753\n",
      "Epoch 1302 Batch   10/81   train_loss = 0.793\n",
      "Epoch 1302 Batch   42/81   train_loss = 0.776\n",
      "Epoch 1302 Batch   74/81   train_loss = 0.784\n",
      "Epoch 1303 Batch   25/81   train_loss = 0.782\n",
      "Epoch 1303 Batch   57/81   train_loss = 0.793\n",
      "Epoch 1304 Batch    8/81   train_loss = 0.781\n",
      "Epoch 1304 Batch   40/81   train_loss = 0.750\n",
      "Epoch 1304 Batch   72/81   train_loss = 0.752\n",
      "Epoch 1305 Batch   23/81   train_loss = 0.765\n",
      "Epoch 1305 Batch   55/81   train_loss = 0.784\n",
      "Epoch 1306 Batch    6/81   train_loss = 0.762\n",
      "Epoch 1306 Batch   38/81   train_loss = 0.787\n",
      "Epoch 1306 Batch   70/81   train_loss = 0.765\n",
      "Epoch 1307 Batch   21/81   train_loss = 0.833\n",
      "Epoch 1307 Batch   53/81   train_loss = 0.775\n",
      "Epoch 1308 Batch    4/81   train_loss = 0.767\n",
      "Epoch 1308 Batch   36/81   train_loss = 0.803\n",
      "Epoch 1308 Batch   68/81   train_loss = 0.797\n",
      "Epoch 1309 Batch   19/81   train_loss = 0.759\n",
      "Epoch 1309 Batch   51/81   train_loss = 0.829\n",
      "Epoch 1310 Batch    2/81   train_loss = 0.793\n",
      "Epoch 1310 Batch   34/81   train_loss = 0.805\n",
      "Epoch 1310 Batch   66/81   train_loss = 0.782\n",
      "Epoch 1311 Batch   17/81   train_loss = 0.794\n",
      "Epoch 1311 Batch   49/81   train_loss = 0.783\n",
      "Epoch 1312 Batch    0/81   train_loss = 0.773\n",
      "Epoch 1312 Batch   32/81   train_loss = 0.767\n",
      "Epoch 1312 Batch   64/81   train_loss = 0.781\n",
      "Epoch 1313 Batch   15/81   train_loss = 0.766\n",
      "Epoch 1313 Batch   47/81   train_loss = 0.771\n",
      "Epoch 1313 Batch   79/81   train_loss = 0.777\n",
      "Epoch 1314 Batch   30/81   train_loss = 0.772\n",
      "Epoch 1314 Batch   62/81   train_loss = 0.778\n",
      "Epoch 1315 Batch   13/81   train_loss = 0.790\n",
      "Epoch 1315 Batch   45/81   train_loss = 0.766\n",
      "Epoch 1315 Batch   77/81   train_loss = 0.796\n",
      "Epoch 1316 Batch   28/81   train_loss = 0.753\n",
      "Epoch 1316 Batch   60/81   train_loss = 0.762\n",
      "Epoch 1317 Batch   11/81   train_loss = 0.766\n",
      "Epoch 1317 Batch   43/81   train_loss = 0.746\n",
      "Epoch 1317 Batch   75/81   train_loss = 0.758\n",
      "Epoch 1318 Batch   26/81   train_loss = 0.787\n",
      "Epoch 1318 Batch   58/81   train_loss = 0.745\n",
      "Epoch 1319 Batch    9/81   train_loss = 0.802\n",
      "Epoch 1319 Batch   41/81   train_loss = 0.740\n",
      "Epoch 1319 Batch   73/81   train_loss = 0.779\n",
      "Epoch 1320 Batch   24/81   train_loss = 0.771\n",
      "Epoch 1320 Batch   56/81   train_loss = 0.775\n",
      "Epoch 1321 Batch    7/81   train_loss = 0.761\n",
      "Epoch 1321 Batch   39/81   train_loss = 0.747\n",
      "Epoch 1321 Batch   71/81   train_loss = 0.763\n",
      "Epoch 1322 Batch   22/81   train_loss = 0.796\n",
      "Epoch 1322 Batch   54/81   train_loss = 0.770\n",
      "Epoch 1323 Batch    5/81   train_loss = 0.779\n",
      "Epoch 1323 Batch   37/81   train_loss = 0.785\n",
      "Epoch 1323 Batch   69/81   train_loss = 0.780\n",
      "Epoch 1324 Batch   20/81   train_loss = 0.766\n",
      "Epoch 1324 Batch   52/81   train_loss = 0.804\n",
      "Epoch 1325 Batch    3/81   train_loss = 0.760\n",
      "Epoch 1325 Batch   35/81   train_loss = 0.763\n",
      "Epoch 1325 Batch   67/81   train_loss = 0.793\n",
      "Epoch 1326 Batch   18/81   train_loss = 0.793\n",
      "Epoch 1326 Batch   50/81   train_loss = 0.763\n",
      "Epoch 1327 Batch    1/81   train_loss = 0.773\n",
      "Epoch 1327 Batch   33/81   train_loss = 0.752\n",
      "Epoch 1327 Batch   65/81   train_loss = 0.797\n",
      "Epoch 1328 Batch   16/81   train_loss = 0.848\n",
      "Epoch 1328 Batch   48/81   train_loss = 0.783\n",
      "Epoch 1328 Batch   80/81   train_loss = 0.803\n",
      "Epoch 1329 Batch   31/81   train_loss = 0.784\n",
      "Epoch 1329 Batch   63/81   train_loss = 0.808\n",
      "Epoch 1330 Batch   14/81   train_loss = 0.798\n",
      "Epoch 1330 Batch   46/81   train_loss = 0.783\n",
      "Epoch 1330 Batch   78/81   train_loss = 0.787\n",
      "Epoch 1331 Batch   29/81   train_loss = 0.773\n",
      "Epoch 1331 Batch   61/81   train_loss = 0.784\n",
      "Epoch 1332 Batch   12/81   train_loss = 0.802\n",
      "Epoch 1332 Batch   44/81   train_loss = 0.799\n",
      "Epoch 1332 Batch   76/81   train_loss = 0.803\n",
      "Epoch 1333 Batch   27/81   train_loss = 0.769\n",
      "Epoch 1333 Batch   59/81   train_loss = 0.771\n",
      "Epoch 1334 Batch   10/81   train_loss = 0.797\n",
      "Epoch 1334 Batch   42/81   train_loss = 0.774\n",
      "Epoch 1334 Batch   74/81   train_loss = 0.779\n",
      "Epoch 1335 Batch   25/81   train_loss = 0.791\n",
      "Epoch 1335 Batch   57/81   train_loss = 0.798\n",
      "Epoch 1336 Batch    8/81   train_loss = 0.791\n",
      "Epoch 1336 Batch   40/81   train_loss = 0.743\n",
      "Epoch 1336 Batch   72/81   train_loss = 0.772\n",
      "Epoch 1337 Batch   23/81   train_loss = 0.769\n",
      "Epoch 1337 Batch   55/81   train_loss = 0.798\n",
      "Epoch 1338 Batch    6/81   train_loss = 0.775\n",
      "Epoch 1338 Batch   38/81   train_loss = 0.796\n",
      "Epoch 1338 Batch   70/81   train_loss = 0.774\n",
      "Epoch 1339 Batch   21/81   train_loss = 0.819\n",
      "Epoch 1339 Batch   53/81   train_loss = 0.770\n",
      "Epoch 1340 Batch    4/81   train_loss = 0.778\n",
      "Epoch 1340 Batch   36/81   train_loss = 0.798\n",
      "Epoch 1340 Batch   68/81   train_loss = 0.801\n",
      "Epoch 1341 Batch   19/81   train_loss = 0.765\n",
      "Epoch 1341 Batch   51/81   train_loss = 0.812\n",
      "Epoch 1342 Batch    2/81   train_loss = 0.782\n",
      "Epoch 1342 Batch   34/81   train_loss = 0.800\n",
      "Epoch 1342 Batch   66/81   train_loss = 0.785\n",
      "Epoch 1343 Batch   17/81   train_loss = 0.785\n",
      "Epoch 1343 Batch   49/81   train_loss = 0.773\n",
      "Epoch 1344 Batch    0/81   train_loss = 0.790\n",
      "Epoch 1344 Batch   32/81   train_loss = 0.751\n",
      "Epoch 1344 Batch   64/81   train_loss = 0.790\n",
      "Epoch 1345 Batch   15/81   train_loss = 0.772\n",
      "Epoch 1345 Batch   47/81   train_loss = 0.769\n",
      "Epoch 1345 Batch   79/81   train_loss = 0.769\n",
      "Epoch 1346 Batch   30/81   train_loss = 0.769\n",
      "Epoch 1346 Batch   62/81   train_loss = 0.778\n",
      "Epoch 1347 Batch   13/81   train_loss = 0.776\n",
      "Epoch 1347 Batch   45/81   train_loss = 0.777\n",
      "Epoch 1347 Batch   77/81   train_loss = 0.781\n",
      "Epoch 1348 Batch   28/81   train_loss = 0.753\n",
      "Epoch 1348 Batch   60/81   train_loss = 0.785\n",
      "Epoch 1349 Batch   11/81   train_loss = 0.769\n",
      "Epoch 1349 Batch   43/81   train_loss = 0.753\n",
      "Epoch 1349 Batch   75/81   train_loss = 0.770\n",
      "Epoch 1350 Batch   26/81   train_loss = 0.779\n",
      "Epoch 1350 Batch   58/81   train_loss = 0.750\n",
      "Epoch 1351 Batch    9/81   train_loss = 0.793\n",
      "Epoch 1351 Batch   41/81   train_loss = 0.751\n",
      "Epoch 1351 Batch   73/81   train_loss = 0.782\n",
      "Epoch 1352 Batch   24/81   train_loss = 0.778\n",
      "Epoch 1352 Batch   56/81   train_loss = 0.786\n",
      "Epoch 1353 Batch    7/81   train_loss = 0.754\n",
      "Epoch 1353 Batch   39/81   train_loss = 0.748\n",
      "Epoch 1353 Batch   71/81   train_loss = 0.770\n",
      "Epoch 1354 Batch   22/81   train_loss = 0.801\n",
      "Epoch 1354 Batch   54/81   train_loss = 0.756\n",
      "Epoch 1355 Batch    5/81   train_loss = 0.786\n",
      "Epoch 1355 Batch   37/81   train_loss = 0.781\n",
      "Epoch 1355 Batch   69/81   train_loss = 0.767\n",
      "Epoch 1356 Batch   20/81   train_loss = 0.752\n",
      "Epoch 1356 Batch   52/81   train_loss = 0.811\n",
      "Epoch 1357 Batch    3/81   train_loss = 0.755\n",
      "Epoch 1357 Batch   35/81   train_loss = 0.765\n",
      "Epoch 1357 Batch   67/81   train_loss = 0.807\n",
      "Epoch 1358 Batch   18/81   train_loss = 0.793\n",
      "Epoch 1358 Batch   50/81   train_loss = 0.763\n",
      "Epoch 1359 Batch    1/81   train_loss = 0.774\n",
      "Epoch 1359 Batch   33/81   train_loss = 0.746\n",
      "Epoch 1359 Batch   65/81   train_loss = 0.801\n",
      "Epoch 1360 Batch   16/81   train_loss = 0.818\n",
      "Epoch 1360 Batch   48/81   train_loss = 0.770\n",
      "Epoch 1360 Batch   80/81   train_loss = 0.789\n",
      "Epoch 1361 Batch   31/81   train_loss = 0.758\n",
      "Epoch 1361 Batch   63/81   train_loss = 0.781\n",
      "Epoch 1362 Batch   14/81   train_loss = 0.773\n",
      "Epoch 1362 Batch   46/81   train_loss = 0.757\n",
      "Epoch 1362 Batch   78/81   train_loss = 0.775\n",
      "Epoch 1363 Batch   29/81   train_loss = 0.760\n",
      "Epoch 1363 Batch   61/81   train_loss = 0.792\n",
      "Epoch 1364 Batch   12/81   train_loss = 0.790\n",
      "Epoch 1364 Batch   44/81   train_loss = 0.778\n",
      "Epoch 1364 Batch   76/81   train_loss = 0.798\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1365 Batch   27/81   train_loss = 0.766\n",
      "Epoch 1365 Batch   59/81   train_loss = 0.766\n",
      "Epoch 1366 Batch   10/81   train_loss = 0.796\n",
      "Epoch 1366 Batch   42/81   train_loss = 0.772\n",
      "Epoch 1366 Batch   74/81   train_loss = 0.775\n",
      "Epoch 1367 Batch   25/81   train_loss = 0.771\n",
      "Epoch 1367 Batch   57/81   train_loss = 0.790\n",
      "Epoch 1368 Batch    8/81   train_loss = 0.796\n",
      "Epoch 1368 Batch   40/81   train_loss = 0.762\n",
      "Epoch 1368 Batch   72/81   train_loss = 0.758\n",
      "Epoch 1369 Batch   23/81   train_loss = 0.788\n",
      "Epoch 1369 Batch   55/81   train_loss = 0.781\n",
      "Epoch 1370 Batch    6/81   train_loss = 0.770\n",
      "Epoch 1370 Batch   38/81   train_loss = 0.797\n",
      "Epoch 1370 Batch   70/81   train_loss = 0.763\n",
      "Epoch 1371 Batch   21/81   train_loss = 0.810\n",
      "Epoch 1371 Batch   53/81   train_loss = 0.763\n",
      "Epoch 1372 Batch    4/81   train_loss = 0.781\n",
      "Epoch 1372 Batch   36/81   train_loss = 0.804\n",
      "Epoch 1372 Batch   68/81   train_loss = 0.813\n",
      "Epoch 1373 Batch   19/81   train_loss = 0.743\n",
      "Epoch 1373 Batch   51/81   train_loss = 0.813\n",
      "Epoch 1374 Batch    2/81   train_loss = 0.794\n",
      "Epoch 1374 Batch   34/81   train_loss = 0.791\n",
      "Epoch 1374 Batch   66/81   train_loss = 0.784\n",
      "Epoch 1375 Batch   17/81   train_loss = 0.776\n",
      "Epoch 1375 Batch   49/81   train_loss = 0.780\n",
      "Epoch 1376 Batch    0/81   train_loss = 0.791\n",
      "Epoch 1376 Batch   32/81   train_loss = 0.769\n",
      "Epoch 1376 Batch   64/81   train_loss = 0.791\n",
      "Epoch 1377 Batch   15/81   train_loss = 0.779\n",
      "Epoch 1377 Batch   47/81   train_loss = 0.787\n",
      "Epoch 1377 Batch   79/81   train_loss = 0.769\n",
      "Epoch 1378 Batch   30/81   train_loss = 0.769\n",
      "Epoch 1378 Batch   62/81   train_loss = 0.789\n",
      "Epoch 1379 Batch   13/81   train_loss = 0.797\n",
      "Epoch 1379 Batch   45/81   train_loss = 0.767\n",
      "Epoch 1379 Batch   77/81   train_loss = 0.794\n",
      "Epoch 1380 Batch   28/81   train_loss = 0.757\n",
      "Epoch 1380 Batch   60/81   train_loss = 0.784\n",
      "Epoch 1381 Batch   11/81   train_loss = 0.778\n",
      "Epoch 1381 Batch   43/81   train_loss = 0.749\n",
      "Epoch 1381 Batch   75/81   train_loss = 0.773\n",
      "Epoch 1382 Batch   26/81   train_loss = 0.778\n",
      "Epoch 1382 Batch   58/81   train_loss = 0.771\n",
      "Epoch 1383 Batch    9/81   train_loss = 0.812\n",
      "Epoch 1383 Batch   41/81   train_loss = 0.756\n",
      "Epoch 1383 Batch   73/81   train_loss = 0.797\n",
      "Epoch 1384 Batch   24/81   train_loss = 0.786\n",
      "Epoch 1384 Batch   56/81   train_loss = 0.796\n",
      "Epoch 1385 Batch    7/81   train_loss = 0.780\n",
      "Epoch 1385 Batch   39/81   train_loss = 0.765\n",
      "Epoch 1385 Batch   71/81   train_loss = 0.768\n",
      "Epoch 1386 Batch   22/81   train_loss = 0.788\n",
      "Epoch 1386 Batch   54/81   train_loss = 0.783\n",
      "Epoch 1387 Batch    5/81   train_loss = 0.790\n",
      "Epoch 1387 Batch   37/81   train_loss = 0.774\n",
      "Epoch 1387 Batch   69/81   train_loss = 0.803\n",
      "Epoch 1388 Batch   20/81   train_loss = 0.758\n",
      "Epoch 1388 Batch   52/81   train_loss = 0.812\n",
      "Epoch 1389 Batch    3/81   train_loss = 0.770\n",
      "Epoch 1389 Batch   35/81   train_loss = 0.756\n",
      "Epoch 1389 Batch   67/81   train_loss = 0.811\n",
      "Epoch 1390 Batch   18/81   train_loss = 0.799\n",
      "Epoch 1390 Batch   50/81   train_loss = 0.770\n",
      "Epoch 1391 Batch    1/81   train_loss = 0.793\n",
      "Epoch 1391 Batch   33/81   train_loss = 0.758\n",
      "Epoch 1391 Batch   65/81   train_loss = 0.791\n",
      "Epoch 1392 Batch   16/81   train_loss = 0.829\n",
      "Epoch 1392 Batch   48/81   train_loss = 0.757\n",
      "Epoch 1392 Batch   80/81   train_loss = 0.782\n",
      "Epoch 1393 Batch   31/81   train_loss = 0.758\n",
      "Epoch 1393 Batch   63/81   train_loss = 0.791\n",
      "Epoch 1394 Batch   14/81   train_loss = 0.791\n",
      "Epoch 1394 Batch   46/81   train_loss = 0.788\n",
      "Epoch 1394 Batch   78/81   train_loss = 0.791\n",
      "Epoch 1395 Batch   29/81   train_loss = 0.770\n",
      "Epoch 1395 Batch   61/81   train_loss = 0.766\n",
      "Epoch 1396 Batch   12/81   train_loss = 0.770\n",
      "Epoch 1396 Batch   44/81   train_loss = 0.791\n",
      "Epoch 1396 Batch   76/81   train_loss = 0.816\n",
      "Epoch 1397 Batch   27/81   train_loss = 0.761\n",
      "Epoch 1397 Batch   59/81   train_loss = 0.767\n",
      "Epoch 1398 Batch   10/81   train_loss = 0.799\n",
      "Epoch 1398 Batch   42/81   train_loss = 0.764\n",
      "Epoch 1398 Batch   74/81   train_loss = 0.784\n",
      "Epoch 1399 Batch   25/81   train_loss = 0.771\n",
      "Epoch 1399 Batch   57/81   train_loss = 0.809\n",
      "Epoch 1400 Batch    8/81   train_loss = 0.794\n",
      "Epoch 1400 Batch   40/81   train_loss = 0.759\n",
      "Epoch 1400 Batch   72/81   train_loss = 0.775\n",
      "Epoch 1401 Batch   23/81   train_loss = 0.772\n",
      "Epoch 1401 Batch   55/81   train_loss = 0.779\n",
      "Epoch 1402 Batch    6/81   train_loss = 0.781\n",
      "Epoch 1402 Batch   38/81   train_loss = 0.808\n",
      "Epoch 1402 Batch   70/81   train_loss = 0.781\n",
      "Epoch 1403 Batch   21/81   train_loss = 0.813\n",
      "Epoch 1403 Batch   53/81   train_loss = 0.769\n",
      "Epoch 1404 Batch    4/81   train_loss = 0.772\n",
      "Epoch 1404 Batch   36/81   train_loss = 0.805\n",
      "Epoch 1404 Batch   68/81   train_loss = 0.812\n",
      "Epoch 1405 Batch   19/81   train_loss = 0.755\n",
      "Epoch 1405 Batch   51/81   train_loss = 0.814\n",
      "Epoch 1406 Batch    2/81   train_loss = 0.816\n",
      "Epoch 1406 Batch   34/81   train_loss = 0.817\n",
      "Epoch 1406 Batch   66/81   train_loss = 0.811\n",
      "Epoch 1407 Batch   17/81   train_loss = 0.802\n",
      "Epoch 1407 Batch   49/81   train_loss = 0.791\n",
      "Epoch 1408 Batch    0/81   train_loss = 0.796\n",
      "Epoch 1408 Batch   32/81   train_loss = 0.789\n",
      "Epoch 1408 Batch   64/81   train_loss = 0.809\n",
      "Epoch 1409 Batch   15/81   train_loss = 0.782\n",
      "Epoch 1409 Batch   47/81   train_loss = 0.800\n",
      "Epoch 1409 Batch   79/81   train_loss = 0.788\n",
      "Epoch 1410 Batch   30/81   train_loss = 0.774\n",
      "Epoch 1410 Batch   62/81   train_loss = 0.769\n",
      "Epoch 1411 Batch   13/81   train_loss = 0.794\n",
      "Epoch 1411 Batch   45/81   train_loss = 0.775\n",
      "Epoch 1411 Batch   77/81   train_loss = 0.794\n",
      "Epoch 1412 Batch   28/81   train_loss = 0.757\n",
      "Epoch 1412 Batch   60/81   train_loss = 0.794\n",
      "Epoch 1413 Batch   11/81   train_loss = 0.767\n",
      "Epoch 1413 Batch   43/81   train_loss = 0.746\n",
      "Epoch 1413 Batch   75/81   train_loss = 0.753\n",
      "Epoch 1414 Batch   26/81   train_loss = 0.780\n",
      "Epoch 1414 Batch   58/81   train_loss = 0.777\n",
      "Epoch 1415 Batch    9/81   train_loss = 0.810\n",
      "Epoch 1415 Batch   41/81   train_loss = 0.750\n",
      "Epoch 1415 Batch   73/81   train_loss = 0.792\n",
      "Epoch 1416 Batch   24/81   train_loss = 0.767\n",
      "Epoch 1416 Batch   56/81   train_loss = 0.787\n",
      "Epoch 1417 Batch    7/81   train_loss = 0.773\n",
      "Epoch 1417 Batch   39/81   train_loss = 0.744\n",
      "Epoch 1417 Batch   71/81   train_loss = 0.764\n",
      "Epoch 1418 Batch   22/81   train_loss = 0.800\n",
      "Epoch 1418 Batch   54/81   train_loss = 0.773\n",
      "Epoch 1419 Batch    5/81   train_loss = 0.797\n",
      "Epoch 1419 Batch   37/81   train_loss = 0.790\n",
      "Epoch 1419 Batch   69/81   train_loss = 0.782\n",
      "Epoch 1420 Batch   20/81   train_loss = 0.768\n",
      "Epoch 1420 Batch   52/81   train_loss = 0.797\n",
      "Epoch 1421 Batch    3/81   train_loss = 0.767\n",
      "Epoch 1421 Batch   35/81   train_loss = 0.759\n",
      "Epoch 1421 Batch   67/81   train_loss = 0.798\n",
      "Epoch 1422 Batch   18/81   train_loss = 0.781\n",
      "Epoch 1422 Batch   50/81   train_loss = 0.759\n",
      "Epoch 1423 Batch    1/81   train_loss = 0.806\n",
      "Epoch 1423 Batch   33/81   train_loss = 0.757\n",
      "Epoch 1423 Batch   65/81   train_loss = 0.816\n",
      "Epoch 1424 Batch   16/81   train_loss = 0.821\n",
      "Epoch 1424 Batch   48/81   train_loss = 0.777\n",
      "Epoch 1424 Batch   80/81   train_loss = 0.784\n",
      "Epoch 1425 Batch   31/81   train_loss = 0.756\n",
      "Epoch 1425 Batch   63/81   train_loss = 0.791\n",
      "Epoch 1426 Batch   14/81   train_loss = 0.808\n",
      "Epoch 1426 Batch   46/81   train_loss = 0.773\n",
      "Epoch 1426 Batch   78/81   train_loss = 0.778\n",
      "Epoch 1427 Batch   29/81   train_loss = 0.756\n",
      "Epoch 1427 Batch   61/81   train_loss = 0.777\n",
      "Epoch 1428 Batch   12/81   train_loss = 0.797\n",
      "Epoch 1428 Batch   44/81   train_loss = 0.780\n",
      "Epoch 1428 Batch   76/81   train_loss = 0.813\n",
      "Epoch 1429 Batch   27/81   train_loss = 0.768\n",
      "Epoch 1429 Batch   59/81   train_loss = 0.762\n",
      "Epoch 1430 Batch   10/81   train_loss = 0.803\n",
      "Epoch 1430 Batch   42/81   train_loss = 0.778\n",
      "Epoch 1430 Batch   74/81   train_loss = 0.769\n",
      "Epoch 1431 Batch   25/81   train_loss = 0.796\n",
      "Epoch 1431 Batch   57/81   train_loss = 0.786\n",
      "Epoch 1432 Batch    8/81   train_loss = 0.787\n",
      "Epoch 1432 Batch   40/81   train_loss = 0.755\n",
      "Epoch 1432 Batch   72/81   train_loss = 0.764\n",
      "Epoch 1433 Batch   23/81   train_loss = 0.772\n",
      "Epoch 1433 Batch   55/81   train_loss = 0.794\n",
      "Epoch 1434 Batch    6/81   train_loss = 0.760\n",
      "Epoch 1434 Batch   38/81   train_loss = 0.794\n",
      "Epoch 1434 Batch   70/81   train_loss = 0.773\n",
      "Epoch 1435 Batch   21/81   train_loss = 0.818\n",
      "Epoch 1435 Batch   53/81   train_loss = 0.759\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1436 Batch    4/81   train_loss = 0.788\n",
      "Epoch 1436 Batch   36/81   train_loss = 0.795\n",
      "Epoch 1436 Batch   68/81   train_loss = 0.797\n",
      "Epoch 1437 Batch   19/81   train_loss = 0.763\n",
      "Epoch 1437 Batch   51/81   train_loss = 0.816\n",
      "Epoch 1438 Batch    2/81   train_loss = 0.790\n",
      "Epoch 1438 Batch   34/81   train_loss = 0.795\n",
      "Epoch 1438 Batch   66/81   train_loss = 0.769\n",
      "Epoch 1439 Batch   17/81   train_loss = 0.799\n",
      "Epoch 1439 Batch   49/81   train_loss = 0.775\n",
      "Epoch 1440 Batch    0/81   train_loss = 0.774\n",
      "Epoch 1440 Batch   32/81   train_loss = 0.759\n",
      "Epoch 1440 Batch   64/81   train_loss = 0.782\n",
      "Epoch 1441 Batch   15/81   train_loss = 0.795\n",
      "Epoch 1441 Batch   47/81   train_loss = 0.772\n",
      "Epoch 1441 Batch   79/81   train_loss = 0.775\n",
      "Epoch 1442 Batch   30/81   train_loss = 0.780\n",
      "Epoch 1442 Batch   62/81   train_loss = 0.775\n",
      "Epoch 1443 Batch   13/81   train_loss = 0.780\n",
      "Epoch 1443 Batch   45/81   train_loss = 0.757\n",
      "Epoch 1443 Batch   77/81   train_loss = 0.790\n",
      "Epoch 1444 Batch   28/81   train_loss = 0.759\n",
      "Epoch 1444 Batch   60/81   train_loss = 0.787\n",
      "Epoch 1445 Batch   11/81   train_loss = 0.781\n",
      "Epoch 1445 Batch   43/81   train_loss = 0.747\n",
      "Epoch 1445 Batch   75/81   train_loss = 0.757\n",
      "Epoch 1446 Batch   26/81   train_loss = 0.788\n",
      "Epoch 1446 Batch   58/81   train_loss = 0.752\n",
      "Epoch 1447 Batch    9/81   train_loss = 0.793\n",
      "Epoch 1447 Batch   41/81   train_loss = 0.731\n",
      "Epoch 1447 Batch   73/81   train_loss = 0.785\n",
      "Epoch 1448 Batch   24/81   train_loss = 0.767\n",
      "Epoch 1448 Batch   56/81   train_loss = 0.784\n",
      "Epoch 1449 Batch    7/81   train_loss = 0.780\n",
      "Epoch 1449 Batch   39/81   train_loss = 0.745\n",
      "Epoch 1449 Batch   71/81   train_loss = 0.751\n",
      "Epoch 1450 Batch   22/81   train_loss = 0.780\n",
      "Epoch 1450 Batch   54/81   train_loss = 0.760\n",
      "Epoch 1451 Batch    5/81   train_loss = 0.789\n",
      "Epoch 1451 Batch   37/81   train_loss = 0.768\n",
      "Epoch 1451 Batch   69/81   train_loss = 0.775\n",
      "Epoch 1452 Batch   20/81   train_loss = 0.763\n",
      "Epoch 1452 Batch   52/81   train_loss = 0.794\n",
      "Epoch 1453 Batch    3/81   train_loss = 0.766\n",
      "Epoch 1453 Batch   35/81   train_loss = 0.765\n",
      "Epoch 1453 Batch   67/81   train_loss = 0.802\n",
      "Epoch 1454 Batch   18/81   train_loss = 0.811\n",
      "Epoch 1454 Batch   50/81   train_loss = 0.768\n",
      "Epoch 1455 Batch    1/81   train_loss = 0.785\n",
      "Epoch 1455 Batch   33/81   train_loss = 0.769\n",
      "Epoch 1455 Batch   65/81   train_loss = 0.782\n",
      "Epoch 1456 Batch   16/81   train_loss = 0.822\n",
      "Epoch 1456 Batch   48/81   train_loss = 0.769\n",
      "Epoch 1456 Batch   80/81   train_loss = 0.798\n",
      "Epoch 1457 Batch   31/81   train_loss = 0.764\n",
      "Epoch 1457 Batch   63/81   train_loss = 0.782\n",
      "Epoch 1458 Batch   14/81   train_loss = 0.795\n",
      "Epoch 1458 Batch   46/81   train_loss = 0.787\n",
      "Epoch 1458 Batch   78/81   train_loss = 0.784\n",
      "Epoch 1459 Batch   29/81   train_loss = 0.768\n",
      "Epoch 1459 Batch   61/81   train_loss = 0.783\n",
      "Epoch 1460 Batch   12/81   train_loss = 0.768\n",
      "Epoch 1460 Batch   44/81   train_loss = 0.781\n",
      "Epoch 1460 Batch   76/81   train_loss = 0.783\n",
      "Epoch 1461 Batch   27/81   train_loss = 0.749\n",
      "Epoch 1461 Batch   59/81   train_loss = 0.740\n",
      "Epoch 1462 Batch   10/81   train_loss = 0.778\n",
      "Epoch 1462 Batch   42/81   train_loss = 0.755\n",
      "Epoch 1462 Batch   74/81   train_loss = 0.763\n",
      "Epoch 1463 Batch   25/81   train_loss = 0.782\n",
      "Epoch 1463 Batch   57/81   train_loss = 0.782\n",
      "Epoch 1464 Batch    8/81   train_loss = 0.798\n",
      "Epoch 1464 Batch   40/81   train_loss = 0.743\n",
      "Epoch 1464 Batch   72/81   train_loss = 0.760\n",
      "Epoch 1465 Batch   23/81   train_loss = 0.764\n",
      "Epoch 1465 Batch   55/81   train_loss = 0.776\n",
      "Epoch 1466 Batch    6/81   train_loss = 0.766\n",
      "Epoch 1466 Batch   38/81   train_loss = 0.802\n",
      "Epoch 1466 Batch   70/81   train_loss = 0.774\n",
      "Epoch 1467 Batch   21/81   train_loss = 0.811\n",
      "Epoch 1467 Batch   53/81   train_loss = 0.758\n",
      "Epoch 1468 Batch    4/81   train_loss = 0.784\n",
      "Epoch 1468 Batch   36/81   train_loss = 0.796\n",
      "Epoch 1468 Batch   68/81   train_loss = 0.790\n",
      "Epoch 1469 Batch   19/81   train_loss = 0.791\n",
      "Epoch 1469 Batch   51/81   train_loss = 0.800\n",
      "Epoch 1470 Batch    2/81   train_loss = 0.803\n",
      "Epoch 1470 Batch   34/81   train_loss = 0.788\n",
      "Epoch 1470 Batch   66/81   train_loss = 0.785\n",
      "Epoch 1471 Batch   17/81   train_loss = 0.821\n",
      "Epoch 1471 Batch   49/81   train_loss = 0.774\n",
      "Epoch 1472 Batch    0/81   train_loss = 0.779\n",
      "Epoch 1472 Batch   32/81   train_loss = 0.761\n",
      "Epoch 1472 Batch   64/81   train_loss = 0.785\n",
      "Epoch 1473 Batch   15/81   train_loss = 0.785\n",
      "Epoch 1473 Batch   47/81   train_loss = 0.771\n",
      "Epoch 1473 Batch   79/81   train_loss = 0.793\n",
      "Epoch 1474 Batch   30/81   train_loss = 0.774\n",
      "Epoch 1474 Batch   62/81   train_loss = 0.763\n",
      "Epoch 1475 Batch   13/81   train_loss = 0.798\n",
      "Epoch 1475 Batch   45/81   train_loss = 0.753\n",
      "Epoch 1475 Batch   77/81   train_loss = 0.782\n",
      "Epoch 1476 Batch   28/81   train_loss = 0.750\n",
      "Epoch 1476 Batch   60/81   train_loss = 0.778\n",
      "Epoch 1477 Batch   11/81   train_loss = 0.773\n",
      "Epoch 1477 Batch   43/81   train_loss = 0.734\n",
      "Epoch 1477 Batch   75/81   train_loss = 0.755\n",
      "Epoch 1478 Batch   26/81   train_loss = 0.775\n",
      "Epoch 1478 Batch   58/81   train_loss = 0.750\n",
      "Epoch 1479 Batch    9/81   train_loss = 0.810\n",
      "Epoch 1479 Batch   41/81   train_loss = 0.747\n",
      "Epoch 1479 Batch   73/81   train_loss = 0.788\n",
      "Epoch 1480 Batch   24/81   train_loss = 0.748\n",
      "Epoch 1480 Batch   56/81   train_loss = 0.795\n",
      "Epoch 1481 Batch    7/81   train_loss = 0.782\n",
      "Epoch 1481 Batch   39/81   train_loss = 0.769\n",
      "Epoch 1481 Batch   71/81   train_loss = 0.746\n",
      "Epoch 1482 Batch   22/81   train_loss = 0.775\n",
      "Epoch 1482 Batch   54/81   train_loss = 0.751\n",
      "Epoch 1483 Batch    5/81   train_loss = 0.783\n",
      "Epoch 1483 Batch   37/81   train_loss = 0.766\n",
      "Epoch 1483 Batch   69/81   train_loss = 0.794\n",
      "Epoch 1484 Batch   20/81   train_loss = 0.762\n",
      "Epoch 1484 Batch   52/81   train_loss = 0.787\n",
      "Epoch 1485 Batch    3/81   train_loss = 0.777\n",
      "Epoch 1485 Batch   35/81   train_loss = 0.764\n",
      "Epoch 1485 Batch   67/81   train_loss = 0.805\n",
      "Epoch 1486 Batch   18/81   train_loss = 0.806\n",
      "Epoch 1486 Batch   50/81   train_loss = 0.770\n",
      "Epoch 1487 Batch    1/81   train_loss = 0.796\n",
      "Epoch 1487 Batch   33/81   train_loss = 0.754\n",
      "Epoch 1487 Batch   65/81   train_loss = 0.792\n",
      "Epoch 1488 Batch   16/81   train_loss = 0.816\n",
      "Epoch 1488 Batch   48/81   train_loss = 0.759\n",
      "Epoch 1488 Batch   80/81   train_loss = 0.782\n",
      "Epoch 1489 Batch   31/81   train_loss = 0.769\n",
      "Epoch 1489 Batch   63/81   train_loss = 0.779\n",
      "Epoch 1490 Batch   14/81   train_loss = 0.787\n",
      "Epoch 1490 Batch   46/81   train_loss = 0.778\n",
      "Epoch 1490 Batch   78/81   train_loss = 0.790\n",
      "Epoch 1491 Batch   29/81   train_loss = 0.776\n",
      "Epoch 1491 Batch   61/81   train_loss = 0.784\n",
      "Epoch 1492 Batch   12/81   train_loss = 0.778\n",
      "Epoch 1492 Batch   44/81   train_loss = 0.781\n",
      "Epoch 1492 Batch   76/81   train_loss = 0.800\n",
      "Epoch 1493 Batch   27/81   train_loss = 0.758\n",
      "Epoch 1493 Batch   59/81   train_loss = 0.762\n",
      "Epoch 1494 Batch   10/81   train_loss = 0.811\n",
      "Epoch 1494 Batch   42/81   train_loss = 0.752\n",
      "Epoch 1494 Batch   74/81   train_loss = 0.767\n",
      "Epoch 1495 Batch   25/81   train_loss = 0.778\n",
      "Epoch 1495 Batch   57/81   train_loss = 0.771\n",
      "Epoch 1496 Batch    8/81   train_loss = 0.778\n",
      "Epoch 1496 Batch   40/81   train_loss = 0.750\n",
      "Epoch 1496 Batch   72/81   train_loss = 0.759\n",
      "Epoch 1497 Batch   23/81   train_loss = 0.767\n",
      "Epoch 1497 Batch   55/81   train_loss = 0.796\n",
      "Epoch 1498 Batch    6/81   train_loss = 0.753\n",
      "Epoch 1498 Batch   38/81   train_loss = 0.785\n",
      "Epoch 1498 Batch   70/81   train_loss = 0.755\n",
      "Epoch 1499 Batch   21/81   train_loss = 0.804\n",
      "Epoch 1499 Batch   53/81   train_loss = 0.766\n",
      "Epoch 1500 Batch    4/81   train_loss = 0.770\n",
      "Epoch 1500 Batch   36/81   train_loss = 0.772\n",
      "Epoch 1500 Batch   68/81   train_loss = 0.773\n",
      "Epoch 1501 Batch   19/81   train_loss = 0.747\n",
      "Epoch 1501 Batch   51/81   train_loss = 0.795\n",
      "Epoch 1502 Batch    2/81   train_loss = 0.778\n",
      "Epoch 1502 Batch   34/81   train_loss = 0.783\n",
      "Epoch 1502 Batch   66/81   train_loss = 0.772\n",
      "Epoch 1503 Batch   17/81   train_loss = 0.779\n",
      "Epoch 1503 Batch   49/81   train_loss = 0.771\n",
      "Epoch 1504 Batch    0/81   train_loss = 0.780\n",
      "Epoch 1504 Batch   32/81   train_loss = 0.757\n",
      "Epoch 1504 Batch   64/81   train_loss = 0.777\n",
      "Epoch 1505 Batch   15/81   train_loss = 0.772\n",
      "Epoch 1505 Batch   47/81   train_loss = 0.754\n",
      "Epoch 1505 Batch   79/81   train_loss = 0.756\n",
      "Epoch 1506 Batch   30/81   train_loss = 0.755\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1506 Batch   62/81   train_loss = 0.781\n",
      "Epoch 1507 Batch   13/81   train_loss = 0.802\n",
      "Epoch 1507 Batch   45/81   train_loss = 0.774\n",
      "Epoch 1507 Batch   77/81   train_loss = 0.772\n",
      "Epoch 1508 Batch   28/81   train_loss = 0.741\n",
      "Epoch 1508 Batch   60/81   train_loss = 0.786\n",
      "Epoch 1509 Batch   11/81   train_loss = 0.772\n",
      "Epoch 1509 Batch   43/81   train_loss = 0.746\n",
      "Epoch 1509 Batch   75/81   train_loss = 0.748\n",
      "Epoch 1510 Batch   26/81   train_loss = 0.777\n",
      "Epoch 1510 Batch   58/81   train_loss = 0.755\n",
      "Epoch 1511 Batch    9/81   train_loss = 0.800\n",
      "Epoch 1511 Batch   41/81   train_loss = 0.749\n",
      "Epoch 1511 Batch   73/81   train_loss = 0.784\n",
      "Epoch 1512 Batch   24/81   train_loss = 0.760\n",
      "Epoch 1512 Batch   56/81   train_loss = 0.767\n",
      "Epoch 1513 Batch    7/81   train_loss = 0.762\n",
      "Epoch 1513 Batch   39/81   train_loss = 0.734\n",
      "Epoch 1513 Batch   71/81   train_loss = 0.750\n",
      "Epoch 1514 Batch   22/81   train_loss = 0.780\n",
      "Epoch 1514 Batch   54/81   train_loss = 0.745\n",
      "Epoch 1515 Batch    5/81   train_loss = 0.782\n",
      "Epoch 1515 Batch   37/81   train_loss = 0.766\n",
      "Epoch 1515 Batch   69/81   train_loss = 0.788\n",
      "Epoch 1516 Batch   20/81   train_loss = 0.760\n",
      "Epoch 1516 Batch   52/81   train_loss = 0.788\n",
      "Epoch 1517 Batch    3/81   train_loss = 0.747\n",
      "Epoch 1517 Batch   35/81   train_loss = 0.748\n",
      "Epoch 1517 Batch   67/81   train_loss = 0.796\n",
      "Epoch 1518 Batch   18/81   train_loss = 0.786\n",
      "Epoch 1518 Batch   50/81   train_loss = 0.769\n",
      "Epoch 1519 Batch    1/81   train_loss = 0.773\n",
      "Epoch 1519 Batch   33/81   train_loss = 0.752\n",
      "Epoch 1519 Batch   65/81   train_loss = 0.775\n",
      "Epoch 1520 Batch   16/81   train_loss = 0.812\n",
      "Epoch 1520 Batch   48/81   train_loss = 0.774\n",
      "Epoch 1520 Batch   80/81   train_loss = 0.773\n",
      "Epoch 1521 Batch   31/81   train_loss = 0.745\n",
      "Epoch 1521 Batch   63/81   train_loss = 0.769\n",
      "Epoch 1522 Batch   14/81   train_loss = 0.792\n",
      "Epoch 1522 Batch   46/81   train_loss = 0.771\n",
      "Epoch 1522 Batch   78/81   train_loss = 0.785\n",
      "Epoch 1523 Batch   29/81   train_loss = 0.760\n",
      "Epoch 1523 Batch   61/81   train_loss = 0.766\n",
      "Epoch 1524 Batch   12/81   train_loss = 0.769\n",
      "Epoch 1524 Batch   44/81   train_loss = 0.767\n",
      "Epoch 1524 Batch   76/81   train_loss = 0.785\n",
      "Epoch 1525 Batch   27/81   train_loss = 0.735\n",
      "Epoch 1525 Batch   59/81   train_loss = 0.746\n",
      "Epoch 1526 Batch   10/81   train_loss = 0.783\n",
      "Epoch 1526 Batch   42/81   train_loss = 0.756\n",
      "Epoch 1526 Batch   74/81   train_loss = 0.776\n",
      "Epoch 1527 Batch   25/81   train_loss = 0.781\n",
      "Epoch 1527 Batch   57/81   train_loss = 0.771\n",
      "Epoch 1528 Batch    8/81   train_loss = 0.786\n",
      "Epoch 1528 Batch   40/81   train_loss = 0.748\n",
      "Epoch 1528 Batch   72/81   train_loss = 0.742\n",
      "Epoch 1529 Batch   23/81   train_loss = 0.762\n",
      "Epoch 1529 Batch   55/81   train_loss = 0.761\n",
      "Epoch 1530 Batch    6/81   train_loss = 0.755\n",
      "Epoch 1530 Batch   38/81   train_loss = 0.784\n",
      "Epoch 1530 Batch   70/81   train_loss = 0.760\n",
      "Epoch 1531 Batch   21/81   train_loss = 0.804\n",
      "Epoch 1531 Batch   53/81   train_loss = 0.759\n",
      "Epoch 1532 Batch    4/81   train_loss = 0.789\n",
      "Epoch 1532 Batch   36/81   train_loss = 0.788\n",
      "Epoch 1532 Batch   68/81   train_loss = 0.784\n",
      "Epoch 1533 Batch   19/81   train_loss = 0.743\n",
      "Epoch 1533 Batch   51/81   train_loss = 0.806\n",
      "Epoch 1534 Batch    2/81   train_loss = 0.784\n",
      "Epoch 1534 Batch   34/81   train_loss = 0.772\n",
      "Epoch 1534 Batch   66/81   train_loss = 0.770\n",
      "Epoch 1535 Batch   17/81   train_loss = 0.778\n",
      "Epoch 1535 Batch   49/81   train_loss = 0.761\n",
      "Epoch 1536 Batch    0/81   train_loss = 0.781\n",
      "Epoch 1536 Batch   32/81   train_loss = 0.755\n",
      "Epoch 1536 Batch   64/81   train_loss = 0.792\n",
      "Epoch 1537 Batch   15/81   train_loss = 0.759\n",
      "Epoch 1537 Batch   47/81   train_loss = 0.773\n",
      "Epoch 1537 Batch   79/81   train_loss = 0.762\n",
      "Epoch 1538 Batch   30/81   train_loss = 0.765\n",
      "Epoch 1538 Batch   62/81   train_loss = 0.768\n",
      "Epoch 1539 Batch   13/81   train_loss = 0.796\n",
      "Epoch 1539 Batch   45/81   train_loss = 0.772\n",
      "Epoch 1539 Batch   77/81   train_loss = 0.792\n",
      "Epoch 1540 Batch   28/81   train_loss = 0.756\n",
      "Epoch 1540 Batch   60/81   train_loss = 0.795\n",
      "Epoch 1541 Batch   11/81   train_loss = 0.776\n",
      "Epoch 1541 Batch   43/81   train_loss = 0.739\n",
      "Epoch 1541 Batch   75/81   train_loss = 0.760\n",
      "Epoch 1542 Batch   26/81   train_loss = 0.768\n",
      "Epoch 1542 Batch   58/81   train_loss = 0.756\n",
      "Epoch 1543 Batch    9/81   train_loss = 0.788\n",
      "Epoch 1543 Batch   41/81   train_loss = 0.744\n",
      "Epoch 1543 Batch   73/81   train_loss = 0.784\n",
      "Epoch 1544 Batch   24/81   train_loss = 0.756\n",
      "Epoch 1544 Batch   56/81   train_loss = 0.772\n",
      "Epoch 1545 Batch    7/81   train_loss = 0.777\n",
      "Epoch 1545 Batch   39/81   train_loss = 0.740\n",
      "Epoch 1545 Batch   71/81   train_loss = 0.759\n",
      "Epoch 1546 Batch   22/81   train_loss = 0.780\n",
      "Epoch 1546 Batch   54/81   train_loss = 0.759\n",
      "Epoch 1547 Batch    5/81   train_loss = 0.797\n",
      "Epoch 1547 Batch   37/81   train_loss = 0.755\n",
      "Epoch 1547 Batch   69/81   train_loss = 0.794\n",
      "Epoch 1548 Batch   20/81   train_loss = 0.750\n",
      "Epoch 1548 Batch   52/81   train_loss = 0.784\n",
      "Epoch 1549 Batch    3/81   train_loss = 0.752\n",
      "Epoch 1549 Batch   35/81   train_loss = 0.750\n",
      "Epoch 1549 Batch   67/81   train_loss = 0.808\n",
      "Epoch 1550 Batch   18/81   train_loss = 0.791\n",
      "Epoch 1550 Batch   50/81   train_loss = 0.745\n",
      "Epoch 1551 Batch    1/81   train_loss = 0.775\n",
      "Epoch 1551 Batch   33/81   train_loss = 0.744\n",
      "Epoch 1551 Batch   65/81   train_loss = 0.785\n",
      "Epoch 1552 Batch   16/81   train_loss = 0.803\n",
      "Epoch 1552 Batch   48/81   train_loss = 0.774\n",
      "Epoch 1552 Batch   80/81   train_loss = 0.780\n",
      "Epoch 1553 Batch   31/81   train_loss = 0.748\n",
      "Epoch 1553 Batch   63/81   train_loss = 0.781\n",
      "Epoch 1554 Batch   14/81   train_loss = 0.776\n",
      "Epoch 1554 Batch   46/81   train_loss = 0.754\n",
      "Epoch 1554 Batch   78/81   train_loss = 0.782\n",
      "Epoch 1555 Batch   29/81   train_loss = 0.763\n",
      "Epoch 1555 Batch   61/81   train_loss = 0.769\n",
      "Epoch 1556 Batch   12/81   train_loss = 0.785\n",
      "Epoch 1556 Batch   44/81   train_loss = 0.772\n",
      "Epoch 1556 Batch   76/81   train_loss = 0.784\n",
      "Epoch 1557 Batch   27/81   train_loss = 0.724\n",
      "Epoch 1557 Batch   59/81   train_loss = 0.748\n",
      "Epoch 1558 Batch   10/81   train_loss = 0.795\n",
      "Epoch 1558 Batch   42/81   train_loss = 0.762\n",
      "Epoch 1558 Batch   74/81   train_loss = 0.767\n",
      "Epoch 1559 Batch   25/81   train_loss = 0.775\n",
      "Epoch 1559 Batch   57/81   train_loss = 0.786\n",
      "Epoch 1560 Batch    8/81   train_loss = 0.777\n",
      "Epoch 1560 Batch   40/81   train_loss = 0.744\n",
      "Epoch 1560 Batch   72/81   train_loss = 0.735\n",
      "Epoch 1561 Batch   23/81   train_loss = 0.746\n",
      "Epoch 1561 Batch   55/81   train_loss = 0.782\n",
      "Epoch 1562 Batch    6/81   train_loss = 0.760\n",
      "Epoch 1562 Batch   38/81   train_loss = 0.772\n",
      "Epoch 1562 Batch   70/81   train_loss = 0.755\n",
      "Epoch 1563 Batch   21/81   train_loss = 0.801\n",
      "Epoch 1563 Batch   53/81   train_loss = 0.755\n",
      "Epoch 1564 Batch    4/81   train_loss = 0.774\n",
      "Epoch 1564 Batch   36/81   train_loss = 0.786\n",
      "Epoch 1564 Batch   68/81   train_loss = 0.775\n",
      "Epoch 1565 Batch   19/81   train_loss = 0.738\n",
      "Epoch 1565 Batch   51/81   train_loss = 0.778\n",
      "Epoch 1566 Batch    2/81   train_loss = 0.777\n",
      "Epoch 1566 Batch   34/81   train_loss = 0.778\n",
      "Epoch 1566 Batch   66/81   train_loss = 0.763\n",
      "Epoch 1567 Batch   17/81   train_loss = 0.776\n",
      "Epoch 1567 Batch   49/81   train_loss = 0.766\n",
      "Epoch 1568 Batch    0/81   train_loss = 0.760\n",
      "Epoch 1568 Batch   32/81   train_loss = 0.737\n",
      "Epoch 1568 Batch   64/81   train_loss = 0.770\n",
      "Epoch 1569 Batch   15/81   train_loss = 0.763\n",
      "Epoch 1569 Batch   47/81   train_loss = 0.753\n",
      "Epoch 1569 Batch   79/81   train_loss = 0.763\n",
      "Epoch 1570 Batch   30/81   train_loss = 0.758\n",
      "Epoch 1570 Batch   62/81   train_loss = 0.755\n",
      "Epoch 1571 Batch   13/81   train_loss = 0.777\n",
      "Epoch 1571 Batch   45/81   train_loss = 0.748\n",
      "Epoch 1571 Batch   77/81   train_loss = 0.771\n",
      "Epoch 1572 Batch   28/81   train_loss = 0.733\n",
      "Epoch 1572 Batch   60/81   train_loss = 0.778\n",
      "Epoch 1573 Batch   11/81   train_loss = 0.768\n",
      "Epoch 1573 Batch   43/81   train_loss = 0.742\n",
      "Epoch 1573 Batch   75/81   train_loss = 0.762\n",
      "Epoch 1574 Batch   26/81   train_loss = 0.784\n",
      "Epoch 1574 Batch   58/81   train_loss = 0.743\n",
      "Epoch 1575 Batch    9/81   train_loss = 0.786\n",
      "Epoch 1575 Batch   41/81   train_loss = 0.733\n",
      "Epoch 1575 Batch   73/81   train_loss = 0.758\n",
      "Epoch 1576 Batch   24/81   train_loss = 0.762\n",
      "Epoch 1576 Batch   56/81   train_loss = 0.761\n",
      "Epoch 1577 Batch    7/81   train_loss = 0.775\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1577 Batch   39/81   train_loss = 0.739\n",
      "Epoch 1577 Batch   71/81   train_loss = 0.765\n",
      "Epoch 1578 Batch   22/81   train_loss = 0.787\n",
      "Epoch 1578 Batch   54/81   train_loss = 0.750\n",
      "Epoch 1579 Batch    5/81   train_loss = 0.776\n",
      "Epoch 1579 Batch   37/81   train_loss = 0.759\n",
      "Epoch 1579 Batch   69/81   train_loss = 0.784\n",
      "Epoch 1580 Batch   20/81   train_loss = 0.767\n",
      "Epoch 1580 Batch   52/81   train_loss = 0.802\n",
      "Epoch 1581 Batch    3/81   train_loss = 0.753\n",
      "Epoch 1581 Batch   35/81   train_loss = 0.757\n",
      "Epoch 1581 Batch   67/81   train_loss = 0.812\n",
      "Epoch 1582 Batch   18/81   train_loss = 0.791\n",
      "Epoch 1582 Batch   50/81   train_loss = 0.762\n",
      "Epoch 1583 Batch    1/81   train_loss = 0.787\n",
      "Epoch 1583 Batch   33/81   train_loss = 0.769\n",
      "Epoch 1583 Batch   65/81   train_loss = 0.792\n",
      "Epoch 1584 Batch   16/81   train_loss = 0.806\n",
      "Epoch 1584 Batch   48/81   train_loss = 0.764\n",
      "Epoch 1584 Batch   80/81   train_loss = 0.780\n",
      "Epoch 1585 Batch   31/81   train_loss = 0.748\n",
      "Epoch 1585 Batch   63/81   train_loss = 0.775\n",
      "Epoch 1586 Batch   14/81   train_loss = 0.781\n",
      "Epoch 1586 Batch   46/81   train_loss = 0.750\n",
      "Epoch 1586 Batch   78/81   train_loss = 0.788\n",
      "Epoch 1587 Batch   29/81   train_loss = 0.766\n",
      "Epoch 1587 Batch   61/81   train_loss = 0.769\n",
      "Epoch 1588 Batch   12/81   train_loss = 0.787\n",
      "Epoch 1588 Batch   44/81   train_loss = 0.769\n",
      "Epoch 1588 Batch   76/81   train_loss = 0.801\n",
      "Epoch 1589 Batch   27/81   train_loss = 0.778\n",
      "Epoch 1589 Batch   59/81   train_loss = 0.779\n",
      "Epoch 1590 Batch   10/81   train_loss = 0.811\n",
      "Epoch 1590 Batch   42/81   train_loss = 0.778\n",
      "Epoch 1590 Batch   74/81   train_loss = 0.787\n",
      "Epoch 1591 Batch   25/81   train_loss = 0.787\n",
      "Epoch 1591 Batch   57/81   train_loss = 0.780\n",
      "Epoch 1592 Batch    8/81   train_loss = 0.794\n",
      "Epoch 1592 Batch   40/81   train_loss = 0.759\n",
      "Epoch 1592 Batch   72/81   train_loss = 0.758\n",
      "Epoch 1593 Batch   23/81   train_loss = 0.767\n",
      "Epoch 1593 Batch   55/81   train_loss = 0.769\n",
      "Epoch 1594 Batch    6/81   train_loss = 0.773\n",
      "Epoch 1594 Batch   38/81   train_loss = 0.792\n",
      "Epoch 1594 Batch   70/81   train_loss = 0.762\n",
      "Epoch 1595 Batch   21/81   train_loss = 0.806\n",
      "Epoch 1595 Batch   53/81   train_loss = 0.760\n",
      "Epoch 1596 Batch    4/81   train_loss = 0.791\n",
      "Epoch 1596 Batch   36/81   train_loss = 0.795\n",
      "Epoch 1596 Batch   68/81   train_loss = 0.783\n",
      "Epoch 1597 Batch   19/81   train_loss = 0.762\n",
      "Epoch 1597 Batch   51/81   train_loss = 0.792\n",
      "Epoch 1598 Batch    2/81   train_loss = 0.788\n",
      "Epoch 1598 Batch   34/81   train_loss = 0.793\n",
      "Epoch 1598 Batch   66/81   train_loss = 0.783\n",
      "Epoch 1599 Batch   17/81   train_loss = 0.785\n",
      "Epoch 1599 Batch   49/81   train_loss = 0.799\n",
      "Epoch 1600 Batch    0/81   train_loss = 0.760\n",
      "Epoch 1600 Batch   32/81   train_loss = 0.757\n",
      "Epoch 1600 Batch   64/81   train_loss = 0.791\n",
      "Epoch 1601 Batch   15/81   train_loss = 0.759\n",
      "Epoch 1601 Batch   47/81   train_loss = 0.778\n",
      "Epoch 1601 Batch   79/81   train_loss = 0.777\n",
      "Epoch 1602 Batch   30/81   train_loss = 0.769\n",
      "Epoch 1602 Batch   62/81   train_loss = 0.773\n",
      "Epoch 1603 Batch   13/81   train_loss = 0.771\n",
      "Epoch 1603 Batch   45/81   train_loss = 0.743\n",
      "Epoch 1603 Batch   77/81   train_loss = 0.794\n",
      "Epoch 1604 Batch   28/81   train_loss = 0.750\n",
      "Epoch 1604 Batch   60/81   train_loss = 0.773\n",
      "Epoch 1605 Batch   11/81   train_loss = 0.788\n",
      "Epoch 1605 Batch   43/81   train_loss = 0.754\n",
      "Epoch 1605 Batch   75/81   train_loss = 0.780\n",
      "Epoch 1606 Batch   26/81   train_loss = 0.777\n",
      "Epoch 1606 Batch   58/81   train_loss = 0.761\n",
      "Epoch 1607 Batch    9/81   train_loss = 0.799\n",
      "Epoch 1607 Batch   41/81   train_loss = 0.742\n",
      "Epoch 1607 Batch   73/81   train_loss = 0.787\n",
      "Epoch 1608 Batch   24/81   train_loss = 0.760\n",
      "Epoch 1608 Batch   56/81   train_loss = 0.782\n",
      "Epoch 1609 Batch    7/81   train_loss = 0.771\n",
      "Epoch 1609 Batch   39/81   train_loss = 0.735\n",
      "Epoch 1609 Batch   71/81   train_loss = 0.760\n",
      "Epoch 1610 Batch   22/81   train_loss = 0.776\n",
      "Epoch 1610 Batch   54/81   train_loss = 0.741\n",
      "Epoch 1611 Batch    5/81   train_loss = 0.788\n",
      "Epoch 1611 Batch   37/81   train_loss = 0.773\n",
      "Epoch 1611 Batch   69/81   train_loss = 0.778\n",
      "Epoch 1612 Batch   20/81   train_loss = 0.774\n",
      "Epoch 1612 Batch   52/81   train_loss = 0.788\n",
      "Epoch 1613 Batch    3/81   train_loss = 0.767\n",
      "Epoch 1613 Batch   35/81   train_loss = 0.751\n",
      "Epoch 1613 Batch   67/81   train_loss = 0.790\n",
      "Epoch 1614 Batch   18/81   train_loss = 0.816\n",
      "Epoch 1614 Batch   50/81   train_loss = 0.776\n",
      "Epoch 1615 Batch    1/81   train_loss = 0.791\n",
      "Epoch 1615 Batch   33/81   train_loss = 0.760\n",
      "Epoch 1615 Batch   65/81   train_loss = 0.797\n",
      "Epoch 1616 Batch   16/81   train_loss = 0.811\n",
      "Epoch 1616 Batch   48/81   train_loss = 0.775\n",
      "Epoch 1616 Batch   80/81   train_loss = 0.800\n",
      "Epoch 1617 Batch   31/81   train_loss = 0.737\n",
      "Epoch 1617 Batch   63/81   train_loss = 0.779\n",
      "Epoch 1618 Batch   14/81   train_loss = 0.780\n",
      "Epoch 1618 Batch   46/81   train_loss = 0.762\n",
      "Epoch 1618 Batch   78/81   train_loss = 0.772\n",
      "Epoch 1619 Batch   29/81   train_loss = 0.767\n",
      "Epoch 1619 Batch   61/81   train_loss = 0.774\n",
      "Epoch 1620 Batch   12/81   train_loss = 0.770\n",
      "Epoch 1620 Batch   44/81   train_loss = 0.771\n",
      "Epoch 1620 Batch   76/81   train_loss = 0.801\n",
      "Epoch 1621 Batch   27/81   train_loss = 0.732\n",
      "Epoch 1621 Batch   59/81   train_loss = 0.761\n",
      "Epoch 1622 Batch   10/81   train_loss = 0.791\n",
      "Epoch 1622 Batch   42/81   train_loss = 0.764\n",
      "Epoch 1622 Batch   74/81   train_loss = 0.772\n",
      "Epoch 1623 Batch   25/81   train_loss = 0.766\n",
      "Epoch 1623 Batch   57/81   train_loss = 0.785\n",
      "Epoch 1624 Batch    8/81   train_loss = 0.786\n",
      "Epoch 1624 Batch   40/81   train_loss = 0.737\n",
      "Epoch 1624 Batch   72/81   train_loss = 0.759\n",
      "Epoch 1625 Batch   23/81   train_loss = 0.756\n",
      "Epoch 1625 Batch   55/81   train_loss = 0.770\n",
      "Epoch 1626 Batch    6/81   train_loss = 0.755\n",
      "Epoch 1626 Batch   38/81   train_loss = 0.786\n",
      "Epoch 1626 Batch   70/81   train_loss = 0.762\n",
      "Epoch 1627 Batch   21/81   train_loss = 0.802\n",
      "Epoch 1627 Batch   53/81   train_loss = 0.756\n",
      "Epoch 1628 Batch    4/81   train_loss = 0.772\n",
      "Epoch 1628 Batch   36/81   train_loss = 0.795\n",
      "Epoch 1628 Batch   68/81   train_loss = 0.791\n",
      "Epoch 1629 Batch   19/81   train_loss = 0.751\n",
      "Epoch 1629 Batch   51/81   train_loss = 0.806\n",
      "Epoch 1630 Batch    2/81   train_loss = 0.795\n",
      "Epoch 1630 Batch   34/81   train_loss = 0.799\n",
      "Epoch 1630 Batch   66/81   train_loss = 0.780\n",
      "Epoch 1631 Batch   17/81   train_loss = 0.787\n",
      "Epoch 1631 Batch   49/81   train_loss = 0.771\n",
      "Epoch 1632 Batch    0/81   train_loss = 0.765\n",
      "Epoch 1632 Batch   32/81   train_loss = 0.750\n",
      "Epoch 1632 Batch   64/81   train_loss = 0.785\n",
      "Epoch 1633 Batch   15/81   train_loss = 0.745\n",
      "Epoch 1633 Batch   47/81   train_loss = 0.751\n",
      "Epoch 1633 Batch   79/81   train_loss = 0.764\n",
      "Epoch 1634 Batch   30/81   train_loss = 0.780\n",
      "Epoch 1634 Batch   62/81   train_loss = 0.772\n",
      "Epoch 1635 Batch   13/81   train_loss = 0.818\n",
      "Epoch 1635 Batch   45/81   train_loss = 0.754\n",
      "Epoch 1635 Batch   77/81   train_loss = 0.785\n",
      "Epoch 1636 Batch   28/81   train_loss = 0.755\n",
      "Epoch 1636 Batch   60/81   train_loss = 0.769\n",
      "Epoch 1637 Batch   11/81   train_loss = 0.767\n",
      "Epoch 1637 Batch   43/81   train_loss = 0.745\n",
      "Epoch 1637 Batch   75/81   train_loss = 0.776\n",
      "Epoch 1638 Batch   26/81   train_loss = 0.771\n",
      "Epoch 1638 Batch   58/81   train_loss = 0.742\n",
      "Epoch 1639 Batch    9/81   train_loss = 0.788\n",
      "Epoch 1639 Batch   41/81   train_loss = 0.739\n",
      "Epoch 1639 Batch   73/81   train_loss = 0.780\n",
      "Epoch 1640 Batch   24/81   train_loss = 0.781\n",
      "Epoch 1640 Batch   56/81   train_loss = 0.789\n",
      "Epoch 1641 Batch    7/81   train_loss = 0.791\n",
      "Epoch 1641 Batch   39/81   train_loss = 0.733\n",
      "Epoch 1641 Batch   71/81   train_loss = 0.765\n",
      "Epoch 1642 Batch   22/81   train_loss = 0.788\n",
      "Epoch 1642 Batch   54/81   train_loss = 0.768\n",
      "Epoch 1643 Batch    5/81   train_loss = 0.795\n",
      "Epoch 1643 Batch   37/81   train_loss = 0.773\n",
      "Epoch 1643 Batch   69/81   train_loss = 0.787\n",
      "Epoch 1644 Batch   20/81   train_loss = 0.768\n",
      "Epoch 1644 Batch   52/81   train_loss = 0.788\n",
      "Epoch 1645 Batch    3/81   train_loss = 0.778\n",
      "Epoch 1645 Batch   35/81   train_loss = 0.760\n",
      "Epoch 1645 Batch   67/81   train_loss = 0.794\n",
      "Epoch 1646 Batch   18/81   train_loss = 0.835\n",
      "Epoch 1646 Batch   50/81   train_loss = 0.763\n",
      "Epoch 1647 Batch    1/81   train_loss = 0.825\n",
      "Epoch 1647 Batch   33/81   train_loss = 0.755\n",
      "Epoch 1647 Batch   65/81   train_loss = 0.808\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1648 Batch   16/81   train_loss = 0.814\n",
      "Epoch 1648 Batch   48/81   train_loss = 0.788\n",
      "Epoch 1648 Batch   80/81   train_loss = 0.815\n",
      "Epoch 1649 Batch   31/81   train_loss = 0.755\n",
      "Epoch 1649 Batch   63/81   train_loss = 0.790\n",
      "Epoch 1650 Batch   14/81   train_loss = 0.803\n",
      "Epoch 1650 Batch   46/81   train_loss = 0.769\n",
      "Epoch 1650 Batch   78/81   train_loss = 0.782\n",
      "Epoch 1651 Batch   29/81   train_loss = 0.768\n",
      "Epoch 1651 Batch   61/81   train_loss = 0.763\n",
      "Epoch 1652 Batch   12/81   train_loss = 0.788\n",
      "Epoch 1652 Batch   44/81   train_loss = 0.781\n",
      "Epoch 1652 Batch   76/81   train_loss = 0.803\n",
      "Epoch 1653 Batch   27/81   train_loss = 0.751\n",
      "Epoch 1653 Batch   59/81   train_loss = 0.756\n",
      "Epoch 1654 Batch   10/81   train_loss = 0.801\n",
      "Epoch 1654 Batch   42/81   train_loss = 0.765\n",
      "Epoch 1654 Batch   74/81   train_loss = 0.765\n",
      "Epoch 1655 Batch   25/81   train_loss = 0.767\n",
      "Epoch 1655 Batch   57/81   train_loss = 0.773\n",
      "Epoch 1656 Batch    8/81   train_loss = 0.790\n",
      "Epoch 1656 Batch   40/81   train_loss = 0.769\n",
      "Epoch 1656 Batch   72/81   train_loss = 0.771\n",
      "Epoch 1657 Batch   23/81   train_loss = 0.766\n",
      "Epoch 1657 Batch   55/81   train_loss = 0.777\n",
      "Epoch 1658 Batch    6/81   train_loss = 0.767\n",
      "Epoch 1658 Batch   38/81   train_loss = 0.787\n",
      "Epoch 1658 Batch   70/81   train_loss = 0.770\n",
      "Epoch 1659 Batch   21/81   train_loss = 0.818\n",
      "Epoch 1659 Batch   53/81   train_loss = 0.757\n",
      "Epoch 1660 Batch    4/81   train_loss = 0.794\n",
      "Epoch 1660 Batch   36/81   train_loss = 0.802\n",
      "Epoch 1660 Batch   68/81   train_loss = 0.791\n",
      "Epoch 1661 Batch   19/81   train_loss = 0.765\n",
      "Epoch 1661 Batch   51/81   train_loss = 0.818\n",
      "Epoch 1662 Batch    2/81   train_loss = 0.787\n",
      "Epoch 1662 Batch   34/81   train_loss = 0.825\n",
      "Epoch 1662 Batch   66/81   train_loss = 0.765\n",
      "Epoch 1663 Batch   17/81   train_loss = 0.798\n",
      "Epoch 1663 Batch   49/81   train_loss = 0.776\n",
      "Epoch 1664 Batch    0/81   train_loss = 0.791\n",
      "Epoch 1664 Batch   32/81   train_loss = 0.767\n",
      "Epoch 1664 Batch   64/81   train_loss = 0.774\n",
      "Epoch 1665 Batch   15/81   train_loss = 0.775\n",
      "Epoch 1665 Batch   47/81   train_loss = 0.788\n",
      "Epoch 1665 Batch   79/81   train_loss = 0.785\n",
      "Epoch 1666 Batch   30/81   train_loss = 0.773\n",
      "Epoch 1666 Batch   62/81   train_loss = 0.762\n",
      "Epoch 1667 Batch   13/81   train_loss = 0.801\n",
      "Epoch 1667 Batch   45/81   train_loss = 0.760\n",
      "Epoch 1667 Batch   77/81   train_loss = 0.797\n",
      "Epoch 1668 Batch   28/81   train_loss = 0.766\n",
      "Epoch 1668 Batch   60/81   train_loss = 0.779\n",
      "Epoch 1669 Batch   11/81   train_loss = 0.773\n",
      "Epoch 1669 Batch   43/81   train_loss = 0.771\n",
      "Epoch 1669 Batch   75/81   train_loss = 0.764\n",
      "Epoch 1670 Batch   26/81   train_loss = 0.767\n",
      "Epoch 1670 Batch   58/81   train_loss = 0.748\n",
      "Epoch 1671 Batch    9/81   train_loss = 0.806\n",
      "Epoch 1671 Batch   41/81   train_loss = 0.737\n",
      "Epoch 1671 Batch   73/81   train_loss = 0.782\n",
      "Epoch 1672 Batch   24/81   train_loss = 0.759\n",
      "Epoch 1672 Batch   56/81   train_loss = 0.779\n",
      "Epoch 1673 Batch    7/81   train_loss = 0.759\n",
      "Epoch 1673 Batch   39/81   train_loss = 0.737\n",
      "Epoch 1673 Batch   71/81   train_loss = 0.750\n",
      "Epoch 1674 Batch   22/81   train_loss = 0.779\n",
      "Epoch 1674 Batch   54/81   train_loss = 0.744\n",
      "Epoch 1675 Batch    5/81   train_loss = 0.776\n",
      "Epoch 1675 Batch   37/81   train_loss = 0.759\n",
      "Epoch 1675 Batch   69/81   train_loss = 0.771\n",
      "Epoch 1676 Batch   20/81   train_loss = 0.752\n",
      "Epoch 1676 Batch   52/81   train_loss = 0.815\n",
      "Epoch 1677 Batch    3/81   train_loss = 0.769\n",
      "Epoch 1677 Batch   35/81   train_loss = 0.756\n",
      "Epoch 1677 Batch   67/81   train_loss = 0.787\n",
      "Epoch 1678 Batch   18/81   train_loss = 0.789\n",
      "Epoch 1678 Batch   50/81   train_loss = 0.763\n",
      "Epoch 1679 Batch    1/81   train_loss = 0.800\n",
      "Epoch 1679 Batch   33/81   train_loss = 0.753\n",
      "Epoch 1679 Batch   65/81   train_loss = 0.805\n",
      "Epoch 1680 Batch   16/81   train_loss = 0.816\n",
      "Epoch 1680 Batch   48/81   train_loss = 0.785\n",
      "Epoch 1680 Batch   80/81   train_loss = 0.811\n",
      "Epoch 1681 Batch   31/81   train_loss = 0.761\n",
      "Epoch 1681 Batch   63/81   train_loss = 0.794\n",
      "Epoch 1682 Batch   14/81   train_loss = 0.799\n",
      "Epoch 1682 Batch   46/81   train_loss = 0.780\n",
      "Epoch 1682 Batch   78/81   train_loss = 0.778\n",
      "Epoch 1683 Batch   29/81   train_loss = 0.776\n",
      "Epoch 1683 Batch   61/81   train_loss = 0.782\n",
      "Epoch 1684 Batch   12/81   train_loss = 0.779\n",
      "Epoch 1684 Batch   44/81   train_loss = 0.779\n",
      "Epoch 1684 Batch   76/81   train_loss = 0.796\n",
      "Epoch 1685 Batch   27/81   train_loss = 0.751\n",
      "Epoch 1685 Batch   59/81   train_loss = 0.753\n",
      "Epoch 1686 Batch   10/81   train_loss = 0.782\n",
      "Epoch 1686 Batch   42/81   train_loss = 0.786\n",
      "Epoch 1686 Batch   74/81   train_loss = 0.773\n",
      "Epoch 1687 Batch   25/81   train_loss = 0.809\n",
      "Epoch 1687 Batch   57/81   train_loss = 0.790\n",
      "Epoch 1688 Batch    8/81   train_loss = 0.788\n",
      "Epoch 1688 Batch   40/81   train_loss = 0.768\n",
      "Epoch 1688 Batch   72/81   train_loss = 0.756\n",
      "Epoch 1689 Batch   23/81   train_loss = 0.762\n",
      "Epoch 1689 Batch   55/81   train_loss = 0.778\n",
      "Epoch 1690 Batch    6/81   train_loss = 0.791\n",
      "Epoch 1690 Batch   38/81   train_loss = 0.792\n",
      "Epoch 1690 Batch   70/81   train_loss = 0.757\n",
      "Epoch 1691 Batch   21/81   train_loss = 0.829\n",
      "Epoch 1691 Batch   53/81   train_loss = 0.755\n",
      "Epoch 1692 Batch    4/81   train_loss = 0.798\n",
      "Epoch 1692 Batch   36/81   train_loss = 0.778\n",
      "Epoch 1692 Batch   68/81   train_loss = 0.796\n",
      "Epoch 1693 Batch   19/81   train_loss = 0.752\n",
      "Epoch 1693 Batch   51/81   train_loss = 0.807\n",
      "Epoch 1694 Batch    2/81   train_loss = 0.780\n",
      "Epoch 1694 Batch   34/81   train_loss = 0.793\n",
      "Epoch 1694 Batch   66/81   train_loss = 0.772\n",
      "Epoch 1695 Batch   17/81   train_loss = 0.776\n",
      "Epoch 1695 Batch   49/81   train_loss = 0.778\n",
      "Epoch 1696 Batch    0/81   train_loss = 0.766\n",
      "Epoch 1696 Batch   32/81   train_loss = 0.742\n",
      "Epoch 1696 Batch   64/81   train_loss = 0.771\n",
      "Epoch 1697 Batch   15/81   train_loss = 0.766\n",
      "Epoch 1697 Batch   47/81   train_loss = 0.759\n",
      "Epoch 1697 Batch   79/81   train_loss = 0.757\n",
      "Epoch 1698 Batch   30/81   train_loss = 0.761\n",
      "Epoch 1698 Batch   62/81   train_loss = 0.756\n",
      "Epoch 1699 Batch   13/81   train_loss = 0.794\n",
      "Epoch 1699 Batch   45/81   train_loss = 0.756\n",
      "Epoch 1699 Batch   77/81   train_loss = 0.776\n",
      "Epoch 1700 Batch   28/81   train_loss = 0.757\n",
      "Epoch 1700 Batch   60/81   train_loss = 0.771\n",
      "Epoch 1701 Batch   11/81   train_loss = 0.774\n",
      "Epoch 1701 Batch   43/81   train_loss = 0.751\n",
      "Epoch 1701 Batch   75/81   train_loss = 0.773\n",
      "Epoch 1702 Batch   26/81   train_loss = 0.762\n",
      "Epoch 1702 Batch   58/81   train_loss = 0.738\n",
      "Epoch 1703 Batch    9/81   train_loss = 0.807\n",
      "Epoch 1703 Batch   41/81   train_loss = 0.743\n",
      "Epoch 1703 Batch   73/81   train_loss = 0.786\n",
      "Epoch 1704 Batch   24/81   train_loss = 0.757\n",
      "Epoch 1704 Batch   56/81   train_loss = 0.768\n",
      "Epoch 1705 Batch    7/81   train_loss = 0.758\n",
      "Epoch 1705 Batch   39/81   train_loss = 0.749\n",
      "Epoch 1705 Batch   71/81   train_loss = 0.776\n",
      "Epoch 1706 Batch   22/81   train_loss = 0.802\n",
      "Epoch 1706 Batch   54/81   train_loss = 0.751\n",
      "Epoch 1707 Batch    5/81   train_loss = 0.790\n",
      "Epoch 1707 Batch   37/81   train_loss = 0.792\n",
      "Epoch 1707 Batch   69/81   train_loss = 0.781\n",
      "Epoch 1708 Batch   20/81   train_loss = 0.762\n",
      "Epoch 1708 Batch   52/81   train_loss = 0.790\n",
      "Epoch 1709 Batch    3/81   train_loss = 0.779\n",
      "Epoch 1709 Batch   35/81   train_loss = 0.764\n",
      "Epoch 1709 Batch   67/81   train_loss = 0.795\n",
      "Epoch 1710 Batch   18/81   train_loss = 0.803\n",
      "Epoch 1710 Batch   50/81   train_loss = 0.759\n",
      "Epoch 1711 Batch    1/81   train_loss = 0.806\n",
      "Epoch 1711 Batch   33/81   train_loss = 0.760\n",
      "Epoch 1711 Batch   65/81   train_loss = 0.810\n",
      "Epoch 1712 Batch   16/81   train_loss = 0.828\n",
      "Epoch 1712 Batch   48/81   train_loss = 0.796\n",
      "Epoch 1712 Batch   80/81   train_loss = 0.794\n",
      "Epoch 1713 Batch   31/81   train_loss = 0.747\n",
      "Epoch 1713 Batch   63/81   train_loss = 0.785\n",
      "Epoch 1714 Batch   14/81   train_loss = 0.793\n",
      "Epoch 1714 Batch   46/81   train_loss = 0.770\n",
      "Epoch 1714 Batch   78/81   train_loss = 0.796\n",
      "Epoch 1715 Batch   29/81   train_loss = 0.791\n",
      "Epoch 1715 Batch   61/81   train_loss = 0.783\n",
      "Epoch 1716 Batch   12/81   train_loss = 0.780\n",
      "Epoch 1716 Batch   44/81   train_loss = 0.784\n",
      "Epoch 1716 Batch   76/81   train_loss = 0.797\n",
      "Epoch 1717 Batch   27/81   train_loss = 0.760\n",
      "Epoch 1717 Batch   59/81   train_loss = 0.771\n",
      "Epoch 1718 Batch   10/81   train_loss = 0.804\n",
      "Epoch 1718 Batch   42/81   train_loss = 0.768\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1718 Batch   74/81   train_loss = 0.802\n",
      "Epoch 1719 Batch   25/81   train_loss = 0.798\n",
      "Epoch 1719 Batch   57/81   train_loss = 0.777\n",
      "Epoch 1720 Batch    8/81   train_loss = 0.800\n",
      "Epoch 1720 Batch   40/81   train_loss = 0.786\n",
      "Epoch 1720 Batch   72/81   train_loss = 0.759\n",
      "Epoch 1721 Batch   23/81   train_loss = 0.783\n",
      "Epoch 1721 Batch   55/81   train_loss = 0.789\n",
      "Epoch 1722 Batch    6/81   train_loss = 0.772\n",
      "Epoch 1722 Batch   38/81   train_loss = 0.771\n",
      "Epoch 1722 Batch   70/81   train_loss = 0.772\n",
      "Epoch 1723 Batch   21/81   train_loss = 0.805\n",
      "Epoch 1723 Batch   53/81   train_loss = 0.756\n",
      "Epoch 1724 Batch    4/81   train_loss = 0.776\n",
      "Epoch 1724 Batch   36/81   train_loss = 0.798\n",
      "Epoch 1724 Batch   68/81   train_loss = 0.817\n",
      "Epoch 1725 Batch   19/81   train_loss = 0.768\n",
      "Epoch 1725 Batch   51/81   train_loss = 0.815\n",
      "Epoch 1726 Batch    2/81   train_loss = 0.801\n",
      "Epoch 1726 Batch   34/81   train_loss = 0.799\n",
      "Epoch 1726 Batch   66/81   train_loss = 0.771\n",
      "Epoch 1727 Batch   17/81   train_loss = 0.799\n",
      "Epoch 1727 Batch   49/81   train_loss = 0.787\n",
      "Epoch 1728 Batch    0/81   train_loss = 0.771\n",
      "Epoch 1728 Batch   32/81   train_loss = 0.755\n",
      "Epoch 1728 Batch   64/81   train_loss = 0.780\n",
      "Epoch 1729 Batch   15/81   train_loss = 0.771\n",
      "Epoch 1729 Batch   47/81   train_loss = 0.788\n",
      "Epoch 1729 Batch   79/81   train_loss = 0.768\n",
      "Epoch 1730 Batch   30/81   train_loss = 0.771\n",
      "Epoch 1730 Batch   62/81   train_loss = 0.786\n",
      "Epoch 1731 Batch   13/81   train_loss = 0.803\n",
      "Epoch 1731 Batch   45/81   train_loss = 0.771\n",
      "Epoch 1731 Batch   77/81   train_loss = 0.798\n",
      "Epoch 1732 Batch   28/81   train_loss = 0.769\n",
      "Epoch 1732 Batch   60/81   train_loss = 0.778\n",
      "Epoch 1733 Batch   11/81   train_loss = 0.798\n",
      "Epoch 1733 Batch   43/81   train_loss = 0.758\n",
      "Epoch 1733 Batch   75/81   train_loss = 0.759\n",
      "Epoch 1734 Batch   26/81   train_loss = 0.790\n",
      "Epoch 1734 Batch   58/81   train_loss = 0.746\n",
      "Epoch 1735 Batch    9/81   train_loss = 0.782\n",
      "Epoch 1735 Batch   41/81   train_loss = 0.728\n",
      "Epoch 1735 Batch   73/81   train_loss = 0.777\n",
      "Epoch 1736 Batch   24/81   train_loss = 0.768\n",
      "Epoch 1736 Batch   56/81   train_loss = 0.785\n",
      "Epoch 1737 Batch    7/81   train_loss = 0.778\n",
      "Epoch 1737 Batch   39/81   train_loss = 0.742\n",
      "Epoch 1737 Batch   71/81   train_loss = 0.767\n",
      "Epoch 1738 Batch   22/81   train_loss = 0.806\n",
      "Epoch 1738 Batch   54/81   train_loss = 0.769\n",
      "Epoch 1739 Batch    5/81   train_loss = 0.785\n",
      "Epoch 1739 Batch   37/81   train_loss = 0.781\n",
      "Epoch 1739 Batch   69/81   train_loss = 0.779\n",
      "Epoch 1740 Batch   20/81   train_loss = 0.782\n",
      "Epoch 1740 Batch   52/81   train_loss = 0.807\n",
      "Epoch 1741 Batch    3/81   train_loss = 0.781\n",
      "Epoch 1741 Batch   35/81   train_loss = 0.767\n",
      "Epoch 1741 Batch   67/81   train_loss = 0.802\n",
      "Epoch 1742 Batch   18/81   train_loss = 0.793\n",
      "Epoch 1742 Batch   50/81   train_loss = 0.771\n",
      "Epoch 1743 Batch    1/81   train_loss = 0.801\n",
      "Epoch 1743 Batch   33/81   train_loss = 0.757\n",
      "Epoch 1743 Batch   65/81   train_loss = 0.810\n",
      "Epoch 1744 Batch   16/81   train_loss = 0.824\n",
      "Epoch 1744 Batch   48/81   train_loss = 0.794\n",
      "Epoch 1744 Batch   80/81   train_loss = 0.803\n",
      "Epoch 1745 Batch   31/81   train_loss = 0.766\n",
      "Epoch 1745 Batch   63/81   train_loss = 0.802\n",
      "Epoch 1746 Batch   14/81   train_loss = 0.780\n",
      "Epoch 1746 Batch   46/81   train_loss = 0.778\n",
      "Epoch 1746 Batch   78/81   train_loss = 0.781\n",
      "Epoch 1747 Batch   29/81   train_loss = 0.772\n",
      "Epoch 1747 Batch   61/81   train_loss = 0.759\n",
      "Epoch 1748 Batch   12/81   train_loss = 0.775\n",
      "Epoch 1748 Batch   44/81   train_loss = 0.782\n",
      "Epoch 1748 Batch   76/81   train_loss = 0.793\n",
      "Epoch 1749 Batch   27/81   train_loss = 0.766\n",
      "Epoch 1749 Batch   59/81   train_loss = 0.756\n",
      "Epoch 1750 Batch   10/81   train_loss = 0.773\n",
      "Epoch 1750 Batch   42/81   train_loss = 0.767\n",
      "Epoch 1750 Batch   74/81   train_loss = 0.771\n",
      "Epoch 1751 Batch   25/81   train_loss = 0.772\n",
      "Epoch 1751 Batch   57/81   train_loss = 0.761\n",
      "Epoch 1752 Batch    8/81   train_loss = 0.789\n",
      "Epoch 1752 Batch   40/81   train_loss = 0.765\n",
      "Epoch 1752 Batch   72/81   train_loss = 0.758\n",
      "Epoch 1753 Batch   23/81   train_loss = 0.767\n",
      "Epoch 1753 Batch   55/81   train_loss = 0.780\n",
      "Epoch 1754 Batch    6/81   train_loss = 0.753\n",
      "Epoch 1754 Batch   38/81   train_loss = 0.790\n",
      "Epoch 1754 Batch   70/81   train_loss = 0.743\n",
      "Epoch 1755 Batch   21/81   train_loss = 0.803\n",
      "Epoch 1755 Batch   53/81   train_loss = 0.766\n",
      "Epoch 1756 Batch    4/81   train_loss = 0.786\n",
      "Epoch 1756 Batch   36/81   train_loss = 0.782\n",
      "Epoch 1756 Batch   68/81   train_loss = 0.787\n",
      "Epoch 1757 Batch   19/81   train_loss = 0.740\n",
      "Epoch 1757 Batch   51/81   train_loss = 0.802\n",
      "Epoch 1758 Batch    2/81   train_loss = 0.778\n",
      "Epoch 1758 Batch   34/81   train_loss = 0.779\n",
      "Epoch 1758 Batch   66/81   train_loss = 0.769\n",
      "Epoch 1759 Batch   17/81   train_loss = 0.779\n",
      "Epoch 1759 Batch   49/81   train_loss = 0.772\n",
      "Epoch 1760 Batch    0/81   train_loss = 0.775\n",
      "Epoch 1760 Batch   32/81   train_loss = 0.763\n",
      "Epoch 1760 Batch   64/81   train_loss = 0.780\n",
      "Epoch 1761 Batch   15/81   train_loss = 0.766\n",
      "Epoch 1761 Batch   47/81   train_loss = 0.787\n",
      "Epoch 1761 Batch   79/81   train_loss = 0.776\n",
      "Epoch 1762 Batch   30/81   train_loss = 0.778\n",
      "Epoch 1762 Batch   62/81   train_loss = 0.772\n",
      "Epoch 1763 Batch   13/81   train_loss = 0.794\n",
      "Epoch 1763 Batch   45/81   train_loss = 0.768\n",
      "Epoch 1763 Batch   77/81   train_loss = 0.795\n",
      "Epoch 1764 Batch   28/81   train_loss = 0.764\n",
      "Epoch 1764 Batch   60/81   train_loss = 0.771\n",
      "Epoch 1765 Batch   11/81   train_loss = 0.769\n",
      "Epoch 1765 Batch   43/81   train_loss = 0.748\n",
      "Epoch 1765 Batch   75/81   train_loss = 0.755\n",
      "Epoch 1766 Batch   26/81   train_loss = 0.776\n",
      "Epoch 1766 Batch   58/81   train_loss = 0.754\n",
      "Epoch 1767 Batch    9/81   train_loss = 0.794\n",
      "Epoch 1767 Batch   41/81   train_loss = 0.739\n",
      "Epoch 1767 Batch   73/81   train_loss = 0.787\n",
      "Epoch 1768 Batch   24/81   train_loss = 0.752\n",
      "Epoch 1768 Batch   56/81   train_loss = 0.780\n",
      "Epoch 1769 Batch    7/81   train_loss = 0.779\n",
      "Epoch 1769 Batch   39/81   train_loss = 0.722\n",
      "Epoch 1769 Batch   71/81   train_loss = 0.760\n",
      "Epoch 1770 Batch   22/81   train_loss = 0.800\n",
      "Epoch 1770 Batch   54/81   train_loss = 0.753\n",
      "Epoch 1771 Batch    5/81   train_loss = 0.779\n",
      "Epoch 1771 Batch   37/81   train_loss = 0.781\n",
      "Epoch 1771 Batch   69/81   train_loss = 0.780\n",
      "Epoch 1772 Batch   20/81   train_loss = 0.769\n",
      "Epoch 1772 Batch   52/81   train_loss = 0.791\n",
      "Epoch 1773 Batch    3/81   train_loss = 0.745\n",
      "Epoch 1773 Batch   35/81   train_loss = 0.776\n",
      "Epoch 1773 Batch   67/81   train_loss = 0.794\n",
      "Epoch 1774 Batch   18/81   train_loss = 0.801\n",
      "Epoch 1774 Batch   50/81   train_loss = 0.757\n",
      "Epoch 1775 Batch    1/81   train_loss = 0.780\n",
      "Epoch 1775 Batch   33/81   train_loss = 0.759\n",
      "Epoch 1775 Batch   65/81   train_loss = 0.793\n",
      "Epoch 1776 Batch   16/81   train_loss = 0.816\n",
      "Epoch 1776 Batch   48/81   train_loss = 0.755\n",
      "Epoch 1776 Batch   80/81   train_loss = 0.793\n",
      "Epoch 1777 Batch   31/81   train_loss = 0.741\n",
      "Epoch 1777 Batch   63/81   train_loss = 0.769\n",
      "Epoch 1778 Batch   14/81   train_loss = 0.778\n",
      "Epoch 1778 Batch   46/81   train_loss = 0.783\n",
      "Epoch 1778 Batch   78/81   train_loss = 0.762\n",
      "Epoch 1779 Batch   29/81   train_loss = 0.787\n",
      "Epoch 1779 Batch   61/81   train_loss = 0.772\n",
      "Epoch 1780 Batch   12/81   train_loss = 0.783\n",
      "Epoch 1780 Batch   44/81   train_loss = 0.776\n",
      "Epoch 1780 Batch   76/81   train_loss = 0.806\n",
      "Epoch 1781 Batch   27/81   train_loss = 0.733\n",
      "Epoch 1781 Batch   59/81   train_loss = 0.764\n",
      "Epoch 1782 Batch   10/81   train_loss = 0.775\n",
      "Epoch 1782 Batch   42/81   train_loss = 0.749\n",
      "Epoch 1782 Batch   74/81   train_loss = 0.770\n",
      "Epoch 1783 Batch   25/81   train_loss = 0.787\n",
      "Epoch 1783 Batch   57/81   train_loss = 0.769\n",
      "Epoch 1784 Batch    8/81   train_loss = 0.771\n",
      "Epoch 1784 Batch   40/81   train_loss = 0.761\n",
      "Epoch 1784 Batch   72/81   train_loss = 0.758\n",
      "Epoch 1785 Batch   23/81   train_loss = 0.764\n",
      "Epoch 1785 Batch   55/81   train_loss = 0.780\n",
      "Epoch 1786 Batch    6/81   train_loss = 0.753\n",
      "Epoch 1786 Batch   38/81   train_loss = 0.780\n",
      "Epoch 1786 Batch   70/81   train_loss = 0.757\n",
      "Epoch 1787 Batch   21/81   train_loss = 0.815\n",
      "Epoch 1787 Batch   53/81   train_loss = 0.772\n",
      "Epoch 1788 Batch    4/81   train_loss = 0.783\n",
      "Epoch 1788 Batch   36/81   train_loss = 0.797\n",
      "Epoch 1788 Batch   68/81   train_loss = 0.790\n",
      "Epoch 1789 Batch   19/81   train_loss = 0.755\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1789 Batch   51/81   train_loss = 0.797\n",
      "Epoch 1790 Batch    2/81   train_loss = 0.780\n",
      "Epoch 1790 Batch   34/81   train_loss = 0.780\n",
      "Epoch 1790 Batch   66/81   train_loss = 0.785\n",
      "Epoch 1791 Batch   17/81   train_loss = 0.763\n",
      "Epoch 1791 Batch   49/81   train_loss = 0.773\n",
      "Epoch 1792 Batch    0/81   train_loss = 0.787\n",
      "Epoch 1792 Batch   32/81   train_loss = 0.758\n",
      "Epoch 1792 Batch   64/81   train_loss = 0.788\n",
      "Epoch 1793 Batch   15/81   train_loss = 0.754\n",
      "Epoch 1793 Batch   47/81   train_loss = 0.763\n",
      "Epoch 1793 Batch   79/81   train_loss = 0.780\n",
      "Epoch 1794 Batch   30/81   train_loss = 0.760\n",
      "Epoch 1794 Batch   62/81   train_loss = 0.766\n",
      "Epoch 1795 Batch   13/81   train_loss = 0.800\n",
      "Epoch 1795 Batch   45/81   train_loss = 0.761\n",
      "Epoch 1795 Batch   77/81   train_loss = 0.782\n",
      "Epoch 1796 Batch   28/81   train_loss = 0.750\n",
      "Epoch 1796 Batch   60/81   train_loss = 0.780\n",
      "Epoch 1797 Batch   11/81   train_loss = 0.805\n",
      "Epoch 1797 Batch   43/81   train_loss = 0.740\n",
      "Epoch 1797 Batch   75/81   train_loss = 0.772\n",
      "Epoch 1798 Batch   26/81   train_loss = 0.783\n",
      "Epoch 1798 Batch   58/81   train_loss = 0.779\n",
      "Epoch 1799 Batch    9/81   train_loss = 0.806\n",
      "Epoch 1799 Batch   41/81   train_loss = 0.735\n",
      "Epoch 1799 Batch   73/81   train_loss = 0.793\n",
      "Epoch 1800 Batch   24/81   train_loss = 0.758\n",
      "Epoch 1800 Batch   56/81   train_loss = 0.777\n",
      "Epoch 1801 Batch    7/81   train_loss = 0.765\n",
      "Epoch 1801 Batch   39/81   train_loss = 0.733\n",
      "Epoch 1801 Batch   71/81   train_loss = 0.754\n",
      "Epoch 1802 Batch   22/81   train_loss = 0.804\n",
      "Epoch 1802 Batch   54/81   train_loss = 0.773\n",
      "Epoch 1803 Batch    5/81   train_loss = 0.776\n",
      "Epoch 1803 Batch   37/81   train_loss = 0.771\n",
      "Epoch 1803 Batch   69/81   train_loss = 0.780\n",
      "Epoch 1804 Batch   20/81   train_loss = 0.768\n",
      "Epoch 1804 Batch   52/81   train_loss = 0.798\n",
      "Epoch 1805 Batch    3/81   train_loss = 0.760\n",
      "Epoch 1805 Batch   35/81   train_loss = 0.747\n",
      "Epoch 1805 Batch   67/81   train_loss = 0.804\n",
      "Epoch 1806 Batch   18/81   train_loss = 0.787\n",
      "Epoch 1806 Batch   50/81   train_loss = 0.771\n",
      "Epoch 1807 Batch    1/81   train_loss = 0.788\n",
      "Epoch 1807 Batch   33/81   train_loss = 0.753\n",
      "Epoch 1807 Batch   65/81   train_loss = 0.779\n",
      "Epoch 1808 Batch   16/81   train_loss = 0.814\n",
      "Epoch 1808 Batch   48/81   train_loss = 0.780\n",
      "Epoch 1808 Batch   80/81   train_loss = 0.809\n",
      "Epoch 1809 Batch   31/81   train_loss = 0.753\n",
      "Epoch 1809 Batch   63/81   train_loss = 0.802\n",
      "Epoch 1810 Batch   14/81   train_loss = 0.802\n",
      "Epoch 1810 Batch   46/81   train_loss = 0.777\n",
      "Epoch 1810 Batch   78/81   train_loss = 0.802\n",
      "Epoch 1811 Batch   29/81   train_loss = 0.786\n",
      "Epoch 1811 Batch   61/81   train_loss = 0.775\n",
      "Epoch 1812 Batch   12/81   train_loss = 0.815\n",
      "Epoch 1812 Batch   44/81   train_loss = 0.789\n",
      "Epoch 1812 Batch   76/81   train_loss = 0.788\n",
      "Epoch 1813 Batch   27/81   train_loss = 0.757\n",
      "Epoch 1813 Batch   59/81   train_loss = 0.768\n",
      "Epoch 1814 Batch   10/81   train_loss = 0.795\n",
      "Epoch 1814 Batch   42/81   train_loss = 0.767\n",
      "Epoch 1814 Batch   74/81   train_loss = 0.799\n",
      "Epoch 1815 Batch   25/81   train_loss = 0.776\n",
      "Epoch 1815 Batch   57/81   train_loss = 0.777\n",
      "Epoch 1816 Batch    8/81   train_loss = 0.792\n",
      "Epoch 1816 Batch   40/81   train_loss = 0.760\n",
      "Epoch 1816 Batch   72/81   train_loss = 0.756\n",
      "Epoch 1817 Batch   23/81   train_loss = 0.773\n",
      "Epoch 1817 Batch   55/81   train_loss = 0.795\n",
      "Epoch 1818 Batch    6/81   train_loss = 0.801\n",
      "Epoch 1818 Batch   38/81   train_loss = 0.783\n",
      "Epoch 1818 Batch   70/81   train_loss = 0.778\n",
      "Epoch 1819 Batch   21/81   train_loss = 0.840\n",
      "Epoch 1819 Batch   53/81   train_loss = 0.776\n",
      "Epoch 1820 Batch    4/81   train_loss = 0.780\n",
      "Epoch 1820 Batch   36/81   train_loss = 0.799\n",
      "Epoch 1820 Batch   68/81   train_loss = 0.795\n",
      "Epoch 1821 Batch   19/81   train_loss = 0.763\n",
      "Epoch 1821 Batch   51/81   train_loss = 0.816\n",
      "Epoch 1822 Batch    2/81   train_loss = 0.797\n",
      "Epoch 1822 Batch   34/81   train_loss = 0.786\n",
      "Epoch 1822 Batch   66/81   train_loss = 0.783\n",
      "Epoch 1823 Batch   17/81   train_loss = 0.785\n",
      "Epoch 1823 Batch   49/81   train_loss = 0.779\n",
      "Epoch 1824 Batch    0/81   train_loss = 0.780\n",
      "Epoch 1824 Batch   32/81   train_loss = 0.768\n",
      "Epoch 1824 Batch   64/81   train_loss = 0.789\n",
      "Epoch 1825 Batch   15/81   train_loss = 0.784\n",
      "Epoch 1825 Batch   47/81   train_loss = 0.762\n",
      "Epoch 1825 Batch   79/81   train_loss = 0.789\n",
      "Epoch 1826 Batch   30/81   train_loss = 0.788\n",
      "Epoch 1826 Batch   62/81   train_loss = 0.760\n",
      "Epoch 1827 Batch   13/81   train_loss = 0.820\n",
      "Epoch 1827 Batch   45/81   train_loss = 0.788\n",
      "Epoch 1827 Batch   77/81   train_loss = 0.800\n",
      "Epoch 1828 Batch   28/81   train_loss = 0.770\n",
      "Epoch 1828 Batch   60/81   train_loss = 0.802\n",
      "Epoch 1829 Batch   11/81   train_loss = 0.785\n",
      "Epoch 1829 Batch   43/81   train_loss = 0.743\n",
      "Epoch 1829 Batch   75/81   train_loss = 0.751\n",
      "Epoch 1830 Batch   26/81   train_loss = 0.771\n",
      "Epoch 1830 Batch   58/81   train_loss = 0.754\n",
      "Epoch 1831 Batch    9/81   train_loss = 0.817\n",
      "Epoch 1831 Batch   41/81   train_loss = 0.735\n",
      "Epoch 1831 Batch   73/81   train_loss = 0.770\n",
      "Epoch 1832 Batch   24/81   train_loss = 0.763\n",
      "Epoch 1832 Batch   56/81   train_loss = 0.788\n",
      "Epoch 1833 Batch    7/81   train_loss = 0.775\n",
      "Epoch 1833 Batch   39/81   train_loss = 0.751\n",
      "Epoch 1833 Batch   71/81   train_loss = 0.742\n",
      "Epoch 1834 Batch   22/81   train_loss = 0.802\n",
      "Epoch 1834 Batch   54/81   train_loss = 0.755\n",
      "Epoch 1835 Batch    5/81   train_loss = 0.789\n",
      "Epoch 1835 Batch   37/81   train_loss = 0.776\n",
      "Epoch 1835 Batch   69/81   train_loss = 0.777\n",
      "Epoch 1836 Batch   20/81   train_loss = 0.768\n",
      "Epoch 1836 Batch   52/81   train_loss = 0.783\n",
      "Epoch 1837 Batch    3/81   train_loss = 0.756\n",
      "Epoch 1837 Batch   35/81   train_loss = 0.757\n",
      "Epoch 1837 Batch   67/81   train_loss = 0.790\n",
      "Epoch 1838 Batch   18/81   train_loss = 0.793\n",
      "Epoch 1838 Batch   50/81   train_loss = 0.769\n",
      "Epoch 1839 Batch    1/81   train_loss = 0.789\n",
      "Epoch 1839 Batch   33/81   train_loss = 0.763\n",
      "Epoch 1839 Batch   65/81   train_loss = 0.781\n",
      "Epoch 1840 Batch   16/81   train_loss = 0.808\n",
      "Epoch 1840 Batch   48/81   train_loss = 0.759\n",
      "Epoch 1840 Batch   80/81   train_loss = 0.777\n",
      "Epoch 1841 Batch   31/81   train_loss = 0.756\n",
      "Epoch 1841 Batch   63/81   train_loss = 0.768\n",
      "Epoch 1842 Batch   14/81   train_loss = 0.783\n",
      "Epoch 1842 Batch   46/81   train_loss = 0.769\n",
      "Epoch 1842 Batch   78/81   train_loss = 0.787\n",
      "Epoch 1843 Batch   29/81   train_loss = 0.788\n",
      "Epoch 1843 Batch   61/81   train_loss = 0.750\n",
      "Epoch 1844 Batch   12/81   train_loss = 0.780\n",
      "Epoch 1844 Batch   44/81   train_loss = 0.759\n",
      "Epoch 1844 Batch   76/81   train_loss = 0.802\n",
      "Epoch 1845 Batch   27/81   train_loss = 0.757\n",
      "Epoch 1845 Batch   59/81   train_loss = 0.749\n",
      "Epoch 1846 Batch   10/81   train_loss = 0.769\n",
      "Epoch 1846 Batch   42/81   train_loss = 0.773\n",
      "Epoch 1846 Batch   74/81   train_loss = 0.765\n",
      "Epoch 1847 Batch   25/81   train_loss = 0.771\n",
      "Epoch 1847 Batch   57/81   train_loss = 0.766\n",
      "Epoch 1848 Batch    8/81   train_loss = 0.778\n",
      "Epoch 1848 Batch   40/81   train_loss = 0.753\n",
      "Epoch 1848 Batch   72/81   train_loss = 0.769\n",
      "Epoch 1849 Batch   23/81   train_loss = 0.767\n",
      "Epoch 1849 Batch   55/81   train_loss = 0.769\n",
      "Epoch 1850 Batch    6/81   train_loss = 0.740\n",
      "Epoch 1850 Batch   38/81   train_loss = 0.775\n",
      "Epoch 1850 Batch   70/81   train_loss = 0.758\n",
      "Epoch 1851 Batch   21/81   train_loss = 0.807\n",
      "Epoch 1851 Batch   53/81   train_loss = 0.757\n",
      "Epoch 1852 Batch    4/81   train_loss = 0.775\n",
      "Epoch 1852 Batch   36/81   train_loss = 0.798\n",
      "Epoch 1852 Batch   68/81   train_loss = 0.815\n",
      "Epoch 1853 Batch   19/81   train_loss = 0.741\n",
      "Epoch 1853 Batch   51/81   train_loss = 0.814\n",
      "Epoch 1854 Batch    2/81   train_loss = 0.769\n",
      "Epoch 1854 Batch   34/81   train_loss = 0.775\n",
      "Epoch 1854 Batch   66/81   train_loss = 0.765\n",
      "Epoch 1855 Batch   17/81   train_loss = 0.786\n",
      "Epoch 1855 Batch   49/81   train_loss = 0.771\n",
      "Epoch 1856 Batch    0/81   train_loss = 0.769\n",
      "Epoch 1856 Batch   32/81   train_loss = 0.745\n",
      "Epoch 1856 Batch   64/81   train_loss = 0.767\n",
      "Epoch 1857 Batch   15/81   train_loss = 0.755\n",
      "Epoch 1857 Batch   47/81   train_loss = 0.757\n",
      "Epoch 1857 Batch   79/81   train_loss = 0.785\n",
      "Epoch 1858 Batch   30/81   train_loss = 0.754\n",
      "Epoch 1858 Batch   62/81   train_loss = 0.761\n",
      "Epoch 1859 Batch   13/81   train_loss = 0.779\n",
      "Epoch 1859 Batch   45/81   train_loss = 0.761\n",
      "Epoch 1859 Batch   77/81   train_loss = 0.795\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1860 Batch   28/81   train_loss = 0.761\n",
      "Epoch 1860 Batch   60/81   train_loss = 0.772\n",
      "Epoch 1861 Batch   11/81   train_loss = 0.784\n",
      "Epoch 1861 Batch   43/81   train_loss = 0.766\n",
      "Epoch 1861 Batch   75/81   train_loss = 0.756\n",
      "Epoch 1862 Batch   26/81   train_loss = 0.792\n",
      "Epoch 1862 Batch   58/81   train_loss = 0.746\n",
      "Epoch 1863 Batch    9/81   train_loss = 0.814\n",
      "Epoch 1863 Batch   41/81   train_loss = 0.716\n",
      "Epoch 1863 Batch   73/81   train_loss = 0.770\n",
      "Epoch 1864 Batch   24/81   train_loss = 0.785\n",
      "Epoch 1864 Batch   56/81   train_loss = 0.770\n",
      "Epoch 1865 Batch    7/81   train_loss = 0.776\n",
      "Epoch 1865 Batch   39/81   train_loss = 0.756\n",
      "Epoch 1865 Batch   71/81   train_loss = 0.742\n",
      "Epoch 1866 Batch   22/81   train_loss = 0.800\n",
      "Epoch 1866 Batch   54/81   train_loss = 0.766\n",
      "Epoch 1867 Batch    5/81   train_loss = 0.765\n",
      "Epoch 1867 Batch   37/81   train_loss = 0.768\n",
      "Epoch 1867 Batch   69/81   train_loss = 0.772\n",
      "Epoch 1868 Batch   20/81   train_loss = 0.758\n",
      "Epoch 1868 Batch   52/81   train_loss = 0.791\n",
      "Epoch 1869 Batch    3/81   train_loss = 0.770\n",
      "Epoch 1869 Batch   35/81   train_loss = 0.767\n",
      "Epoch 1869 Batch   67/81   train_loss = 0.817\n",
      "Epoch 1870 Batch   18/81   train_loss = 0.816\n",
      "Epoch 1870 Batch   50/81   train_loss = 0.756\n",
      "Epoch 1871 Batch    1/81   train_loss = 0.794\n",
      "Epoch 1871 Batch   33/81   train_loss = 0.764\n",
      "Epoch 1871 Batch   65/81   train_loss = 0.805\n",
      "Epoch 1872 Batch   16/81   train_loss = 0.828\n",
      "Epoch 1872 Batch   48/81   train_loss = 0.778\n",
      "Epoch 1872 Batch   80/81   train_loss = 0.806\n",
      "Epoch 1873 Batch   31/81   train_loss = 0.762\n",
      "Epoch 1873 Batch   63/81   train_loss = 0.789\n",
      "Epoch 1874 Batch   14/81   train_loss = 0.790\n",
      "Epoch 1874 Batch   46/81   train_loss = 0.771\n",
      "Epoch 1874 Batch   78/81   train_loss = 0.767\n",
      "Epoch 1875 Batch   29/81   train_loss = 0.782\n",
      "Epoch 1875 Batch   61/81   train_loss = 0.776\n",
      "Epoch 1876 Batch   12/81   train_loss = 0.780\n",
      "Epoch 1876 Batch   44/81   train_loss = 0.762\n",
      "Epoch 1876 Batch   76/81   train_loss = 0.792\n",
      "Epoch 1877 Batch   27/81   train_loss = 0.764\n",
      "Epoch 1877 Batch   59/81   train_loss = 0.768\n",
      "Epoch 1878 Batch   10/81   train_loss = 0.786\n",
      "Epoch 1878 Batch   42/81   train_loss = 0.762\n",
      "Epoch 1878 Batch   74/81   train_loss = 0.780\n",
      "Epoch 1879 Batch   25/81   train_loss = 0.774\n",
      "Epoch 1879 Batch   57/81   train_loss = 0.774\n",
      "Epoch 1880 Batch    8/81   train_loss = 0.776\n",
      "Epoch 1880 Batch   40/81   train_loss = 0.761\n",
      "Epoch 1880 Batch   72/81   train_loss = 0.762\n",
      "Epoch 1881 Batch   23/81   train_loss = 0.764\n",
      "Epoch 1881 Batch   55/81   train_loss = 0.761\n",
      "Epoch 1882 Batch    6/81   train_loss = 0.740\n",
      "Epoch 1882 Batch   38/81   train_loss = 0.771\n",
      "Epoch 1882 Batch   70/81   train_loss = 0.763\n",
      "Epoch 1883 Batch   21/81   train_loss = 0.802\n",
      "Epoch 1883 Batch   53/81   train_loss = 0.763\n",
      "Epoch 1884 Batch    4/81   train_loss = 0.767\n",
      "Epoch 1884 Batch   36/81   train_loss = 0.775\n",
      "Epoch 1884 Batch   68/81   train_loss = 0.801\n",
      "Epoch 1885 Batch   19/81   train_loss = 0.754\n",
      "Epoch 1885 Batch   51/81   train_loss = 0.799\n",
      "Epoch 1886 Batch    2/81   train_loss = 0.787\n",
      "Epoch 1886 Batch   34/81   train_loss = 0.788\n",
      "Epoch 1886 Batch   66/81   train_loss = 0.769\n",
      "Epoch 1887 Batch   17/81   train_loss = 0.775\n",
      "Epoch 1887 Batch   49/81   train_loss = 0.762\n",
      "Epoch 1888 Batch    0/81   train_loss = 0.793\n",
      "Epoch 1888 Batch   32/81   train_loss = 0.748\n",
      "Epoch 1888 Batch   64/81   train_loss = 0.766\n",
      "Epoch 1889 Batch   15/81   train_loss = 0.772\n",
      "Epoch 1889 Batch   47/81   train_loss = 0.775\n",
      "Epoch 1889 Batch   79/81   train_loss = 0.777\n",
      "Epoch 1890 Batch   30/81   train_loss = 0.775\n",
      "Epoch 1890 Batch   62/81   train_loss = 0.787\n",
      "Epoch 1891 Batch   13/81   train_loss = 0.793\n",
      "Epoch 1891 Batch   45/81   train_loss = 0.757\n",
      "Epoch 1891 Batch   77/81   train_loss = 0.789\n",
      "Epoch 1892 Batch   28/81   train_loss = 0.738\n",
      "Epoch 1892 Batch   60/81   train_loss = 0.783\n",
      "Epoch 1893 Batch   11/81   train_loss = 0.783\n",
      "Epoch 1893 Batch   43/81   train_loss = 0.753\n",
      "Epoch 1893 Batch   75/81   train_loss = 0.757\n",
      "Epoch 1894 Batch   26/81   train_loss = 0.753\n",
      "Epoch 1894 Batch   58/81   train_loss = 0.724\n",
      "Epoch 1895 Batch    9/81   train_loss = 0.813\n",
      "Epoch 1895 Batch   41/81   train_loss = 0.810\n",
      "Epoch 1895 Batch   73/81   train_loss = 0.783\n",
      "Epoch 1896 Batch   24/81   train_loss = 0.789\n",
      "Epoch 1896 Batch   56/81   train_loss = 0.782\n",
      "Epoch 1897 Batch    7/81   train_loss = 0.777\n",
      "Epoch 1897 Batch   39/81   train_loss = 0.736\n",
      "Epoch 1897 Batch   71/81   train_loss = 0.753\n",
      "Epoch 1898 Batch   22/81   train_loss = 0.783\n",
      "Epoch 1898 Batch   54/81   train_loss = 0.771\n",
      "Epoch 1899 Batch    5/81   train_loss = 0.783\n",
      "Epoch 1899 Batch   37/81   train_loss = 0.777\n",
      "Epoch 1899 Batch   69/81   train_loss = 0.789\n",
      "Epoch 1900 Batch   20/81   train_loss = 0.768\n",
      "Epoch 1900 Batch   52/81   train_loss = 0.799\n",
      "Epoch 1901 Batch    3/81   train_loss = 0.764\n",
      "Epoch 1901 Batch   35/81   train_loss = 0.757\n",
      "Epoch 1901 Batch   67/81   train_loss = 0.796\n",
      "Epoch 1902 Batch   18/81   train_loss = 0.783\n",
      "Epoch 1902 Batch   50/81   train_loss = 0.754\n",
      "Epoch 1903 Batch    1/81   train_loss = 0.778\n",
      "Epoch 1903 Batch   33/81   train_loss = 0.740\n",
      "Epoch 1903 Batch   65/81   train_loss = 0.794\n",
      "Epoch 1904 Batch   16/81   train_loss = 0.828\n",
      "Epoch 1904 Batch   48/81   train_loss = 0.769\n",
      "Epoch 1904 Batch   80/81   train_loss = 0.791\n",
      "Epoch 1905 Batch   31/81   train_loss = 0.752\n",
      "Epoch 1905 Batch   63/81   train_loss = 0.793\n",
      "Epoch 1906 Batch   14/81   train_loss = 0.781\n",
      "Epoch 1906 Batch   46/81   train_loss = 0.767\n",
      "Epoch 1906 Batch   78/81   train_loss = 0.792\n",
      "Epoch 1907 Batch   29/81   train_loss = 0.783\n",
      "Epoch 1907 Batch   61/81   train_loss = 0.775\n",
      "Epoch 1908 Batch   12/81   train_loss = 0.794\n",
      "Epoch 1908 Batch   44/81   train_loss = 0.769\n",
      "Epoch 1908 Batch   76/81   train_loss = 0.796\n",
      "Epoch 1909 Batch   27/81   train_loss = 0.761\n",
      "Epoch 1909 Batch   59/81   train_loss = 0.757\n",
      "Epoch 1910 Batch   10/81   train_loss = 0.804\n",
      "Epoch 1910 Batch   42/81   train_loss = 0.773\n",
      "Epoch 1910 Batch   74/81   train_loss = 0.785\n",
      "Epoch 1911 Batch   25/81   train_loss = 0.789\n",
      "Epoch 1911 Batch   57/81   train_loss = 0.778\n",
      "Epoch 1912 Batch    8/81   train_loss = 0.780\n",
      "Epoch 1912 Batch   40/81   train_loss = 0.756\n",
      "Epoch 1912 Batch   72/81   train_loss = 0.769\n",
      "Epoch 1913 Batch   23/81   train_loss = 0.768\n",
      "Epoch 1913 Batch   55/81   train_loss = 0.776\n",
      "Epoch 1914 Batch    6/81   train_loss = 0.771\n",
      "Epoch 1914 Batch   38/81   train_loss = 0.811\n",
      "Epoch 1914 Batch   70/81   train_loss = 0.797\n",
      "Epoch 1915 Batch   21/81   train_loss = 0.811\n",
      "Epoch 1915 Batch   53/81   train_loss = 0.762\n",
      "Epoch 1916 Batch    4/81   train_loss = 0.802\n",
      "Epoch 1916 Batch   36/81   train_loss = 0.777\n",
      "Epoch 1916 Batch   68/81   train_loss = 0.811\n",
      "Epoch 1917 Batch   19/81   train_loss = 0.751\n",
      "Epoch 1917 Batch   51/81   train_loss = 0.804\n",
      "Epoch 1918 Batch    2/81   train_loss = 0.781\n",
      "Epoch 1918 Batch   34/81   train_loss = 0.791\n",
      "Epoch 1918 Batch   66/81   train_loss = 0.802\n",
      "Epoch 1919 Batch   17/81   train_loss = 0.778\n",
      "Epoch 1919 Batch   49/81   train_loss = 0.764\n",
      "Epoch 1920 Batch    0/81   train_loss = 0.785\n",
      "Epoch 1920 Batch   32/81   train_loss = 0.753\n",
      "Epoch 1920 Batch   64/81   train_loss = 0.761\n",
      "Epoch 1921 Batch   15/81   train_loss = 0.762\n",
      "Epoch 1921 Batch   47/81   train_loss = 0.772\n",
      "Epoch 1921 Batch   79/81   train_loss = 0.761\n",
      "Epoch 1922 Batch   30/81   train_loss = 0.755\n",
      "Epoch 1922 Batch   62/81   train_loss = 0.759\n",
      "Epoch 1923 Batch   13/81   train_loss = 0.793\n",
      "Epoch 1923 Batch   45/81   train_loss = 0.755\n",
      "Epoch 1923 Batch   77/81   train_loss = 0.787\n",
      "Epoch 1924 Batch   28/81   train_loss = 0.768\n",
      "Epoch 1924 Batch   60/81   train_loss = 0.789\n",
      "Epoch 1925 Batch   11/81   train_loss = 0.758\n",
      "Epoch 1925 Batch   43/81   train_loss = 0.750\n",
      "Epoch 1925 Batch   75/81   train_loss = 0.743\n",
      "Epoch 1926 Batch   26/81   train_loss = 0.765\n",
      "Epoch 1926 Batch   58/81   train_loss = 0.741\n",
      "Epoch 1927 Batch    9/81   train_loss = 0.811\n",
      "Epoch 1927 Batch   41/81   train_loss = 0.739\n",
      "Epoch 1927 Batch   73/81   train_loss = 0.756\n",
      "Epoch 1928 Batch   24/81   train_loss = 0.755\n",
      "Epoch 1928 Batch   56/81   train_loss = 0.757\n",
      "Epoch 1929 Batch    7/81   train_loss = 0.766\n",
      "Epoch 1929 Batch   39/81   train_loss = 0.732\n",
      "Epoch 1929 Batch   71/81   train_loss = 0.733\n",
      "Epoch 1930 Batch   22/81   train_loss = 0.780\n",
      "Epoch 1930 Batch   54/81   train_loss = 0.757\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1931 Batch    5/81   train_loss = 0.784\n",
      "Epoch 1931 Batch   37/81   train_loss = 0.755\n",
      "Epoch 1931 Batch   69/81   train_loss = 0.769\n",
      "Epoch 1932 Batch   20/81   train_loss = 0.763\n",
      "Epoch 1932 Batch   52/81   train_loss = 0.792\n",
      "Epoch 1933 Batch    3/81   train_loss = 0.758\n",
      "Epoch 1933 Batch   35/81   train_loss = 0.751\n",
      "Epoch 1933 Batch   67/81   train_loss = 0.793\n",
      "Epoch 1934 Batch   18/81   train_loss = 0.780\n",
      "Epoch 1934 Batch   50/81   train_loss = 0.767\n",
      "Epoch 1935 Batch    1/81   train_loss = 0.782\n",
      "Epoch 1935 Batch   33/81   train_loss = 0.780\n",
      "Epoch 1935 Batch   65/81   train_loss = 0.802\n",
      "Epoch 1936 Batch   16/81   train_loss = 0.820\n",
      "Epoch 1936 Batch   48/81   train_loss = 0.776\n",
      "Epoch 1936 Batch   80/81   train_loss = 0.799\n",
      "Epoch 1937 Batch   31/81   train_loss = 0.779\n",
      "Epoch 1937 Batch   63/81   train_loss = 0.799\n",
      "Epoch 1938 Batch   14/81   train_loss = 0.784\n",
      "Epoch 1938 Batch   46/81   train_loss = 0.760\n",
      "Epoch 1938 Batch   78/81   train_loss = 0.771\n",
      "Epoch 1939 Batch   29/81   train_loss = 0.769\n",
      "Epoch 1939 Batch   61/81   train_loss = 0.774\n",
      "Epoch 1940 Batch   12/81   train_loss = 0.794\n",
      "Epoch 1940 Batch   44/81   train_loss = 0.778\n",
      "Epoch 1940 Batch   76/81   train_loss = 0.798\n",
      "Epoch 1941 Batch   27/81   train_loss = 0.749\n",
      "Epoch 1941 Batch   59/81   train_loss = 0.756\n",
      "Epoch 1942 Batch   10/81   train_loss = 0.790\n",
      "Epoch 1942 Batch   42/81   train_loss = 0.780\n",
      "Epoch 1942 Batch   74/81   train_loss = 0.771\n",
      "Epoch 1943 Batch   25/81   train_loss = 0.767\n",
      "Epoch 1943 Batch   57/81   train_loss = 0.776\n",
      "Epoch 1944 Batch    8/81   train_loss = 0.790\n",
      "Epoch 1944 Batch   40/81   train_loss = 0.767\n",
      "Epoch 1944 Batch   72/81   train_loss = 0.758\n",
      "Epoch 1945 Batch   23/81   train_loss = 0.777\n",
      "Epoch 1945 Batch   55/81   train_loss = 0.797\n",
      "Epoch 1946 Batch    6/81   train_loss = 0.765\n",
      "Epoch 1946 Batch   38/81   train_loss = 0.774\n",
      "Epoch 1946 Batch   70/81   train_loss = 0.790\n",
      "Epoch 1947 Batch   21/81   train_loss = 0.815\n",
      "Epoch 1947 Batch   53/81   train_loss = 0.766\n",
      "Epoch 1948 Batch    4/81   train_loss = 0.777\n",
      "Epoch 1948 Batch   36/81   train_loss = 0.781\n",
      "Epoch 1948 Batch   68/81   train_loss = 0.794\n",
      "Epoch 1949 Batch   19/81   train_loss = 0.759\n",
      "Epoch 1949 Batch   51/81   train_loss = 0.796\n",
      "Epoch 1950 Batch    2/81   train_loss = 0.783\n",
      "Epoch 1950 Batch   34/81   train_loss = 0.785\n",
      "Epoch 1950 Batch   66/81   train_loss = 0.784\n",
      "Epoch 1951 Batch   17/81   train_loss = 0.788\n",
      "Epoch 1951 Batch   49/81   train_loss = 0.775\n",
      "Epoch 1952 Batch    0/81   train_loss = 0.758\n",
      "Epoch 1952 Batch   32/81   train_loss = 0.748\n",
      "Epoch 1952 Batch   64/81   train_loss = 0.762\n",
      "Epoch 1953 Batch   15/81   train_loss = 0.764\n",
      "Epoch 1953 Batch   47/81   train_loss = 0.769\n",
      "Epoch 1953 Batch   79/81   train_loss = 0.763\n",
      "Epoch 1954 Batch   30/81   train_loss = 0.750\n",
      "Epoch 1954 Batch   62/81   train_loss = 0.761\n",
      "Epoch 1955 Batch   13/81   train_loss = 0.792\n",
      "Epoch 1955 Batch   45/81   train_loss = 0.754\n",
      "Epoch 1955 Batch   77/81   train_loss = 0.773\n",
      "Epoch 1956 Batch   28/81   train_loss = 0.749\n",
      "Epoch 1956 Batch   60/81   train_loss = 0.775\n",
      "Epoch 1957 Batch   11/81   train_loss = 0.769\n",
      "Epoch 1957 Batch   43/81   train_loss = 0.755\n",
      "Epoch 1957 Batch   75/81   train_loss = 0.771\n",
      "Epoch 1958 Batch   26/81   train_loss = 0.758\n",
      "Epoch 1958 Batch   58/81   train_loss = 0.732\n",
      "Epoch 1959 Batch    9/81   train_loss = 0.779\n",
      "Epoch 1959 Batch   41/81   train_loss = 0.714\n",
      "Epoch 1959 Batch   73/81   train_loss = 0.776\n",
      "Epoch 1960 Batch   24/81   train_loss = 0.763\n",
      "Epoch 1960 Batch   56/81   train_loss = 0.765\n",
      "Epoch 1961 Batch    7/81   train_loss = 0.766\n",
      "Epoch 1961 Batch   39/81   train_loss = 0.727\n",
      "Epoch 1961 Batch   71/81   train_loss = 0.751\n",
      "Epoch 1962 Batch   22/81   train_loss = 0.781\n",
      "Epoch 1962 Batch   54/81   train_loss = 0.766\n",
      "Epoch 1963 Batch    5/81   train_loss = 0.787\n",
      "Epoch 1963 Batch   37/81   train_loss = 0.773\n",
      "Epoch 1963 Batch   69/81   train_loss = 0.776\n",
      "Epoch 1964 Batch   20/81   train_loss = 0.769\n",
      "Epoch 1964 Batch   52/81   train_loss = 0.783\n",
      "Epoch 1965 Batch    3/81   train_loss = 0.756\n",
      "Epoch 1965 Batch   35/81   train_loss = 0.751\n",
      "Epoch 1965 Batch   67/81   train_loss = 0.792\n",
      "Epoch 1966 Batch   18/81   train_loss = 0.771\n",
      "Epoch 1966 Batch   50/81   train_loss = 0.747\n",
      "Epoch 1967 Batch    1/81   train_loss = 0.773\n",
      "Epoch 1967 Batch   33/81   train_loss = 0.747\n",
      "Epoch 1967 Batch   65/81   train_loss = 0.802\n",
      "Epoch 1968 Batch   16/81   train_loss = 0.810\n",
      "Epoch 1968 Batch   48/81   train_loss = 0.772\n",
      "Epoch 1968 Batch   80/81   train_loss = 0.776\n",
      "Epoch 1969 Batch   31/81   train_loss = 0.766\n",
      "Epoch 1969 Batch   63/81   train_loss = 0.777\n",
      "Epoch 1970 Batch   14/81   train_loss = 0.779\n",
      "Epoch 1970 Batch   46/81   train_loss = 0.759\n",
      "Epoch 1970 Batch   78/81   train_loss = 0.751\n",
      "Epoch 1971 Batch   29/81   train_loss = 0.759\n",
      "Epoch 1971 Batch   61/81   train_loss = 0.748\n",
      "Epoch 1972 Batch   12/81   train_loss = 0.763\n",
      "Epoch 1972 Batch   44/81   train_loss = 0.763\n",
      "Epoch 1972 Batch   76/81   train_loss = 0.785\n",
      "Epoch 1973 Batch   27/81   train_loss = 0.733\n",
      "Epoch 1973 Batch   59/81   train_loss = 0.772\n",
      "Epoch 1974 Batch   10/81   train_loss = 0.788\n",
      "Epoch 1974 Batch   42/81   train_loss = 0.751\n",
      "Epoch 1974 Batch   74/81   train_loss = 0.757\n",
      "Epoch 1975 Batch   25/81   train_loss = 0.772\n",
      "Epoch 1975 Batch   57/81   train_loss = 0.772\n",
      "Epoch 1976 Batch    8/81   train_loss = 0.775\n",
      "Epoch 1976 Batch   40/81   train_loss = 0.746\n",
      "Epoch 1976 Batch   72/81   train_loss = 0.748\n",
      "Epoch 1977 Batch   23/81   train_loss = 0.758\n",
      "Epoch 1977 Batch   55/81   train_loss = 0.765\n",
      "Epoch 1978 Batch    6/81   train_loss = 0.764\n",
      "Epoch 1978 Batch   38/81   train_loss = 0.777\n",
      "Epoch 1978 Batch   70/81   train_loss = 0.759\n",
      "Epoch 1979 Batch   21/81   train_loss = 0.808\n",
      "Epoch 1979 Batch   53/81   train_loss = 0.755\n",
      "Epoch 1980 Batch    4/81   train_loss = 0.783\n",
      "Epoch 1980 Batch   36/81   train_loss = 0.788\n",
      "Epoch 1980 Batch   68/81   train_loss = 0.787\n",
      "Epoch 1981 Batch   19/81   train_loss = 0.755\n",
      "Epoch 1981 Batch   51/81   train_loss = 0.813\n",
      "Epoch 1982 Batch    2/81   train_loss = 0.791\n",
      "Epoch 1982 Batch   34/81   train_loss = 0.783\n",
      "Epoch 1982 Batch   66/81   train_loss = 0.771\n",
      "Epoch 1983 Batch   17/81   train_loss = 0.770\n",
      "Epoch 1983 Batch   49/81   train_loss = 0.750\n",
      "Epoch 1984 Batch    0/81   train_loss = 0.758\n",
      "Epoch 1984 Batch   32/81   train_loss = 0.769\n",
      "Epoch 1984 Batch   64/81   train_loss = 0.763\n",
      "Epoch 1985 Batch   15/81   train_loss = 0.776\n",
      "Epoch 1985 Batch   47/81   train_loss = 0.780\n",
      "Epoch 1985 Batch   79/81   train_loss = 0.780\n",
      "Epoch 1986 Batch   30/81   train_loss = 0.763\n",
      "Epoch 1986 Batch   62/81   train_loss = 0.759\n",
      "Epoch 1987 Batch   13/81   train_loss = 0.810\n",
      "Epoch 1987 Batch   45/81   train_loss = 0.770\n",
      "Epoch 1987 Batch   77/81   train_loss = 0.778\n",
      "Epoch 1988 Batch   28/81   train_loss = 0.753\n",
      "Epoch 1988 Batch   60/81   train_loss = 0.784\n",
      "Epoch 1989 Batch   11/81   train_loss = 0.785\n",
      "Epoch 1989 Batch   43/81   train_loss = 0.759\n",
      "Epoch 1989 Batch   75/81   train_loss = 0.761\n",
      "Epoch 1990 Batch   26/81   train_loss = 0.770\n",
      "Epoch 1990 Batch   58/81   train_loss = 0.742\n",
      "Epoch 1991 Batch    9/81   train_loss = 0.798\n",
      "Epoch 1991 Batch   41/81   train_loss = 0.743\n",
      "Epoch 1991 Batch   73/81   train_loss = 0.784\n",
      "Epoch 1992 Batch   24/81   train_loss = 0.764\n",
      "Epoch 1992 Batch   56/81   train_loss = 0.780\n",
      "Epoch 1993 Batch    7/81   train_loss = 0.773\n",
      "Epoch 1993 Batch   39/81   train_loss = 0.731\n",
      "Epoch 1993 Batch   71/81   train_loss = 0.736\n",
      "Epoch 1994 Batch   22/81   train_loss = 0.784\n",
      "Epoch 1994 Batch   54/81   train_loss = 0.754\n",
      "Epoch 1995 Batch    5/81   train_loss = 0.779\n",
      "Epoch 1995 Batch   37/81   train_loss = 0.765\n",
      "Epoch 1995 Batch   69/81   train_loss = 0.754\n",
      "Epoch 1996 Batch   20/81   train_loss = 0.757\n",
      "Epoch 1996 Batch   52/81   train_loss = 0.796\n",
      "Epoch 1997 Batch    3/81   train_loss = 0.747\n",
      "Epoch 1997 Batch   35/81   train_loss = 0.761\n",
      "Epoch 1997 Batch   67/81   train_loss = 0.779\n",
      "Epoch 1998 Batch   18/81   train_loss = 0.774\n",
      "Epoch 1998 Batch   50/81   train_loss = 0.765\n",
      "Epoch 1999 Batch    1/81   train_loss = 0.791\n",
      "Epoch 1999 Batch   33/81   train_loss = 0.748\n",
      "Epoch 1999 Batch   65/81   train_loss = 0.773\n",
      "Epoch 2000 Batch   16/81   train_loss = 0.809\n",
      "Epoch 2000 Batch   48/81   train_loss = 0.776\n",
      "Epoch 2000 Batch   80/81   train_loss = 0.813\n",
      "Epoch 2001 Batch   31/81   train_loss = 0.747\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2001 Batch   63/81   train_loss = 0.773\n",
      "Epoch 2002 Batch   14/81   train_loss = 0.782\n",
      "Epoch 2002 Batch   46/81   train_loss = 0.761\n",
      "Epoch 2002 Batch   78/81   train_loss = 0.779\n",
      "Epoch 2003 Batch   29/81   train_loss = 0.770\n",
      "Epoch 2003 Batch   61/81   train_loss = 0.763\n",
      "Epoch 2004 Batch   12/81   train_loss = 0.777\n",
      "Epoch 2004 Batch   44/81   train_loss = 0.767\n",
      "Epoch 2004 Batch   76/81   train_loss = 0.814\n",
      "Epoch 2005 Batch   27/81   train_loss = 0.756\n",
      "Epoch 2005 Batch   59/81   train_loss = 0.744\n",
      "Epoch 2006 Batch   10/81   train_loss = 0.773\n",
      "Epoch 2006 Batch   42/81   train_loss = 0.772\n",
      "Epoch 2006 Batch   74/81   train_loss = 0.783\n",
      "Epoch 2007 Batch   25/81   train_loss = 0.787\n",
      "Epoch 2007 Batch   57/81   train_loss = 0.776\n",
      "Epoch 2008 Batch    8/81   train_loss = 0.769\n",
      "Epoch 2008 Batch   40/81   train_loss = 0.746\n",
      "Epoch 2008 Batch   72/81   train_loss = 0.746\n",
      "Epoch 2009 Batch   23/81   train_loss = 0.753\n",
      "Epoch 2009 Batch   55/81   train_loss = 0.782\n",
      "Epoch 2010 Batch    6/81   train_loss = 0.751\n",
      "Epoch 2010 Batch   38/81   train_loss = 0.757\n",
      "Epoch 2010 Batch   70/81   train_loss = 0.755\n",
      "Epoch 2011 Batch   21/81   train_loss = 0.810\n",
      "Epoch 2011 Batch   53/81   train_loss = 0.786\n",
      "Epoch 2012 Batch    4/81   train_loss = 0.782\n",
      "Epoch 2012 Batch   36/81   train_loss = 0.794\n",
      "Epoch 2012 Batch   68/81   train_loss = 0.790\n",
      "Epoch 2013 Batch   19/81   train_loss = 0.772\n",
      "Epoch 2013 Batch   51/81   train_loss = 0.792\n",
      "Epoch 2014 Batch    2/81   train_loss = 0.786\n",
      "Epoch 2014 Batch   34/81   train_loss = 0.782\n",
      "Epoch 2014 Batch   66/81   train_loss = 0.766\n",
      "Epoch 2015 Batch   17/81   train_loss = 0.764\n",
      "Epoch 2015 Batch   49/81   train_loss = 0.760\n",
      "Epoch 2016 Batch    0/81   train_loss = 0.769\n",
      "Epoch 2016 Batch   32/81   train_loss = 0.773\n",
      "Epoch 2016 Batch   64/81   train_loss = 0.776\n",
      "Epoch 2017 Batch   15/81   train_loss = 0.759\n",
      "Epoch 2017 Batch   47/81   train_loss = 0.777\n",
      "Epoch 2017 Batch   79/81   train_loss = 0.766\n",
      "Epoch 2018 Batch   30/81   train_loss = 0.759\n",
      "Epoch 2018 Batch   62/81   train_loss = 0.744\n",
      "Epoch 2019 Batch   13/81   train_loss = 0.793\n",
      "Epoch 2019 Batch   45/81   train_loss = 0.756\n",
      "Epoch 2019 Batch   77/81   train_loss = 0.793\n",
      "Epoch 2020 Batch   28/81   train_loss = 0.758\n",
      "Epoch 2020 Batch   60/81   train_loss = 0.775\n",
      "Epoch 2021 Batch   11/81   train_loss = 0.764\n",
      "Epoch 2021 Batch   43/81   train_loss = 0.764\n",
      "Epoch 2021 Batch   75/81   train_loss = 0.767\n",
      "Epoch 2022 Batch   26/81   train_loss = 0.752\n",
      "Epoch 2022 Batch   58/81   train_loss = 0.731\n",
      "Epoch 2023 Batch    9/81   train_loss = 0.792\n",
      "Epoch 2023 Batch   41/81   train_loss = 0.720\n",
      "Epoch 2023 Batch   73/81   train_loss = 0.776\n",
      "Epoch 2024 Batch   24/81   train_loss = 0.756\n",
      "Epoch 2024 Batch   56/81   train_loss = 0.766\n",
      "Epoch 2025 Batch    7/81   train_loss = 0.745\n",
      "Epoch 2025 Batch   39/81   train_loss = 0.721\n",
      "Epoch 2025 Batch   71/81   train_loss = 0.746\n",
      "Epoch 2026 Batch   22/81   train_loss = 0.779\n",
      "Epoch 2026 Batch   54/81   train_loss = 0.747\n",
      "Epoch 2027 Batch    5/81   train_loss = 0.786\n",
      "Epoch 2027 Batch   37/81   train_loss = 0.754\n",
      "Epoch 2027 Batch   69/81   train_loss = 0.757\n",
      "Epoch 2028 Batch   20/81   train_loss = 0.753\n",
      "Epoch 2028 Batch   52/81   train_loss = 0.783\n",
      "Epoch 2029 Batch    3/81   train_loss = 0.748\n",
      "Epoch 2029 Batch   35/81   train_loss = 0.738\n",
      "Epoch 2029 Batch   67/81   train_loss = 0.778\n",
      "Epoch 2030 Batch   18/81   train_loss = 0.760\n",
      "Epoch 2030 Batch   50/81   train_loss = 0.744\n",
      "Epoch 2031 Batch    1/81   train_loss = 0.781\n",
      "Epoch 2031 Batch   33/81   train_loss = 0.729\n",
      "Epoch 2031 Batch   65/81   train_loss = 0.785\n",
      "Epoch 2032 Batch   16/81   train_loss = 0.795\n",
      "Epoch 2032 Batch   48/81   train_loss = 0.759\n",
      "Epoch 2032 Batch   80/81   train_loss = 0.788\n",
      "Epoch 2033 Batch   31/81   train_loss = 0.729\n",
      "Epoch 2033 Batch   63/81   train_loss = 0.761\n",
      "Epoch 2034 Batch   14/81   train_loss = 0.775\n",
      "Epoch 2034 Batch   46/81   train_loss = 0.764\n",
      "Epoch 2034 Batch   78/81   train_loss = 0.778\n",
      "Epoch 2035 Batch   29/81   train_loss = 0.759\n",
      "Epoch 2035 Batch   61/81   train_loss = 0.763\n",
      "Epoch 2036 Batch   12/81   train_loss = 0.766\n",
      "Epoch 2036 Batch   44/81   train_loss = 0.749\n",
      "Epoch 2036 Batch   76/81   train_loss = 0.807\n",
      "Epoch 2037 Batch   27/81   train_loss = 0.749\n",
      "Epoch 2037 Batch   59/81   train_loss = 0.761\n",
      "Epoch 2038 Batch   10/81   train_loss = 0.770\n",
      "Epoch 2038 Batch   42/81   train_loss = 0.759\n",
      "Epoch 2038 Batch   74/81   train_loss = 0.762\n",
      "Epoch 2039 Batch   25/81   train_loss = 0.781\n",
      "Epoch 2039 Batch   57/81   train_loss = 0.755\n",
      "Epoch 2040 Batch    8/81   train_loss = 0.764\n",
      "Epoch 2040 Batch   40/81   train_loss = 0.762\n",
      "Epoch 2040 Batch   72/81   train_loss = 0.757\n",
      "Epoch 2041 Batch   23/81   train_loss = 0.762\n",
      "Epoch 2041 Batch   55/81   train_loss = 0.773\n",
      "Epoch 2042 Batch    6/81   train_loss = 0.765\n",
      "Epoch 2042 Batch   38/81   train_loss = 0.785\n",
      "Epoch 2042 Batch   70/81   train_loss = 0.756\n",
      "Epoch 2043 Batch   21/81   train_loss = 0.804\n",
      "Epoch 2043 Batch   53/81   train_loss = 0.773\n",
      "Epoch 2044 Batch    4/81   train_loss = 0.770\n",
      "Epoch 2044 Batch   36/81   train_loss = 0.805\n",
      "Epoch 2044 Batch   68/81   train_loss = 0.793\n",
      "Epoch 2045 Batch   19/81   train_loss = 0.741\n",
      "Epoch 2045 Batch   51/81   train_loss = 0.801\n",
      "Epoch 2046 Batch    2/81   train_loss = 0.774\n",
      "Epoch 2046 Batch   34/81   train_loss = 0.770\n",
      "Epoch 2046 Batch   66/81   train_loss = 0.788\n",
      "Epoch 2047 Batch   17/81   train_loss = 0.777\n",
      "Epoch 2047 Batch   49/81   train_loss = 0.756\n",
      "Epoch 2048 Batch    0/81   train_loss = 0.772\n",
      "Epoch 2048 Batch   32/81   train_loss = 0.752\n",
      "Epoch 2048 Batch   64/81   train_loss = 0.785\n",
      "Epoch 2049 Batch   15/81   train_loss = 0.763\n",
      "Epoch 2049 Batch   47/81   train_loss = 0.745\n",
      "Epoch 2049 Batch   79/81   train_loss = 0.781\n",
      "Epoch 2050 Batch   30/81   train_loss = 0.750\n",
      "Epoch 2050 Batch   62/81   train_loss = 0.749\n",
      "Epoch 2051 Batch   13/81   train_loss = 0.784\n",
      "Epoch 2051 Batch   45/81   train_loss = 0.759\n",
      "Epoch 2051 Batch   77/81   train_loss = 0.773\n",
      "Epoch 2052 Batch   28/81   train_loss = 0.743\n",
      "Epoch 2052 Batch   60/81   train_loss = 0.767\n",
      "Epoch 2053 Batch   11/81   train_loss = 0.803\n",
      "Epoch 2053 Batch   43/81   train_loss = 0.742\n",
      "Epoch 2053 Batch   75/81   train_loss = 0.771\n",
      "Epoch 2054 Batch   26/81   train_loss = 0.756\n",
      "Epoch 2054 Batch   58/81   train_loss = 0.753\n",
      "Epoch 2055 Batch    9/81   train_loss = 0.828\n",
      "Epoch 2055 Batch   41/81   train_loss = 0.728\n",
      "Epoch 2055 Batch   73/81   train_loss = 0.795\n",
      "Epoch 2056 Batch   24/81   train_loss = 0.757\n",
      "Epoch 2056 Batch   56/81   train_loss = 0.764\n",
      "Epoch 2057 Batch    7/81   train_loss = 0.778\n",
      "Epoch 2057 Batch   39/81   train_loss = 0.745\n",
      "Epoch 2057 Batch   71/81   train_loss = 0.757\n",
      "Epoch 2058 Batch   22/81   train_loss = 0.802\n",
      "Epoch 2058 Batch   54/81   train_loss = 0.764\n",
      "Epoch 2059 Batch    5/81   train_loss = 0.783\n",
      "Epoch 2059 Batch   37/81   train_loss = 0.767\n",
      "Epoch 2059 Batch   69/81   train_loss = 0.765\n",
      "Epoch 2060 Batch   20/81   train_loss = 0.771\n",
      "Epoch 2060 Batch   52/81   train_loss = 0.800\n",
      "Epoch 2061 Batch    3/81   train_loss = 0.756\n",
      "Epoch 2061 Batch   35/81   train_loss = 0.757\n",
      "Epoch 2061 Batch   67/81   train_loss = 0.808\n",
      "Epoch 2062 Batch   18/81   train_loss = 0.784\n",
      "Epoch 2062 Batch   50/81   train_loss = 0.779\n",
      "Epoch 2063 Batch    1/81   train_loss = 0.784\n",
      "Epoch 2063 Batch   33/81   train_loss = 0.746\n",
      "Epoch 2063 Batch   65/81   train_loss = 0.803\n",
      "Epoch 2064 Batch   16/81   train_loss = 0.814\n",
      "Epoch 2064 Batch   48/81   train_loss = 0.781\n",
      "Epoch 2064 Batch   80/81   train_loss = 0.785\n",
      "Epoch 2065 Batch   31/81   train_loss = 0.766\n",
      "Epoch 2065 Batch   63/81   train_loss = 0.766\n",
      "Epoch 2066 Batch   14/81   train_loss = 0.786\n",
      "Epoch 2066 Batch   46/81   train_loss = 0.763\n",
      "Epoch 2066 Batch   78/81   train_loss = 0.769\n",
      "Epoch 2067 Batch   29/81   train_loss = 0.765\n",
      "Epoch 2067 Batch   61/81   train_loss = 0.780\n",
      "Epoch 2068 Batch   12/81   train_loss = 0.769\n",
      "Epoch 2068 Batch   44/81   train_loss = 0.760\n",
      "Epoch 2068 Batch   76/81   train_loss = 0.784\n",
      "Epoch 2069 Batch   27/81   train_loss = 0.735\n",
      "Epoch 2069 Batch   59/81   train_loss = 0.756\n",
      "Epoch 2070 Batch   10/81   train_loss = 0.784\n",
      "Epoch 2070 Batch   42/81   train_loss = 0.750\n",
      "Epoch 2070 Batch   74/81   train_loss = 0.773\n",
      "Epoch 2071 Batch   25/81   train_loss = 0.764\n",
      "Epoch 2071 Batch   57/81   train_loss = 0.765\n",
      "Epoch 2072 Batch    8/81   train_loss = 0.781\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2072 Batch   40/81   train_loss = 0.751\n",
      "Epoch 2072 Batch   72/81   train_loss = 0.750\n",
      "Epoch 2073 Batch   23/81   train_loss = 0.767\n",
      "Epoch 2073 Batch   55/81   train_loss = 0.763\n",
      "Epoch 2074 Batch    6/81   train_loss = 0.757\n",
      "Epoch 2074 Batch   38/81   train_loss = 0.787\n",
      "Epoch 2074 Batch   70/81   train_loss = 0.752\n",
      "Epoch 2075 Batch   21/81   train_loss = 0.803\n",
      "Epoch 2075 Batch   53/81   train_loss = 0.752\n",
      "Epoch 2076 Batch    4/81   train_loss = 0.782\n",
      "Epoch 2076 Batch   36/81   train_loss = 0.791\n",
      "Epoch 2076 Batch   68/81   train_loss = 0.792\n",
      "Epoch 2077 Batch   19/81   train_loss = 0.745\n",
      "Epoch 2077 Batch   51/81   train_loss = 0.819\n",
      "Epoch 2078 Batch    2/81   train_loss = 0.789\n",
      "Epoch 2078 Batch   34/81   train_loss = 0.784\n",
      "Epoch 2078 Batch   66/81   train_loss = 0.781\n",
      "Epoch 2079 Batch   17/81   train_loss = 0.788\n",
      "Epoch 2079 Batch   49/81   train_loss = 0.767\n",
      "Epoch 2080 Batch    0/81   train_loss = 0.781\n",
      "Epoch 2080 Batch   32/81   train_loss = 0.741\n",
      "Epoch 2080 Batch   64/81   train_loss = 0.758\n",
      "Epoch 2081 Batch   15/81   train_loss = 0.752\n",
      "Epoch 2081 Batch   47/81   train_loss = 0.769\n",
      "Epoch 2081 Batch   79/81   train_loss = 0.780\n",
      "Epoch 2082 Batch   30/81   train_loss = 0.764\n",
      "Epoch 2082 Batch   62/81   train_loss = 0.756\n",
      "Epoch 2083 Batch   13/81   train_loss = 0.807\n",
      "Epoch 2083 Batch   45/81   train_loss = 0.787\n",
      "Epoch 2083 Batch   77/81   train_loss = 0.787\n",
      "Epoch 2084 Batch   28/81   train_loss = 0.750\n",
      "Epoch 2084 Batch   60/81   train_loss = 0.762\n",
      "Epoch 2085 Batch   11/81   train_loss = 0.778\n",
      "Epoch 2085 Batch   43/81   train_loss = 0.740\n",
      "Epoch 2085 Batch   75/81   train_loss = 0.754\n",
      "Epoch 2086 Batch   26/81   train_loss = 0.750\n",
      "Epoch 2086 Batch   58/81   train_loss = 0.735\n",
      "Epoch 2087 Batch    9/81   train_loss = 0.790\n",
      "Epoch 2087 Batch   41/81   train_loss = 0.736\n",
      "Epoch 2087 Batch   73/81   train_loss = 0.789\n",
      "Epoch 2088 Batch   24/81   train_loss = 0.755\n",
      "Epoch 2088 Batch   56/81   train_loss = 0.750\n",
      "Epoch 2089 Batch    7/81   train_loss = 0.763\n",
      "Epoch 2089 Batch   39/81   train_loss = 0.742\n",
      "Epoch 2089 Batch   71/81   train_loss = 0.734\n",
      "Epoch 2090 Batch   22/81   train_loss = 0.786\n",
      "Epoch 2090 Batch   54/81   train_loss = 0.757\n",
      "Epoch 2091 Batch    5/81   train_loss = 0.767\n",
      "Epoch 2091 Batch   37/81   train_loss = 0.775\n",
      "Epoch 2091 Batch   69/81   train_loss = 0.757\n",
      "Epoch 2092 Batch   20/81   train_loss = 0.750\n",
      "Epoch 2092 Batch   52/81   train_loss = 0.776\n",
      "Epoch 2093 Batch    3/81   train_loss = 0.753\n",
      "Epoch 2093 Batch   35/81   train_loss = 0.748\n",
      "Epoch 2093 Batch   67/81   train_loss = 0.770\n",
      "Epoch 2094 Batch   18/81   train_loss = 0.774\n",
      "Epoch 2094 Batch   50/81   train_loss = 0.742\n",
      "Epoch 2095 Batch    1/81   train_loss = 0.772\n",
      "Epoch 2095 Batch   33/81   train_loss = 0.743\n",
      "Epoch 2095 Batch   65/81   train_loss = 0.806\n",
      "Epoch 2096 Batch   16/81   train_loss = 0.830\n",
      "Epoch 2096 Batch   48/81   train_loss = 0.767\n",
      "Epoch 2096 Batch   80/81   train_loss = 0.780\n",
      "Epoch 2097 Batch   31/81   train_loss = 0.767\n",
      "Epoch 2097 Batch   63/81   train_loss = 0.788\n",
      "Epoch 2098 Batch   14/81   train_loss = 0.794\n",
      "Epoch 2098 Batch   46/81   train_loss = 0.778\n",
      "Epoch 2098 Batch   78/81   train_loss = 0.760\n",
      "Epoch 2099 Batch   29/81   train_loss = 0.775\n",
      "Epoch 2099 Batch   61/81   train_loss = 0.774\n",
      "Epoch 2100 Batch   12/81   train_loss = 0.787\n",
      "Epoch 2100 Batch   44/81   train_loss = 0.778\n",
      "Epoch 2100 Batch   76/81   train_loss = 0.800\n",
      "Epoch 2101 Batch   27/81   train_loss = 0.763\n",
      "Epoch 2101 Batch   59/81   train_loss = 0.773\n",
      "Epoch 2102 Batch   10/81   train_loss = 0.798\n",
      "Epoch 2102 Batch   42/81   train_loss = 0.757\n",
      "Epoch 2102 Batch   74/81   train_loss = 0.770\n",
      "Epoch 2103 Batch   25/81   train_loss = 0.774\n",
      "Epoch 2103 Batch   57/81   train_loss = 0.764\n",
      "Epoch 2104 Batch    8/81   train_loss = 0.789\n",
      "Epoch 2104 Batch   40/81   train_loss = 0.747\n",
      "Epoch 2104 Batch   72/81   train_loss = 0.748\n",
      "Epoch 2105 Batch   23/81   train_loss = 0.759\n",
      "Epoch 2105 Batch   55/81   train_loss = 0.794\n",
      "Epoch 2106 Batch    6/81   train_loss = 0.751\n",
      "Epoch 2106 Batch   38/81   train_loss = 0.767\n",
      "Epoch 2106 Batch   70/81   train_loss = 0.766\n",
      "Epoch 2107 Batch   21/81   train_loss = 0.811\n",
      "Epoch 2107 Batch   53/81   train_loss = 0.743\n",
      "Epoch 2108 Batch    4/81   train_loss = 0.768\n",
      "Epoch 2108 Batch   36/81   train_loss = 0.782\n",
      "Epoch 2108 Batch   68/81   train_loss = 0.783\n",
      "Epoch 2109 Batch   19/81   train_loss = 0.748\n",
      "Epoch 2109 Batch   51/81   train_loss = 0.808\n",
      "Epoch 2110 Batch    2/81   train_loss = 0.780\n",
      "Epoch 2110 Batch   34/81   train_loss = 0.765\n",
      "Epoch 2110 Batch   66/81   train_loss = 0.758\n",
      "Epoch 2111 Batch   17/81   train_loss = 0.764\n",
      "Epoch 2111 Batch   49/81   train_loss = 0.757\n",
      "Epoch 2112 Batch    0/81   train_loss = 0.756\n",
      "Epoch 2112 Batch   32/81   train_loss = 0.749\n",
      "Epoch 2112 Batch   64/81   train_loss = 0.762\n",
      "Epoch 2113 Batch   15/81   train_loss = 0.754\n",
      "Epoch 2113 Batch   47/81   train_loss = 0.755\n",
      "Epoch 2113 Batch   79/81   train_loss = 0.768\n",
      "Epoch 2114 Batch   30/81   train_loss = 0.748\n",
      "Epoch 2114 Batch   62/81   train_loss = 0.746\n",
      "Epoch 2115 Batch   13/81   train_loss = 0.788\n",
      "Epoch 2115 Batch   45/81   train_loss = 0.770\n",
      "Epoch 2115 Batch   77/81   train_loss = 0.763\n",
      "Epoch 2116 Batch   28/81   train_loss = 0.742\n",
      "Epoch 2116 Batch   60/81   train_loss = 0.763\n",
      "Epoch 2117 Batch   11/81   train_loss = 0.762\n",
      "Epoch 2117 Batch   43/81   train_loss = 0.738\n",
      "Epoch 2117 Batch   75/81   train_loss = 0.748\n",
      "Epoch 2118 Batch   26/81   train_loss = 0.755\n",
      "Epoch 2118 Batch   58/81   train_loss = 0.726\n",
      "Epoch 2119 Batch    9/81   train_loss = 0.797\n",
      "Epoch 2119 Batch   41/81   train_loss = 0.725\n",
      "Epoch 2119 Batch   73/81   train_loss = 0.783\n",
      "Epoch 2120 Batch   24/81   train_loss = 0.754\n",
      "Epoch 2120 Batch   56/81   train_loss = 0.750\n",
      "Epoch 2121 Batch    7/81   train_loss = 0.758\n",
      "Epoch 2121 Batch   39/81   train_loss = 0.740\n",
      "Epoch 2121 Batch   71/81   train_loss = 0.734\n",
      "Epoch 2122 Batch   22/81   train_loss = 0.790\n",
      "Epoch 2122 Batch   54/81   train_loss = 0.757\n",
      "Epoch 2123 Batch    5/81   train_loss = 0.779\n",
      "Epoch 2123 Batch   37/81   train_loss = 0.761\n",
      "Epoch 2123 Batch   69/81   train_loss = 0.765\n",
      "Epoch 2124 Batch   20/81   train_loss = 0.757\n",
      "Epoch 2124 Batch   52/81   train_loss = 0.793\n",
      "Epoch 2125 Batch    3/81   train_loss = 0.758\n",
      "Epoch 2125 Batch   35/81   train_loss = 0.752\n",
      "Epoch 2125 Batch   67/81   train_loss = 0.783\n",
      "Epoch 2126 Batch   18/81   train_loss = 0.783\n",
      "Epoch 2126 Batch   50/81   train_loss = 0.740\n",
      "Epoch 2127 Batch    1/81   train_loss = 0.798\n",
      "Epoch 2127 Batch   33/81   train_loss = 0.752\n",
      "Epoch 2127 Batch   65/81   train_loss = 0.800\n",
      "Epoch 2128 Batch   16/81   train_loss = 0.806\n",
      "Epoch 2128 Batch   48/81   train_loss = 0.763\n",
      "Epoch 2128 Batch   80/81   train_loss = 0.792\n",
      "Epoch 2129 Batch   31/81   train_loss = 0.742\n",
      "Epoch 2129 Batch   63/81   train_loss = 0.775\n",
      "Epoch 2130 Batch   14/81   train_loss = 0.771\n",
      "Epoch 2130 Batch   46/81   train_loss = 0.755\n",
      "Epoch 2130 Batch   78/81   train_loss = 0.765\n",
      "Epoch 2131 Batch   29/81   train_loss = 0.769\n",
      "Epoch 2131 Batch   61/81   train_loss = 0.755\n",
      "Epoch 2132 Batch   12/81   train_loss = 0.775\n",
      "Epoch 2132 Batch   44/81   train_loss = 0.784\n",
      "Epoch 2132 Batch   76/81   train_loss = 0.797\n",
      "Epoch 2133 Batch   27/81   train_loss = 0.764\n",
      "Epoch 2133 Batch   59/81   train_loss = 0.769\n",
      "Epoch 2134 Batch   10/81   train_loss = 0.806\n",
      "Epoch 2134 Batch   42/81   train_loss = 0.749\n",
      "Epoch 2134 Batch   74/81   train_loss = 0.763\n",
      "Epoch 2135 Batch   25/81   train_loss = 0.772\n",
      "Epoch 2135 Batch   57/81   train_loss = 0.768\n",
      "Epoch 2136 Batch    8/81   train_loss = 0.769\n",
      "Epoch 2136 Batch   40/81   train_loss = 0.743\n",
      "Epoch 2136 Batch   72/81   train_loss = 0.757\n",
      "Epoch 2137 Batch   23/81   train_loss = 0.760\n",
      "Epoch 2137 Batch   55/81   train_loss = 0.786\n",
      "Epoch 2138 Batch    6/81   train_loss = 0.776\n",
      "Epoch 2138 Batch   38/81   train_loss = 0.789\n",
      "Epoch 2138 Batch   70/81   train_loss = 0.778\n",
      "Epoch 2139 Batch   21/81   train_loss = 0.824\n",
      "Epoch 2139 Batch   53/81   train_loss = 0.764\n",
      "Epoch 2140 Batch    4/81   train_loss = 0.773\n",
      "Epoch 2140 Batch   36/81   train_loss = 0.787\n",
      "Epoch 2140 Batch   68/81   train_loss = 0.789\n",
      "Epoch 2141 Batch   19/81   train_loss = 0.771\n",
      "Epoch 2141 Batch   51/81   train_loss = 0.825\n",
      "Epoch 2142 Batch    2/81   train_loss = 0.780\n",
      "Epoch 2142 Batch   34/81   train_loss = 0.786\n",
      "Epoch 2142 Batch   66/81   train_loss = 0.769\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2143 Batch   17/81   train_loss = 0.789\n",
      "Epoch 2143 Batch   49/81   train_loss = 0.754\n",
      "Epoch 2144 Batch    0/81   train_loss = 0.788\n",
      "Epoch 2144 Batch   32/81   train_loss = 0.764\n",
      "Epoch 2144 Batch   64/81   train_loss = 0.766\n",
      "Epoch 2145 Batch   15/81   train_loss = 0.752\n",
      "Epoch 2145 Batch   47/81   train_loss = 0.760\n",
      "Epoch 2145 Batch   79/81   train_loss = 0.774\n",
      "Epoch 2146 Batch   30/81   train_loss = 0.765\n",
      "Epoch 2146 Batch   62/81   train_loss = 0.742\n",
      "Epoch 2147 Batch   13/81   train_loss = 0.791\n",
      "Epoch 2147 Batch   45/81   train_loss = 0.769\n",
      "Epoch 2147 Batch   77/81   train_loss = 0.791\n",
      "Epoch 2148 Batch   28/81   train_loss = 0.753\n",
      "Epoch 2148 Batch   60/81   train_loss = 0.762\n",
      "Epoch 2149 Batch   11/81   train_loss = 0.794\n",
      "Epoch 2149 Batch   43/81   train_loss = 0.739\n",
      "Epoch 2149 Batch   75/81   train_loss = 0.770\n",
      "Epoch 2150 Batch   26/81   train_loss = 0.770\n",
      "Epoch 2150 Batch   58/81   train_loss = 0.753\n",
      "Epoch 2151 Batch    9/81   train_loss = 0.788\n",
      "Epoch 2151 Batch   41/81   train_loss = 0.730\n",
      "Epoch 2151 Batch   73/81   train_loss = 0.763\n",
      "Epoch 2152 Batch   24/81   train_loss = 0.761\n",
      "Epoch 2152 Batch   56/81   train_loss = 0.794\n",
      "Epoch 2153 Batch    7/81   train_loss = 0.763\n",
      "Epoch 2153 Batch   39/81   train_loss = 0.743\n",
      "Epoch 2153 Batch   71/81   train_loss = 0.754\n",
      "Epoch 2154 Batch   22/81   train_loss = 0.801\n",
      "Epoch 2154 Batch   54/81   train_loss = 0.742\n",
      "Epoch 2155 Batch    5/81   train_loss = 0.786\n",
      "Epoch 2155 Batch   37/81   train_loss = 0.763\n",
      "Epoch 2155 Batch   69/81   train_loss = 0.760\n",
      "Epoch 2156 Batch   20/81   train_loss = 0.754\n",
      "Epoch 2156 Batch   52/81   train_loss = 0.793\n",
      "Epoch 2157 Batch    3/81   train_loss = 0.755\n",
      "Epoch 2157 Batch   35/81   train_loss = 0.752\n",
      "Epoch 2157 Batch   67/81   train_loss = 0.782\n",
      "Epoch 2158 Batch   18/81   train_loss = 0.780\n",
      "Epoch 2158 Batch   50/81   train_loss = 0.746\n",
      "Epoch 2159 Batch    1/81   train_loss = 0.770\n",
      "Epoch 2159 Batch   33/81   train_loss = 0.743\n",
      "Epoch 2159 Batch   65/81   train_loss = 0.780\n",
      "Epoch 2160 Batch   16/81   train_loss = 0.816\n",
      "Epoch 2160 Batch   48/81   train_loss = 0.770\n",
      "Epoch 2160 Batch   80/81   train_loss = 0.808\n",
      "Epoch 2161 Batch   31/81   train_loss = 0.751\n",
      "Epoch 2161 Batch   63/81   train_loss = 0.786\n",
      "Epoch 2162 Batch   14/81   train_loss = 0.783\n",
      "Epoch 2162 Batch   46/81   train_loss = 0.770\n",
      "Epoch 2162 Batch   78/81   train_loss = 0.776\n",
      "Epoch 2163 Batch   29/81   train_loss = 0.764\n",
      "Epoch 2163 Batch   61/81   train_loss = 0.792\n",
      "Epoch 2164 Batch   12/81   train_loss = 0.781\n",
      "Epoch 2164 Batch   44/81   train_loss = 0.769\n",
      "Epoch 2164 Batch   76/81   train_loss = 0.805\n",
      "Epoch 2165 Batch   27/81   train_loss = 0.772\n",
      "Epoch 2165 Batch   59/81   train_loss = 0.771\n",
      "Epoch 2166 Batch   10/81   train_loss = 0.796\n",
      "Epoch 2166 Batch   42/81   train_loss = 0.756\n",
      "Epoch 2166 Batch   74/81   train_loss = 0.768\n",
      "Epoch 2167 Batch   25/81   train_loss = 0.778\n",
      "Epoch 2167 Batch   57/81   train_loss = 0.775\n",
      "Epoch 2168 Batch    8/81   train_loss = 0.761\n",
      "Epoch 2168 Batch   40/81   train_loss = 0.753\n",
      "Epoch 2168 Batch   72/81   train_loss = 0.751\n",
      "Epoch 2169 Batch   23/81   train_loss = 0.762\n",
      "Epoch 2169 Batch   55/81   train_loss = 0.769\n",
      "Epoch 2170 Batch    6/81   train_loss = 0.766\n",
      "Epoch 2170 Batch   38/81   train_loss = 0.768\n",
      "Epoch 2170 Batch   70/81   train_loss = 0.760\n",
      "Epoch 2171 Batch   21/81   train_loss = 0.794\n",
      "Epoch 2171 Batch   53/81   train_loss = 0.757\n",
      "Epoch 2172 Batch    4/81   train_loss = 0.786\n",
      "Epoch 2172 Batch   36/81   train_loss = 0.795\n",
      "Epoch 2172 Batch   68/81   train_loss = 0.801\n",
      "Epoch 2173 Batch   19/81   train_loss = 0.763\n",
      "Epoch 2173 Batch   51/81   train_loss = 0.830\n",
      "Epoch 2174 Batch    2/81   train_loss = 0.787\n",
      "Epoch 2174 Batch   34/81   train_loss = 0.794\n",
      "Epoch 2174 Batch   66/81   train_loss = 0.784\n",
      "Epoch 2175 Batch   17/81   train_loss = 0.795\n",
      "Epoch 2175 Batch   49/81   train_loss = 0.770\n",
      "Epoch 2176 Batch    0/81   train_loss = 0.789\n",
      "Epoch 2176 Batch   32/81   train_loss = 0.754\n",
      "Epoch 2176 Batch   64/81   train_loss = 0.763\n",
      "Epoch 2177 Batch   15/81   train_loss = 0.768\n",
      "Epoch 2177 Batch   47/81   train_loss = 0.759\n",
      "Epoch 2177 Batch   79/81   train_loss = 0.777\n",
      "Epoch 2178 Batch   30/81   train_loss = 0.767\n",
      "Epoch 2178 Batch   62/81   train_loss = 0.766\n",
      "Epoch 2179 Batch   13/81   train_loss = 0.797\n",
      "Epoch 2179 Batch   45/81   train_loss = 0.779\n",
      "Epoch 2179 Batch   77/81   train_loss = 0.801\n",
      "Epoch 2180 Batch   28/81   train_loss = 0.751\n",
      "Epoch 2180 Batch   60/81   train_loss = 0.783\n",
      "Epoch 2181 Batch   11/81   train_loss = 0.790\n",
      "Epoch 2181 Batch   43/81   train_loss = 0.760\n",
      "Epoch 2181 Batch   75/81   train_loss = 0.753\n",
      "Epoch 2182 Batch   26/81   train_loss = 0.769\n",
      "Epoch 2182 Batch   58/81   train_loss = 0.762\n",
      "Epoch 2183 Batch    9/81   train_loss = 0.797\n",
      "Epoch 2183 Batch   41/81   train_loss = 0.746\n",
      "Epoch 2183 Batch   73/81   train_loss = 0.773\n",
      "Epoch 2184 Batch   24/81   train_loss = 0.763\n",
      "Epoch 2184 Batch   56/81   train_loss = 0.796\n",
      "Epoch 2185 Batch    7/81   train_loss = 0.748\n",
      "Epoch 2185 Batch   39/81   train_loss = 0.742\n",
      "Epoch 2185 Batch   71/81   train_loss = 0.746\n",
      "Epoch 2186 Batch   22/81   train_loss = 0.785\n",
      "Epoch 2186 Batch   54/81   train_loss = 0.755\n",
      "Epoch 2187 Batch    5/81   train_loss = 0.775\n",
      "Epoch 2187 Batch   37/81   train_loss = 0.774\n",
      "Epoch 2187 Batch   69/81   train_loss = 0.764\n",
      "Epoch 2188 Batch   20/81   train_loss = 0.744\n",
      "Epoch 2188 Batch   52/81   train_loss = 0.799\n",
      "Epoch 2189 Batch    3/81   train_loss = 0.755\n",
      "Epoch 2189 Batch   35/81   train_loss = 0.772\n",
      "Epoch 2189 Batch   67/81   train_loss = 0.792\n",
      "Epoch 2190 Batch   18/81   train_loss = 0.786\n",
      "Epoch 2190 Batch   50/81   train_loss = 0.747\n",
      "Epoch 2191 Batch    1/81   train_loss = 0.789\n",
      "Epoch 2191 Batch   33/81   train_loss = 0.744\n",
      "Epoch 2191 Batch   65/81   train_loss = 0.793\n",
      "Epoch 2192 Batch   16/81   train_loss = 0.835\n",
      "Epoch 2192 Batch   48/81   train_loss = 0.776\n",
      "Epoch 2192 Batch   80/81   train_loss = 0.789\n",
      "Epoch 2193 Batch   31/81   train_loss = 0.743\n",
      "Epoch 2193 Batch   63/81   train_loss = 0.758\n",
      "Epoch 2194 Batch   14/81   train_loss = 0.792\n",
      "Epoch 2194 Batch   46/81   train_loss = 0.777\n",
      "Epoch 2194 Batch   78/81   train_loss = 0.776\n",
      "Epoch 2195 Batch   29/81   train_loss = 0.776\n",
      "Epoch 2195 Batch   61/81   train_loss = 0.769\n",
      "Epoch 2196 Batch   12/81   train_loss = 0.763\n",
      "Epoch 2196 Batch   44/81   train_loss = 0.747\n",
      "Epoch 2196 Batch   76/81   train_loss = 0.793\n",
      "Epoch 2197 Batch   27/81   train_loss = 0.765\n",
      "Epoch 2197 Batch   59/81   train_loss = 0.781\n",
      "Epoch 2198 Batch   10/81   train_loss = 0.778\n",
      "Epoch 2198 Batch   42/81   train_loss = 0.775\n",
      "Epoch 2198 Batch   74/81   train_loss = 0.755\n",
      "Epoch 2199 Batch   25/81   train_loss = 0.769\n",
      "Epoch 2199 Batch   57/81   train_loss = 0.767\n",
      "Epoch 2200 Batch    8/81   train_loss = 0.772\n",
      "Epoch 2200 Batch   40/81   train_loss = 0.747\n",
      "Epoch 2200 Batch   72/81   train_loss = 0.752\n",
      "Epoch 2201 Batch   23/81   train_loss = 0.754\n",
      "Epoch 2201 Batch   55/81   train_loss = 0.776\n",
      "Epoch 2202 Batch    6/81   train_loss = 0.765\n",
      "Epoch 2202 Batch   38/81   train_loss = 0.774\n",
      "Epoch 2202 Batch   70/81   train_loss = 0.750\n",
      "Epoch 2203 Batch   21/81   train_loss = 0.808\n",
      "Epoch 2203 Batch   53/81   train_loss = 0.767\n",
      "Epoch 2204 Batch    4/81   train_loss = 0.774\n",
      "Epoch 2204 Batch   36/81   train_loss = 0.792\n",
      "Epoch 2204 Batch   68/81   train_loss = 0.800\n",
      "Epoch 2205 Batch   19/81   train_loss = 0.762\n",
      "Epoch 2205 Batch   51/81   train_loss = 0.809\n",
      "Epoch 2206 Batch    2/81   train_loss = 0.786\n",
      "Epoch 2206 Batch   34/81   train_loss = 0.824\n",
      "Epoch 2206 Batch   66/81   train_loss = 0.799\n",
      "Epoch 2207 Batch   17/81   train_loss = 0.791\n",
      "Epoch 2207 Batch   49/81   train_loss = 0.786\n",
      "Epoch 2208 Batch    0/81   train_loss = 0.789\n",
      "Epoch 2208 Batch   32/81   train_loss = 0.782\n",
      "Epoch 2208 Batch   64/81   train_loss = 0.786\n",
      "Epoch 2209 Batch   15/81   train_loss = 0.776\n",
      "Epoch 2209 Batch   47/81   train_loss = 0.787\n",
      "Epoch 2209 Batch   79/81   train_loss = 0.790\n",
      "Epoch 2210 Batch   30/81   train_loss = 0.770\n",
      "Epoch 2210 Batch   62/81   train_loss = 0.785\n",
      "Epoch 2211 Batch   13/81   train_loss = 0.800\n",
      "Epoch 2211 Batch   45/81   train_loss = 0.774\n",
      "Epoch 2211 Batch   77/81   train_loss = 0.833\n",
      "Epoch 2212 Batch   28/81   train_loss = 0.756\n",
      "Epoch 2212 Batch   60/81   train_loss = 0.788\n",
      "Epoch 2213 Batch   11/81   train_loss = 0.780\n",
      "Epoch 2213 Batch   43/81   train_loss = 0.746\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2213 Batch   75/81   train_loss = 0.766\n",
      "Epoch 2214 Batch   26/81   train_loss = 0.765\n",
      "Epoch 2214 Batch   58/81   train_loss = 0.751\n",
      "Epoch 2215 Batch    9/81   train_loss = 0.815\n",
      "Epoch 2215 Batch   41/81   train_loss = 0.737\n",
      "Epoch 2215 Batch   73/81   train_loss = 0.802\n",
      "Epoch 2216 Batch   24/81   train_loss = 0.761\n",
      "Epoch 2216 Batch   56/81   train_loss = 0.796\n",
      "Epoch 2217 Batch    7/81   train_loss = 0.773\n",
      "Epoch 2217 Batch   39/81   train_loss = 0.748\n",
      "Epoch 2217 Batch   71/81   train_loss = 0.745\n",
      "Epoch 2218 Batch   22/81   train_loss = 0.793\n",
      "Epoch 2218 Batch   54/81   train_loss = 0.763\n",
      "Epoch 2219 Batch    5/81   train_loss = 0.796\n",
      "Epoch 2219 Batch   37/81   train_loss = 0.771\n",
      "Epoch 2219 Batch   69/81   train_loss = 0.780\n",
      "Epoch 2220 Batch   20/81   train_loss = 0.754\n",
      "Epoch 2220 Batch   52/81   train_loss = 0.805\n",
      "Epoch 2221 Batch    3/81   train_loss = 0.759\n",
      "Epoch 2221 Batch   35/81   train_loss = 0.758\n",
      "Epoch 2221 Batch   67/81   train_loss = 0.778\n",
      "Epoch 2222 Batch   18/81   train_loss = 0.768\n",
      "Epoch 2222 Batch   50/81   train_loss = 0.753\n",
      "Epoch 2223 Batch    1/81   train_loss = 0.779\n",
      "Epoch 2223 Batch   33/81   train_loss = 0.738\n",
      "Epoch 2223 Batch   65/81   train_loss = 0.788\n",
      "Epoch 2224 Batch   16/81   train_loss = 0.805\n",
      "Epoch 2224 Batch   48/81   train_loss = 0.761\n",
      "Epoch 2224 Batch   80/81   train_loss = 0.798\n",
      "Epoch 2225 Batch   31/81   train_loss = 0.740\n",
      "Epoch 2225 Batch   63/81   train_loss = 0.765\n",
      "Epoch 2226 Batch   14/81   train_loss = 0.794\n",
      "Epoch 2226 Batch   46/81   train_loss = 0.761\n",
      "Epoch 2226 Batch   78/81   train_loss = 0.771\n",
      "Epoch 2227 Batch   29/81   train_loss = 0.778\n",
      "Epoch 2227 Batch   61/81   train_loss = 0.774\n",
      "Epoch 2228 Batch   12/81   train_loss = 0.768\n",
      "Epoch 2228 Batch   44/81   train_loss = 0.763\n",
      "Epoch 2228 Batch   76/81   train_loss = 0.793\n",
      "Epoch 2229 Batch   27/81   train_loss = 0.770\n",
      "Epoch 2229 Batch   59/81   train_loss = 0.754\n",
      "Epoch 2230 Batch   10/81   train_loss = 0.794\n",
      "Epoch 2230 Batch   42/81   train_loss = 0.757\n",
      "Epoch 2230 Batch   74/81   train_loss = 0.760\n",
      "Epoch 2231 Batch   25/81   train_loss = 0.779\n",
      "Epoch 2231 Batch   57/81   train_loss = 0.779\n",
      "Epoch 2232 Batch    8/81   train_loss = 0.775\n",
      "Epoch 2232 Batch   40/81   train_loss = 0.755\n",
      "Epoch 2232 Batch   72/81   train_loss = 0.752\n",
      "Epoch 2233 Batch   23/81   train_loss = 0.754\n",
      "Epoch 2233 Batch   55/81   train_loss = 0.805\n",
      "Epoch 2234 Batch    6/81   train_loss = 0.756\n",
      "Epoch 2234 Batch   38/81   train_loss = 0.788\n",
      "Epoch 2234 Batch   70/81   train_loss = 0.746\n",
      "Epoch 2235 Batch   21/81   train_loss = 0.815\n",
      "Epoch 2235 Batch   53/81   train_loss = 0.753\n",
      "Epoch 2236 Batch    4/81   train_loss = 0.783\n",
      "Epoch 2236 Batch   36/81   train_loss = 0.803\n",
      "Epoch 2236 Batch   68/81   train_loss = 0.781\n",
      "Epoch 2237 Batch   19/81   train_loss = 0.755\n",
      "Epoch 2237 Batch   51/81   train_loss = 0.800\n",
      "Epoch 2238 Batch    2/81   train_loss = 0.772\n",
      "Epoch 2238 Batch   34/81   train_loss = 0.785\n",
      "Epoch 2238 Batch   66/81   train_loss = 0.766\n",
      "Epoch 2239 Batch   17/81   train_loss = 0.799\n",
      "Epoch 2239 Batch   49/81   train_loss = 0.773\n",
      "Epoch 2240 Batch    0/81   train_loss = 0.783\n",
      "Epoch 2240 Batch   32/81   train_loss = 0.763\n",
      "Epoch 2240 Batch   64/81   train_loss = 0.774\n",
      "Epoch 2241 Batch   15/81   train_loss = 0.765\n",
      "Epoch 2241 Batch   47/81   train_loss = 0.752\n",
      "Epoch 2241 Batch   79/81   train_loss = 0.777\n",
      "Epoch 2242 Batch   30/81   train_loss = 0.770\n",
      "Epoch 2242 Batch   62/81   train_loss = 0.755\n",
      "Epoch 2243 Batch   13/81   train_loss = 0.767\n",
      "Epoch 2243 Batch   45/81   train_loss = 0.764\n",
      "Epoch 2243 Batch   77/81   train_loss = 0.770\n",
      "Epoch 2244 Batch   28/81   train_loss = 0.755\n",
      "Epoch 2244 Batch   60/81   train_loss = 0.769\n",
      "Epoch 2245 Batch   11/81   train_loss = 0.780\n",
      "Epoch 2245 Batch   43/81   train_loss = 0.730\n",
      "Epoch 2245 Batch   75/81   train_loss = 0.752\n",
      "Epoch 2246 Batch   26/81   train_loss = 0.752\n",
      "Epoch 2246 Batch   58/81   train_loss = 0.737\n",
      "Epoch 2247 Batch    9/81   train_loss = 0.794\n",
      "Epoch 2247 Batch   41/81   train_loss = 0.722\n",
      "Epoch 2247 Batch   73/81   train_loss = 0.775\n",
      "Epoch 2248 Batch   24/81   train_loss = 0.752\n",
      "Epoch 2248 Batch   56/81   train_loss = 0.773\n",
      "Epoch 2249 Batch    7/81   train_loss = 0.750\n",
      "Epoch 2249 Batch   39/81   train_loss = 0.743\n",
      "Epoch 2249 Batch   71/81   train_loss = 0.762\n",
      "Epoch 2250 Batch   22/81   train_loss = 0.797\n",
      "Epoch 2250 Batch   54/81   train_loss = 0.765\n",
      "Epoch 2251 Batch    5/81   train_loss = 0.791\n",
      "Epoch 2251 Batch   37/81   train_loss = 0.766\n",
      "Epoch 2251 Batch   69/81   train_loss = 0.760\n",
      "Epoch 2252 Batch   20/81   train_loss = 0.745\n",
      "Epoch 2252 Batch   52/81   train_loss = 0.799\n",
      "Epoch 2253 Batch    3/81   train_loss = 0.750\n",
      "Epoch 2253 Batch   35/81   train_loss = 0.754\n",
      "Epoch 2253 Batch   67/81   train_loss = 0.769\n",
      "Epoch 2254 Batch   18/81   train_loss = 0.782\n",
      "Epoch 2254 Batch   50/81   train_loss = 0.751\n",
      "Epoch 2255 Batch    1/81   train_loss = 0.767\n",
      "Epoch 2255 Batch   33/81   train_loss = 0.726\n",
      "Epoch 2255 Batch   65/81   train_loss = 0.788\n",
      "Epoch 2256 Batch   16/81   train_loss = 0.796\n",
      "Epoch 2256 Batch   48/81   train_loss = 0.763\n",
      "Epoch 2256 Batch   80/81   train_loss = 0.781\n",
      "Epoch 2257 Batch   31/81   train_loss = 0.737\n",
      "Epoch 2257 Batch   63/81   train_loss = 0.768\n",
      "Epoch 2258 Batch   14/81   train_loss = 0.759\n",
      "Epoch 2258 Batch   46/81   train_loss = 0.764\n",
      "Epoch 2258 Batch   78/81   train_loss = 0.761\n",
      "Epoch 2259 Batch   29/81   train_loss = 0.762\n",
      "Epoch 2259 Batch   61/81   train_loss = 0.769\n",
      "Epoch 2260 Batch   12/81   train_loss = 0.745\n",
      "Epoch 2260 Batch   44/81   train_loss = 0.769\n",
      "Epoch 2260 Batch   76/81   train_loss = 0.783\n",
      "Epoch 2261 Batch   27/81   train_loss = 0.753\n",
      "Epoch 2261 Batch   59/81   train_loss = 0.745\n",
      "Epoch 2262 Batch   10/81   train_loss = 0.768\n",
      "Epoch 2262 Batch   42/81   train_loss = 0.765\n",
      "Epoch 2262 Batch   74/81   train_loss = 0.761\n",
      "Epoch 2263 Batch   25/81   train_loss = 0.777\n",
      "Epoch 2263 Batch   57/81   train_loss = 0.773\n",
      "Epoch 2264 Batch    8/81   train_loss = 0.783\n",
      "Epoch 2264 Batch   40/81   train_loss = 0.764\n",
      "Epoch 2264 Batch   72/81   train_loss = 0.746\n",
      "Epoch 2265 Batch   23/81   train_loss = 0.751\n",
      "Epoch 2265 Batch   55/81   train_loss = 0.766\n",
      "Epoch 2266 Batch    6/81   train_loss = 0.775\n",
      "Epoch 2266 Batch   38/81   train_loss = 0.780\n",
      "Epoch 2266 Batch   70/81   train_loss = 0.755\n",
      "Epoch 2267 Batch   21/81   train_loss = 0.808\n",
      "Epoch 2267 Batch   53/81   train_loss = 0.755\n",
      "Epoch 2268 Batch    4/81   train_loss = 0.802\n",
      "Epoch 2268 Batch   36/81   train_loss = 0.793\n",
      "Epoch 2268 Batch   68/81   train_loss = 0.788\n",
      "Epoch 2269 Batch   19/81   train_loss = 0.760\n",
      "Epoch 2269 Batch   51/81   train_loss = 0.804\n",
      "Epoch 2270 Batch    2/81   train_loss = 0.808\n",
      "Epoch 2270 Batch   34/81   train_loss = 0.795\n",
      "Epoch 2270 Batch   66/81   train_loss = 0.784\n",
      "Epoch 2271 Batch   17/81   train_loss = 0.793\n",
      "Epoch 2271 Batch   49/81   train_loss = 0.784\n",
      "Epoch 2272 Batch    0/81   train_loss = 0.790\n",
      "Epoch 2272 Batch   32/81   train_loss = 0.757\n",
      "Epoch 2272 Batch   64/81   train_loss = 0.782\n",
      "Epoch 2273 Batch   15/81   train_loss = 0.773\n",
      "Epoch 2273 Batch   47/81   train_loss = 0.753\n",
      "Epoch 2273 Batch   79/81   train_loss = 0.781\n",
      "Epoch 2274 Batch   30/81   train_loss = 0.777\n",
      "Epoch 2274 Batch   62/81   train_loss = 0.783\n",
      "Epoch 2275 Batch   13/81   train_loss = 0.830\n",
      "Epoch 2275 Batch   45/81   train_loss = 0.777\n",
      "Epoch 2275 Batch   77/81   train_loss = 0.795\n",
      "Epoch 2276 Batch   28/81   train_loss = 0.767\n",
      "Epoch 2276 Batch   60/81   train_loss = 0.778\n",
      "Epoch 2277 Batch   11/81   train_loss = 0.783\n",
      "Epoch 2277 Batch   43/81   train_loss = 0.749\n",
      "Epoch 2277 Batch   75/81   train_loss = 0.768\n",
      "Epoch 2278 Batch   26/81   train_loss = 0.769\n",
      "Epoch 2278 Batch   58/81   train_loss = 0.745\n",
      "Epoch 2279 Batch    9/81   train_loss = 0.800\n",
      "Epoch 2279 Batch   41/81   train_loss = 0.729\n",
      "Epoch 2279 Batch   73/81   train_loss = 0.774\n",
      "Epoch 2280 Batch   24/81   train_loss = 0.762\n",
      "Epoch 2280 Batch   56/81   train_loss = 0.767\n",
      "Epoch 2281 Batch    7/81   train_loss = 0.754\n",
      "Epoch 2281 Batch   39/81   train_loss = 0.733\n",
      "Epoch 2281 Batch   71/81   train_loss = 0.748\n",
      "Epoch 2282 Batch   22/81   train_loss = 0.793\n",
      "Epoch 2282 Batch   54/81   train_loss = 0.750\n",
      "Epoch 2283 Batch    5/81   train_loss = 0.799\n",
      "Epoch 2283 Batch   37/81   train_loss = 0.764\n",
      "Epoch 2283 Batch   69/81   train_loss = 0.760\n",
      "Epoch 2284 Batch   20/81   train_loss = 0.771\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2284 Batch   52/81   train_loss = 0.805\n",
      "Epoch 2285 Batch    3/81   train_loss = 0.765\n",
      "Epoch 2285 Batch   35/81   train_loss = 0.759\n",
      "Epoch 2285 Batch   67/81   train_loss = 0.785\n",
      "Epoch 2286 Batch   18/81   train_loss = 0.792\n",
      "Epoch 2286 Batch   50/81   train_loss = 0.776\n",
      "Epoch 2287 Batch    1/81   train_loss = 0.777\n",
      "Epoch 2287 Batch   33/81   train_loss = 0.766\n",
      "Epoch 2287 Batch   65/81   train_loss = 0.796\n",
      "Epoch 2288 Batch   16/81   train_loss = 0.802\n",
      "Epoch 2288 Batch   48/81   train_loss = 0.775\n",
      "Epoch 2288 Batch   80/81   train_loss = 0.786\n",
      "Epoch 2289 Batch   31/81   train_loss = 0.756\n",
      "Epoch 2289 Batch   63/81   train_loss = 0.760\n",
      "Epoch 2290 Batch   14/81   train_loss = 0.776\n",
      "Epoch 2290 Batch   46/81   train_loss = 0.751\n",
      "Epoch 2290 Batch   78/81   train_loss = 0.778\n",
      "Epoch 2291 Batch   29/81   train_loss = 0.758\n",
      "Epoch 2291 Batch   61/81   train_loss = 0.778\n",
      "Epoch 2292 Batch   12/81   train_loss = 0.772\n",
      "Epoch 2292 Batch   44/81   train_loss = 0.850\n",
      "Epoch 2292 Batch   76/81   train_loss = 0.836\n",
      "Epoch 2293 Batch   27/81   train_loss = 0.787\n",
      "Epoch 2293 Batch   59/81   train_loss = 0.768\n",
      "Epoch 2294 Batch   10/81   train_loss = 0.797\n",
      "Epoch 2294 Batch   42/81   train_loss = 0.795\n",
      "Epoch 2294 Batch   74/81   train_loss = 0.777\n",
      "Epoch 2295 Batch   25/81   train_loss = 0.777\n",
      "Epoch 2295 Batch   57/81   train_loss = 0.776\n",
      "Epoch 2296 Batch    8/81   train_loss = 0.774\n",
      "Epoch 2296 Batch   40/81   train_loss = 0.763\n",
      "Epoch 2296 Batch   72/81   train_loss = 0.729\n",
      "Epoch 2297 Batch   23/81   train_loss = 0.764\n",
      "Epoch 2297 Batch   55/81   train_loss = 0.771\n",
      "Epoch 2298 Batch    6/81   train_loss = 0.755\n",
      "Epoch 2298 Batch   38/81   train_loss = 0.794\n",
      "Epoch 2298 Batch   70/81   train_loss = 0.772\n",
      "Epoch 2299 Batch   21/81   train_loss = 0.802\n",
      "Epoch 2299 Batch   53/81   train_loss = 0.745\n",
      "Epoch 2300 Batch    4/81   train_loss = 0.778\n",
      "Epoch 2300 Batch   36/81   train_loss = 0.781\n",
      "Epoch 2300 Batch   68/81   train_loss = 0.774\n",
      "Epoch 2301 Batch   19/81   train_loss = 0.750\n",
      "Epoch 2301 Batch   51/81   train_loss = 0.802\n",
      "Epoch 2302 Batch    2/81   train_loss = 0.792\n",
      "Epoch 2302 Batch   34/81   train_loss = 0.799\n",
      "Epoch 2302 Batch   66/81   train_loss = 0.783\n",
      "Epoch 2303 Batch   17/81   train_loss = 0.778\n",
      "Epoch 2303 Batch   49/81   train_loss = 0.769\n",
      "Epoch 2304 Batch    0/81   train_loss = 0.758\n",
      "Epoch 2304 Batch   32/81   train_loss = 0.751\n",
      "Epoch 2304 Batch   64/81   train_loss = 0.766\n",
      "Epoch 2305 Batch   15/81   train_loss = 0.766\n",
      "Epoch 2305 Batch   47/81   train_loss = 0.740\n",
      "Epoch 2305 Batch   79/81   train_loss = 0.754\n",
      "Epoch 2306 Batch   30/81   train_loss = 0.767\n",
      "Epoch 2306 Batch   62/81   train_loss = 0.762\n",
      "Epoch 2307 Batch   13/81   train_loss = 0.786\n",
      "Epoch 2307 Batch   45/81   train_loss = 0.783\n",
      "Epoch 2307 Batch   77/81   train_loss = 0.783\n",
      "Epoch 2308 Batch   28/81   train_loss = 0.738\n",
      "Epoch 2308 Batch   60/81   train_loss = 0.764\n",
      "Epoch 2309 Batch   11/81   train_loss = 0.751\n",
      "Epoch 2309 Batch   43/81   train_loss = 0.744\n",
      "Epoch 2309 Batch   75/81   train_loss = 0.746\n",
      "Epoch 2310 Batch   26/81   train_loss = 0.766\n",
      "Epoch 2310 Batch   58/81   train_loss = 0.741\n",
      "Epoch 2311 Batch    9/81   train_loss = 0.795\n",
      "Epoch 2311 Batch   41/81   train_loss = 0.752\n",
      "Epoch 2311 Batch   73/81   train_loss = 0.764\n",
      "Epoch 2312 Batch   24/81   train_loss = 0.752\n",
      "Epoch 2312 Batch   56/81   train_loss = 0.771\n",
      "Epoch 2313 Batch    7/81   train_loss = 0.768\n",
      "Epoch 2313 Batch   39/81   train_loss = 0.742\n",
      "Epoch 2313 Batch   71/81   train_loss = 0.757\n",
      "Epoch 2314 Batch   22/81   train_loss = 0.798\n",
      "Epoch 2314 Batch   54/81   train_loss = 0.768\n",
      "Epoch 2315 Batch    5/81   train_loss = 0.790\n",
      "Epoch 2315 Batch   37/81   train_loss = 0.773\n",
      "Epoch 2315 Batch   69/81   train_loss = 0.774\n",
      "Epoch 2316 Batch   20/81   train_loss = 0.760\n",
      "Epoch 2316 Batch   52/81   train_loss = 0.816\n",
      "Epoch 2317 Batch    3/81   train_loss = 0.762\n",
      "Epoch 2317 Batch   35/81   train_loss = 0.785\n",
      "Epoch 2317 Batch   67/81   train_loss = 0.802\n",
      "Epoch 2318 Batch   18/81   train_loss = 0.811\n",
      "Epoch 2318 Batch   50/81   train_loss = 0.771\n",
      "Epoch 2319 Batch    1/81   train_loss = 0.802\n",
      "Epoch 2319 Batch   33/81   train_loss = 0.769\n",
      "Epoch 2319 Batch   65/81   train_loss = 0.795\n",
      "Epoch 2320 Batch   16/81   train_loss = 0.803\n",
      "Epoch 2320 Batch   48/81   train_loss = 0.780\n",
      "Epoch 2320 Batch   80/81   train_loss = 0.795\n",
      "Epoch 2321 Batch   31/81   train_loss = 0.757\n",
      "Epoch 2321 Batch   63/81   train_loss = 0.781\n",
      "Epoch 2322 Batch   14/81   train_loss = 0.789\n",
      "Epoch 2322 Batch   46/81   train_loss = 0.784\n",
      "Epoch 2322 Batch   78/81   train_loss = 0.780\n",
      "Epoch 2323 Batch   29/81   train_loss = 0.766\n",
      "Epoch 2323 Batch   61/81   train_loss = 0.774\n",
      "Epoch 2324 Batch   12/81   train_loss = 0.758\n",
      "Epoch 2324 Batch   44/81   train_loss = 0.783\n",
      "Epoch 2324 Batch   76/81   train_loss = 0.781\n",
      "Epoch 2325 Batch   27/81   train_loss = 0.772\n",
      "Epoch 2325 Batch   59/81   train_loss = 0.779\n",
      "Epoch 2326 Batch   10/81   train_loss = 0.797\n",
      "Epoch 2326 Batch   42/81   train_loss = 0.765\n",
      "Epoch 2326 Batch   74/81   train_loss = 0.762\n",
      "Epoch 2327 Batch   25/81   train_loss = 0.773\n",
      "Epoch 2327 Batch   57/81   train_loss = 0.788\n",
      "Epoch 2328 Batch    8/81   train_loss = 0.788\n",
      "Epoch 2328 Batch   40/81   train_loss = 0.759\n",
      "Epoch 2328 Batch   72/81   train_loss = 0.749\n",
      "Epoch 2329 Batch   23/81   train_loss = 0.761\n",
      "Epoch 2329 Batch   55/81   train_loss = 0.772\n",
      "Epoch 2330 Batch    6/81   train_loss = 0.775\n",
      "Epoch 2330 Batch   38/81   train_loss = 0.783\n",
      "Epoch 2330 Batch   70/81   train_loss = 0.769\n",
      "Epoch 2331 Batch   21/81   train_loss = 0.803\n",
      "Epoch 2331 Batch   53/81   train_loss = 0.759\n",
      "Epoch 2332 Batch    4/81   train_loss = 0.768\n",
      "Epoch 2332 Batch   36/81   train_loss = 0.794\n",
      "Epoch 2332 Batch   68/81   train_loss = 0.784\n",
      "Epoch 2333 Batch   19/81   train_loss = 0.758\n",
      "Epoch 2333 Batch   51/81   train_loss = 0.802\n",
      "Epoch 2334 Batch    2/81   train_loss = 0.787\n",
      "Epoch 2334 Batch   34/81   train_loss = 0.798\n",
      "Epoch 2334 Batch   66/81   train_loss = 0.784\n",
      "Epoch 2335 Batch   17/81   train_loss = 0.796\n",
      "Epoch 2335 Batch   49/81   train_loss = 0.813\n",
      "Epoch 2336 Batch    0/81   train_loss = 0.795\n",
      "Epoch 2336 Batch   32/81   train_loss = 0.760\n",
      "Epoch 2336 Batch   64/81   train_loss = 0.788\n",
      "Epoch 2337 Batch   15/81   train_loss = 0.773\n",
      "Epoch 2337 Batch   47/81   train_loss = 0.761\n",
      "Epoch 2337 Batch   79/81   train_loss = 0.765\n",
      "Epoch 2338 Batch   30/81   train_loss = 0.778\n",
      "Epoch 2338 Batch   62/81   train_loss = 0.760\n",
      "Epoch 2339 Batch   13/81   train_loss = 0.782\n",
      "Epoch 2339 Batch   45/81   train_loss = 0.779\n",
      "Epoch 2339 Batch   77/81   train_loss = 0.783\n",
      "Epoch 2340 Batch   28/81   train_loss = 0.739\n",
      "Epoch 2340 Batch   60/81   train_loss = 0.772\n",
      "Epoch 2341 Batch   11/81   train_loss = 0.774\n",
      "Epoch 2341 Batch   43/81   train_loss = 0.751\n",
      "Epoch 2341 Batch   75/81   train_loss = 0.753\n",
      "Epoch 2342 Batch   26/81   train_loss = 0.758\n",
      "Epoch 2342 Batch   58/81   train_loss = 0.750\n",
      "Epoch 2343 Batch    9/81   train_loss = 0.803\n",
      "Epoch 2343 Batch   41/81   train_loss = 0.734\n",
      "Epoch 2343 Batch   73/81   train_loss = 0.786\n",
      "Epoch 2344 Batch   24/81   train_loss = 0.773\n",
      "Epoch 2344 Batch   56/81   train_loss = 0.783\n",
      "Epoch 2345 Batch    7/81   train_loss = 0.785\n",
      "Epoch 2345 Batch   39/81   train_loss = 0.755\n",
      "Epoch 2345 Batch   71/81   train_loss = 0.763\n",
      "Epoch 2346 Batch   22/81   train_loss = 0.802\n",
      "Epoch 2346 Batch   54/81   train_loss = 0.778\n",
      "Epoch 2347 Batch    5/81   train_loss = 0.803\n",
      "Epoch 2347 Batch   37/81   train_loss = 0.768\n",
      "Epoch 2347 Batch   69/81   train_loss = 0.787\n",
      "Epoch 2348 Batch   20/81   train_loss = 0.761\n",
      "Epoch 2348 Batch   52/81   train_loss = 0.811\n",
      "Epoch 2349 Batch    3/81   train_loss = 0.773\n",
      "Epoch 2349 Batch   35/81   train_loss = 0.782\n",
      "Epoch 2349 Batch   67/81   train_loss = 0.791\n",
      "Epoch 2350 Batch   18/81   train_loss = 0.785\n",
      "Epoch 2350 Batch   50/81   train_loss = 0.760\n",
      "Epoch 2351 Batch    1/81   train_loss = 0.800\n",
      "Epoch 2351 Batch   33/81   train_loss = 0.761\n",
      "Epoch 2351 Batch   65/81   train_loss = 0.809\n",
      "Epoch 2352 Batch   16/81   train_loss = 0.818\n",
      "Epoch 2352 Batch   48/81   train_loss = 0.807\n",
      "Epoch 2352 Batch   80/81   train_loss = 0.811\n",
      "Epoch 2353 Batch   31/81   train_loss = 0.769\n",
      "Epoch 2353 Batch   63/81   train_loss = 0.785\n",
      "Epoch 2354 Batch   14/81   train_loss = 0.800\n",
      "Epoch 2354 Batch   46/81   train_loss = 0.782\n",
      "Epoch 2354 Batch   78/81   train_loss = 0.796\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2355 Batch   29/81   train_loss = 0.774\n",
      "Epoch 2355 Batch   61/81   train_loss = 0.780\n",
      "Epoch 2356 Batch   12/81   train_loss = 0.779\n",
      "Epoch 2356 Batch   44/81   train_loss = 0.773\n",
      "Epoch 2356 Batch   76/81   train_loss = 0.791\n",
      "Epoch 2357 Batch   27/81   train_loss = 0.760\n",
      "Epoch 2357 Batch   59/81   train_loss = 0.777\n",
      "Epoch 2358 Batch   10/81   train_loss = 0.782\n",
      "Epoch 2358 Batch   42/81   train_loss = 0.769\n",
      "Epoch 2358 Batch   74/81   train_loss = 0.776\n",
      "Epoch 2359 Batch   25/81   train_loss = 0.773\n",
      "Epoch 2359 Batch   57/81   train_loss = 0.783\n",
      "Epoch 2360 Batch    8/81   train_loss = 0.792\n",
      "Epoch 2360 Batch   40/81   train_loss = 0.753\n",
      "Epoch 2360 Batch   72/81   train_loss = 0.764\n",
      "Epoch 2361 Batch   23/81   train_loss = 0.766\n",
      "Epoch 2361 Batch   55/81   train_loss = 0.811\n",
      "Epoch 2362 Batch    6/81   train_loss = 0.767\n",
      "Epoch 2362 Batch   38/81   train_loss = 0.793\n",
      "Epoch 2362 Batch   70/81   train_loss = 0.762\n",
      "Epoch 2363 Batch   21/81   train_loss = 0.821\n",
      "Epoch 2363 Batch   53/81   train_loss = 0.758\n",
      "Epoch 2364 Batch    4/81   train_loss = 0.792\n",
      "Epoch 2364 Batch   36/81   train_loss = 0.817\n",
      "Epoch 2364 Batch   68/81   train_loss = 0.816\n",
      "Epoch 2365 Batch   19/81   train_loss = 0.750\n",
      "Epoch 2365 Batch   51/81   train_loss = 0.802\n",
      "Epoch 2366 Batch    2/81   train_loss = 0.797\n",
      "Epoch 2366 Batch   34/81   train_loss = 0.805\n",
      "Epoch 2366 Batch   66/81   train_loss = 0.785\n",
      "Epoch 2367 Batch   17/81   train_loss = 0.787\n",
      "Epoch 2367 Batch   49/81   train_loss = 0.792\n",
      "Epoch 2368 Batch    0/81   train_loss = 0.785\n",
      "Epoch 2368 Batch   32/81   train_loss = 0.765\n",
      "Epoch 2368 Batch   64/81   train_loss = 0.765\n",
      "Epoch 2369 Batch   15/81   train_loss = 0.778\n",
      "Epoch 2369 Batch   47/81   train_loss = 0.774\n",
      "Epoch 2369 Batch   79/81   train_loss = 0.791\n",
      "Epoch 2370 Batch   30/81   train_loss = 0.766\n",
      "Epoch 2370 Batch   62/81   train_loss = 0.785\n",
      "Epoch 2371 Batch   13/81   train_loss = 0.783\n",
      "Epoch 2371 Batch   45/81   train_loss = 0.763\n",
      "Epoch 2371 Batch   77/81   train_loss = 0.800\n",
      "Epoch 2372 Batch   28/81   train_loss = 0.748\n",
      "Epoch 2372 Batch   60/81   train_loss = 0.782\n",
      "Epoch 2373 Batch   11/81   train_loss = 0.780\n",
      "Epoch 2373 Batch   43/81   train_loss = 0.748\n",
      "Epoch 2373 Batch   75/81   train_loss = 0.767\n",
      "Epoch 2374 Batch   26/81   train_loss = 0.767\n",
      "Epoch 2374 Batch   58/81   train_loss = 0.774\n",
      "Epoch 2375 Batch    9/81   train_loss = 0.811\n",
      "Epoch 2375 Batch   41/81   train_loss = 0.718\n",
      "Epoch 2375 Batch   73/81   train_loss = 0.782\n",
      "Epoch 2376 Batch   24/81   train_loss = 0.758\n",
      "Epoch 2376 Batch   56/81   train_loss = 0.796\n",
      "Epoch 2377 Batch    7/81   train_loss = 0.796\n",
      "Epoch 2377 Batch   39/81   train_loss = 0.732\n",
      "Epoch 2377 Batch   71/81   train_loss = 0.750\n",
      "Epoch 2378 Batch   22/81   train_loss = 0.802\n",
      "Epoch 2378 Batch   54/81   train_loss = 0.768\n",
      "Epoch 2379 Batch    5/81   train_loss = 0.829\n",
      "Epoch 2379 Batch   37/81   train_loss = 0.777\n",
      "Epoch 2379 Batch   69/81   train_loss = 0.776\n",
      "Epoch 2380 Batch   20/81   train_loss = 0.755\n",
      "Epoch 2380 Batch   52/81   train_loss = 0.794\n",
      "Epoch 2381 Batch    3/81   train_loss = 0.763\n",
      "Epoch 2381 Batch   35/81   train_loss = 0.760\n",
      "Epoch 2381 Batch   67/81   train_loss = 0.804\n",
      "Epoch 2382 Batch   18/81   train_loss = 0.787\n",
      "Epoch 2382 Batch   50/81   train_loss = 0.752\n",
      "Epoch 2383 Batch    1/81   train_loss = 0.794\n",
      "Epoch 2383 Batch   33/81   train_loss = 0.774\n",
      "Epoch 2383 Batch   65/81   train_loss = 0.814\n",
      "Epoch 2384 Batch   16/81   train_loss = 0.846\n",
      "Epoch 2384 Batch   48/81   train_loss = 0.786\n",
      "Epoch 2384 Batch   80/81   train_loss = 0.807\n",
      "Epoch 2385 Batch   31/81   train_loss = 0.764\n",
      "Epoch 2385 Batch   63/81   train_loss = 0.801\n",
      "Epoch 2386 Batch   14/81   train_loss = 0.803\n",
      "Epoch 2386 Batch   46/81   train_loss = 0.766\n",
      "Epoch 2386 Batch   78/81   train_loss = 0.786\n",
      "Epoch 2387 Batch   29/81   train_loss = 0.778\n",
      "Epoch 2387 Batch   61/81   train_loss = 0.782\n",
      "Epoch 2388 Batch   12/81   train_loss = 0.790\n",
      "Epoch 2388 Batch   44/81   train_loss = 0.812\n",
      "Epoch 2388 Batch   76/81   train_loss = 0.792\n",
      "Epoch 2389 Batch   27/81   train_loss = 0.746\n",
      "Epoch 2389 Batch   59/81   train_loss = 0.750\n",
      "Epoch 2390 Batch   10/81   train_loss = 0.801\n",
      "Epoch 2390 Batch   42/81   train_loss = 0.764\n",
      "Epoch 2390 Batch   74/81   train_loss = 0.781\n",
      "Epoch 2391 Batch   25/81   train_loss = 0.794\n",
      "Epoch 2391 Batch   57/81   train_loss = 0.781\n",
      "Epoch 2392 Batch    8/81   train_loss = 0.791\n",
      "Epoch 2392 Batch   40/81   train_loss = 0.764\n",
      "Epoch 2392 Batch   72/81   train_loss = 0.777\n",
      "Epoch 2393 Batch   23/81   train_loss = 0.770\n",
      "Epoch 2393 Batch   55/81   train_loss = 0.778\n",
      "Epoch 2394 Batch    6/81   train_loss = 0.766\n",
      "Epoch 2394 Batch   38/81   train_loss = 0.788\n",
      "Epoch 2394 Batch   70/81   train_loss = 0.767\n",
      "Epoch 2395 Batch   21/81   train_loss = 0.825\n",
      "Epoch 2395 Batch   53/81   train_loss = 0.759\n",
      "Epoch 2396 Batch    4/81   train_loss = 0.794\n",
      "Epoch 2396 Batch   36/81   train_loss = 0.806\n",
      "Epoch 2396 Batch   68/81   train_loss = 0.802\n",
      "Epoch 2397 Batch   19/81   train_loss = 0.771\n",
      "Epoch 2397 Batch   51/81   train_loss = 0.800\n",
      "Epoch 2398 Batch    2/81   train_loss = 0.789\n",
      "Epoch 2398 Batch   34/81   train_loss = 0.792\n",
      "Epoch 2398 Batch   66/81   train_loss = 0.776\n",
      "Epoch 2399 Batch   17/81   train_loss = 0.801\n",
      "Epoch 2399 Batch   49/81   train_loss = 0.775\n",
      "Epoch 2400 Batch    0/81   train_loss = 0.766\n",
      "Epoch 2400 Batch   32/81   train_loss = 0.758\n",
      "Epoch 2400 Batch   64/81   train_loss = 0.775\n",
      "Epoch 2401 Batch   15/81   train_loss = 0.762\n",
      "Epoch 2401 Batch   47/81   train_loss = 0.773\n",
      "Epoch 2401 Batch   79/81   train_loss = 0.774\n",
      "Epoch 2402 Batch   30/81   train_loss = 0.747\n",
      "Epoch 2402 Batch   62/81   train_loss = 0.770\n",
      "Epoch 2403 Batch   13/81   train_loss = 0.799\n",
      "Epoch 2403 Batch   45/81   train_loss = 0.777\n",
      "Epoch 2403 Batch   77/81   train_loss = 0.784\n",
      "Epoch 2404 Batch   28/81   train_loss = 0.733\n",
      "Epoch 2404 Batch   60/81   train_loss = 0.760\n",
      "Epoch 2405 Batch   11/81   train_loss = 0.760\n",
      "Epoch 2405 Batch   43/81   train_loss = 0.749\n",
      "Epoch 2405 Batch   75/81   train_loss = 0.766\n",
      "Epoch 2406 Batch   26/81   train_loss = 0.765\n",
      "Epoch 2406 Batch   58/81   train_loss = 0.748\n",
      "Epoch 2407 Batch    9/81   train_loss = 0.789\n",
      "Epoch 2407 Batch   41/81   train_loss = 0.739\n",
      "Epoch 2407 Batch   73/81   train_loss = 0.778\n",
      "Epoch 2408 Batch   24/81   train_loss = 0.748\n",
      "Epoch 2408 Batch   56/81   train_loss = 0.779\n",
      "Epoch 2409 Batch    7/81   train_loss = 0.783\n",
      "Epoch 2409 Batch   39/81   train_loss = 0.725\n",
      "Epoch 2409 Batch   71/81   train_loss = 0.740\n",
      "Epoch 2410 Batch   22/81   train_loss = 0.801\n",
      "Epoch 2410 Batch   54/81   train_loss = 0.780\n",
      "Epoch 2411 Batch    5/81   train_loss = 0.798\n",
      "Epoch 2411 Batch   37/81   train_loss = 0.770\n",
      "Epoch 2411 Batch   69/81   train_loss = 0.757\n",
      "Epoch 2412 Batch   20/81   train_loss = 0.759\n",
      "Epoch 2412 Batch   52/81   train_loss = 0.816\n",
      "Epoch 2413 Batch    3/81   train_loss = 0.773\n",
      "Epoch 2413 Batch   35/81   train_loss = 0.767\n",
      "Epoch 2413 Batch   67/81   train_loss = 0.779\n",
      "Epoch 2414 Batch   18/81   train_loss = 0.792\n",
      "Epoch 2414 Batch   50/81   train_loss = 0.751\n",
      "Epoch 2415 Batch    1/81   train_loss = 0.791\n",
      "Epoch 2415 Batch   33/81   train_loss = 0.765\n",
      "Epoch 2415 Batch   65/81   train_loss = 0.809\n",
      "Epoch 2416 Batch   16/81   train_loss = 0.829\n",
      "Epoch 2416 Batch   48/81   train_loss = 0.777\n",
      "Epoch 2416 Batch   80/81   train_loss = 0.793\n",
      "Epoch 2417 Batch   31/81   train_loss = 0.760\n",
      "Epoch 2417 Batch   63/81   train_loss = 0.780\n",
      "Epoch 2418 Batch   14/81   train_loss = 0.785\n",
      "Epoch 2418 Batch   46/81   train_loss = 0.769\n",
      "Epoch 2418 Batch   78/81   train_loss = 0.786\n",
      "Epoch 2419 Batch   29/81   train_loss = 0.771\n",
      "Epoch 2419 Batch   61/81   train_loss = 0.782\n",
      "Epoch 2420 Batch   12/81   train_loss = 0.771\n",
      "Epoch 2420 Batch   44/81   train_loss = 0.781\n",
      "Epoch 2420 Batch   76/81   train_loss = 0.790\n",
      "Epoch 2421 Batch   27/81   train_loss = 0.744\n",
      "Epoch 2421 Batch   59/81   train_loss = 0.746\n",
      "Epoch 2422 Batch   10/81   train_loss = 0.792\n",
      "Epoch 2422 Batch   42/81   train_loss = 0.769\n",
      "Epoch 2422 Batch   74/81   train_loss = 0.753\n",
      "Epoch 2423 Batch   25/81   train_loss = 0.768\n",
      "Epoch 2423 Batch   57/81   train_loss = 0.785\n",
      "Epoch 2424 Batch    8/81   train_loss = 0.779\n",
      "Epoch 2424 Batch   40/81   train_loss = 0.763\n",
      "Epoch 2424 Batch   72/81   train_loss = 0.751\n",
      "Epoch 2425 Batch   23/81   train_loss = 0.753\n",
      "Epoch 2425 Batch   55/81   train_loss = 0.770\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2426 Batch    6/81   train_loss = 0.758\n",
      "Epoch 2426 Batch   38/81   train_loss = 0.791\n",
      "Epoch 2426 Batch   70/81   train_loss = 0.767\n",
      "Epoch 2427 Batch   21/81   train_loss = 0.813\n",
      "Epoch 2427 Batch   53/81   train_loss = 0.745\n",
      "Epoch 2428 Batch    4/81   train_loss = 0.796\n",
      "Epoch 2428 Batch   36/81   train_loss = 0.805\n",
      "Epoch 2428 Batch   68/81   train_loss = 0.790\n",
      "Epoch 2429 Batch   19/81   train_loss = 0.741\n",
      "Epoch 2429 Batch   51/81   train_loss = 0.804\n",
      "Epoch 2430 Batch    2/81   train_loss = 0.803\n",
      "Epoch 2430 Batch   34/81   train_loss = 0.819\n",
      "Epoch 2430 Batch   66/81   train_loss = 0.789\n",
      "Epoch 2431 Batch   17/81   train_loss = 0.791\n",
      "Epoch 2431 Batch   49/81   train_loss = 0.788\n",
      "Epoch 2432 Batch    0/81   train_loss = 0.792\n",
      "Epoch 2432 Batch   32/81   train_loss = 0.774\n",
      "Epoch 2432 Batch   64/81   train_loss = 0.807\n",
      "Epoch 2433 Batch   15/81   train_loss = 0.778\n",
      "Epoch 2433 Batch   47/81   train_loss = 0.785\n",
      "Epoch 2433 Batch   79/81   train_loss = 0.797\n",
      "Epoch 2434 Batch   30/81   train_loss = 0.770\n",
      "Epoch 2434 Batch   62/81   train_loss = 0.808\n",
      "Epoch 2435 Batch   13/81   train_loss = 0.823\n",
      "Epoch 2435 Batch   45/81   train_loss = 0.783\n",
      "Epoch 2435 Batch   77/81   train_loss = 0.843\n",
      "Epoch 2436 Batch   28/81   train_loss = 0.769\n",
      "Epoch 2436 Batch   60/81   train_loss = 0.781\n",
      "Epoch 2437 Batch   11/81   train_loss = 0.795\n",
      "Epoch 2437 Batch   43/81   train_loss = 0.757\n",
      "Epoch 2437 Batch   75/81   train_loss = 0.778\n",
      "Epoch 2438 Batch   26/81   train_loss = 0.791\n",
      "Epoch 2438 Batch   58/81   train_loss = 0.761\n",
      "Epoch 2439 Batch    9/81   train_loss = 0.813\n",
      "Epoch 2439 Batch   41/81   train_loss = 0.758\n",
      "Epoch 2439 Batch   73/81   train_loss = 0.811\n",
      "Epoch 2440 Batch   24/81   train_loss = 0.780\n",
      "Epoch 2440 Batch   56/81   train_loss = 0.787\n",
      "Epoch 2441 Batch    7/81   train_loss = 0.791\n",
      "Epoch 2441 Batch   39/81   train_loss = 0.750\n",
      "Epoch 2441 Batch   71/81   train_loss = 0.768\n",
      "Epoch 2442 Batch   22/81   train_loss = 0.816\n",
      "Epoch 2442 Batch   54/81   train_loss = 0.772\n",
      "Epoch 2443 Batch    5/81   train_loss = 0.811\n",
      "Epoch 2443 Batch   37/81   train_loss = 0.760\n",
      "Epoch 2443 Batch   69/81   train_loss = 0.776\n",
      "Epoch 2444 Batch   20/81   train_loss = 0.771\n",
      "Epoch 2444 Batch   52/81   train_loss = 0.832\n",
      "Epoch 2445 Batch    3/81   train_loss = 0.776\n",
      "Epoch 2445 Batch   35/81   train_loss = 0.771\n",
      "Epoch 2445 Batch   67/81   train_loss = 0.796\n",
      "Epoch 2446 Batch   18/81   train_loss = 0.807\n",
      "Epoch 2446 Batch   50/81   train_loss = 0.770\n",
      "Epoch 2447 Batch    1/81   train_loss = 0.798\n",
      "Epoch 2447 Batch   33/81   train_loss = 0.765\n",
      "Epoch 2447 Batch   65/81   train_loss = 0.819\n",
      "Epoch 2448 Batch   16/81   train_loss = 0.825\n",
      "Epoch 2448 Batch   48/81   train_loss = 0.786\n",
      "Epoch 2448 Batch   80/81   train_loss = 0.793\n",
      "Epoch 2449 Batch   31/81   train_loss = 0.765\n",
      "Epoch 2449 Batch   63/81   train_loss = 0.798\n",
      "Epoch 2450 Batch   14/81   train_loss = 0.802\n",
      "Epoch 2450 Batch   46/81   train_loss = 0.778\n",
      "Epoch 2450 Batch   78/81   train_loss = 0.782\n",
      "Epoch 2451 Batch   29/81   train_loss = 0.756\n",
      "Epoch 2451 Batch   61/81   train_loss = 0.774\n",
      "Epoch 2452 Batch   12/81   train_loss = 0.803\n",
      "Epoch 2452 Batch   44/81   train_loss = 0.774\n",
      "Epoch 2452 Batch   76/81   train_loss = 0.800\n",
      "Epoch 2453 Batch   27/81   train_loss = 0.764\n",
      "Epoch 2453 Batch   59/81   train_loss = 0.781\n",
      "Epoch 2454 Batch   10/81   train_loss = 0.798\n",
      "Epoch 2454 Batch   42/81   train_loss = 0.792\n",
      "Epoch 2454 Batch   74/81   train_loss = 0.779\n",
      "Epoch 2455 Batch   25/81   train_loss = 0.804\n",
      "Epoch 2455 Batch   57/81   train_loss = 0.796\n",
      "Epoch 2456 Batch    8/81   train_loss = 0.793\n",
      "Epoch 2456 Batch   40/81   train_loss = 0.775\n",
      "Epoch 2456 Batch   72/81   train_loss = 0.780\n",
      "Epoch 2457 Batch   23/81   train_loss = 0.763\n",
      "Epoch 2457 Batch   55/81   train_loss = 0.773\n",
      "Epoch 2458 Batch    6/81   train_loss = 0.779\n",
      "Epoch 2458 Batch   38/81   train_loss = 0.788\n",
      "Epoch 2458 Batch   70/81   train_loss = 0.755\n",
      "Epoch 2459 Batch   21/81   train_loss = 0.811\n",
      "Epoch 2459 Batch   53/81   train_loss = 0.782\n",
      "Epoch 2460 Batch    4/81   train_loss = 0.803\n",
      "Epoch 2460 Batch   36/81   train_loss = 0.790\n",
      "Epoch 2460 Batch   68/81   train_loss = 0.788\n",
      "Epoch 2461 Batch   19/81   train_loss = 0.747\n",
      "Epoch 2461 Batch   51/81   train_loss = 0.798\n",
      "Epoch 2462 Batch    2/81   train_loss = 0.800\n",
      "Epoch 2462 Batch   34/81   train_loss = 0.800\n",
      "Epoch 2462 Batch   66/81   train_loss = 0.774\n",
      "Epoch 2463 Batch   17/81   train_loss = 0.779\n",
      "Epoch 2463 Batch   49/81   train_loss = 0.779\n",
      "Epoch 2464 Batch    0/81   train_loss = 0.794\n",
      "Epoch 2464 Batch   32/81   train_loss = 0.767\n",
      "Epoch 2464 Batch   64/81   train_loss = 0.756\n",
      "Epoch 2465 Batch   15/81   train_loss = 0.760\n",
      "Epoch 2465 Batch   47/81   train_loss = 0.763\n",
      "Epoch 2465 Batch   79/81   train_loss = 0.766\n",
      "Epoch 2466 Batch   30/81   train_loss = 0.754\n",
      "Epoch 2466 Batch   62/81   train_loss = 0.786\n",
      "Epoch 2467 Batch   13/81   train_loss = 0.784\n",
      "Epoch 2467 Batch   45/81   train_loss = 0.755\n",
      "Epoch 2467 Batch   77/81   train_loss = 0.799\n",
      "Epoch 2468 Batch   28/81   train_loss = 0.766\n",
      "Epoch 2468 Batch   60/81   train_loss = 0.760\n",
      "Epoch 2469 Batch   11/81   train_loss = 0.777\n",
      "Epoch 2469 Batch   43/81   train_loss = 0.744\n",
      "Epoch 2469 Batch   75/81   train_loss = 0.755\n",
      "Epoch 2470 Batch   26/81   train_loss = 0.782\n",
      "Epoch 2470 Batch   58/81   train_loss = 0.760\n",
      "Epoch 2471 Batch    9/81   train_loss = 0.796\n",
      "Epoch 2471 Batch   41/81   train_loss = 0.743\n",
      "Epoch 2471 Batch   73/81   train_loss = 0.786\n",
      "Epoch 2472 Batch   24/81   train_loss = 0.755\n",
      "Epoch 2472 Batch   56/81   train_loss = 0.794\n",
      "Epoch 2473 Batch    7/81   train_loss = 0.774\n",
      "Epoch 2473 Batch   39/81   train_loss = 0.738\n",
      "Epoch 2473 Batch   71/81   train_loss = 0.749\n",
      "Epoch 2474 Batch   22/81   train_loss = 0.792\n",
      "Epoch 2474 Batch   54/81   train_loss = 0.771\n",
      "Epoch 2475 Batch    5/81   train_loss = 0.778\n",
      "Epoch 2475 Batch   37/81   train_loss = 0.755\n",
      "Epoch 2475 Batch   69/81   train_loss = 0.777\n",
      "Epoch 2476 Batch   20/81   train_loss = 0.756\n",
      "Epoch 2476 Batch   52/81   train_loss = 0.795\n",
      "Epoch 2477 Batch    3/81   train_loss = 0.758\n",
      "Epoch 2477 Batch   35/81   train_loss = 0.745\n",
      "Epoch 2477 Batch   67/81   train_loss = 0.780\n",
      "Epoch 2478 Batch   18/81   train_loss = 0.786\n",
      "Epoch 2478 Batch   50/81   train_loss = 0.760\n",
      "Epoch 2479 Batch    1/81   train_loss = 0.799\n",
      "Epoch 2479 Batch   33/81   train_loss = 0.743\n",
      "Epoch 2479 Batch   65/81   train_loss = 0.799\n",
      "Epoch 2480 Batch   16/81   train_loss = 0.821\n",
      "Epoch 2480 Batch   48/81   train_loss = 0.792\n",
      "Epoch 2480 Batch   80/81   train_loss = 0.784\n",
      "Epoch 2481 Batch   31/81   train_loss = 0.759\n",
      "Epoch 2481 Batch   63/81   train_loss = 0.777\n",
      "Epoch 2482 Batch   14/81   train_loss = 0.785\n",
      "Epoch 2482 Batch   46/81   train_loss = 0.767\n",
      "Epoch 2482 Batch   78/81   train_loss = 0.780\n",
      "Epoch 2483 Batch   29/81   train_loss = 0.764\n",
      "Epoch 2483 Batch   61/81   train_loss = 0.755\n",
      "Epoch 2484 Batch   12/81   train_loss = 0.786\n",
      "Epoch 2484 Batch   44/81   train_loss = 0.766\n",
      "Epoch 2484 Batch   76/81   train_loss = 0.783\n",
      "Epoch 2485 Batch   27/81   train_loss = 0.754\n",
      "Epoch 2485 Batch   59/81   train_loss = 0.763\n",
      "Epoch 2486 Batch   10/81   train_loss = 0.799\n",
      "Epoch 2486 Batch   42/81   train_loss = 0.758\n",
      "Epoch 2486 Batch   74/81   train_loss = 0.769\n",
      "Epoch 2487 Batch   25/81   train_loss = 0.792\n",
      "Epoch 2487 Batch   57/81   train_loss = 0.781\n",
      "Epoch 2488 Batch    8/81   train_loss = 0.778\n",
      "Epoch 2488 Batch   40/81   train_loss = 0.751\n",
      "Epoch 2488 Batch   72/81   train_loss = 0.777\n",
      "Epoch 2489 Batch   23/81   train_loss = 0.761\n",
      "Epoch 2489 Batch   55/81   train_loss = 0.813\n",
      "Epoch 2490 Batch    6/81   train_loss = 0.768\n",
      "Epoch 2490 Batch   38/81   train_loss = 0.784\n",
      "Epoch 2490 Batch   70/81   train_loss = 0.766\n",
      "Epoch 2491 Batch   21/81   train_loss = 0.810\n",
      "Epoch 2491 Batch   53/81   train_loss = 0.759\n",
      "Epoch 2492 Batch    4/81   train_loss = 0.791\n",
      "Epoch 2492 Batch   36/81   train_loss = 0.815\n",
      "Epoch 2492 Batch   68/81   train_loss = 0.818\n",
      "Epoch 2493 Batch   19/81   train_loss = 0.755\n",
      "Epoch 2493 Batch   51/81   train_loss = 0.810\n",
      "Epoch 2494 Batch    2/81   train_loss = 0.795\n",
      "Epoch 2494 Batch   34/81   train_loss = 0.810\n",
      "Epoch 2494 Batch   66/81   train_loss = 0.776\n",
      "Epoch 2495 Batch   17/81   train_loss = 0.794\n",
      "Epoch 2495 Batch   49/81   train_loss = 0.787\n",
      "Epoch 2496 Batch    0/81   train_loss = 0.779\n",
      "Epoch 2496 Batch   32/81   train_loss = 0.794\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2496 Batch   64/81   train_loss = 0.766\n",
      "Epoch 2497 Batch   15/81   train_loss = 0.754\n",
      "Epoch 2497 Batch   47/81   train_loss = 0.751\n",
      "Epoch 2497 Batch   79/81   train_loss = 0.781\n",
      "Epoch 2498 Batch   30/81   train_loss = 0.746\n",
      "Epoch 2498 Batch   62/81   train_loss = 0.791\n",
      "Epoch 2499 Batch   13/81   train_loss = 0.796\n",
      "Epoch 2499 Batch   45/81   train_loss = 0.807\n",
      "Epoch 2499 Batch   77/81   train_loss = 0.819\n",
      "Epoch 2500 Batch   28/81   train_loss = 0.783\n",
      "Epoch 2500 Batch   60/81   train_loss = 0.800\n",
      "Epoch 2501 Batch   11/81   train_loss = 0.769\n",
      "Epoch 2501 Batch   43/81   train_loss = 0.745\n",
      "Epoch 2501 Batch   75/81   train_loss = 0.778\n",
      "Epoch 2502 Batch   26/81   train_loss = 0.802\n",
      "Epoch 2502 Batch   58/81   train_loss = 0.754\n",
      "Epoch 2503 Batch    9/81   train_loss = 0.816\n",
      "Epoch 2503 Batch   41/81   train_loss = 0.730\n",
      "Epoch 2503 Batch   73/81   train_loss = 0.782\n",
      "Epoch 2504 Batch   24/81   train_loss = 0.765\n",
      "Epoch 2504 Batch   56/81   train_loss = 0.791\n",
      "Epoch 2505 Batch    7/81   train_loss = 0.777\n",
      "Epoch 2505 Batch   39/81   train_loss = 0.750\n",
      "Epoch 2505 Batch   71/81   train_loss = 0.748\n",
      "Epoch 2506 Batch   22/81   train_loss = 0.804\n",
      "Epoch 2506 Batch   54/81   train_loss = 0.772\n",
      "Epoch 2507 Batch    5/81   train_loss = 0.798\n",
      "Epoch 2507 Batch   37/81   train_loss = 0.762\n",
      "Epoch 2507 Batch   69/81   train_loss = 0.775\n",
      "Epoch 2508 Batch   20/81   train_loss = 0.768\n",
      "Epoch 2508 Batch   52/81   train_loss = 0.824\n",
      "Epoch 2509 Batch    3/81   train_loss = 0.761\n",
      "Epoch 2509 Batch   35/81   train_loss = 0.761\n",
      "Epoch 2509 Batch   67/81   train_loss = 0.796\n",
      "Epoch 2510 Batch   18/81   train_loss = 0.811\n",
      "Epoch 2510 Batch   50/81   train_loss = 0.765\n",
      "Epoch 2511 Batch    1/81   train_loss = 0.789\n",
      "Epoch 2511 Batch   33/81   train_loss = 0.756\n",
      "Epoch 2511 Batch   65/81   train_loss = 0.802\n",
      "Epoch 2512 Batch   16/81   train_loss = 0.816\n",
      "Epoch 2512 Batch   48/81   train_loss = 0.781\n",
      "Epoch 2512 Batch   80/81   train_loss = 0.801\n",
      "Epoch 2513 Batch   31/81   train_loss = 0.790\n",
      "Epoch 2513 Batch   63/81   train_loss = 0.783\n",
      "Epoch 2514 Batch   14/81   train_loss = 0.801\n",
      "Epoch 2514 Batch   46/81   train_loss = 0.768\n",
      "Epoch 2514 Batch   78/81   train_loss = 0.773\n",
      "Epoch 2515 Batch   29/81   train_loss = 0.764\n",
      "Epoch 2515 Batch   61/81   train_loss = 0.772\n",
      "Epoch 2516 Batch   12/81   train_loss = 0.790\n",
      "Epoch 2516 Batch   44/81   train_loss = 0.781\n",
      "Epoch 2516 Batch   76/81   train_loss = 0.779\n",
      "Epoch 2517 Batch   27/81   train_loss = 0.763\n",
      "Epoch 2517 Batch   59/81   train_loss = 0.742\n",
      "Epoch 2518 Batch   10/81   train_loss = 0.799\n",
      "Epoch 2518 Batch   42/81   train_loss = 0.759\n",
      "Epoch 2518 Batch   74/81   train_loss = 0.767\n",
      "Epoch 2519 Batch   25/81   train_loss = 0.796\n",
      "Epoch 2519 Batch   57/81   train_loss = 0.788\n",
      "Epoch 2520 Batch    8/81   train_loss = 0.782\n",
      "Epoch 2520 Batch   40/81   train_loss = 0.763\n",
      "Epoch 2520 Batch   72/81   train_loss = 0.766\n",
      "Epoch 2521 Batch   23/81   train_loss = 0.764\n",
      "Epoch 2521 Batch   55/81   train_loss = 0.781\n",
      "Epoch 2522 Batch    6/81   train_loss = 0.758\n",
      "Epoch 2522 Batch   38/81   train_loss = 0.792\n",
      "Epoch 2522 Batch   70/81   train_loss = 0.751\n",
      "Epoch 2523 Batch   21/81   train_loss = 0.813\n",
      "Epoch 2523 Batch   53/81   train_loss = 0.768\n",
      "Epoch 2524 Batch    4/81   train_loss = 0.787\n",
      "Epoch 2524 Batch   36/81   train_loss = 0.795\n",
      "Epoch 2524 Batch   68/81   train_loss = 0.799\n",
      "Epoch 2525 Batch   19/81   train_loss = 0.748\n",
      "Epoch 2525 Batch   51/81   train_loss = 0.812\n",
      "Epoch 2526 Batch    2/81   train_loss = 0.789\n",
      "Epoch 2526 Batch   34/81   train_loss = 0.829\n",
      "Epoch 2526 Batch   66/81   train_loss = 0.770\n",
      "Epoch 2527 Batch   17/81   train_loss = 0.785\n",
      "Epoch 2527 Batch   49/81   train_loss = 0.773\n",
      "Epoch 2528 Batch    0/81   train_loss = 0.783\n",
      "Epoch 2528 Batch   32/81   train_loss = 0.768\n",
      "Epoch 2528 Batch   64/81   train_loss = 0.757\n",
      "Epoch 2529 Batch   15/81   train_loss = 0.751\n",
      "Epoch 2529 Batch   47/81   train_loss = 0.767\n",
      "Epoch 2529 Batch   79/81   train_loss = 0.766\n",
      "Epoch 2530 Batch   30/81   train_loss = 0.765\n",
      "Epoch 2530 Batch   62/81   train_loss = 0.770\n",
      "Epoch 2531 Batch   13/81   train_loss = 0.811\n",
      "Epoch 2531 Batch   45/81   train_loss = 0.792\n",
      "Epoch 2531 Batch   77/81   train_loss = 0.797\n",
      "Epoch 2532 Batch   28/81   train_loss = 0.763\n",
      "Epoch 2532 Batch   60/81   train_loss = 0.801\n",
      "Epoch 2533 Batch   11/81   train_loss = 0.805\n",
      "Epoch 2533 Batch   43/81   train_loss = 0.750\n",
      "Epoch 2533 Batch   75/81   train_loss = 0.757\n",
      "Epoch 2534 Batch   26/81   train_loss = 0.781\n",
      "Epoch 2534 Batch   58/81   train_loss = 0.773\n",
      "Epoch 2535 Batch    9/81   train_loss = 0.816\n",
      "Epoch 2535 Batch   41/81   train_loss = 0.745\n",
      "Epoch 2535 Batch   73/81   train_loss = 0.789\n",
      "Epoch 2536 Batch   24/81   train_loss = 0.792\n",
      "Epoch 2536 Batch   56/81   train_loss = 0.812\n",
      "Epoch 2537 Batch    7/81   train_loss = 0.777\n",
      "Epoch 2537 Batch   39/81   train_loss = 0.746\n",
      "Epoch 2537 Batch   71/81   train_loss = 0.792\n",
      "Epoch 2538 Batch   22/81   train_loss = 0.813\n",
      "Epoch 2538 Batch   54/81   train_loss = 0.763\n",
      "Epoch 2539 Batch    5/81   train_loss = 0.815\n",
      "Epoch 2539 Batch   37/81   train_loss = 0.789\n",
      "Epoch 2539 Batch   69/81   train_loss = 0.775\n",
      "Epoch 2540 Batch   20/81   train_loss = 0.766\n",
      "Epoch 2540 Batch   52/81   train_loss = 0.820\n",
      "Epoch 2541 Batch    3/81   train_loss = 0.778\n",
      "Epoch 2541 Batch   35/81   train_loss = 0.784\n",
      "Epoch 2541 Batch   67/81   train_loss = 0.808\n",
      "Epoch 2542 Batch   18/81   train_loss = 0.791\n",
      "Epoch 2542 Batch   50/81   train_loss = 0.771\n",
      "Epoch 2543 Batch    1/81   train_loss = 0.799\n",
      "Epoch 2543 Batch   33/81   train_loss = 0.764\n",
      "Epoch 2543 Batch   65/81   train_loss = 0.835\n",
      "Epoch 2544 Batch   16/81   train_loss = 0.844\n",
      "Epoch 2544 Batch   48/81   train_loss = 0.793\n",
      "Epoch 2544 Batch   80/81   train_loss = 0.801\n",
      "Epoch 2545 Batch   31/81   train_loss = 0.771\n",
      "Epoch 2545 Batch   63/81   train_loss = 0.810\n",
      "Epoch 2546 Batch   14/81   train_loss = 0.792\n",
      "Epoch 2546 Batch   46/81   train_loss = 0.783\n",
      "Epoch 2546 Batch   78/81   train_loss = 0.809\n",
      "Epoch 2547 Batch   29/81   train_loss = 0.792\n",
      "Epoch 2547 Batch   61/81   train_loss = 0.786\n",
      "Epoch 2548 Batch   12/81   train_loss = 0.800\n",
      "Epoch 2548 Batch   44/81   train_loss = 0.791\n",
      "Epoch 2548 Batch   76/81   train_loss = 0.821\n",
      "Epoch 2549 Batch   27/81   train_loss = 0.771\n",
      "Epoch 2549 Batch   59/81   train_loss = 0.765\n",
      "Epoch 2550 Batch   10/81   train_loss = 0.795\n",
      "Epoch 2550 Batch   42/81   train_loss = 0.783\n",
      "Epoch 2550 Batch   74/81   train_loss = 0.782\n",
      "Epoch 2551 Batch   25/81   train_loss = 0.785\n",
      "Epoch 2551 Batch   57/81   train_loss = 0.794\n",
      "Epoch 2552 Batch    8/81   train_loss = 0.795\n",
      "Epoch 2552 Batch   40/81   train_loss = 0.758\n",
      "Epoch 2552 Batch   72/81   train_loss = 0.765\n",
      "Epoch 2553 Batch   23/81   train_loss = 0.785\n",
      "Epoch 2553 Batch   55/81   train_loss = 0.782\n",
      "Epoch 2554 Batch    6/81   train_loss = 0.795\n",
      "Epoch 2554 Batch   38/81   train_loss = 0.804\n",
      "Epoch 2554 Batch   70/81   train_loss = 0.793\n",
      "Epoch 2555 Batch   21/81   train_loss = 0.823\n",
      "Epoch 2555 Batch   53/81   train_loss = 0.758\n",
      "Epoch 2556 Batch    4/81   train_loss = 0.792\n",
      "Epoch 2556 Batch   36/81   train_loss = 0.796\n",
      "Epoch 2556 Batch   68/81   train_loss = 0.789\n",
      "Epoch 2557 Batch   19/81   train_loss = 0.746\n",
      "Epoch 2557 Batch   51/81   train_loss = 0.809\n",
      "Epoch 2558 Batch    2/81   train_loss = 0.789\n",
      "Epoch 2558 Batch   34/81   train_loss = 0.811\n",
      "Epoch 2558 Batch   66/81   train_loss = 0.762\n",
      "Epoch 2559 Batch   17/81   train_loss = 0.789\n",
      "Epoch 2559 Batch   49/81   train_loss = 0.787\n",
      "Epoch 2560 Batch    0/81   train_loss = 0.783\n",
      "Epoch 2560 Batch   32/81   train_loss = 0.759\n",
      "Epoch 2560 Batch   64/81   train_loss = 0.768\n",
      "Epoch 2561 Batch   15/81   train_loss = 0.774\n",
      "Epoch 2561 Batch   47/81   train_loss = 0.761\n",
      "Epoch 2561 Batch   79/81   train_loss = 0.762\n",
      "Epoch 2562 Batch   30/81   train_loss = 0.790\n",
      "Epoch 2562 Batch   62/81   train_loss = 0.777\n",
      "Epoch 2563 Batch   13/81   train_loss = 0.796\n",
      "Epoch 2563 Batch   45/81   train_loss = 0.768\n",
      "Epoch 2563 Batch   77/81   train_loss = 0.788\n",
      "Epoch 2564 Batch   28/81   train_loss = 0.746\n",
      "Epoch 2564 Batch   60/81   train_loss = 0.767\n",
      "Epoch 2565 Batch   11/81   train_loss = 0.779\n",
      "Epoch 2565 Batch   43/81   train_loss = 0.752\n",
      "Epoch 2565 Batch   75/81   train_loss = 0.764\n",
      "Epoch 2566 Batch   26/81   train_loss = 0.765\n",
      "Epoch 2566 Batch   58/81   train_loss = 0.752\n",
      "Epoch 2567 Batch    9/81   train_loss = 0.804\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2567 Batch   41/81   train_loss = 0.746\n",
      "Epoch 2567 Batch   73/81   train_loss = 0.802\n",
      "Epoch 2568 Batch   24/81   train_loss = 0.747\n",
      "Epoch 2568 Batch   56/81   train_loss = 0.809\n",
      "Epoch 2569 Batch    7/81   train_loss = 0.805\n",
      "Epoch 2569 Batch   39/81   train_loss = 0.745\n",
      "Epoch 2569 Batch   71/81   train_loss = 0.747\n",
      "Epoch 2570 Batch   22/81   train_loss = 0.794\n",
      "Epoch 2570 Batch   54/81   train_loss = 0.768\n",
      "Epoch 2571 Batch    5/81   train_loss = 0.795\n",
      "Epoch 2571 Batch   37/81   train_loss = 0.771\n",
      "Epoch 2571 Batch   69/81   train_loss = 0.779\n",
      "Epoch 2572 Batch   20/81   train_loss = 0.760\n",
      "Epoch 2572 Batch   52/81   train_loss = 0.800\n",
      "Epoch 2573 Batch    3/81   train_loss = 0.754\n",
      "Epoch 2573 Batch   35/81   train_loss = 0.772\n",
      "Epoch 2573 Batch   67/81   train_loss = 0.814\n",
      "Epoch 2574 Batch   18/81   train_loss = 0.776\n",
      "Epoch 2574 Batch   50/81   train_loss = 0.775\n",
      "Epoch 2575 Batch    1/81   train_loss = 0.808\n",
      "Epoch 2575 Batch   33/81   train_loss = 0.754\n",
      "Epoch 2575 Batch   65/81   train_loss = 0.823\n",
      "Epoch 2576 Batch   16/81   train_loss = 0.832\n",
      "Epoch 2576 Batch   48/81   train_loss = 0.779\n",
      "Epoch 2576 Batch   80/81   train_loss = 0.793\n",
      "Epoch 2577 Batch   31/81   train_loss = 0.743\n",
      "Epoch 2577 Batch   63/81   train_loss = 0.783\n",
      "Epoch 2578 Batch   14/81   train_loss = 0.785\n",
      "Epoch 2578 Batch   46/81   train_loss = 0.773\n",
      "Epoch 2578 Batch   78/81   train_loss = 0.810\n",
      "Epoch 2579 Batch   29/81   train_loss = 0.784\n",
      "Epoch 2579 Batch   61/81   train_loss = 0.792\n",
      "Epoch 2580 Batch   12/81   train_loss = 0.801\n",
      "Epoch 2580 Batch   44/81   train_loss = 0.805\n",
      "Epoch 2580 Batch   76/81   train_loss = 0.824\n",
      "Epoch 2581 Batch   27/81   train_loss = 0.783\n",
      "Epoch 2581 Batch   59/81   train_loss = 0.769\n",
      "Epoch 2582 Batch   10/81   train_loss = 0.807\n",
      "Epoch 2582 Batch   42/81   train_loss = 0.768\n",
      "Epoch 2582 Batch   74/81   train_loss = 0.789\n",
      "Epoch 2583 Batch   25/81   train_loss = 0.769\n",
      "Epoch 2583 Batch   57/81   train_loss = 0.800\n",
      "Epoch 2584 Batch    8/81   train_loss = 0.798\n",
      "Epoch 2584 Batch   40/81   train_loss = 0.766\n",
      "Epoch 2584 Batch   72/81   train_loss = 0.789\n",
      "Epoch 2585 Batch   23/81   train_loss = 0.782\n",
      "Epoch 2585 Batch   55/81   train_loss = 0.794\n",
      "Epoch 2586 Batch    6/81   train_loss = 0.769\n",
      "Epoch 2586 Batch   38/81   train_loss = 0.779\n",
      "Epoch 2586 Batch   70/81   train_loss = 0.778\n",
      "Epoch 2587 Batch   21/81   train_loss = 0.819\n",
      "Epoch 2587 Batch   53/81   train_loss = 0.774\n",
      "Epoch 2588 Batch    4/81   train_loss = 0.794\n",
      "Epoch 2588 Batch   36/81   train_loss = 0.801\n",
      "Epoch 2588 Batch   68/81   train_loss = 0.815\n",
      "Epoch 2589 Batch   19/81   train_loss = 0.761\n",
      "Epoch 2589 Batch   51/81   train_loss = 0.823\n",
      "Epoch 2590 Batch    2/81   train_loss = 0.826\n",
      "Epoch 2590 Batch   34/81   train_loss = 0.822\n",
      "Epoch 2590 Batch   66/81   train_loss = 0.797\n",
      "Epoch 2591 Batch   17/81   train_loss = 0.805\n",
      "Epoch 2591 Batch   49/81   train_loss = 0.787\n",
      "Epoch 2592 Batch    0/81   train_loss = 0.818\n",
      "Epoch 2592 Batch   32/81   train_loss = 0.781\n",
      "Epoch 2592 Batch   64/81   train_loss = 0.822\n",
      "Epoch 2593 Batch   15/81   train_loss = 0.783\n",
      "Epoch 2593 Batch   47/81   train_loss = 0.803\n",
      "Epoch 2593 Batch   79/81   train_loss = 0.799\n",
      "Epoch 2594 Batch   30/81   train_loss = 0.786\n",
      "Epoch 2594 Batch   62/81   train_loss = 0.789\n",
      "Epoch 2595 Batch   13/81   train_loss = 0.814\n",
      "Epoch 2595 Batch   45/81   train_loss = 0.780\n",
      "Epoch 2595 Batch   77/81   train_loss = 0.812\n",
      "Epoch 2596 Batch   28/81   train_loss = 0.763\n",
      "Epoch 2596 Batch   60/81   train_loss = 0.772\n",
      "Epoch 2597 Batch   11/81   train_loss = 0.792\n",
      "Epoch 2597 Batch   43/81   train_loss = 0.760\n",
      "Epoch 2597 Batch   75/81   train_loss = 0.768\n",
      "Epoch 2598 Batch   26/81   train_loss = 0.784\n",
      "Epoch 2598 Batch   58/81   train_loss = 0.760\n",
      "Epoch 2599 Batch    9/81   train_loss = 0.827\n",
      "Epoch 2599 Batch   41/81   train_loss = 0.734\n",
      "Epoch 2599 Batch   73/81   train_loss = 0.770\n",
      "Epoch 2600 Batch   24/81   train_loss = 0.758\n",
      "Epoch 2600 Batch   56/81   train_loss = 0.803\n",
      "Epoch 2601 Batch    7/81   train_loss = 0.773\n",
      "Epoch 2601 Batch   39/81   train_loss = 0.758\n",
      "Epoch 2601 Batch   71/81   train_loss = 0.751\n",
      "Epoch 2602 Batch   22/81   train_loss = 0.781\n",
      "Epoch 2602 Batch   54/81   train_loss = 0.776\n",
      "Epoch 2603 Batch    5/81   train_loss = 0.812\n",
      "Epoch 2603 Batch   37/81   train_loss = 0.781\n",
      "Epoch 2603 Batch   69/81   train_loss = 0.774\n",
      "Epoch 2604 Batch   20/81   train_loss = 0.758\n",
      "Epoch 2604 Batch   52/81   train_loss = 0.799\n",
      "Epoch 2605 Batch    3/81   train_loss = 0.751\n",
      "Epoch 2605 Batch   35/81   train_loss = 0.747\n",
      "Epoch 2605 Batch   67/81   train_loss = 0.794\n",
      "Epoch 2606 Batch   18/81   train_loss = 0.815\n",
      "Epoch 2606 Batch   50/81   train_loss = 0.775\n",
      "Epoch 2607 Batch    1/81   train_loss = 0.797\n",
      "Epoch 2607 Batch   33/81   train_loss = 0.757\n",
      "Epoch 2607 Batch   65/81   train_loss = 0.820\n",
      "Epoch 2608 Batch   16/81   train_loss = 0.835\n",
      "Epoch 2608 Batch   48/81   train_loss = 0.804\n",
      "Epoch 2608 Batch   80/81   train_loss = 0.804\n",
      "Epoch 2609 Batch   31/81   train_loss = 0.761\n",
      "Epoch 2609 Batch   63/81   train_loss = 0.833\n",
      "Epoch 2610 Batch   14/81   train_loss = 0.811\n",
      "Epoch 2610 Batch   46/81   train_loss = 0.772\n",
      "Epoch 2610 Batch   78/81   train_loss = 0.800\n",
      "Epoch 2611 Batch   29/81   train_loss = 0.762\n",
      "Epoch 2611 Batch   61/81   train_loss = 0.780\n",
      "Epoch 2612 Batch   12/81   train_loss = 0.789\n",
      "Epoch 2612 Batch   44/81   train_loss = 0.789\n",
      "Epoch 2612 Batch   76/81   train_loss = 0.800\n",
      "Epoch 2613 Batch   27/81   train_loss = 0.767\n",
      "Epoch 2613 Batch   59/81   train_loss = 0.770\n",
      "Epoch 2614 Batch   10/81   train_loss = 0.795\n",
      "Epoch 2614 Batch   42/81   train_loss = 0.780\n",
      "Epoch 2614 Batch   74/81   train_loss = 0.760\n",
      "Epoch 2615 Batch   25/81   train_loss = 0.797\n",
      "Epoch 2615 Batch   57/81   train_loss = 0.801\n",
      "Epoch 2616 Batch    8/81   train_loss = 0.799\n",
      "Epoch 2616 Batch   40/81   train_loss = 0.745\n",
      "Epoch 2616 Batch   72/81   train_loss = 0.770\n",
      "Epoch 2617 Batch   23/81   train_loss = 0.780\n",
      "Epoch 2617 Batch   55/81   train_loss = 0.782\n",
      "Epoch 2618 Batch    6/81   train_loss = 0.761\n",
      "Epoch 2618 Batch   38/81   train_loss = 0.789\n",
      "Epoch 2618 Batch   70/81   train_loss = 0.793\n",
      "Epoch 2619 Batch   21/81   train_loss = 0.831\n",
      "Epoch 2619 Batch   53/81   train_loss = 0.760\n",
      "Epoch 2620 Batch    4/81   train_loss = 0.781\n",
      "Epoch 2620 Batch   36/81   train_loss = 0.779\n",
      "Epoch 2620 Batch   68/81   train_loss = 0.793\n",
      "Epoch 2621 Batch   19/81   train_loss = 0.760\n",
      "Epoch 2621 Batch   51/81   train_loss = 0.829\n",
      "Epoch 2622 Batch    2/81   train_loss = 0.791\n",
      "Epoch 2622 Batch   34/81   train_loss = 0.806\n",
      "Epoch 2622 Batch   66/81   train_loss = 0.770\n",
      "Epoch 2623 Batch   17/81   train_loss = 0.791\n",
      "Epoch 2623 Batch   49/81   train_loss = 0.780\n",
      "Epoch 2624 Batch    0/81   train_loss = 0.797\n",
      "Epoch 2624 Batch   32/81   train_loss = 0.761\n",
      "Epoch 2624 Batch   64/81   train_loss = 0.802\n",
      "Epoch 2625 Batch   15/81   train_loss = 0.771\n",
      "Epoch 2625 Batch   47/81   train_loss = 0.778\n",
      "Epoch 2625 Batch   79/81   train_loss = 0.780\n",
      "Epoch 2626 Batch   30/81   train_loss = 0.776\n",
      "Epoch 2626 Batch   62/81   train_loss = 0.797\n",
      "Epoch 2627 Batch   13/81   train_loss = 0.804\n",
      "Epoch 2627 Batch   45/81   train_loss = 0.774\n",
      "Epoch 2627 Batch   77/81   train_loss = 0.800\n",
      "Epoch 2628 Batch   28/81   train_loss = 0.775\n",
      "Epoch 2628 Batch   60/81   train_loss = 0.790\n",
      "Epoch 2629 Batch   11/81   train_loss = 0.785\n",
      "Epoch 2629 Batch   43/81   train_loss = 0.761\n",
      "Epoch 2629 Batch   75/81   train_loss = 0.776\n",
      "Epoch 2630 Batch   26/81   train_loss = 0.786\n",
      "Epoch 2630 Batch   58/81   train_loss = 0.761\n",
      "Epoch 2631 Batch    9/81   train_loss = 0.800\n",
      "Epoch 2631 Batch   41/81   train_loss = 0.758\n",
      "Epoch 2631 Batch   73/81   train_loss = 0.807\n",
      "Epoch 2632 Batch   24/81   train_loss = 0.790\n",
      "Epoch 2632 Batch   56/81   train_loss = 0.792\n",
      "Epoch 2633 Batch    7/81   train_loss = 0.800\n",
      "Epoch 2633 Batch   39/81   train_loss = 0.766\n",
      "Epoch 2633 Batch   71/81   train_loss = 0.752\n",
      "Epoch 2634 Batch   22/81   train_loss = 0.786\n",
      "Epoch 2634 Batch   54/81   train_loss = 0.765\n",
      "Epoch 2635 Batch    5/81   train_loss = 0.803\n",
      "Epoch 2635 Batch   37/81   train_loss = 0.778\n",
      "Epoch 2635 Batch   69/81   train_loss = 0.790\n",
      "Epoch 2636 Batch   20/81   train_loss = 0.780\n",
      "Epoch 2636 Batch   52/81   train_loss = 0.803\n",
      "Epoch 2637 Batch    3/81   train_loss = 0.781\n",
      "Epoch 2637 Batch   35/81   train_loss = 0.775\n",
      "Epoch 2637 Batch   67/81   train_loss = 0.810\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2638 Batch   18/81   train_loss = 0.799\n",
      "Epoch 2638 Batch   50/81   train_loss = 0.778\n",
      "Epoch 2639 Batch    1/81   train_loss = 0.807\n",
      "Epoch 2639 Batch   33/81   train_loss = 0.755\n",
      "Epoch 2639 Batch   65/81   train_loss = 0.816\n",
      "Epoch 2640 Batch   16/81   train_loss = 0.837\n",
      "Epoch 2640 Batch   48/81   train_loss = 0.799\n",
      "Epoch 2640 Batch   80/81   train_loss = 0.799\n",
      "Epoch 2641 Batch   31/81   train_loss = 0.780\n",
      "Epoch 2641 Batch   63/81   train_loss = 0.794\n",
      "Epoch 2642 Batch   14/81   train_loss = 0.797\n",
      "Epoch 2642 Batch   46/81   train_loss = 0.773\n",
      "Epoch 2642 Batch   78/81   train_loss = 0.787\n",
      "Epoch 2643 Batch   29/81   train_loss = 0.786\n",
      "Epoch 2643 Batch   61/81   train_loss = 0.781\n",
      "Epoch 2644 Batch   12/81   train_loss = 0.775\n",
      "Epoch 2644 Batch   44/81   train_loss = 0.774\n",
      "Epoch 2644 Batch   76/81   train_loss = 0.786\n",
      "Epoch 2645 Batch   27/81   train_loss = 0.770\n",
      "Epoch 2645 Batch   59/81   train_loss = 0.757\n",
      "Epoch 2646 Batch   10/81   train_loss = 0.799\n",
      "Epoch 2646 Batch   42/81   train_loss = 0.775\n",
      "Epoch 2646 Batch   74/81   train_loss = 0.784\n",
      "Epoch 2647 Batch   25/81   train_loss = 0.786\n",
      "Epoch 2647 Batch   57/81   train_loss = 0.792\n",
      "Epoch 2648 Batch    8/81   train_loss = 0.776\n",
      "Epoch 2648 Batch   40/81   train_loss = 0.747\n",
      "Epoch 2648 Batch   72/81   train_loss = 0.753\n",
      "Epoch 2649 Batch   23/81   train_loss = 0.780\n",
      "Epoch 2649 Batch   55/81   train_loss = 0.794\n",
      "Epoch 2650 Batch    6/81   train_loss = 0.790\n",
      "Epoch 2650 Batch   38/81   train_loss = 0.796\n",
      "Epoch 2650 Batch   70/81   train_loss = 0.769\n",
      "Epoch 2651 Batch   21/81   train_loss = 0.822\n",
      "Epoch 2651 Batch   53/81   train_loss = 0.774\n",
      "Epoch 2652 Batch    4/81   train_loss = 0.797\n",
      "Epoch 2652 Batch   36/81   train_loss = 0.788\n",
      "Epoch 2652 Batch   68/81   train_loss = 0.805\n",
      "Epoch 2653 Batch   19/81   train_loss = 0.746\n",
      "Epoch 2653 Batch   51/81   train_loss = 0.807\n",
      "Epoch 2654 Batch    2/81   train_loss = 0.787\n",
      "Epoch 2654 Batch   34/81   train_loss = 0.792\n",
      "Epoch 2654 Batch   66/81   train_loss = 0.784\n",
      "Epoch 2655 Batch   17/81   train_loss = 0.794\n",
      "Epoch 2655 Batch   49/81   train_loss = 0.791\n",
      "Epoch 2656 Batch    0/81   train_loss = 0.793\n",
      "Epoch 2656 Batch   32/81   train_loss = 0.770\n",
      "Epoch 2656 Batch   64/81   train_loss = 0.778\n",
      "Epoch 2657 Batch   15/81   train_loss = 0.765\n",
      "Epoch 2657 Batch   47/81   train_loss = 0.770\n",
      "Epoch 2657 Batch   79/81   train_loss = 0.772\n",
      "Epoch 2658 Batch   30/81   train_loss = 0.771\n",
      "Epoch 2658 Batch   62/81   train_loss = 0.783\n",
      "Epoch 2659 Batch   13/81   train_loss = 0.802\n",
      "Epoch 2659 Batch   45/81   train_loss = 0.773\n",
      "Epoch 2659 Batch   77/81   train_loss = 0.788\n",
      "Epoch 2660 Batch   28/81   train_loss = 0.751\n",
      "Epoch 2660 Batch   60/81   train_loss = 0.777\n",
      "Epoch 2661 Batch   11/81   train_loss = 0.788\n",
      "Epoch 2661 Batch   43/81   train_loss = 0.744\n",
      "Epoch 2661 Batch   75/81   train_loss = 0.773\n",
      "Epoch 2662 Batch   26/81   train_loss = 0.776\n",
      "Epoch 2662 Batch   58/81   train_loss = 0.743\n",
      "Epoch 2663 Batch    9/81   train_loss = 0.794\n",
      "Epoch 2663 Batch   41/81   train_loss = 0.755\n",
      "Epoch 2663 Batch   73/81   train_loss = 0.797\n",
      "Epoch 2664 Batch   24/81   train_loss = 0.761\n",
      "Epoch 2664 Batch   56/81   train_loss = 0.801\n",
      "Epoch 2665 Batch    7/81   train_loss = 0.785\n",
      "Epoch 2665 Batch   39/81   train_loss = 0.745\n",
      "Epoch 2665 Batch   71/81   train_loss = 0.745\n",
      "Epoch 2666 Batch   22/81   train_loss = 0.794\n",
      "Epoch 2666 Batch   54/81   train_loss = 0.771\n",
      "Epoch 2667 Batch    5/81   train_loss = 0.813\n",
      "Epoch 2667 Batch   37/81   train_loss = 0.767\n",
      "Epoch 2667 Batch   69/81   train_loss = 0.795\n",
      "Epoch 2668 Batch   20/81   train_loss = 0.765\n",
      "Epoch 2668 Batch   52/81   train_loss = 0.805\n",
      "Epoch 2669 Batch    3/81   train_loss = 0.776\n",
      "Epoch 2669 Batch   35/81   train_loss = 0.771\n",
      "Epoch 2669 Batch   67/81   train_loss = 0.797\n",
      "Epoch 2670 Batch   18/81   train_loss = 0.816\n",
      "Epoch 2670 Batch   50/81   train_loss = 0.757\n",
      "Epoch 2671 Batch    1/81   train_loss = 0.791\n",
      "Epoch 2671 Batch   33/81   train_loss = 0.765\n",
      "Epoch 2671 Batch   65/81   train_loss = 0.803\n",
      "Epoch 2672 Batch   16/81   train_loss = 0.811\n",
      "Epoch 2672 Batch   48/81   train_loss = 0.784\n",
      "Epoch 2672 Batch   80/81   train_loss = 0.799\n",
      "Epoch 2673 Batch   31/81   train_loss = 0.788\n",
      "Epoch 2673 Batch   63/81   train_loss = 0.783\n",
      "Epoch 2674 Batch   14/81   train_loss = 0.795\n",
      "Epoch 2674 Batch   46/81   train_loss = 0.972\n",
      "Epoch 2674 Batch   78/81   train_loss = 0.791\n",
      "Epoch 2675 Batch   29/81   train_loss = 0.779\n",
      "Epoch 2675 Batch   61/81   train_loss = 0.801\n",
      "Epoch 2676 Batch   12/81   train_loss = 0.807\n",
      "Epoch 2676 Batch   44/81   train_loss = 0.810\n",
      "Epoch 2676 Batch   76/81   train_loss = 0.803\n",
      "Epoch 2677 Batch   27/81   train_loss = 0.783\n",
      "Epoch 2677 Batch   59/81   train_loss = 0.769\n",
      "Epoch 2678 Batch   10/81   train_loss = 0.805\n",
      "Epoch 2678 Batch   42/81   train_loss = 0.794\n",
      "Epoch 2678 Batch   74/81   train_loss = 0.785\n",
      "Epoch 2679 Batch   25/81   train_loss = 0.818\n",
      "Epoch 2679 Batch   57/81   train_loss = 0.790\n",
      "Epoch 2680 Batch    8/81   train_loss = 0.814\n",
      "Epoch 2680 Batch   40/81   train_loss = 0.775\n",
      "Epoch 2680 Batch   72/81   train_loss = 0.789\n",
      "Epoch 2681 Batch   23/81   train_loss = 0.798\n",
      "Epoch 2681 Batch   55/81   train_loss = 0.791\n",
      "Epoch 2682 Batch    6/81   train_loss = 0.806\n",
      "Epoch 2682 Batch   38/81   train_loss = 0.792\n",
      "Epoch 2682 Batch   70/81   train_loss = 0.782\n",
      "Epoch 2683 Batch   21/81   train_loss = 0.826\n",
      "Epoch 2683 Batch   53/81   train_loss = 0.787\n",
      "Epoch 2684 Batch    4/81   train_loss = 0.804\n",
      "Epoch 2684 Batch   36/81   train_loss = 0.805\n",
      "Epoch 2684 Batch   68/81   train_loss = 0.813\n",
      "Epoch 2685 Batch   19/81   train_loss = 0.757\n",
      "Epoch 2685 Batch   51/81   train_loss = 0.806\n",
      "Epoch 2686 Batch    2/81   train_loss = 0.798\n",
      "Epoch 2686 Batch   34/81   train_loss = 0.804\n",
      "Epoch 2686 Batch   66/81   train_loss = 0.785\n",
      "Epoch 2687 Batch   17/81   train_loss = 0.789\n",
      "Epoch 2687 Batch   49/81   train_loss = 0.783\n",
      "Epoch 2688 Batch    0/81   train_loss = 0.793\n",
      "Epoch 2688 Batch   32/81   train_loss = 0.750\n",
      "Epoch 2688 Batch   64/81   train_loss = 0.790\n",
      "Epoch 2689 Batch   15/81   train_loss = 0.779\n",
      "Epoch 2689 Batch   47/81   train_loss = 0.778\n",
      "Epoch 2689 Batch   79/81   train_loss = 0.793\n",
      "Epoch 2690 Batch   30/81   train_loss = 0.784\n",
      "Epoch 2690 Batch   62/81   train_loss = 0.772\n",
      "Epoch 2691 Batch   13/81   train_loss = 0.804\n",
      "Epoch 2691 Batch   45/81   train_loss = 0.769\n",
      "Epoch 2691 Batch   77/81   train_loss = 0.819\n",
      "Epoch 2692 Batch   28/81   train_loss = 0.763\n",
      "Epoch 2692 Batch   60/81   train_loss = 0.793\n",
      "Epoch 2693 Batch   11/81   train_loss = 0.790\n",
      "Epoch 2693 Batch   43/81   train_loss = 0.748\n",
      "Epoch 2693 Batch   75/81   train_loss = 0.764\n",
      "Epoch 2694 Batch   26/81   train_loss = 0.781\n",
      "Epoch 2694 Batch   58/81   train_loss = 0.764\n",
      "Epoch 2695 Batch    9/81   train_loss = 0.799\n",
      "Epoch 2695 Batch   41/81   train_loss = 0.737\n",
      "Epoch 2695 Batch   73/81   train_loss = 0.784\n",
      "Epoch 2696 Batch   24/81   train_loss = 0.756\n",
      "Epoch 2696 Batch   56/81   train_loss = 0.776\n",
      "Epoch 2697 Batch    7/81   train_loss = 0.772\n",
      "Epoch 2697 Batch   39/81   train_loss = 0.739\n",
      "Epoch 2697 Batch   71/81   train_loss = 0.743\n",
      "Epoch 2698 Batch   22/81   train_loss = 0.793\n",
      "Epoch 2698 Batch   54/81   train_loss = 0.755\n",
      "Epoch 2699 Batch    5/81   train_loss = 0.779\n",
      "Epoch 2699 Batch   37/81   train_loss = 0.757\n",
      "Epoch 2699 Batch   69/81   train_loss = 0.754\n",
      "Epoch 2700 Batch   20/81   train_loss = 0.758\n",
      "Epoch 2700 Batch   52/81   train_loss = 0.809\n",
      "Epoch 2701 Batch    3/81   train_loss = 0.784\n",
      "Epoch 2701 Batch   35/81   train_loss = 0.766\n",
      "Epoch 2701 Batch   67/81   train_loss = 0.782\n",
      "Epoch 2702 Batch   18/81   train_loss = 0.778\n",
      "Epoch 2702 Batch   50/81   train_loss = 0.747\n",
      "Epoch 2703 Batch    1/81   train_loss = 0.774\n",
      "Epoch 2703 Batch   33/81   train_loss = 0.755\n",
      "Epoch 2703 Batch   65/81   train_loss = 0.794\n",
      "Epoch 2704 Batch   16/81   train_loss = 0.830\n",
      "Epoch 2704 Batch   48/81   train_loss = 0.782\n",
      "Epoch 2704 Batch   80/81   train_loss = 0.802\n",
      "Epoch 2705 Batch   31/81   train_loss = 0.771\n",
      "Epoch 2705 Batch   63/81   train_loss = 0.800\n",
      "Epoch 2706 Batch   14/81   train_loss = 0.794\n",
      "Epoch 2706 Batch   46/81   train_loss = 0.777\n",
      "Epoch 2706 Batch   78/81   train_loss = 0.787\n",
      "Epoch 2707 Batch   29/81   train_loss = 0.765\n",
      "Epoch 2707 Batch   61/81   train_loss = 0.768\n",
      "Epoch 2708 Batch   12/81   train_loss = 0.801\n",
      "Epoch 2708 Batch   44/81   train_loss = 0.781\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2708 Batch   76/81   train_loss = 0.808\n",
      "Epoch 2709 Batch   27/81   train_loss = 0.758\n",
      "Epoch 2709 Batch   59/81   train_loss = 0.771\n",
      "Epoch 2710 Batch   10/81   train_loss = 0.823\n",
      "Epoch 2710 Batch   42/81   train_loss = 0.773\n",
      "Epoch 2710 Batch   74/81   train_loss = 0.794\n",
      "Epoch 2711 Batch   25/81   train_loss = 0.786\n",
      "Epoch 2711 Batch   57/81   train_loss = 0.764\n",
      "Epoch 2712 Batch    8/81   train_loss = 0.776\n",
      "Epoch 2712 Batch   40/81   train_loss = 0.754\n",
      "Epoch 2712 Batch   72/81   train_loss = 0.774\n",
      "Epoch 2713 Batch   23/81   train_loss = 0.780\n",
      "Epoch 2713 Batch   55/81   train_loss = 0.801\n",
      "Epoch 2714 Batch    6/81   train_loss = 0.772\n",
      "Epoch 2714 Batch   38/81   train_loss = 0.780\n",
      "Epoch 2714 Batch   70/81   train_loss = 0.762\n",
      "Epoch 2715 Batch   21/81   train_loss = 0.806\n",
      "Epoch 2715 Batch   53/81   train_loss = 0.748\n",
      "Epoch 2716 Batch    4/81   train_loss = 0.788\n",
      "Epoch 2716 Batch   36/81   train_loss = 0.777\n",
      "Epoch 2716 Batch   68/81   train_loss = 0.785\n",
      "Epoch 2717 Batch   19/81   train_loss = 0.744\n",
      "Epoch 2717 Batch   51/81   train_loss = 0.812\n",
      "Epoch 2718 Batch    2/81   train_loss = 0.779\n",
      "Epoch 2718 Batch   34/81   train_loss = 0.765\n",
      "Epoch 2718 Batch   66/81   train_loss = 0.776\n",
      "Epoch 2719 Batch   17/81   train_loss = 0.767\n",
      "Epoch 2719 Batch   49/81   train_loss = 0.752\n",
      "Epoch 2720 Batch    0/81   train_loss = 0.783\n",
      "Epoch 2720 Batch   32/81   train_loss = 0.757\n",
      "Epoch 2720 Batch   64/81   train_loss = 0.762\n",
      "Epoch 2721 Batch   15/81   train_loss = 0.765\n",
      "Epoch 2721 Batch   47/81   train_loss = 0.776\n",
      "Epoch 2721 Batch   79/81   train_loss = 0.770\n",
      "Epoch 2722 Batch   30/81   train_loss = 0.758\n",
      "Epoch 2722 Batch   62/81   train_loss = 0.786\n",
      "Epoch 2723 Batch   13/81   train_loss = 0.791\n",
      "Epoch 2723 Batch   45/81   train_loss = 0.756\n",
      "Epoch 2723 Batch   77/81   train_loss = 0.787\n",
      "Epoch 2724 Batch   28/81   train_loss = 0.752\n",
      "Epoch 2724 Batch   60/81   train_loss = 0.780\n",
      "Epoch 2725 Batch   11/81   train_loss = 0.780\n",
      "Epoch 2725 Batch   43/81   train_loss = 0.735\n",
      "Epoch 2725 Batch   75/81   train_loss = 0.756\n",
      "Epoch 2726 Batch   26/81   train_loss = 0.771\n",
      "Epoch 2726 Batch   58/81   train_loss = 0.754\n",
      "Epoch 2727 Batch    9/81   train_loss = 0.787\n",
      "Epoch 2727 Batch   41/81   train_loss = 0.735\n",
      "Epoch 2727 Batch   73/81   train_loss = 0.786\n",
      "Epoch 2728 Batch   24/81   train_loss = 0.766\n",
      "Epoch 2728 Batch   56/81   train_loss = 0.789\n",
      "Epoch 2729 Batch    7/81   train_loss = 0.772\n",
      "Epoch 2729 Batch   39/81   train_loss = 0.732\n",
      "Epoch 2729 Batch   71/81   train_loss = 0.756\n",
      "Epoch 2730 Batch   22/81   train_loss = 0.790\n",
      "Epoch 2730 Batch   54/81   train_loss = 0.763\n",
      "Epoch 2731 Batch    5/81   train_loss = 0.812\n",
      "Epoch 2731 Batch   37/81   train_loss = 0.778\n",
      "Epoch 2731 Batch   69/81   train_loss = 0.780\n",
      "Epoch 2732 Batch   20/81   train_loss = 0.774\n",
      "Epoch 2732 Batch   52/81   train_loss = 0.799\n",
      "Epoch 2733 Batch    3/81   train_loss = 0.772\n",
      "Epoch 2733 Batch   35/81   train_loss = 0.760\n",
      "Epoch 2733 Batch   67/81   train_loss = 0.810\n",
      "Epoch 2734 Batch   18/81   train_loss = 0.807\n",
      "Epoch 2734 Batch   50/81   train_loss = 0.769\n",
      "Epoch 2735 Batch    1/81   train_loss = 0.800\n",
      "Epoch 2735 Batch   33/81   train_loss = 0.768\n",
      "Epoch 2735 Batch   65/81   train_loss = 0.806\n",
      "Epoch 2736 Batch   16/81   train_loss = 0.850\n",
      "Epoch 2736 Batch   48/81   train_loss = 0.783\n",
      "Epoch 2736 Batch   80/81   train_loss = 0.803\n",
      "Epoch 2737 Batch   31/81   train_loss = 0.770\n",
      "Epoch 2737 Batch   63/81   train_loss = 0.790\n",
      "Epoch 2738 Batch   14/81   train_loss = 0.783\n",
      "Epoch 2738 Batch   46/81   train_loss = 0.775\n",
      "Epoch 2738 Batch   78/81   train_loss = 0.781\n",
      "Epoch 2739 Batch   29/81   train_loss = 0.799\n",
      "Epoch 2739 Batch   61/81   train_loss = 0.787\n",
      "Epoch 2740 Batch   12/81   train_loss = 0.784\n",
      "Epoch 2740 Batch   44/81   train_loss = 0.764\n",
      "Epoch 2740 Batch   76/81   train_loss = 0.801\n",
      "Epoch 2741 Batch   27/81   train_loss = 0.762\n",
      "Epoch 2741 Batch   59/81   train_loss = 0.769\n",
      "Epoch 2742 Batch   10/81   train_loss = 0.796\n",
      "Epoch 2742 Batch   42/81   train_loss = 0.753\n",
      "Epoch 2742 Batch   74/81   train_loss = 0.783\n",
      "Epoch 2743 Batch   25/81   train_loss = 0.774\n",
      "Epoch 2743 Batch   57/81   train_loss = 0.800\n",
      "Epoch 2744 Batch    8/81   train_loss = 0.791\n",
      "Epoch 2744 Batch   40/81   train_loss = 0.745\n",
      "Epoch 2744 Batch   72/81   train_loss = 0.763\n",
      "Epoch 2745 Batch   23/81   train_loss = 0.779\n",
      "Epoch 2745 Batch   55/81   train_loss = 0.781\n",
      "Epoch 2746 Batch    6/81   train_loss = 0.759\n",
      "Epoch 2746 Batch   38/81   train_loss = 0.772\n",
      "Epoch 2746 Batch   70/81   train_loss = 0.758\n",
      "Epoch 2747 Batch   21/81   train_loss = 0.803\n",
      "Epoch 2747 Batch   53/81   train_loss = 0.781\n",
      "Epoch 2748 Batch    4/81   train_loss = 0.799\n",
      "Epoch 2748 Batch   36/81   train_loss = 0.807\n",
      "Epoch 2748 Batch   68/81   train_loss = 0.796\n",
      "Epoch 2749 Batch   19/81   train_loss = 0.743\n",
      "Epoch 2749 Batch   51/81   train_loss = 0.811\n",
      "Epoch 2750 Batch    2/81   train_loss = 0.805\n",
      "Epoch 2750 Batch   34/81   train_loss = 0.790\n",
      "Epoch 2750 Batch   66/81   train_loss = 0.778\n",
      "Epoch 2751 Batch   17/81   train_loss = 0.782\n",
      "Epoch 2751 Batch   49/81   train_loss = 0.766\n",
      "Epoch 2752 Batch    0/81   train_loss = 0.792\n",
      "Epoch 2752 Batch   32/81   train_loss = 0.748\n",
      "Epoch 2752 Batch   64/81   train_loss = 0.777\n",
      "Epoch 2753 Batch   15/81   train_loss = 0.770\n",
      "Epoch 2753 Batch   47/81   train_loss = 0.788\n",
      "Epoch 2753 Batch   79/81   train_loss = 0.771\n",
      "Epoch 2754 Batch   30/81   train_loss = 0.753\n",
      "Epoch 2754 Batch   62/81   train_loss = 0.784\n",
      "Epoch 2755 Batch   13/81   train_loss = 0.831\n",
      "Epoch 2755 Batch   45/81   train_loss = 0.780\n",
      "Epoch 2755 Batch   77/81   train_loss = 0.786\n",
      "Epoch 2756 Batch   28/81   train_loss = 0.766\n",
      "Epoch 2756 Batch   60/81   train_loss = 0.801\n",
      "Epoch 2757 Batch   11/81   train_loss = 0.779\n",
      "Epoch 2757 Batch   43/81   train_loss = 0.742\n",
      "Epoch 2757 Batch   75/81   train_loss = 0.770\n",
      "Epoch 2758 Batch   26/81   train_loss = 0.779\n",
      "Epoch 2758 Batch   58/81   train_loss = 0.749\n",
      "Epoch 2759 Batch    9/81   train_loss = 0.809\n",
      "Epoch 2759 Batch   41/81   train_loss = 0.741\n",
      "Epoch 2759 Batch   73/81   train_loss = 0.771\n",
      "Epoch 2760 Batch   24/81   train_loss = 0.749\n",
      "Epoch 2760 Batch   56/81   train_loss = 0.786\n",
      "Epoch 2761 Batch    7/81   train_loss = 0.780\n",
      "Epoch 2761 Batch   39/81   train_loss = 0.753\n",
      "Epoch 2761 Batch   71/81   train_loss = 0.762\n",
      "Epoch 2762 Batch   22/81   train_loss = 0.799\n",
      "Epoch 2762 Batch   54/81   train_loss = 0.774\n",
      "Epoch 2763 Batch    5/81   train_loss = 0.796\n",
      "Epoch 2763 Batch   37/81   train_loss = 0.763\n",
      "Epoch 2763 Batch   69/81   train_loss = 0.773\n",
      "Epoch 2764 Batch   20/81   train_loss = 0.781\n",
      "Epoch 2764 Batch   52/81   train_loss = 0.818\n",
      "Epoch 2765 Batch    3/81   train_loss = 0.758\n",
      "Epoch 2765 Batch   35/81   train_loss = 0.771\n",
      "Epoch 2765 Batch   67/81   train_loss = 0.800\n",
      "Epoch 2766 Batch   18/81   train_loss = 0.794\n",
      "Epoch 2766 Batch   50/81   train_loss = 0.762\n",
      "Epoch 2767 Batch    1/81   train_loss = 0.792\n",
      "Epoch 2767 Batch   33/81   train_loss = 0.736\n",
      "Epoch 2767 Batch   65/81   train_loss = 0.791\n",
      "Epoch 2768 Batch   16/81   train_loss = 0.796\n",
      "Epoch 2768 Batch   48/81   train_loss = 0.761\n",
      "Epoch 2768 Batch   80/81   train_loss = 0.790\n",
      "Epoch 2769 Batch   31/81   train_loss = 0.770\n",
      "Epoch 2769 Batch   63/81   train_loss = 0.799\n",
      "Epoch 2770 Batch   14/81   train_loss = 0.795\n",
      "Epoch 2770 Batch   46/81   train_loss = 0.765\n",
      "Epoch 2770 Batch   78/81   train_loss = 0.771\n",
      "Epoch 2771 Batch   29/81   train_loss = 0.766\n",
      "Epoch 2771 Batch   61/81   train_loss = 0.768\n",
      "Epoch 2772 Batch   12/81   train_loss = 0.788\n",
      "Epoch 2772 Batch   44/81   train_loss = 0.755\n",
      "Epoch 2772 Batch   76/81   train_loss = 0.785\n",
      "Epoch 2773 Batch   27/81   train_loss = 0.750\n",
      "Epoch 2773 Batch   59/81   train_loss = 0.753\n",
      "Epoch 2774 Batch   10/81   train_loss = 0.776\n",
      "Epoch 2774 Batch   42/81   train_loss = 0.759\n",
      "Epoch 2774 Batch   74/81   train_loss = 0.762\n",
      "Epoch 2775 Batch   25/81   train_loss = 0.743\n",
      "Epoch 2775 Batch   57/81   train_loss = 0.765\n",
      "Epoch 2776 Batch    8/81   train_loss = 0.774\n",
      "Epoch 2776 Batch   40/81   train_loss = 0.729\n",
      "Epoch 2776 Batch   72/81   train_loss = 0.774\n",
      "Epoch 2777 Batch   23/81   train_loss = 0.780\n",
      "Epoch 2777 Batch   55/81   train_loss = 0.792\n",
      "Epoch 2778 Batch    6/81   train_loss = 0.759\n",
      "Epoch 2778 Batch   38/81   train_loss = 0.792\n",
      "Epoch 2778 Batch   70/81   train_loss = 0.755\n",
      "Epoch 2779 Batch   21/81   train_loss = 0.801\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2779 Batch   53/81   train_loss = 0.761\n",
      "Epoch 2780 Batch    4/81   train_loss = 0.790\n",
      "Epoch 2780 Batch   36/81   train_loss = 0.787\n",
      "Epoch 2780 Batch   68/81   train_loss = 0.767\n",
      "Epoch 2781 Batch   19/81   train_loss = 0.754\n",
      "Epoch 2781 Batch   51/81   train_loss = 0.808\n",
      "Epoch 2782 Batch    2/81   train_loss = 0.797\n",
      "Epoch 2782 Batch   34/81   train_loss = 0.806\n",
      "Epoch 2782 Batch   66/81   train_loss = 0.782\n",
      "Epoch 2783 Batch   17/81   train_loss = 0.798\n",
      "Epoch 2783 Batch   49/81   train_loss = 0.771\n",
      "Epoch 2784 Batch    0/81   train_loss = 0.787\n",
      "Epoch 2784 Batch   32/81   train_loss = 0.766\n",
      "Epoch 2784 Batch   64/81   train_loss = 0.774\n",
      "Epoch 2785 Batch   15/81   train_loss = 0.774\n",
      "Epoch 2785 Batch   47/81   train_loss = 0.772\n",
      "Epoch 2785 Batch   79/81   train_loss = 0.768\n",
      "Epoch 2786 Batch   30/81   train_loss = 0.771\n",
      "Epoch 2786 Batch   62/81   train_loss = 0.781\n",
      "Epoch 2787 Batch   13/81   train_loss = 0.794\n",
      "Epoch 2787 Batch   45/81   train_loss = 0.786\n",
      "Epoch 2787 Batch   77/81   train_loss = 0.811\n",
      "Epoch 2788 Batch   28/81   train_loss = 0.793\n",
      "Epoch 2788 Batch   60/81   train_loss = 0.788\n",
      "Epoch 2789 Batch   11/81   train_loss = 0.795\n",
      "Epoch 2789 Batch   43/81   train_loss = 0.759\n",
      "Epoch 2789 Batch   75/81   train_loss = 0.757\n",
      "Epoch 2790 Batch   26/81   train_loss = 0.815\n",
      "Epoch 2790 Batch   58/81   train_loss = 0.773\n",
      "Epoch 2791 Batch    9/81   train_loss = 0.817\n",
      "Epoch 2791 Batch   41/81   train_loss = 0.765\n",
      "Epoch 2791 Batch   73/81   train_loss = 0.809\n",
      "Epoch 2792 Batch   24/81   train_loss = 0.780\n",
      "Epoch 2792 Batch   56/81   train_loss = 0.809\n",
      "Epoch 2793 Batch    7/81   train_loss = 0.786\n",
      "Epoch 2793 Batch   39/81   train_loss = 0.761\n",
      "Epoch 2793 Batch   71/81   train_loss = 0.786\n",
      "Epoch 2794 Batch   22/81   train_loss = 0.810\n",
      "Epoch 2794 Batch   54/81   train_loss = 0.797\n",
      "Epoch 2795 Batch    5/81   train_loss = 0.804\n",
      "Epoch 2795 Batch   37/81   train_loss = 0.801\n",
      "Epoch 2795 Batch   69/81   train_loss = 0.793\n",
      "Epoch 2796 Batch   20/81   train_loss = 0.795\n",
      "Epoch 2796 Batch   52/81   train_loss = 0.811\n",
      "Epoch 2797 Batch    3/81   train_loss = 0.781\n",
      "Epoch 2797 Batch   35/81   train_loss = 0.803\n",
      "Epoch 2797 Batch   67/81   train_loss = 0.809\n",
      "Epoch 2798 Batch   18/81   train_loss = 0.818\n",
      "Epoch 2798 Batch   50/81   train_loss = 0.806\n",
      "Epoch 2799 Batch    1/81   train_loss = 0.870\n",
      "Epoch 2799 Batch   33/81   train_loss = 0.796\n",
      "Epoch 2799 Batch   65/81   train_loss = 0.817\n",
      "Epoch 2800 Batch   16/81   train_loss = 0.844\n",
      "Epoch 2800 Batch   48/81   train_loss = 0.799\n",
      "Epoch 2800 Batch   80/81   train_loss = 0.806\n",
      "Epoch 2801 Batch   31/81   train_loss = 0.796\n",
      "Epoch 2801 Batch   63/81   train_loss = 0.796\n",
      "Epoch 2802 Batch   14/81   train_loss = 0.793\n",
      "Epoch 2802 Batch   46/81   train_loss = 0.788\n",
      "Epoch 2802 Batch   78/81   train_loss = 0.798\n",
      "Epoch 2803 Batch   29/81   train_loss = 0.808\n",
      "Epoch 2803 Batch   61/81   train_loss = 0.787\n",
      "Epoch 2804 Batch   12/81   train_loss = 0.803\n",
      "Epoch 2804 Batch   44/81   train_loss = 0.805\n",
      "Epoch 2804 Batch   76/81   train_loss = 0.817\n",
      "Epoch 2805 Batch   27/81   train_loss = 0.829\n",
      "Epoch 2805 Batch   59/81   train_loss = 0.777\n",
      "Epoch 2806 Batch   10/81   train_loss = 0.810\n",
      "Epoch 2806 Batch   42/81   train_loss = 0.774\n",
      "Epoch 2806 Batch   74/81   train_loss = 0.772\n",
      "Epoch 2807 Batch   25/81   train_loss = 0.785\n",
      "Epoch 2807 Batch   57/81   train_loss = 0.803\n",
      "Epoch 2808 Batch    8/81   train_loss = 0.796\n",
      "Epoch 2808 Batch   40/81   train_loss = 0.753\n",
      "Epoch 2808 Batch   72/81   train_loss = 0.766\n",
      "Epoch 2809 Batch   23/81   train_loss = 0.789\n",
      "Epoch 2809 Batch   55/81   train_loss = 0.787\n",
      "Epoch 2810 Batch    6/81   train_loss = 0.779\n",
      "Epoch 2810 Batch   38/81   train_loss = 0.790\n",
      "Epoch 2810 Batch   70/81   train_loss = 0.766\n",
      "Epoch 2811 Batch   21/81   train_loss = 0.802\n",
      "Epoch 2811 Batch   53/81   train_loss = 0.764\n",
      "Epoch 2812 Batch    4/81   train_loss = 0.780\n",
      "Epoch 2812 Batch   36/81   train_loss = 0.800\n",
      "Epoch 2812 Batch   68/81   train_loss = 0.788\n",
      "Epoch 2813 Batch   19/81   train_loss = 0.748\n",
      "Epoch 2813 Batch   51/81   train_loss = 0.793\n",
      "Epoch 2814 Batch    2/81   train_loss = 0.797\n",
      "Epoch 2814 Batch   34/81   train_loss = 0.799\n",
      "Epoch 2814 Batch   66/81   train_loss = 0.768\n",
      "Epoch 2815 Batch   17/81   train_loss = 0.782\n",
      "Epoch 2815 Batch   49/81   train_loss = 0.781\n",
      "Epoch 2816 Batch    0/81   train_loss = 0.796\n",
      "Epoch 2816 Batch   32/81   train_loss = 0.761\n",
      "Epoch 2816 Batch   64/81   train_loss = 0.783\n",
      "Epoch 2817 Batch   15/81   train_loss = 0.769\n",
      "Epoch 2817 Batch   47/81   train_loss = 0.791\n",
      "Epoch 2817 Batch   79/81   train_loss = 0.784\n",
      "Epoch 2818 Batch   30/81   train_loss = 0.774\n",
      "Epoch 2818 Batch   62/81   train_loss = 0.768\n",
      "Epoch 2819 Batch   13/81   train_loss = 0.798\n",
      "Epoch 2819 Batch   45/81   train_loss = 0.787\n",
      "Epoch 2819 Batch   77/81   train_loss = 0.789\n",
      "Epoch 2820 Batch   28/81   train_loss = 0.757\n",
      "Epoch 2820 Batch   60/81   train_loss = 0.794\n",
      "Epoch 2821 Batch   11/81   train_loss = 0.792\n",
      "Epoch 2821 Batch   43/81   train_loss = 0.743\n",
      "Epoch 2821 Batch   75/81   train_loss = 0.779\n",
      "Epoch 2822 Batch   26/81   train_loss = 0.790\n",
      "Epoch 2822 Batch   58/81   train_loss = 0.757\n",
      "Epoch 2823 Batch    9/81   train_loss = 0.798\n",
      "Epoch 2823 Batch   41/81   train_loss = 0.744\n",
      "Epoch 2823 Batch   73/81   train_loss = 0.817\n",
      "Epoch 2824 Batch   24/81   train_loss = 0.794\n",
      "Epoch 2824 Batch   56/81   train_loss = 0.824\n",
      "Epoch 2825 Batch    7/81   train_loss = 0.795\n",
      "Epoch 2825 Batch   39/81   train_loss = 0.753\n",
      "Epoch 2825 Batch   71/81   train_loss = 0.770\n",
      "Epoch 2826 Batch   22/81   train_loss = 0.830\n",
      "Epoch 2826 Batch   54/81   train_loss = 0.806\n",
      "Epoch 2827 Batch    5/81   train_loss = 0.804\n",
      "Epoch 2827 Batch   37/81   train_loss = 0.785\n",
      "Epoch 2827 Batch   69/81   train_loss = 0.785\n",
      "Epoch 2828 Batch   20/81   train_loss = 0.771\n",
      "Epoch 2828 Batch   52/81   train_loss = 0.818\n",
      "Epoch 2829 Batch    3/81   train_loss = 0.793\n",
      "Epoch 2829 Batch   35/81   train_loss = 0.783\n",
      "Epoch 2829 Batch   67/81   train_loss = 0.812\n",
      "Epoch 2830 Batch   18/81   train_loss = 0.802\n",
      "Epoch 2830 Batch   50/81   train_loss = 0.772\n",
      "Epoch 2831 Batch    1/81   train_loss = 0.811\n",
      "Epoch 2831 Batch   33/81   train_loss = 0.756\n",
      "Epoch 2831 Batch   65/81   train_loss = 0.822\n",
      "Epoch 2832 Batch   16/81   train_loss = 0.845\n",
      "Epoch 2832 Batch   48/81   train_loss = 0.792\n",
      "Epoch 2832 Batch   80/81   train_loss = 0.804\n",
      "Epoch 2833 Batch   31/81   train_loss = 0.773\n",
      "Epoch 2833 Batch   63/81   train_loss = 0.768\n",
      "Epoch 2834 Batch   14/81   train_loss = 0.802\n",
      "Epoch 2834 Batch   46/81   train_loss = 0.756\n",
      "Epoch 2834 Batch   78/81   train_loss = 0.783\n",
      "Epoch 2835 Batch   29/81   train_loss = 0.785\n",
      "Epoch 2835 Batch   61/81   train_loss = 0.777\n",
      "Epoch 2836 Batch   12/81   train_loss = 0.777\n",
      "Epoch 2836 Batch   44/81   train_loss = 0.783\n",
      "Epoch 2836 Batch   76/81   train_loss = 0.807\n",
      "Epoch 2837 Batch   27/81   train_loss = 0.774\n",
      "Epoch 2837 Batch   59/81   train_loss = 0.759\n",
      "Epoch 2838 Batch   10/81   train_loss = 0.796\n",
      "Epoch 2838 Batch   42/81   train_loss = 0.767\n",
      "Epoch 2838 Batch   74/81   train_loss = 0.778\n",
      "Epoch 2839 Batch   25/81   train_loss = 0.794\n",
      "Epoch 2839 Batch   57/81   train_loss = 0.775\n",
      "Epoch 2840 Batch    8/81   train_loss = 0.784\n",
      "Epoch 2840 Batch   40/81   train_loss = 0.746\n",
      "Epoch 2840 Batch   72/81   train_loss = 0.763\n",
      "Epoch 2841 Batch   23/81   train_loss = 0.766\n",
      "Epoch 2841 Batch   55/81   train_loss = 0.784\n",
      "Epoch 2842 Batch    6/81   train_loss = 0.777\n",
      "Epoch 2842 Batch   38/81   train_loss = 0.795\n",
      "Epoch 2842 Batch   70/81   train_loss = 0.759\n",
      "Epoch 2843 Batch   21/81   train_loss = 0.824\n",
      "Epoch 2843 Batch   53/81   train_loss = 0.773\n",
      "Epoch 2844 Batch    4/81   train_loss = 0.795\n",
      "Epoch 2844 Batch   36/81   train_loss = 0.821\n",
      "Epoch 2844 Batch   68/81   train_loss = 0.782\n",
      "Epoch 2845 Batch   19/81   train_loss = 0.754\n",
      "Epoch 2845 Batch   51/81   train_loss = 0.793\n",
      "Epoch 2846 Batch    2/81   train_loss = 0.792\n",
      "Epoch 2846 Batch   34/81   train_loss = 0.793\n",
      "Epoch 2846 Batch   66/81   train_loss = 0.772\n",
      "Epoch 2847 Batch   17/81   train_loss = 0.777\n",
      "Epoch 2847 Batch   49/81   train_loss = 0.772\n",
      "Epoch 2848 Batch    0/81   train_loss = 0.793\n",
      "Epoch 2848 Batch   32/81   train_loss = 0.758\n",
      "Epoch 2848 Batch   64/81   train_loss = 0.807\n",
      "Epoch 2849 Batch   15/81   train_loss = 0.799\n",
      "Epoch 2849 Batch   47/81   train_loss = 0.773\n",
      "Epoch 2849 Batch   79/81   train_loss = 0.779\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2850 Batch   30/81   train_loss = 0.773\n",
      "Epoch 2850 Batch   62/81   train_loss = 0.776\n",
      "Epoch 2851 Batch   13/81   train_loss = 0.804\n",
      "Epoch 2851 Batch   45/81   train_loss = 0.777\n",
      "Epoch 2851 Batch   77/81   train_loss = 0.815\n",
      "Epoch 2852 Batch   28/81   train_loss = 0.774\n",
      "Epoch 2852 Batch   60/81   train_loss = 0.792\n",
      "Epoch 2853 Batch   11/81   train_loss = 0.788\n",
      "Epoch 2853 Batch   43/81   train_loss = 0.739\n",
      "Epoch 2853 Batch   75/81   train_loss = 0.754\n",
      "Epoch 2854 Batch   26/81   train_loss = 0.775\n",
      "Epoch 2854 Batch   58/81   train_loss = 0.744\n",
      "Epoch 2855 Batch    9/81   train_loss = 0.804\n",
      "Epoch 2855 Batch   41/81   train_loss = 0.742\n",
      "Epoch 2855 Batch   73/81   train_loss = 0.773\n",
      "Epoch 2856 Batch   24/81   train_loss = 0.773\n",
      "Epoch 2856 Batch   56/81   train_loss = 0.813\n",
      "Epoch 2857 Batch    7/81   train_loss = 0.806\n",
      "Epoch 2857 Batch   39/81   train_loss = 0.750\n",
      "Epoch 2857 Batch   71/81   train_loss = 0.781\n",
      "Epoch 2858 Batch   22/81   train_loss = 0.811\n",
      "Epoch 2858 Batch   54/81   train_loss = 0.764\n",
      "Epoch 2859 Batch    5/81   train_loss = 0.798\n",
      "Epoch 2859 Batch   37/81   train_loss = 0.777\n",
      "Epoch 2859 Batch   69/81   train_loss = 0.761\n",
      "Epoch 2860 Batch   20/81   train_loss = 0.778\n",
      "Epoch 2860 Batch   52/81   train_loss = 0.812\n",
      "Epoch 2861 Batch    3/81   train_loss = 0.751\n",
      "Epoch 2861 Batch   35/81   train_loss = 0.770\n",
      "Epoch 2861 Batch   67/81   train_loss = 0.813\n",
      "Epoch 2862 Batch   18/81   train_loss = 0.792\n",
      "Epoch 2862 Batch   50/81   train_loss = 0.782\n",
      "Epoch 2863 Batch    1/81   train_loss = 0.805\n",
      "Epoch 2863 Batch   33/81   train_loss = 0.764\n",
      "Epoch 2863 Batch   65/81   train_loss = 0.802\n",
      "Epoch 2864 Batch   16/81   train_loss = 0.835\n",
      "Epoch 2864 Batch   48/81   train_loss = 0.782\n",
      "Epoch 2864 Batch   80/81   train_loss = 0.794\n",
      "Epoch 2865 Batch   31/81   train_loss = 0.768\n",
      "Epoch 2865 Batch   63/81   train_loss = 0.792\n",
      "Epoch 2866 Batch   14/81   train_loss = 0.812\n",
      "Epoch 2866 Batch   46/81   train_loss = 0.777\n",
      "Epoch 2866 Batch   78/81   train_loss = 0.787\n",
      "Epoch 2867 Batch   29/81   train_loss = 0.794\n",
      "Epoch 2867 Batch   61/81   train_loss = 0.798\n",
      "Epoch 2868 Batch   12/81   train_loss = 0.805\n",
      "Epoch 2868 Batch   44/81   train_loss = 0.776\n",
      "Epoch 2868 Batch   76/81   train_loss = 0.809\n",
      "Epoch 2869 Batch   27/81   train_loss = 0.773\n",
      "Epoch 2869 Batch   59/81   train_loss = 0.782\n",
      "Epoch 2870 Batch   10/81   train_loss = 0.827\n",
      "Epoch 2870 Batch   42/81   train_loss = 0.783\n",
      "Epoch 2870 Batch   74/81   train_loss = 0.811\n",
      "Epoch 2871 Batch   25/81   train_loss = 0.783\n",
      "Epoch 2871 Batch   57/81   train_loss = 0.796\n",
      "Epoch 2872 Batch    8/81   train_loss = 0.789\n",
      "Epoch 2872 Batch   40/81   train_loss = 0.765\n",
      "Epoch 2872 Batch   72/81   train_loss = 0.762\n",
      "Epoch 2873 Batch   23/81   train_loss = 0.772\n",
      "Epoch 2873 Batch   55/81   train_loss = 0.786\n",
      "Epoch 2874 Batch    6/81   train_loss = 0.760\n",
      "Epoch 2874 Batch   38/81   train_loss = 0.787\n",
      "Epoch 2874 Batch   70/81   train_loss = 0.775\n",
      "Epoch 2875 Batch   21/81   train_loss = 0.804\n",
      "Epoch 2875 Batch   53/81   train_loss = 0.765\n",
      "Epoch 2876 Batch    4/81   train_loss = 0.789\n",
      "Epoch 2876 Batch   36/81   train_loss = 0.787\n",
      "Epoch 2876 Batch   68/81   train_loss = 0.804\n",
      "Epoch 2877 Batch   19/81   train_loss = 0.753\n",
      "Epoch 2877 Batch   51/81   train_loss = 0.829\n",
      "Epoch 2878 Batch    2/81   train_loss = 0.791\n",
      "Epoch 2878 Batch   34/81   train_loss = 0.814\n",
      "Epoch 2878 Batch   66/81   train_loss = 0.780\n",
      "Epoch 2879 Batch   17/81   train_loss = 0.765\n",
      "Epoch 2879 Batch   49/81   train_loss = 0.770\n",
      "Epoch 2880 Batch    0/81   train_loss = 0.795\n",
      "Epoch 2880 Batch   32/81   train_loss = 0.763\n",
      "Epoch 2880 Batch   64/81   train_loss = 0.772\n",
      "Epoch 2881 Batch   15/81   train_loss = 0.773\n",
      "Epoch 2881 Batch   47/81   train_loss = 0.773\n",
      "Epoch 2881 Batch   79/81   train_loss = 0.780\n",
      "Epoch 2882 Batch   30/81   train_loss = 0.770\n",
      "Epoch 2882 Batch   62/81   train_loss = 0.773\n",
      "Epoch 2883 Batch   13/81   train_loss = 0.783\n",
      "Epoch 2883 Batch   45/81   train_loss = 0.769\n",
      "Epoch 2883 Batch   77/81   train_loss = 0.791\n",
      "Epoch 2884 Batch   28/81   train_loss = 0.740\n",
      "Epoch 2884 Batch   60/81   train_loss = 0.762\n",
      "Epoch 2885 Batch   11/81   train_loss = 0.775\n",
      "Epoch 2885 Batch   43/81   train_loss = 0.738\n",
      "Epoch 2885 Batch   75/81   train_loss = 0.751\n",
      "Epoch 2886 Batch   26/81   train_loss = 0.780\n",
      "Epoch 2886 Batch   58/81   train_loss = 0.737\n",
      "Epoch 2887 Batch    9/81   train_loss = 0.787\n",
      "Epoch 2887 Batch   41/81   train_loss = 0.728\n",
      "Epoch 2887 Batch   73/81   train_loss = 0.771\n",
      "Epoch 2888 Batch   24/81   train_loss = 0.765\n",
      "Epoch 2888 Batch   56/81   train_loss = 0.786\n",
      "Epoch 2889 Batch    7/81   train_loss = 0.797\n",
      "Epoch 2889 Batch   39/81   train_loss = 0.738\n",
      "Epoch 2889 Batch   71/81   train_loss = 0.760\n",
      "Epoch 2890 Batch   22/81   train_loss = 0.785\n",
      "Epoch 2890 Batch   54/81   train_loss = 0.769\n",
      "Epoch 2891 Batch    5/81   train_loss = 0.791\n",
      "Epoch 2891 Batch   37/81   train_loss = 0.750\n",
      "Epoch 2891 Batch   69/81   train_loss = 0.758\n",
      "Epoch 2892 Batch   20/81   train_loss = 0.772\n",
      "Epoch 2892 Batch   52/81   train_loss = 0.809\n",
      "Epoch 2893 Batch    3/81   train_loss = 0.765\n",
      "Epoch 2893 Batch   35/81   train_loss = 0.800\n",
      "Epoch 2893 Batch   67/81   train_loss = 0.800\n",
      "Epoch 2894 Batch   18/81   train_loss = 0.795\n",
      "Epoch 2894 Batch   50/81   train_loss = 0.768\n",
      "Epoch 2895 Batch    1/81   train_loss = 0.802\n",
      "Epoch 2895 Batch   33/81   train_loss = 0.788\n",
      "Epoch 2895 Batch   65/81   train_loss = 0.786\n",
      "Epoch 2896 Batch   16/81   train_loss = 0.831\n",
      "Epoch 2896 Batch   48/81   train_loss = 0.776\n",
      "Epoch 2896 Batch   80/81   train_loss = 0.788\n",
      "Epoch 2897 Batch   31/81   train_loss = 0.764\n",
      "Epoch 2897 Batch   63/81   train_loss = 0.797\n",
      "Epoch 2898 Batch   14/81   train_loss = 0.792\n",
      "Epoch 2898 Batch   46/81   train_loss = 0.772\n",
      "Epoch 2898 Batch   78/81   train_loss = 0.765\n",
      "Epoch 2899 Batch   29/81   train_loss = 0.755\n",
      "Epoch 2899 Batch   61/81   train_loss = 0.783\n",
      "Epoch 2900 Batch   12/81   train_loss = 0.792\n",
      "Epoch 2900 Batch   44/81   train_loss = 0.774\n",
      "Epoch 2900 Batch   76/81   train_loss = 0.799\n",
      "Epoch 2901 Batch   27/81   train_loss = 0.738\n",
      "Epoch 2901 Batch   59/81   train_loss = 0.754\n",
      "Epoch 2902 Batch   10/81   train_loss = 0.806\n",
      "Epoch 2902 Batch   42/81   train_loss = 0.756\n",
      "Epoch 2902 Batch   74/81   train_loss = 0.789\n",
      "Epoch 2903 Batch   25/81   train_loss = 0.797\n",
      "Epoch 2903 Batch   57/81   train_loss = 0.784\n",
      "Epoch 2904 Batch    8/81   train_loss = 0.788\n",
      "Epoch 2904 Batch   40/81   train_loss = 0.763\n",
      "Epoch 2904 Batch   72/81   train_loss = 0.766\n",
      "Epoch 2905 Batch   23/81   train_loss = 0.760\n",
      "Epoch 2905 Batch   55/81   train_loss = 0.788\n",
      "Epoch 2906 Batch    6/81   train_loss = 0.786\n",
      "Epoch 2906 Batch   38/81   train_loss = 0.796\n",
      "Epoch 2906 Batch   70/81   train_loss = 0.767\n",
      "Epoch 2907 Batch   21/81   train_loss = 0.840\n",
      "Epoch 2907 Batch   53/81   train_loss = 0.777\n",
      "Epoch 2908 Batch    4/81   train_loss = 0.800\n",
      "Epoch 2908 Batch   36/81   train_loss = 0.813\n",
      "Epoch 2908 Batch   68/81   train_loss = 0.803\n",
      "Epoch 2909 Batch   19/81   train_loss = 0.749\n",
      "Epoch 2909 Batch   51/81   train_loss = 0.825\n",
      "Epoch 2910 Batch    2/81   train_loss = 0.814\n",
      "Epoch 2910 Batch   34/81   train_loss = 0.827\n",
      "Epoch 2910 Batch   66/81   train_loss = 0.790\n",
      "Epoch 2911 Batch   17/81   train_loss = 0.783\n",
      "Epoch 2911 Batch   49/81   train_loss = 0.790\n",
      "Epoch 2912 Batch    0/81   train_loss = 0.815\n",
      "Epoch 2912 Batch   32/81   train_loss = 0.756\n",
      "Epoch 2912 Batch   64/81   train_loss = 0.785\n",
      "Epoch 2913 Batch   15/81   train_loss = 0.790\n",
      "Epoch 2913 Batch   47/81   train_loss = 0.781\n",
      "Epoch 2913 Batch   79/81   train_loss = 0.784\n",
      "Epoch 2914 Batch   30/81   train_loss = 0.795\n",
      "Epoch 2914 Batch   62/81   train_loss = 0.796\n",
      "Epoch 2915 Batch   13/81   train_loss = 0.804\n",
      "Epoch 2915 Batch   45/81   train_loss = 0.777\n",
      "Epoch 2915 Batch   77/81   train_loss = 0.802\n",
      "Epoch 2916 Batch   28/81   train_loss = 0.745\n",
      "Epoch 2916 Batch   60/81   train_loss = 0.786\n",
      "Epoch 2917 Batch   11/81   train_loss = 0.777\n",
      "Epoch 2917 Batch   43/81   train_loss = 0.748\n",
      "Epoch 2917 Batch   75/81   train_loss = 0.763\n",
      "Epoch 2918 Batch   26/81   train_loss = 0.769\n",
      "Epoch 2918 Batch   58/81   train_loss = 0.762\n",
      "Epoch 2919 Batch    9/81   train_loss = 0.791\n",
      "Epoch 2919 Batch   41/81   train_loss = 0.719\n",
      "Epoch 2919 Batch   73/81   train_loss = 0.783\n",
      "Epoch 2920 Batch   24/81   train_loss = 0.754\n",
      "Epoch 2920 Batch   56/81   train_loss = 0.800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2921 Batch    7/81   train_loss = 0.778\n",
      "Epoch 2921 Batch   39/81   train_loss = 0.770\n",
      "Epoch 2921 Batch   71/81   train_loss = 0.772\n",
      "Epoch 2922 Batch   22/81   train_loss = 0.781\n",
      "Epoch 2922 Batch   54/81   train_loss = 0.761\n",
      "Epoch 2923 Batch    5/81   train_loss = 0.773\n",
      "Epoch 2923 Batch   37/81   train_loss = 0.766\n",
      "Epoch 2923 Batch   69/81   train_loss = 0.768\n",
      "Epoch 2924 Batch   20/81   train_loss = 0.761\n",
      "Epoch 2924 Batch   52/81   train_loss = 0.797\n",
      "Epoch 2925 Batch    3/81   train_loss = 0.755\n",
      "Epoch 2925 Batch   35/81   train_loss = 0.775\n",
      "Epoch 2925 Batch   67/81   train_loss = 0.792\n",
      "Epoch 2926 Batch   18/81   train_loss = 0.799\n",
      "Epoch 2926 Batch   50/81   train_loss = 0.755\n",
      "Epoch 2927 Batch    1/81   train_loss = 0.790\n",
      "Epoch 2927 Batch   33/81   train_loss = 0.746\n",
      "Epoch 2927 Batch   65/81   train_loss = 0.801\n",
      "Epoch 2928 Batch   16/81   train_loss = 0.826\n",
      "Epoch 2928 Batch   48/81   train_loss = 0.773\n",
      "Epoch 2928 Batch   80/81   train_loss = 0.803\n",
      "Epoch 2929 Batch   31/81   train_loss = 0.775\n",
      "Epoch 2929 Batch   63/81   train_loss = 0.791\n",
      "Epoch 2930 Batch   14/81   train_loss = 0.802\n",
      "Epoch 2930 Batch   46/81   train_loss = 0.784\n",
      "Epoch 2930 Batch   78/81   train_loss = 0.801\n",
      "Epoch 2931 Batch   29/81   train_loss = 0.760\n",
      "Epoch 2931 Batch   61/81   train_loss = 0.771\n",
      "Epoch 2932 Batch   12/81   train_loss = 0.810\n",
      "Epoch 2932 Batch   44/81   train_loss = 0.791\n",
      "Epoch 2932 Batch   76/81   train_loss = 0.810\n",
      "Epoch 2933 Batch   27/81   train_loss = 0.758\n",
      "Epoch 2933 Batch   59/81   train_loss = 0.771\n",
      "Epoch 2934 Batch   10/81   train_loss = 0.794\n",
      "Epoch 2934 Batch   42/81   train_loss = 0.790\n",
      "Epoch 2934 Batch   74/81   train_loss = 0.792\n",
      "Epoch 2935 Batch   25/81   train_loss = 0.783\n",
      "Epoch 2935 Batch   57/81   train_loss = 0.803\n",
      "Epoch 2936 Batch    8/81   train_loss = 0.822\n",
      "Epoch 2936 Batch   40/81   train_loss = 0.754\n",
      "Epoch 2936 Batch   72/81   train_loss = 0.767\n",
      "Epoch 2937 Batch   23/81   train_loss = 0.782\n",
      "Epoch 2937 Batch   55/81   train_loss = 0.792\n",
      "Epoch 2938 Batch    6/81   train_loss = 0.808\n",
      "Epoch 2938 Batch   38/81   train_loss = 0.830\n",
      "Epoch 2938 Batch   70/81   train_loss = 0.793\n",
      "Epoch 2939 Batch   21/81   train_loss = 0.830\n",
      "Epoch 2939 Batch   53/81   train_loss = 0.779\n",
      "Epoch 2940 Batch    4/81   train_loss = 0.823\n",
      "Epoch 2940 Batch   36/81   train_loss = 0.825\n",
      "Epoch 2940 Batch   68/81   train_loss = 0.790\n",
      "Epoch 2941 Batch   19/81   train_loss = 0.769\n",
      "Epoch 2941 Batch   51/81   train_loss = 0.809\n",
      "Epoch 2942 Batch    2/81   train_loss = 0.790\n",
      "Epoch 2942 Batch   34/81   train_loss = 0.791\n",
      "Epoch 2942 Batch   66/81   train_loss = 0.772\n",
      "Epoch 2943 Batch   17/81   train_loss = 0.780\n",
      "Epoch 2943 Batch   49/81   train_loss = 0.779\n",
      "Epoch 2944 Batch    0/81   train_loss = 0.793\n",
      "Epoch 2944 Batch   32/81   train_loss = 0.748\n",
      "Epoch 2944 Batch   64/81   train_loss = 0.780\n",
      "Epoch 2945 Batch   15/81   train_loss = 0.767\n",
      "Epoch 2945 Batch   47/81   train_loss = 0.783\n",
      "Epoch 2945 Batch   79/81   train_loss = 0.804\n",
      "Epoch 2946 Batch   30/81   train_loss = 0.780\n",
      "Epoch 2946 Batch   62/81   train_loss = 0.783\n",
      "Epoch 2947 Batch   13/81   train_loss = 0.799\n",
      "Epoch 2947 Batch   45/81   train_loss = 0.772\n",
      "Epoch 2947 Batch   77/81   train_loss = 0.794\n",
      "Epoch 2948 Batch   28/81   train_loss = 0.747\n",
      "Epoch 2948 Batch   60/81   train_loss = 0.799\n",
      "Epoch 2949 Batch   11/81   train_loss = 0.799\n",
      "Epoch 2949 Batch   43/81   train_loss = 0.756\n",
      "Epoch 2949 Batch   75/81   train_loss = 0.769\n",
      "Epoch 2950 Batch   26/81   train_loss = 0.774\n",
      "Epoch 2950 Batch   58/81   train_loss = 0.755\n",
      "Epoch 2951 Batch    9/81   train_loss = 0.797\n",
      "Epoch 2951 Batch   41/81   train_loss = 0.754\n",
      "Epoch 2951 Batch   73/81   train_loss = 0.801\n",
      "Epoch 2952 Batch   24/81   train_loss = 0.761\n",
      "Epoch 2952 Batch   56/81   train_loss = 0.811\n",
      "Epoch 2953 Batch    7/81   train_loss = 0.809\n",
      "Epoch 2953 Batch   39/81   train_loss = 0.763\n",
      "Epoch 2953 Batch   71/81   train_loss = 0.781\n",
      "Epoch 2954 Batch   22/81   train_loss = 0.801\n",
      "Epoch 2954 Batch   54/81   train_loss = 0.778\n",
      "Epoch 2955 Batch    5/81   train_loss = 0.802\n",
      "Epoch 2955 Batch   37/81   train_loss = 0.774\n",
      "Epoch 2955 Batch   69/81   train_loss = 0.761\n",
      "Epoch 2956 Batch   20/81   train_loss = 0.772\n",
      "Epoch 2956 Batch   52/81   train_loss = 0.808\n",
      "Epoch 2957 Batch    3/81   train_loss = 0.756\n",
      "Epoch 2957 Batch   35/81   train_loss = 0.792\n",
      "Epoch 2957 Batch   67/81   train_loss = 0.791\n",
      "Epoch 2958 Batch   18/81   train_loss = 0.797\n",
      "Epoch 2958 Batch   50/81   train_loss = 0.772\n",
      "Epoch 2959 Batch    1/81   train_loss = 0.808\n",
      "Epoch 2959 Batch   33/81   train_loss = 0.761\n",
      "Epoch 2959 Batch   65/81   train_loss = 0.805\n",
      "Epoch 2960 Batch   16/81   train_loss = 0.858\n",
      "Epoch 2960 Batch   48/81   train_loss = 0.806\n",
      "Epoch 2960 Batch   80/81   train_loss = 0.829\n",
      "Epoch 2961 Batch   31/81   train_loss = 0.799\n",
      "Epoch 2961 Batch   63/81   train_loss = 0.799\n",
      "Epoch 2962 Batch   14/81   train_loss = 0.808\n",
      "Epoch 2962 Batch   46/81   train_loss = 0.762\n",
      "Epoch 2962 Batch   78/81   train_loss = 0.780\n",
      "Epoch 2963 Batch   29/81   train_loss = 0.769\n",
      "Epoch 2963 Batch   61/81   train_loss = 0.768\n",
      "Epoch 2964 Batch   12/81   train_loss = 0.808\n",
      "Epoch 2964 Batch   44/81   train_loss = 0.794\n",
      "Epoch 2964 Batch   76/81   train_loss = 0.812\n",
      "Epoch 2965 Batch   27/81   train_loss = 0.756\n",
      "Epoch 2965 Batch   59/81   train_loss = 0.781\n",
      "Epoch 2966 Batch   10/81   train_loss = 0.803\n",
      "Epoch 2966 Batch   42/81   train_loss = 0.792\n",
      "Epoch 2966 Batch   74/81   train_loss = 0.799\n",
      "Epoch 2967 Batch   25/81   train_loss = 0.807\n",
      "Epoch 2967 Batch   57/81   train_loss = 0.802\n",
      "Epoch 2968 Batch    8/81   train_loss = 0.802\n",
      "Epoch 2968 Batch   40/81   train_loss = 0.767\n",
      "Epoch 2968 Batch   72/81   train_loss = 0.776\n",
      "Epoch 2969 Batch   23/81   train_loss = 0.784\n",
      "Epoch 2969 Batch   55/81   train_loss = 0.794\n",
      "Epoch 2970 Batch    6/81   train_loss = 0.779\n",
      "Epoch 2970 Batch   38/81   train_loss = 0.790\n",
      "Epoch 2970 Batch   70/81   train_loss = 0.771\n",
      "Epoch 2971 Batch   21/81   train_loss = 0.830\n",
      "Epoch 2971 Batch   53/81   train_loss = 0.779\n",
      "Epoch 2972 Batch    4/81   train_loss = 0.812\n",
      "Epoch 2972 Batch   36/81   train_loss = 0.798\n",
      "Epoch 2972 Batch   68/81   train_loss = 0.794\n",
      "Epoch 2973 Batch   19/81   train_loss = 0.773\n",
      "Epoch 2973 Batch   51/81   train_loss = 0.810\n",
      "Epoch 2974 Batch    2/81   train_loss = 0.789\n",
      "Epoch 2974 Batch   34/81   train_loss = 0.800\n",
      "Epoch 2974 Batch   66/81   train_loss = 0.764\n",
      "Epoch 2975 Batch   17/81   train_loss = 0.773\n",
      "Epoch 2975 Batch   49/81   train_loss = 0.775\n",
      "Epoch 2976 Batch    0/81   train_loss = 0.797\n",
      "Epoch 2976 Batch   32/81   train_loss = 0.752\n",
      "Epoch 2976 Batch   64/81   train_loss = 0.776\n",
      "Epoch 2977 Batch   15/81   train_loss = 0.764\n",
      "Epoch 2977 Batch   47/81   train_loss = 0.778\n",
      "Epoch 2977 Batch   79/81   train_loss = 0.781\n",
      "Epoch 2978 Batch   30/81   train_loss = 0.768\n",
      "Epoch 2978 Batch   62/81   train_loss = 0.799\n",
      "Epoch 2979 Batch   13/81   train_loss = 0.803\n",
      "Epoch 2979 Batch   45/81   train_loss = 0.771\n",
      "Epoch 2979 Batch   77/81   train_loss = 0.779\n",
      "Epoch 2980 Batch   28/81   train_loss = 0.747\n",
      "Epoch 2980 Batch   60/81   train_loss = 0.764\n",
      "Epoch 2981 Batch   11/81   train_loss = 0.762\n",
      "Epoch 2981 Batch   43/81   train_loss = 0.731\n",
      "Epoch 2981 Batch   75/81   train_loss = 0.742\n",
      "Epoch 2982 Batch   26/81   train_loss = 0.776\n",
      "Epoch 2982 Batch   58/81   train_loss = 0.754\n",
      "Epoch 2983 Batch    9/81   train_loss = 0.793\n",
      "Epoch 2983 Batch   41/81   train_loss = 0.739\n",
      "Epoch 2983 Batch   73/81   train_loss = 0.790\n",
      "Epoch 2984 Batch   24/81   train_loss = 0.744\n",
      "Epoch 2984 Batch   56/81   train_loss = 0.784\n",
      "Epoch 2985 Batch    7/81   train_loss = 0.773\n",
      "Epoch 2985 Batch   39/81   train_loss = 0.766\n",
      "Epoch 2985 Batch   71/81   train_loss = 0.764\n",
      "Epoch 2986 Batch   22/81   train_loss = 0.792\n",
      "Epoch 2986 Batch   54/81   train_loss = 0.771\n",
      "Epoch 2987 Batch    5/81   train_loss = 0.798\n",
      "Epoch 2987 Batch   37/81   train_loss = 0.746\n",
      "Epoch 2987 Batch   69/81   train_loss = 0.769\n",
      "Epoch 2988 Batch   20/81   train_loss = 0.790\n",
      "Epoch 2988 Batch   52/81   train_loss = 0.806\n",
      "Epoch 2989 Batch    3/81   train_loss = 0.765\n",
      "Epoch 2989 Batch   35/81   train_loss = 0.775\n",
      "Epoch 2989 Batch   67/81   train_loss = 0.807\n",
      "Epoch 2990 Batch   18/81   train_loss = 0.811\n",
      "Epoch 2990 Batch   50/81   train_loss = 0.795\n",
      "Epoch 2991 Batch    1/81   train_loss = 0.825\n",
      "Epoch 2991 Batch   33/81   train_loss = 0.758\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2991 Batch   65/81   train_loss = 0.860\n",
      "Epoch 2992 Batch   16/81   train_loss = 0.894\n",
      "Epoch 2992 Batch   48/81   train_loss = 0.799\n",
      "Epoch 2992 Batch   80/81   train_loss = 0.842\n",
      "Epoch 2993 Batch   31/81   train_loss = 0.795\n",
      "Epoch 2993 Batch   63/81   train_loss = 0.819\n",
      "Epoch 2994 Batch   14/81   train_loss = 0.809\n",
      "Epoch 2994 Batch   46/81   train_loss = 0.791\n",
      "Epoch 2994 Batch   78/81   train_loss = 0.814\n",
      "Epoch 2995 Batch   29/81   train_loss = 0.784\n",
      "Epoch 2995 Batch   61/81   train_loss = 0.808\n",
      "Epoch 2996 Batch   12/81   train_loss = 0.801\n",
      "Epoch 2996 Batch   44/81   train_loss = 0.808\n",
      "Epoch 2996 Batch   76/81   train_loss = 0.830\n",
      "Epoch 2997 Batch   27/81   train_loss = 0.783\n",
      "Epoch 2997 Batch   59/81   train_loss = 0.804\n",
      "Epoch 2998 Batch   10/81   train_loss = 0.835\n",
      "Epoch 2998 Batch   42/81   train_loss = 0.783\n",
      "Epoch 2998 Batch   74/81   train_loss = 0.795\n",
      "Epoch 2999 Batch   25/81   train_loss = 0.803\n",
      "Epoch 2999 Batch   57/81   train_loss = 0.790\n",
      "Model Trained and Saved\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "\n",
    "batches = get_batches(int_text, batch_size, seq_length)\n",
    "\n",
    "with tf.Session(graph=train_graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for epoch_i in range(num_epochs):\n",
    "        state = sess.run(initial_state, {input_text: batches[0][0]})\n",
    "\n",
    "        for batch_i, (x, y) in enumerate(batches):\n",
    "            feed = {\n",
    "                input_text: x,\n",
    "                targets: y,\n",
    "                initial_state: state,\n",
    "                lr: learning_rate}\n",
    "            train_loss, state, _ = sess.run([cost, final_state, train_op], feed)\n",
    "\n",
    "            # Show every <show_every_n_batches> batches\n",
    "            if (epoch_i * len(batches) + batch_i) % show_every_n_batches == 0:\n",
    "                print('Epoch {:>3} Batch {:>4}/{}   train_loss = {:.3f}'.format(\n",
    "                    epoch_i,\n",
    "                    batch_i,\n",
    "                    len(batches),\n",
    "                    train_loss))\n",
    "\n",
    "    # Save Model\n",
    "    saver = tf.train.Saver()\n",
    "    saver.save(sess, save_dir)\n",
    "    print('Model Trained and Saved')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Parameters\n",
    "Save `seq_length` and `save_dir` for generating a new TV script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "# Save parameters for checkpoint\n",
    "helper.save_params((seq_length, save_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/test/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import helper\n",
    "import problem_unittests as tests\n",
    "\n",
    "_, vocab_to_int, int_to_vocab, token_dict = helper.load_preprocess()\n",
    "seq_length, load_dir = helper.load_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement Generate Functions\n",
    "### Get Tensors\n",
    "Get tensors from `loaded_graph` using the function [`get_tensor_by_name()`](https://www.tensorflow.org/api_docs/python/tf/Graph#get_tensor_by_name).  Get the tensors using the following names:\n",
    "- \"input:0\"\n",
    "- \"initial_state:0\"\n",
    "- \"final_state:0\"\n",
    "- \"probs:0\"\n",
    "\n",
    "Return the tensors in the following tuple `(InputTensor, InitialStateTensor, FinalStateTensor, ProbsTensor)` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def get_tensors(loaded_graph):\n",
    "    \"\"\"\n",
    "    Get input, initial state, final state, and probabilities tensor from <loaded_graph>\n",
    "    :param loaded_graph: TensorFlow graph loaded from file\n",
    "    :return: Tuple (InputTensor, InitialStateTensor, FinalStateTensor, ProbsTensor)\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    input_tensor = loaded_graph.get_tensor_by_name('input:0')\n",
    "    initial_state_tensor = loaded_graph.get_tensor_by_name('initial_state:0')\n",
    "    final_state_tensor = loaded_graph.get_tensor_by_name('final_state:0')\n",
    "    probs_tensor = loaded_graph.get_tensor_by_name('probs:0')\n",
    "    \n",
    "    return input_tensor, initial_state_tensor, final_state_tensor, probs_tensor\n",
    "\n",
    "\n",
    "\n",
    "tests.test_get_tensors(get_tensors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose Word\n",
    "Implement the `pick_word()` function to select the next word using `probabilities`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def pick_word(probabilities, int_to_vocab):\n",
    "    \"\"\"\n",
    "    Pick the next word in the generated text\n",
    "    :param probabilities: Probabilites of the next word\n",
    "    :param int_to_vocab: Dictionary of word ids as the keys and words as the values\n",
    "    :return: String of the predicted word\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    next_word = np.random.choice(len(int_to_vocab), p=probabilities)\n",
    "    prediction = int_to_vocab[next_word]\n",
    "    return prediction\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "tests.test_pick_word(pick_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Product Description\n",
    "This will generate the Description for your product.  Set `gen_length` to the length of product description you want to generate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./savejeans\n",
      "mufti red slim washed fit jeans: don a sophisticated look this season by wearing pair of red jeans from the latest collection mufti. exclusively designed for mufti men, these slim-fit exhibit 5-pocket design. team cotton with graphic t-shirt and sneakers to stand out crowd.\n",
      "\n",
      "\n",
      "mufti red slim washed mid rise fit jeans: laidback look is in and you can also flaunt the same as adorn these jeans from mufti unltd assuring stretch comfort, are perfect at comfort. just team with a polo t-shirt pair of jeans. tag button closure concealed zip fly for utility. red shirt lace-ups. whiskered pattern on front belt loops five pockets brand hem 1 distinctive impressions area fitted waist fastening tapered-leg design slim ripped detail side twin thread work back washed effect 5-pocket styling single right leg whisker detailing coin pocket\n",
      "\n",
      "\n",
      "mufti red skinny jeans embellished fit: elevate all for the streets in style wearing these jeans by mufti star. skinny fit and soft material of will keep you at ease day long. feel rush as push a skateboard from hilltop pair sleeveless t-shirt.\n",
      "\n",
      "\n",
      "mufti red slim light washed mid rise fit jeans: red in colour, these jeans from celio*, featuring a cool washed effect are smart, contemporary take on 5-pocket denims. fashioned polycotton spandex fabric, slim fit promise unmatched comfort all day long. adorn and full shirts.\n",
      "\n",
      "\n",
      "mufti red slim washed mid rise fit jeans: lay your hands and chic look this pair of red jeans from mufti will add to like a style statement. mid-washed effect all over distinctive design giving them classic as well stylish stretch. these can be teamed with sneakers t shoes stylish.\n",
      "\n",
      "\n",
      "mufti red slim washed fit jeans: an essential pick for denim lovers in giving its red colour, these jeans by mufti 21 promise mid-rise from colors of benetton will be excellent streetwear styling. wear slim fit with a t-shirt and pair shoes to finish off\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import webcolors\n",
    "import helper\n",
    "import time\n",
    "\n",
    "def remove_duplicates(li):\n",
    "    my_set = set()\n",
    "    res = []\n",
    "    for e in li:\n",
    "        if e not in my_set:\n",
    "            res.append(e)\n",
    "            my_set.add(e)\n",
    "\n",
    "    return res\n",
    "\n",
    "def get_brandname():\n",
    "\n",
    "    data_dir = './data/simpsons/jeans.txt'\n",
    "    text = helper.load_data(data_dir)\n",
    "    # Ignore notice, since we don't use it for analysing the data\n",
    "    text = text[:]\n",
    "    brands = []\n",
    "    alldesc = text.split(\"\\n\\n\")\n",
    "    for eachdesc in alldesc:\n",
    "        eachtitle = eachdesc.split(\":\")[0]\n",
    "        brandname = eachtitle.split(\" \")[0]\n",
    "        brandname = brandname.replace(\"\\n\",'')\n",
    "        brandname = brandname.replace(\"\\ufeff\",'')\n",
    "        brandname = brandname.lower()\n",
    "        brands.append(brandname)\n",
    "    brands = set(brands)\n",
    "    brands = list(brands)\n",
    "    brands.remove('and')\n",
    "    brands.remove('')\n",
    "    brands.remove('all')\n",
    "    brands.remove('the')\n",
    "    brands.remove('none')\n",
    "    brands.remove('rare')\n",
    "    brands.remove('cotton')\n",
    "    brands.remove('classic')\n",
    "    brands.append('cooper')\n",
    "    brands.append('pg3')\n",
    "    brands.append('solly')\n",
    "    brands.append('one')\n",
    "    brands.append('n')\n",
    "    brands.append('keech')\n",
    "    brands.append('hilfiger')\n",
    "    brands.append('star')\n",
    "    brands.append('players')\n",
    "    \n",
    "    clrlst = get_colors()\n",
    "    brands = list(set(brands)-set(clrlst))\n",
    "    return brands\n",
    "\n",
    "def get_colors():\n",
    "    list_of_colors = []\n",
    "    listcolors = webcolors.CSS3_NAMES_TO_HEX\n",
    "    for all in listcolors:\n",
    "        cl = all.split(\":\")[0]\n",
    "        list_of_colors.append(cl)\n",
    "    list_of_colors.append('mauve')\n",
    "    list_of_colors.append('wine')\n",
    "    return list_of_colors\n",
    "\n",
    "def get_style():\n",
    "    list_of_styles = ['skinny', 'slim', 'regular']\n",
    "    return list_of_styles\n",
    "\n",
    "def changetemplate(desc_op,features_str):\n",
    "    \n",
    "    #Getting brand names from text files to identify brand in the features string\n",
    "    brandnames = get_brandname()\n",
    "    brand_instr = ''\n",
    "    for allbrands in brandnames:\n",
    "        for allwords in features_str.split(\" \"):\n",
    "            if allbrands == allwords:\n",
    "                brand_instr = allbrands\n",
    "                break\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    #getting all colors to identify the color in the features string\n",
    "    colorlist = get_colors()\n",
    "    color_instr = ''\n",
    "    for allcolors in colorlist:\n",
    "        for allword in desc_op.split(\":\")[0].split(\" \"):\n",
    "            if allcolors == allword:\n",
    "                color_instr = allcolors\n",
    "                break\n",
    "    for allcolors in colorlist:\n",
    "        for allwords in features_str.split(\" \"):\n",
    "            if allcolors == allwords:\n",
    "                color_instr = allcolors\n",
    "                break\n",
    "                \n",
    "    #getting all styles to identify the styles in the features string\n",
    "    stylelist = get_style()\n",
    "    style_instr = ''\n",
    "    for allcolors in stylelist:\n",
    "        for allword in desc_op.split(\":\")[0].split(\" \"):\n",
    "            if allcolors == allword:\n",
    "                style_instr = allcolors\n",
    "                break\n",
    "    for allcolors in stylelist:\n",
    "        for allwords in features_str.split(\" \"):\n",
    "            if allcolors == allwords:\n",
    "                style_instr = allcolors\n",
    "                break\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # to append the brand, color and style in the title if not present\n",
    "\n",
    "    allwordsintitle = desc_op.split(\":\")[0]\n",
    "    \n",
    "    for allwords in allwordsintitle.split(\" \"):\n",
    "        if allwords == style_instr:\n",
    "            flg = 0\n",
    "        else:\n",
    "            allwordsintitle = style_instr + \" \" + allwordsintitle\n",
    "            title = desc_op.split(\":\")[0]\n",
    "            allwordsintitle = title.replace(desc_op.split(\":\")[0],allwordsintitle)\n",
    "            break\n",
    "\n",
    "    for allwords in allwordsintitle.split(\" \"):\n",
    "        if allwords == color_instr:\n",
    "            flg = 0\n",
    "        else:\n",
    "            allwordsintitle = color_instr + \" \" + allwordsintitle\n",
    "            title = desc_op.split(\":\")[0]\n",
    "            allwordsintitle = title.replace(desc_op.split(\":\")[0],allwordsintitle)\n",
    "            break\n",
    "\n",
    "    for allwords in allwordsintitle.split(\" \"):\n",
    "        if allwords == brand_instr:\n",
    "            flg = 0\n",
    "        else:\n",
    "            allwordsintitle = brand_instr + \" \" + allwordsintitle\n",
    "            title = desc_op.split(\":\")[0]\n",
    "            allwordsintitle = title.replace(desc_op.split(\":\")[0],allwordsintitle)\n",
    "            break\n",
    "            \n",
    "    \n",
    "\n",
    "\n",
    "    #to replace brand names and colours in the description \n",
    "\n",
    "    allwordsindescription = desc_op.split(\":\")[1]\n",
    "\n",
    "    for brandnm in brandnames:\n",
    "        for allwords in allwordsintitle.split(\" \"):\n",
    "            if brandnm == allwords or brandnm+\".\" == allwords or brandnm+\",\" == allwords or brandnm+\";\" == allwords:\n",
    "                allwordsintitle = allwordsintitle.replace(brandnm,brand_instr)\n",
    "        for allwords in allwordsindescription.split(\" \"):\n",
    "            if brandnm == allwords or brandnm+\".\" == allwords or brandnm+\",\" == allwords or brandnm+\";\" == allwords:\n",
    "                allwordsindescription = allwordsindescription.replace(brandnm,brand_instr)\n",
    "                \n",
    "    allwordsintitlelist = allwordsintitle.split(\" \")\n",
    "    allwordsintitlelist = remove_duplicates(allwordsintitlelist)\n",
    "    allwordsintitle = ' '.join(allwordsintitlelist)\n",
    "\n",
    "    \n",
    "    allwordsindescriptionlist = allwordsindescription.split(\" \")\n",
    "    allwordsindescriptionlist = remove_duplicates(allwordsindescriptionlist)\n",
    "    allwordsindescription = ' '.join(allwordsindescriptionlist)\n",
    "    \n",
    "    desc_op = desc_op.replace(desc_op.split(\":\")[0],allwordsintitle)\n",
    "    desc_op = desc_op.replace(desc_op.split(\":\")[1],allwordsindescription)\n",
    "\n",
    "\n",
    "    for clrs in colorlist:\n",
    "        for allwords in allwordsintitle.split(\" \"):\n",
    "            if clrs == allwords or clrs+\".\" == allwords or clrs+\",\" == allwords or clrs+\";\" == allwords:\n",
    "                allwordsintitle = allwordsintitle.replace(clrs,color_instr)\n",
    "        for allwords in allwordsindescription.split(\" \"):\n",
    "            if clrs == allwords or clrs+\".\" == allwords or clrs+\",\" == allwords or clrs+\";\" == allwords:\n",
    "                allwordsindescription = allwordsindescription.replace(clrs,color_instr)\n",
    "\n",
    "\n",
    "    allwordsintitlelist = allwordsintitle.split(\" \")\n",
    "    allwordsintitlelist = remove_duplicates(allwordsintitlelist)\n",
    "    allwordsintitle = ' '.join(allwordsintitlelist)\n",
    "    \n",
    "\n",
    "    desc_op = desc_op.replace(desc_op.split(\":\")[0],allwordsintitle)\n",
    "    desc_op = desc_op.replace(desc_op.split(\":\")[1],allwordsindescription)\n",
    "    \n",
    "    for brandnm in stylelist:\n",
    "        for allwords in allwordsintitle.split(\" \"):\n",
    "            if brandnm == allwords or brandnm+\".\" == allwords or brandnm+\",\" == allwords or brandnm+\";\" == allwords:\n",
    "                allwordsintitle = allwordsintitle.replace(brandnm,style_instr)\n",
    "        for allwords in allwordsindescription.split(\" \"):\n",
    "            if brandnm == allwords or brandnm+\".\" == allwords or brandnm+\",\" == allwords or brandnm+\";\" == allwords:\n",
    "                allwordsindescription = allwordsindescription.replace(brandnm,style_instr)\n",
    "                \n",
    "    allwordsintitlelist = allwordsintitle.split(\" \")\n",
    "    allwordsintitlelist = remove_duplicates(allwordsintitlelist)\n",
    "    allwordsintitle = ' '.join(allwordsintitlelist)\n",
    "\n",
    "    \n",
    "    allwordsindescriptionlist = allwordsindescription.split(\" \")\n",
    "    allwordsindescriptionlist = remove_duplicates(allwordsindescriptionlist)\n",
    "    allwordsindescription = ' '.join(allwordsindescriptionlist)\n",
    "    \n",
    "    desc_op = desc_op.replace(desc_op.split(\":\")[0],allwordsintitle)\n",
    "    desc_op = desc_op.replace(desc_op.split(\":\")[1],allwordsindescription)\n",
    "    print(desc_op+\"\\n\\n\")\n",
    "\n",
    "\n",
    "gen_length = 500\n",
    "\n",
    "features_str = 'mufti red jeans'\n",
    "\n",
    "brandnamesall = get_brandname()\n",
    "prime_word = ''\n",
    "for allb in brandnamesall:\n",
    "    for wrds in features_str.split(\" \"):\n",
    "        if allb == wrds:\n",
    "            prime_word = allb\n",
    "            break\n",
    "\n",
    "\n",
    "loaded_graph = tf.Graph()\n",
    "with tf.Session(graph=loaded_graph) as sess:\n",
    "    # Load saved model\n",
    "    loader = tf.train.import_meta_graph(load_dir + '.meta')\n",
    "    loader.restore(sess, load_dir)\n",
    "\n",
    "    # Get Tensors from loaded model\n",
    "    input_text, initial_state, final_state, probs = get_tensors(loaded_graph)\n",
    "\n",
    "    # Sentences generation setup\n",
    "    gen_sentences = [prime_word]\n",
    "    prev_state = sess.run(initial_state, {input_text: np.array([[1]])})\n",
    "\n",
    "    # Generate sentences\n",
    "    for n in range(gen_length):\n",
    "        # Dynamic Input\n",
    "        dyn_input = [[vocab_to_int[word] for word in gen_sentences[-seq_length:]]]\n",
    "        dyn_seq_length = len(dyn_input[0])\n",
    "\n",
    "        # Get Prediction\n",
    "        probabilities, prev_state = sess.run(\n",
    "            [probs, final_state],\n",
    "            {input_text: dyn_input, initial_state: prev_state})\n",
    "        \n",
    "        pred_word = pick_word(probabilities[dyn_seq_length-1], int_to_vocab)\n",
    "\n",
    "        gen_sentences.append(pred_word)\n",
    "    \n",
    "    # Remove tokens\n",
    "    tv_script = ' '.join(gen_sentences)\n",
    "    for key, token in token_dict.items():\n",
    "        ending = ' ' if key in ['\\n', '(', '\"'] else ''\n",
    "        tv_script = tv_script.replace(' ' + token.lower(), key)\n",
    "    tv_script = tv_script.replace('\\n ', '\\n')\n",
    "    tv_script = tv_script.replace('( ', '(')\n",
    "    \n",
    "descriptions = tv_script.split(\"\\n\\n\")\n",
    "desc_op = ''\n",
    "for description in descriptions:\n",
    "    try:\n",
    "        x = description.split(\":\")[0]\n",
    "        y = description.split(\":\")[1]\n",
    "        desc = description\n",
    "        descls = desc.split()\n",
    "        desc = ' '.join(descls)\n",
    "        #print(desc+\"\\n\\n\")\n",
    "        changetemplate(desc,features_str)\n",
    "        \n",
    "    except:\n",
    "        flag = 0\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
